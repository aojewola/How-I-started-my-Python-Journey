{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 4 + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import wordcloud as wc\n",
    "text = \"Hi Asimi Ojewola, this is to let you know that I will be in the Moon tomorrow to say Hello to the Star\"\n",
    "stopword = set(wc.STOPWORDS)\n",
    "stopword \n",
    "word = wc.WordCloud(stopwords=stopword, background_color=\"grey\").generate(text)\n",
    "plt.figure(figsize = (10,5))\n",
    "plt.imshow(word, interpolation = \"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallo World\n"
     ]
    }
   ],
   "source": [
    "print (\"Hallo World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 2, 1]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[2,1] * int(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Python\"==\"python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "False<True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is my birthday\n"
     ]
    }
   ],
   "source": [
    "if 1 == True:\n",
    "    print ('it is my birthday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     /|\n",
      "    / |\n",
      "   /  |\n",
      "  /   |\n",
      " /    |\n",
      "/_____|\n"
     ]
    }
   ],
   "source": [
    "print('     /|')\n",
    "print('    / |')\n",
    "print('   /  |')\n",
    "print('  /   |')\n",
    "print(' /    |')\n",
    "print('/_____|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mariam is a stubborn girl\n",
      "Mariam was born in 2016\n",
      "Mariam can disturb Karma too much\n",
      "Mariam will soon learn her lessons when she grows older\n"
     ]
    }
   ],
   "source": [
    "print (\"Mariam is a stubborn girl\")\n",
    "print(\"Mariam was born in 2016\")\n",
    "print(\"Mariam can disturb Karma too much\")\n",
    "print (\"Mariam will soon learn her lessons when she grows older\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mariamis a stubborn girl\n",
      "Mariamwas born in,2016\n",
      "Mariamcan disturbKarma too much,\n",
      "Mariamwill soon learn her lessons when she grows older.\n"
     ]
    }
   ],
   "source": [
    "girl_name = \"Mariam\"\n",
    "boy_name = \"Karma\"\n",
    "girl_birth_year = 2016\n",
    "\n",
    "print (girl_name + \"is a stubborn girl\")\n",
    "print(girl_name + \"was born in,\" + str(girl_birth_year))\n",
    "print(girl_name + \"can disturb\" + boy_name, \"too much,\")\n",
    "print (girl_name + \"will soon learn her lessons when she grows older.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mariamis a stubborn girl,\n",
      "Mariamwas born in,2016\n",
      "Mariamcan disturb too much,\n",
      "Mariamwill soon learn her lessons when she grows older.\n"
     ]
    }
   ],
   "source": [
    "girl_name = \"Mariam\"\n",
    "boy_name = \"Karma\"\n",
    "girl_birth_year = \"2016\"\n",
    "\n",
    "print (girl_name + \"is a stubborn girl,\")\n",
    "print(girl_name + \"was born in,\" + girl_birth_year)\n",
    "print(girl_name + \"can disturb too much,\")\n",
    "print (girl_name + \"will soon learn her lessons when she grows older.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zaidat is older than Ola\n"
     ]
    }
   ],
   "source": [
    "girl_name = \"Zaidat\"\n",
    "guy_name = \"Ola\"\n",
    "print(girl_name + \" is older than \" + guy_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Use single _underscore or **astericks** to get italics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "phrase = \"Munirat\" + \"Fatimoh\"\n",
    "print(len(phrase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "phrase = \"Munirat\" + \"Fatimoh\"\n",
    "print(phrase.upper().isupper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "phrase = \"Munirat\" + \"Fatimoh\"\n",
    "print(phrase.upper().isupper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "num = 23\n",
    "num += 3\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "n\n",
      "14\n",
      "Human Intelligent\n"
     ]
    }
   ],
   "source": [
    "mariam = \"Artificial \" + \"Intelligent\"\n",
    "print(len(mariam))\n",
    "print(mariam[20])\n",
    "print(mariam.index(\"e\"))\n",
    "print(mariam.replace(\"Artificial\", \"Human\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ola', 'wale']\n"
     ]
    }
   ],
   "source": [
    "akin = \"ola wale\"\n",
    "print(akin.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Munirt\n",
      "Kamal\n"
     ]
    }
   ],
   "source": [
    "print(\"Munirt\\nKamal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = (\"Please enter your name: \")\n",
    "print(\"You are welcom to my channel \" + name + \"!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num = {1,2, 3, 4, 5}\n",
    "print(max(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (2+3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(3, 1)\n",
      "[(1, 2)]\n",
      "[(1, 3), (1, 2)]\n"
     ]
    }
   ],
   "source": [
    "akin = [(1,2), (3,1)]\n",
    "x,y = akin\n",
    "print(x)\n",
    "print(y)\n",
    "akin.pop()\n",
    "print(akin)\n",
    "akin.insert(0, (1,3))\n",
    "print(akin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 21, 25, 90, 100]\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "x = [21, 25,90,100, 10]\n",
    "y = 9\n",
    "x.sort()\n",
    "print((x))\n",
    "#return statement\n",
    "def cube(num):\n",
    "    return num*num*num\n",
    "print(cube(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "def cube(num):\n",
    "    print(num*num*num)\n",
    "cube(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst  = \"aa\", \"aa\"\n",
    "lst.count(\"aa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "if \" !\".isalpha:\n",
    "    print (1)\n",
    "else:\n",
    "    print (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'h e l l o w o r l d'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strin = \"hello world\"\n",
    "joined = \" \".join(char for char in strin if not char==\" \")\n",
    "joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d', 'e', 'h', 'l', 'o', 'r', 'w'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kk', 'kk', 'kk', 'aa', 'aa', 'bb', 'bb', 'cc', 'cc']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined = [\"aa\", \"aa\", \"bb\",\"bb\", \"cc\", \"kk\", \"cc\",\"kk\",\"kk\"]\n",
    "sorted(joined, key=lambda x: joined.count(x), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'hello', 'world']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" hello world\".split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__iadd__',\n",
       " '__imul__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__rmul__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'append',\n",
       " 'clear',\n",
       " 'copy',\n",
       " 'count',\n",
       " 'extend',\n",
       " 'index',\n",
       " 'insert',\n",
       " 'pop',\n",
       " 'remove',\n",
       " 'reverse',\n",
       " 'sort']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "if not \" \".isalpha():\n",
    "    print(True)\n",
    "else: \n",
    "    print(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are not okay\n"
     ]
    }
   ],
   "source": [
    "if 1 == 0:\n",
    "   print(\"am okay\")\n",
    "else:\n",
    "   print('You are not okay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will give her my poff poff\n"
     ]
    }
   ],
   "source": [
    "Mariam_gives = True\n",
    "\n",
    "if Mariam_gives:\n",
    "   print ('I will give her my poff poff')\n",
    "else:\n",
    "   print('I wont give her my poff poff')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simple calculator\n",
    "num1 = float(input('Enter the first number'))\n",
    "operator = input ('Enter the operator')\n",
    "num2 = input ('Enter the second number')\n",
    "operator2 = input ('Enter the operator')\n",
    "num3 = input ('Enter the third number')\n",
    "\n",
    "if operator ==\"+\" and operator2 == \"*\":\n",
    "   print(num1+num2*num3)\n",
    "elif operator == \"*\" and operator == \"+\":\n",
    "   print(num1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mango': 50, 'apple': 50, 'orange': 100}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionaries\n",
    "fruits = {'mango': 23, 'apple': 50, 'orange':100}\n",
    "fruits[\"mango\"] = fruits[\"apple\"]\n",
    "fruits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['mango', 'apple', 'orange'])\n"
     ]
    }
   ],
   "source": [
    "fruits = {'mango': 23, 'apple': 50, 'orange':100}\n",
    "print(fruits.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('orange', 100), ('apple', 50), ('mango', 23)}\n"
     ]
    }
   ],
   "source": [
    "fruits = {'mango': 23, 'apple': 50, 'orange':100}\n",
    "print(set(fruits.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([23, 50, 100])\n"
     ]
    }
   ],
   "source": [
    "fruits = {'mango': 23, 'apple': 50, 'orange':100}\n",
    "print(fruits.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#while loop\n",
    "i = -1\n",
    "while i > 0:\n",
    "    print(i)\n",
    "    i += 1\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "#for loop\n",
    "i = 6\n",
    "if i <10:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your guess: Mari\n",
      "Enter your guess: Mariam\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "#a guessing game\n",
    "actual_value = \"Mariam\"\n",
    "guess = \" \"\n",
    "guess_count = 0\n",
    "guess_limit = 3\n",
    "out_of_guess = False\n",
    "\n",
    "while guess != actual_value and not(out_of_guess):\n",
    "    if guess_count < guess_limit:\n",
    "        guess = input (\"Enter your guess: \")\n",
    "        guess_count += 1\n",
    "    else:\n",
    "        out_of_guess = True\n",
    "        print(\"You lose!\")\n",
    "\n",
    "if guess == actual_value:\n",
    "   print(\"You Win!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bhhhhhhhhhhhehhhhhhhhhhhshhhhhhhhhhhthhhhhhhhhhh"
     ]
    }
   ],
   "source": [
    "for i in \"best\":\n",
    "   print(i, end =\"hhhhhhhhhhh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's me\n",
      "They are friends of friend\n",
      "They are friends of friend\n",
      "They are friends of friend\n"
     ]
    }
   ],
   "source": [
    "friends = [\"akin\", \"Baruwa\", \"Zainab\",\"Ola\"]\n",
    "for friend in friends:\n",
    "    if friend == \"akin\":\n",
    "     print(\"It's me\")\n",
    "    else:\n",
    "     print(\"They are friends of friend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "9\n",
      "6\n",
      "2\n",
      "5\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#Nested loop\n",
    "results = [ [2,3,4], [9,6,2], [5,0,1]]\n",
    "for row in results:\n",
    "    for i in row:\n",
    "       print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "Fizz\n",
      "4\n",
      "Buzz\n",
      "Fizz\n",
      "7\n",
      "8\n",
      "Fizz\n",
      "Buzz\n",
      "11\n",
      "Fizz\n",
      "13\n",
      "14\n",
      "FizzBuzz\n"
     ]
    }
   ],
   "source": [
    "def fizzBuzz(n):\n",
    "    # Write your code here\n",
    "    for i in range(1, n+1):\n",
    "        if int(i)%3 == 0 and int(i)%5 == 0:\n",
    "            print(\"FizzBuzz\")\n",
    "        elif int(i)%3 ==0 and int(i)%5 != 0:\n",
    "           print(\"Fizz\")\n",
    "        elif int(i)%5 == 0 and int(i)%3 != 0:\n",
    "           print(\"Buzz\")\n",
    "        else:\n",
    "            print(i)\n",
    "fizzBuzz(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mango\n",
      "apple\n",
      "lemon\n"
     ]
    }
   ],
   "source": [
    "fruits = {'mango':21, \"apple\":23, \"lemon\":75}\n",
    "for fruit in fruits:\n",
    "    print(fruit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " pGsswgord\n"
     ]
    }
   ],
   "source": [
    "#Translation\n",
    "def translate(word):\n",
    "    translation = \" \"\n",
    "    for letter in word:\n",
    "        if letter in \"aeiou\":\n",
    "            translation += \"g\"\n",
    "        if letter in \"AEIOU\":\n",
    "            translation += \"G\"\n",
    "        else:\n",
    "            translation += letter\n",
    "    print(translation)\n",
    "translate(\"pAssword\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GmyGgod\n"
     ]
    }
   ],
   "source": [
    "def translate(phrase):\n",
    "    translation = \"\"\n",
    "    for letter in phrase:\n",
    "        if letter in \"aeiou\":\n",
    "            translation = translation + \"g\"\n",
    "         #print (translation)\n",
    "        if letter in \"AEIOU\":\n",
    "            translation = translation + \"G\"\n",
    "        #print (translation)\n",
    "        else:\n",
    "            translation = translation + letter\n",
    "    print(translation)\n",
    "            \n",
    "translate(\"OmyGod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your number here:jhhjh\n",
      "Invalid Input\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    nummber = int(input(\"Enter your number here:\"))\n",
    "    print(print)\n",
    "except:\n",
    "    print(\"Invalid Input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5]\n",
      "[2, 4, 6]\n"
     ]
    }
   ],
   "source": [
    "nums = [1, 2, 3, 4, 5, 6]\n",
    "plusOneNums = [x + 1 for x in nums]\n",
    "oddNums = [x for x in nums if x % 2 == 1]\n",
    "print(oddNums)\n",
    "oddNumsPlusOne = [x + 1 for x in nums if x % 2 == 1]\n",
    "print(oddNumsPlusOne)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 1), ('b', 2), ('c', 3), ('e', 4)]\n"
     ]
    }
   ],
   "source": [
    "ak = [1,2,3,4]\n",
    "ak1= [\"a\",\"b\",\"c\",\"e\"]\n",
    "\n",
    "table =[x for x in zip(ak1, ak)]\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2, 'c': 3, 'e': 4}\n"
     ]
    }
   ],
   "source": [
    "ak = [1,2,3,4]\n",
    "ak1= [\"a\",\"b\",\"c\",\"e\"]\n",
    "\n",
    "table ={x:x1 for x,x1 in zip(ak1, ak)}\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "no num is repeated twice\n",
      "no num is repeated twice\n",
      "no num is repeated twice\n"
     ]
    }
   ],
   "source": [
    "def mode(lists):\n",
    "    for num in lists:\n",
    "        if lists[num] == num:\n",
    "            print(num)\n",
    "        else:\n",
    "            print(\"no num is repeated twice\")\n",
    "mode([1,1,1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apples 2.0\n",
      "oranges 1.5\n",
      "pears 1.75\n"
     ]
    }
   ],
   "source": [
    "pairList = [('apples', 2.00), ('oranges', 1.50), ('pears', 1.75)]\n",
    "for fruit, price in pairList:\n",
    "    print(fruit, price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter vagghhghghgg\n",
      "ghghgg\n"
     ]
    }
   ],
   "source": [
    "def add(a,b):\n",
    "    return a+b\n",
    "print(input(add((\"Enter va\"), (\"gghh\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "none\n",
      "none\n",
      "none\n"
     ]
    }
   ],
   "source": [
    "def BuyLotOfFruits(OrderLists):\n",
    "    \n",
    "    FruitsPrice = [('apple', 20), ('orange', 23), ('lemon',12)]\n",
    "    for fruit, price in FruitsPrice:\n",
    "      if fruit in FruitsPrice:\n",
    "        print(price)\n",
    "      else:\n",
    "        print(\"none\")\n",
    "BuyLotOfFruits('orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weird\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def integ(n):\n",
    "      if n%2!=0:\n",
    "        print('Weird')\n",
    "      else:\n",
    "        print('not Weird')\n",
    "integ(10011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hallo word'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = \"  Hallo word  \"\n",
    "num.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "1\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "def maths(a,b):\n",
    "    print(a + b)\n",
    "    print(a-b)\n",
    "    print (a*b)\n",
    "\n",
    "maths(3,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is\n"
     ]
    }
   ],
   "source": [
    "def math(n):\n",
    "   t = n\n",
    "   for k in range(t,-t):\n",
    "     print(k*k)\n",
    "math(2)\n",
    "print('is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your order here: pears\n",
      "The cost of pears is 70\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def BuyListOfFruits(OrderLists):\n",
    "    #OrderLists = str(input())\n",
    "    FruitsPrice = { \"mango\": 50, \"apple\": 100, \"lemon\": 38,\"pears\":70}\n",
    "    for Fruits,Price in FruitsPrice.items():\n",
    "        if OrderLists==Fruits:\n",
    "             print(f\"The cost of {Fruits} is {Price}\")\n",
    "              #if Fruit == Order:\n",
    "                #print('The cost of ' + Fruit i + Price)\n",
    "        #else:\n",
    "            #print(\"none\")\n",
    "BuyListOfFruits(input(\"Enter your order here: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cube(num):\n",
    "   return num*num*num\n",
    "result=cube(4)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_leap(year):\n",
    "    leap = False\n",
    "    y = [range(1900,10^5)]\n",
    "    for year in range(1900, 1000001):\n",
    "        if year%4==0 and year%100==0 and year%400==0:\n",
    "           not leap\n",
    "        elif year%4==0 and year%100!=0:\n",
    "            leap\n",
    "        elif year%100 ==0 and year%400!=0:\n",
    "            leap\n",
    "        elif year%4==0 and year%400!=0:\n",
    "            leap\n",
    "    return noleap\n",
    "is_leap(199)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(1, 4)5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "y = input(range(1,4))\n",
    "for i in y:\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_leap(year):\n",
    "    leap = False\n",
    "    for year in range(1900, 1000001):\n",
    "        if year%400 != 0:\n",
    "          print (leap)  \n",
    "        else:\n",
    "           print (not leap)\n",
    "\n",
    "is_leap(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a number here:2\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "def num(n):\n",
    "    number=0\n",
    "    for i in range(1,n+1):\n",
    "        number = i-1+i\n",
    "        print(number, end='')\n",
    "    print()  \n",
    "num(int(input('Enter a number here:' )))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter any phrase here: jhjhjh\n",
      "j\n",
      "j\n",
      "j\n"
     ]
    }
   ],
   "source": [
    "phrase = str(input('Enter any phrase here: '))\n",
    "for i in phrase:\n",
    "    if phrase.index(i)%2==0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = str(input(\"Enter your word here: \"))\n",
    "num = int(input(\"Enter an integer here: \"))\n",
    "\n",
    "while num<len(words):\n",
    "    words.rstrip(n)\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check(list):\n",
    "    list = []\n",
    "    n = len(list)-1\n",
    "    same = True\n",
    "    return list[0] == list[-n]\n",
    "    #n = len(list)\n",
    "    #if list[0] == n-1:\n",
    "        #print()\n",
    "print(check(input(\"Enter your list here: \")))\n",
    "\n",
    "__name__ ='__main__'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \n",
      "\n",
      "1\n",
      "4 \n",
      "\n",
      "4\n",
      "1 \n",
      "\n",
      "1\n",
      "4 \n",
      "\n",
      "4\n",
      "1 \n",
      "\n",
      "1\n",
      "4 \n",
      "\n",
      "4\n",
      "1 \n",
      "\n",
      "1\n",
      "4 \n",
      "\n",
      "4\n",
      "1 \n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "company_name = \"Logomotor\"\n",
    "for i in company_name:\n",
    "    print(company_name.count(i[0:len(i)]), \"\\n\")\n",
    "    print(company_name.count(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 5, 10]\n",
      "[1, 5, 9, 10]\n",
      "[9, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "def num(n):\n",
    "    #N = int(raw_input())\n",
    "    print(n)\n",
    "    n.remove(6)\n",
    "    n.append(9)\n",
    "    n.append(1)\n",
    "    n.sort()\n",
    "    print(n)\n",
    "    n.pop()\n",
    "    n.reverse()\n",
    "    print(n)\n",
    "num([6,5,10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in\n"
     ]
    }
   ],
   "source": [
    "k ='olawake, was in my bad last nyt'\n",
    "t = k.split(\" \")\n",
    "print(t[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2 2\n",
      "3 3 3\n",
      "4 4 4 4\n",
      "5 5 5 5 5\n"
     ]
    }
   ],
   "source": [
    "def num(n):\n",
    "   print(n[0])\n",
    "   print(n[1], n[1])\n",
    "   print(n[2], n[2], n[2])\n",
    "   print(n[3], n[3], n[3], n[3])\n",
    "   print(n[4], n[4], n[4], n[4],n[4])\n",
    "num([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "n = [1,2,3,4,5]\n",
    "for i in n:\n",
    "    print(i)\n",
    "for t in n:\n",
    "    print(t)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a number here: ada\n",
      "['a', 'd', 'a']\n",
      "['a', 'd', 'a']\n",
      "['a', 'd', 'a']\n"
     ]
    }
   ],
   "source": [
    "k = str(input(\"Enter a number here: \"))\n",
    "k1=list()\n",
    "k1.extend(k)\n",
    "print(k1)\n",
    "k1.reverse()\n",
    "print(k1)\n",
    "if k1 == k1.reverse():\n",
    "    print(\"the same\", k1)\n",
    "    print(bool(k))\n",
    "else:\n",
    "    print(k1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = ['lohmor']\n",
    "if m == m.reverse():\n",
    "  print(m[-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    for k in range(i):\n",
    "        print(i, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = int(input(\"Enter your list here: \"))\n",
    "n2= int(input(\"Enter your second list here: \"))\n",
    "for i in n1:\n",
    "    for l in n2:\n",
    "        if i%2 !=0 and l%2==0:\n",
    "            print(i,l, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = [1,2,3,4,5]\n",
    "for i in n:\n",
    "    print(n[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "akin asimi ojewola 1 2\n",
      "['akin', 'asimi', 'ojewola', '1', '2']\n"
     ]
    }
   ],
   "source": [
    "x = input()\n",
    "k= x.split()\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'odoo'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a719286e508f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0modoo\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0modoo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlogin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mres_users\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0modoo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregistry\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'res.users'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'odoo'"
     ]
    }
   ],
   "source": [
    "!pip install odoo\n",
    "import odoo\n",
    "import odoo.exceptions\n",
    "\n",
    "def login(db, login, password):\n",
    "    res_users = odoo.registry(db)['res.users']\n",
    "    try:\n",
    "        return res_users._login(db, login, password)\n",
    "    except odoo.exceptions.AccessDenied:\n",
    "        return False\n",
    "\n",
    "def check(db, uid, passwd):\n",
    "    res_users = odoo.registry(db)['res.users']\n",
    "    return res_users.check(db, uid, passwd)\n",
    "\n",
    "def compute_session_token(session, env):\n",
    "    self = env['res.users'].browse(session.uid)\n",
    "    return self._compute_session_token(session.sid)\n",
    "\n",
    "def check_session(session, env):\n",
    "    self = env['res.users'].browse(session.uid)\n",
    "    expected = self._compute_session_token(session.sid)\n",
    "    if expected and odoo.tools.misc.consteq(expected, session.session_token):\n",
    "        return True\n",
    "    self._invalidate_session_cache()\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "me = \"Akin\"\n",
    "my_name = \" \"\n",
    "guess_count = 0\n",
    "guess_limit = 4\n",
    "out_of_guess = False\n",
    "while me != my_name and not (out_of_guess):\n",
    "    if guess_count != guess_limit:\n",
    "        my_name = input(\"Guess my name here: \")\n",
    "        guess_count += 1\n",
    "    else:\n",
    "        out_of_guess = True\n",
    "if out_of_guess:\n",
    "    print(\"Out of guesses, You lose!\")\n",
    "else:\n",
    "    print(\"You win!\") \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'triangle', 'circle', 'Polygon', 'Rhombus'}\n"
     ]
    }
   ],
   "source": [
    "setofShapes = [\"circle\",\"triangle\", \"Rhombus\",\"Polygon\"]\n",
    "lime = set(setofShapes)\n",
    "setoflime2 = [\"tricycle\", \"\"]\n",
    "lime2 = set(setoflime2)\n",
    "k= lime or lime2\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asonpmt\n"
     ]
    }
   ],
   "source": [
    "w = \"ason\"\n",
    "y = \"pmt\"\n",
    "r=''.join(w+y)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x =1\n",
    "r=np.dot(x,1)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stringBalanceCheck(s1, s2):\n",
    "    flag = True\n",
    "    for i in s1:\n",
    "        if i in s2:\n",
    "            continue   \n",
    "        else:\n",
    "            flag = False\n",
    "    return flag\n",
    "stringBalanceCheck('Pe', 'Pynative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#wordcount\n",
    "string = input(\"Enter the sentence for word count: \").lower()\n",
    "count = 0\n",
    "t = string.split()\n",
    "string.count(\"usa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count(k1):\n",
    "    dicc = {}\n",
    "    for i in k1:\n",
    "        h = k1.count(i)\n",
    "        dicc[i]=h\n",
    "    print(dicc)\n",
    "count(\"pynativepynvepynative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your dim: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def dim(arr):\n",
    "   c = np.ones([arr, arr])\n",
    "   return c\n",
    "dim(int(input(\"Enter your dim: \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def dim(arr):\n",
    "   t = np.random.randint(2,3, size =(arr))\n",
    "   number = np.random.randint(2,3, size=(t))\n",
    "   return number\n",
    "dim(int(input(\"Enter your dim: \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23, 11, 17, 24, 36, 12]\n"
     ]
    }
   ],
   "source": [
    "listOne = [10, 20, 23, 11, 17]\n",
    "listTwo = [13, 43, 24, 36, 12]\n",
    "listThree=[]\n",
    "for i in listOne:\n",
    "    if i %2 !=0:\n",
    "        listThree.append(i)\n",
    "for k in listTwo:\n",
    "    if k%2==0:\n",
    "        listThree.append(k)\n",
    "print(listThree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 12 18 \n",
      "[6, 12, 18, 4, 12, 20, 28]\n"
     ]
    }
   ],
   "source": [
    "listOne = [3, 6, 9, 12, 15, 18, 21]\n",
    "listTwo = [4, 8, 12, 16, 20, 24, 28]\n",
    "listThree=[]\n",
    "for i in range(len(listOne)):\n",
    "    if i%2!=0:\n",
    "        listThree.append(listOne[i])\n",
    "        print(listOne[i], end=\" \")\n",
    "for k in range(len(listTwo)):\n",
    "    if k%2==0:\n",
    "        listThree.append(listTwo[k])\n",
    "print(f'\\n{listThree}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the myList contains [1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "mylist = [1,2,3]\n",
    "myda = \"myList\"\n",
    "#print( \"the %s contain %f\" % (myda, mylist))\n",
    "print(f\"the {myda} contains {mylist}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "x = object()\n",
    "y = object()\n",
    "\n",
    "x_list = [x] * 10\n",
    "y_list = [y] * 10\n",
    "\n",
    "k = [x_list+y_list] * 10\n",
    "print(len(k))\n",
    "\n",
    "if k.count(y) == 10:\n",
    "    print(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34, 54, None, 67, 89, 43, 94, 11]\n"
     ]
    }
   ],
   "source": [
    "Original= [34, 54, 67, 89, 11, 43, 94]\n",
    "Original.remove(Original[4])\n",
    "Original.insert(2, )\n",
    "Original.append(11)\n",
    "print(Original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'p': 3, 'y': 3, 'n': 3, 'a': 2, 't': 2, 'i': 2, 'v': 3, 'e': 3}\n"
     ]
    }
   ],
   "source": [
    "inputStr = \"pynativepynvepynative\"\n",
    "countDict = dict()\n",
    "for char in inputStr:\n",
    "  count = inputStr.count(char)\n",
    "  countDict[char]=count\n",
    "print(countDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original list  [11, 45, 8, 11, 23, 45, 23, 45, 11, 89]\n",
      "Printing count of each item  {11: 3, 45: 3, 8: 1, 23: 2, 89: 1}\n"
     ]
    }
   ],
   "source": [
    "Original1 = [11, 45, 8, 11, 23, 45, 23, 45,11, 89]\n",
    "print(\"Original list \", Original1)\n",
    "dico = {}\n",
    "count = 0\n",
    "for i in Original:\n",
    "    count = Original.count(i)\n",
    "    dico[i]=count\n",
    "    #print(count)\n",
    "print(\"Printing count of each item \", dico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original list  [11, 45, 8, 11, 23, 45, 23, 45, 89]\n",
      "Printing count of each item   {11: 2, 45: 3, 8: 1, 23: 2, 89: 1}\n"
     ]
    }
   ],
   "source": [
    "sampleList = [11, 45, 8, 11, 23, 45, 23, 45, 89]\n",
    "print(\"Original list \", sampleList)\n",
    "\n",
    "countDict = dict()\n",
    "for item in sampleList:\n",
    "  if item in countDict:\n",
    "    countDict[item] += 1\n",
    "  else:\n",
    "    countDict[item] = 1\n",
    "  \n",
    "print(\"Printing count of each item  \",countDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First List [2, 3, 4, 5, 6, 7, 8]\n",
      "First List [4, 9, 16, 25, 36, 49, 64]\n",
      "(2, 4) (3, 9) (4, 16) (5, 25) (6, 36) (7, 49) (8, 64) "
     ]
    }
   ],
   "source": [
    "First = [2, 3, 4, 5, 6, 7, 8]\n",
    "Second = [4, 9, 16, 25, 36, 49, 64]\n",
    "print(\"First List\", First)\n",
    "print(\"First List\", Second)\n",
    "for i in range(len(First)):\n",
    "    if len(First)==len(Second):\n",
    "        y= tuple([First[i],Second[i]])\n",
    "        print(y, end=\" \" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First List [2, 3, 4, 5, 6, 7, 8]\n",
      "First List [4, 9, 16, 25, 36, 49, 64]\n",
      "{(6, 36), (8, 64), (4, 16), (5, 25), (3, 9), (7, 49), (2, 4)}\n"
     ]
    }
   ],
   "source": [
    "First = [2, 3, 4, 5, 6, 7, 8]\n",
    "Second = [4, 9, 16, 25, 36, 49, 64]\n",
    "print(\"First List\", First)\n",
    "print(\"First List\", Second)\n",
    "First1 = (First)\n",
    "Second2 = (Second)\n",
    "Third = zip(First1, Second2)\n",
    "print(set(Third))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Set  {43, 34, 27}\n",
      "Second Set  {34, 43, 48, 53, 22, 27, 93}\n",
      "First set is subset of second set  True\n",
      "Second set is subset of First set  False\n",
      "First set is Super set of second set  False\n",
      "Second set is Super set of first set  True\n",
      "First set  set()\n",
      "Second set  {34, 43, 48, 53, 22, 27, 93}\n"
     ]
    }
   ],
   "source": [
    "firstSet = {27, 43, 34}\n",
    "secondSet = {34, 93, 22, 27, 43, 53, 48}\n",
    "print(\"First Set \", firstSet)\n",
    "print(\"Second Set \", secondSet)\n",
    "a1 = set(firstSet)\n",
    "a2 = set(secondSet)\n",
    "a3 = firstSet.issubset(secondSet)\n",
    "a4 = secondSet.issubset(firstSet)\n",
    "print(\"First set is subset of second set \", a3)\n",
    "print(\"Second set is subset of First set \", a4)\n",
    "a5 = firstSet.issuperset(secondSet)\n",
    "a6 = secondSet.issuperset(firstSet)\n",
    "print(\"First set is Super set of second set \", a5)\n",
    "print(\"Second set is Super set of first set \", a6)\n",
    "if firstSet.issubset(secondSet):\n",
    "    firstSet.clear()\n",
    "    print(\"First set \", firstSet)\n",
    "if secondSet.issubset(firstSet):\n",
    "    secondSet.clear()\n",
    "    print(secondSet)\n",
    "else:\n",
    "    print(\"Second set \", secondSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after removing unwanted elemnts from list [47, 69, 76, 97]\n"
     ]
    }
   ],
   "source": [
    "rollNumber = [47, 64, 69, 37, 76, 83, 95, 97]\n",
    "sampleDict ={'Jhon':47, 'Emma':69, 'Kelly':76, 'Jason':97, \"ojo\":20}\n",
    "a1 =[]\n",
    "for i in rollNumber:\n",
    "    for k in sampleDict.values():\n",
    "        if k == i:\n",
    "            a1.append(i)\n",
    "print(\"after removing unwanted elemnts from list\", a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List - [47, 64, 69, 37, 76, 83, 95, 97]\n",
      "Dictionary -  {'Jhon': 47, 'Emma': 69, 'Kelly': 76, 'Jason': 97}\n",
      "after removing unwanted elemnts from list  [47, 69, 76, 97]\n"
     ]
    }
   ],
   "source": [
    "rollNumber  = [47, 64, 69, 37, 76, 83, 95, 97]\n",
    "sampleDict  ={'Jhon':47, 'Emma':69, 'Kelly':76, 'Jason':97} \n",
    "\n",
    "print(\"List -\", rollNumber)\n",
    "print(\"Dictionary - \", sampleDict)\n",
    "\n",
    "rollNumber[:] = [item for item in rollNumber if item in sampleDict.values()]\n",
    "print(\"after removing unwanted elemnts from list \", rollNumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python code to illustrate  \n",
    "# regression using data set \n",
    "import matplotlib \n",
    "matplotlib.use('GTKAgg') \n",
    "   \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "from sklearn import datasets, linear_model \n",
    "import pandas as pd \n",
    "   \n",
    "# Load CSV and columns \n",
    "df = pd.read_csv(\"Housing.csv\") \n",
    "   \n",
    "Y = df['price'] \n",
    "X = df['lotsize'] \n",
    "   \n",
    "X=X.reshape(len(X),1) \n",
    "Y=Y.reshape(len(Y),1) \n",
    "   \n",
    "# Split the data into training/testing sets \n",
    "X_train = X[:-250] \n",
    "X_test = X[-250:] \n",
    "   \n",
    "# Split the targets into training/testing sets \n",
    "Y_train = Y[:-250] \n",
    "Y_test = Y[-250:] \n",
    "   \n",
    "# Plot outputs \n",
    "plt.scatter(X_test, Y_test,  color='black') \n",
    "plt.title('Test Data') \n",
    "plt.xlabel('Size') \n",
    "plt.ylabel('Price') \n",
    "plt.xticks(()) \n",
    "plt.yticks(()) \n",
    "   \n",
    "  \n",
    "# Create linear regression object \n",
    "regr = linear_model.LinearRegression() \n",
    "   \n",
    "# Train the model using the training sets \n",
    "regr.fit(X_train, Y_train) \n",
    "   \n",
    "# Plot outputs \n",
    "plt.plot(X_test, regr.predict(X_test), color='red',linewidth=3) \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique list  [53, 47, 52, 44, 54]\n"
     ]
    }
   ],
   "source": [
    "speed  ={'jan':47, 'feb':52, 'march':47, 'April':44, 'May':52, 'June':53,\n",
    "          'july':54, 'Aug':44, 'Sept':54} \n",
    "r = []\n",
    "o = []\n",
    "p = []\n",
    "for d in speed.values():\n",
    "    r.append(d)\n",
    "for i in range(len(r)):\n",
    "    j = r.count(r[i])\n",
    "    p.append(j)\n",
    "    if j==1:\n",
    "        o.append(r[i])\n",
    "for m in r:\n",
    "    if m not in o:\n",
    "        o.append(m)\n",
    "print(\"unique list \", o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary's values -  dict_values([47, 52, 47, 44, 52, 53, 54, 44, 54])\n",
      "unique list [47, 52, 44, 53, 54]\n"
     ]
    }
   ],
   "source": [
    "speed  ={'jan':47, 'feb':52, 'march':47, 'April':44, 'May':52, 'June':53,\n",
    "          'july':54, 'Aug':44, 'Sept':54} \n",
    "\n",
    "print(\"Dictionary's values - \", speed.values())\n",
    "\n",
    "speedList = list()\n",
    "for item in speed.values():\n",
    "  if item not in speedList:\n",
    "    speedList.append(item)\n",
    "print(\"unique list\", speedList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65, 99, 41, 45, 87, 94]\n",
      "(65, 99, 41, 45, 87, 94)\n",
      "99\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "sampleList = [87, 45, 41, 65, 94, 41, 99, 94]\n",
    "y=list(set(sampleList))\n",
    "print(y)\n",
    "y = tuple(y)\n",
    "print(y)\n",
    "print(max(y))\n",
    "print(min(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original list [87, 52, 44, 53, 54, 87, 52, 53]\n",
      "unique list [44, 52, 53, 54, 87]\n",
      "tuple  (44, 52, 53, 54, 87)\n",
      "Minimum number is:  44\n",
      "Maximum number is:  87\n"
     ]
    }
   ],
   "source": [
    "sampleList = [87, 52, 44, 53, 54, 87, 52, 53]\n",
    "\n",
    "print(\"Original list\", sampleList)\n",
    "\n",
    "sampleList = list(set(sampleList))\n",
    "print(\"unique list\", sampleList)\n",
    "\n",
    "tuple = tuple(sampleList)\n",
    "print(\"tuple \", tuple)\n",
    "\n",
    "print(\"Minimum number is: \", min(tuple))\n",
    "print(\"Maximum number is: \", max(tuple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5368926557, 7127932947]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "r=[]\n",
    "for i in range(100):\n",
    "    y= random.randrange(1000000000,9999999999)  \n",
    "    r.append(y)\n",
    "random.sample(r, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating 100 random lottery tickets\n",
      "Lucky 2 lottery tickets are [5167365049, 1708117294]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "lottery_tickets_list = []\n",
    "print(\"creating 100 random lottery tickets\")\n",
    "for i in range(100):\n",
    "    lottery_tickets_list.append(random.randrange(1000000000, 9999999999))\n",
    "\n",
    "winners = random.sample(lottery_tickets_list, 2)\n",
    "print(\"Lucky 2 lottery tickets are\", winners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secure random OTP is  512391 "
     ]
    }
   ],
   "source": [
    "import random\n",
    "r=[]\n",
    "for i in range(100):\n",
    "   y= random.randrange(999999)  \n",
    "   r.append(y)\n",
    "w= random.sample(r, 1)\n",
    "for g2 in w:\n",
    "    print(\"Secure random OTP is \", g2, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 6 digit random OTP\n",
      "Secure random OTP is  805870\n"
     ]
    }
   ],
   "source": [
    "import secrets\n",
    "\n",
    "#Getting systemRandom class instance out of secrets module\n",
    "secretsGenerator = secrets.SystemRandom()\n",
    "\n",
    "print(\"Generating 6 digit random OTP\")\n",
    "otp = secretsGenerator.randrange(100000, 999999)\n",
    "\n",
    "print(\"Secure random OTP is \", otp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a word here: jjjhjhjh\n",
      "['h', 'j']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "q = input(\"Enter a word here: \")\n",
    "for i in range(len(q)):\n",
    "   w = random.choice(q)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSkKvPdmaYHwWjG"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "#secretnum = secrets.SystemRandom\n",
    "stringlength = 15\n",
    "letters = string.ascii_letters\n",
    "for i in range(stringlength):\n",
    "    k = \"\". join(random.choice(letters))\n",
    "    print(k, end= \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secure Password is  e;Nrpj9C5T\n",
      "Reset password URL Link\n",
      "https://demo.com/user/jhon/reset=Ffo\n"
     ]
    }
   ],
   "source": [
    "import secrets\n",
    "import string\n",
    "\n",
    "stringSource  = string.ascii_letters + string.digits + string.punctuation\n",
    "password = secrets.choice(string.ascii_lowercase)\n",
    "password += secrets.choice(string.ascii_uppercase)\n",
    "password += secrets.choice(string.digits)\n",
    "password += secrets.choice(string.punctuation)\n",
    "\n",
    "for i in range(6):\n",
    "    password += secrets.choice(stringSource)\n",
    "\n",
    "char_list = list(password)\n",
    "secrets.SystemRandom().shuffle(char_list)\n",
    "password = ''.join(char_list)\n",
    "print (\"Secure Password is \", password)\n",
    "\n",
    "print (\"Reset password URL Link\")\n",
    "SecureURL = \"https://demo.com/user/jhon/reset=\"\n",
    "SecureURL+=secrets.token_urlsafe(2)\n",
    "\n",
    "print(SecureURL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4c08c84db38a9574\n",
      "b'K\\xa6\\xbb(\\xab'\n"
     ]
    }
   ],
   "source": [
    "import secrets\n",
    "y = secrets.token_hex(8)\n",
    "u = secrets.token_bytes(5)\n",
    "print(y)\n",
    "print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NPhG+6}L!-"
     ]
    }
   ],
   "source": [
    "import secrets\n",
    "import string\n",
    "import random\n",
    "\n",
    "v=string.ascii_letters+string.digits+string.punctuation\n",
    "for z in range(10):\n",
    "    t= secrets.choice(v)\n",
    "    print(t, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https:\\\\akinasim.com\\x07bout\\reset=BvIAsg'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import secrets\n",
    "url1 = \"https:\\\\akinasim.com\\about\\reset=\"\n",
    "url1 += secrets.token_urlsafe(4)\n",
    "url1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.034999059188493775\n",
      "1.255263201486166\n",
      "0.04393303108595251\n"
     ]
    }
   ],
   "source": [
    "#First random float number must be between 0.1 and 1\n",
    "#Second random float number must be between 9.5 and 99.5\n",
    "import random\n",
    "import secrets\n",
    "\n",
    "#for ii in range(5):\n",
    "t = random.random()\n",
    "print(t)\n",
    "g1 = random.uniform(1, 2)\n",
    "print(g1)\n",
    "print(t*g1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selecting same number of a dice\n",
      "5\n",
      "3\n",
      "6\n",
      "1\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "dice = [1, 2, 3, 4, 5, 6]\n",
    "print(\"Randomly selecting same number of a dice\")\n",
    "for i in range(5):\n",
    "    random.seed()\n",
    "    print(random.choice(dice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,6):\n",
    "    for k in range(i):\n",
    "        for b in range(k):\n",
    "            print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing random date between 1/1/2016  and  12/12/2018\n",
      "Random Date =  01/17/2016\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "def getRandomDate(startDate, endDate ):\n",
    "    print(\"Printing random date between\", startDate, \" and \", endDate)\n",
    "    randomGenerator = random.random()\n",
    "    dateFormat = '%m/%d/%Y'\n",
    "\n",
    "    startTime = time.mktime(time.strptime(startDate, dateFormat))\n",
    "    endTime = time.mktime(time.strptime(endDate, dateFormat))\n",
    "\n",
    "    randomTime = startTime + randomGenerator * (endTime - startTime)\n",
    "    randomDate = time.strftime(dateFormat, time.localtime(randomTime))\n",
    "    return randomDate\n",
    "\n",
    "print (\"Random Date = \", getRandomDate(\"1/1/2016\", \"12/12/2018\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-04 -0.0021004676818847656\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "import time\n",
    "c=time.time()\n",
    "v= date.today()\n",
    "print(v, c-time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "medal = (\"Gold\", \"Silver\", \"Bronze\")\n",
    "y_pos = np.arange(len(medal))\n",
    "Argentina = [18,24,28]\n",
    "plt.bar(y_pos, Argentina, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, medal)\n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "t = input(\"num: \")\n",
    "for i in range(int(t)):\n",
    "    N = input().split(\" \")\n",
    "    command = N[0]\n",
    "    if command == 'insert':\n",
    "       result.insert(int(N[1]), int(N[2]))\n",
    "    if command == 'print':\n",
    "        print(result)\n",
    "    if command == 'remove':\n",
    "       result.remove(int(N[1]))\n",
    "    if command == 'append':\n",
    "        result.append(int(N[1]))\n",
    "    if command == 'reverse':\n",
    "       result.reverse()\n",
    "       print(result)\n",
    "    if command == 'pop':\n",
    "       result.pop()\n",
    "    if command == 'sort':\n",
    "       result.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndxGRJKRyFxFJQip3EZEk9P8BiEgN31GE9Y4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17f236770f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [0,5,9,10,15]\n",
    "y = [0,1,2,3,4]\n",
    "plt.plot(x,y)\n",
    "plt.xticks(np.arange(min(x), max(x)+1, 2.0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school</th>\n",
       "      <th>attendance</th>\n",
       "      <th>population</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>23</td>\n",
       "      <td>90</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "      <td>12</td>\n",
       "      <td>67</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c</td>\n",
       "      <td>87</td>\n",
       "      <td>85</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d</td>\n",
       "      <td>43</td>\n",
       "      <td>29</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e</td>\n",
       "      <td>33</td>\n",
       "      <td>44</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>f</td>\n",
       "      <td>17</td>\n",
       "      <td>32</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>g</td>\n",
       "      <td>18</td>\n",
       "      <td>61</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>h</td>\n",
       "      <td>55</td>\n",
       "      <td>77</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i</td>\n",
       "      <td>76</td>\n",
       "      <td>99</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  school  attendance  population  score\n",
       "0      a          23          90     55\n",
       "1      b          12          67     66\n",
       "2      c          87          85     81\n",
       "3      d          43          29     90\n",
       "4      e          33          44     22\n",
       "5      f          17          32     52\n",
       "6      g          18          61     19\n",
       "7      h          55          77     34\n",
       "8      i          76          99     38"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"testData.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count1 = 0\n",
    "count2=0\n",
    "count3=0\n",
    "count4=0\n",
    "for i in df[\"score\"]:\n",
    "    if i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-183-f572b0ad0508>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'0-10'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'11-20'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'21-30'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'31-40'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'41-50'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'51-60'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'61-70'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'70+'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0max\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcountplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"score\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"population\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\seaborn\\categorical.py\u001b[0m in \u001b[0;36mcountplot\u001b[1;34m(x, y, hue, data, order, hue_order, orient, color, palette, saturation, dodge, ax, **kwargs)\u001b[0m\n\u001b[0;32m   3551\u001b[0m                           \u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mci\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_boot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munits\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3552\u001b[0m                           \u001b[0morient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaturation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3553\u001b[1;33m                           errcolor, errwidth, capsize, dodge)\n\u001b[0m\u001b[0;32m   3554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3555\u001b[0m     \u001b[0mplotter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"count\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\seaborn\\categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, hue, data, order, hue_order, estimator, ci, n_boot, units, orient, color, palette, saturation, errcolor, errwidth, capsize, dodge)\u001b[0m\n\u001b[0;32m   1605\u001b[0m         \u001b[1;34m\"\"\"Initialize the plotter.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1606\u001b[0m         self.establish_variables(x, y, hue, data, orient,\n\u001b[1;32m-> 1607\u001b[1;33m                                  order, hue_order, units)\n\u001b[0m\u001b[0;32m   1608\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestablish_colors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaturation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1609\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimate_statistic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mci\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_boot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\seaborn\\categorical.py\u001b[0m in \u001b[0;36mestablish_variables\u001b[1;34m(self, x, y, hue, data, orient, order, hue_order, units)\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[1;31m# See if we need to get variables from `data`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m                 \u001b[0mhue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "features = ['0-10', '11-20', '21-30', '31-40', '41-50', '51-60','61-70','70+']\n",
    "ax=sns.countplot(x=\"score\", data=\"population\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school</th>\n",
       "      <th>attendance</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>population</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>d</td>\n",
       "      <td>43</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>f</td>\n",
       "      <td>17</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>e</td>\n",
       "      <td>33</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>g</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>b</td>\n",
       "      <td>12</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>h</td>\n",
       "      <td>55</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>c</td>\n",
       "      <td>87</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>a</td>\n",
       "      <td>23</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>i</td>\n",
       "      <td>76</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           school  attendance  score\n",
       "population                          \n",
       "29              d          43     90\n",
       "32              f          17     52\n",
       "44              e          33     22\n",
       "61              g          18     19\n",
       "67              b          12     66\n",
       "77              h          55     34\n",
       "85              c          87     81\n",
       "90              a          23     55\n",
       "99              i          76     38"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = df.groupby(\"population\")\n",
    "t.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      school\n",
       "score       \n",
       "19         g\n",
       "22         e\n",
       "34         h\n",
       "38         i\n",
       "52         f\n",
       "55         a\n",
       "66         b\n",
       "81         c\n",
       "90         d"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gp_1 = df[['score', 'school']].groupby('score')\n",
    "df_gp_1.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>attendance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>55</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>66</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>81</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>90</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score  attendance\n",
       "0     19          18\n",
       "1     22          33\n",
       "2     34          55\n",
       "3     38          76\n",
       "4     52          17\n",
       "5     55          23\n",
       "6     66          12\n",
       "7     81          87\n",
       "8     90          43"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gp_1 = df[['score', 'attendance']].groupby('score').agg(np.max).reset_index()\n",
    "df_gp_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>55</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>66</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>81</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>90</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score  population\n",
       "0     19          61\n",
       "1     22          44\n",
       "2     34          77\n",
       "3     38          99\n",
       "4     52          32\n",
       "5     55          90\n",
       "6     66          67\n",
       "7     81          85\n",
       "8     90          29"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gp_2 = df[['score', 'population']].groupby('score').agg(np.mean).reset_index()\n",
    "df_gp_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "can not merge DataFrame with instance of type <class 'pandas.core.groupby.DataFrameGroupBy'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-144-db0d39fbd626>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_gp2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_gp_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_gp_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"score\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_gp2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator)\u001b[0m\n\u001b[0;32m     51\u001b[0m                          \u001b[0mright_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mright_on\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mleft_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                          \u001b[0mright_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mright_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m                          copy=copy, indicator=indicator)\n\u001b[0m\u001b[0;32m     54\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator)\u001b[0m\n\u001b[0;32m    529\u001b[0m             raise ValueError(\n\u001b[0;32m    530\u001b[0m                 \u001b[1;34m'can not merge DataFrame with instance of '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m                 'type {0}'.format(type(left)))\n\u001b[0m\u001b[0;32m    532\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m             raise ValueError(\n",
      "\u001b[1;31mValueError\u001b[0m: can not merge DataFrame with instance of type <class 'pandas.core.groupby.DataFrameGroupBy'>"
     ]
    }
   ],
   "source": [
    "df_gp2 = pd.merge(df_gp_1, df_gp_2, how =\"inner\")\n",
    "df_gp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_gp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-139-da4a16cfcdec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_gp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormalize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfreq\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_gp' is not defined"
     ]
    }
   ],
   "source": [
    "freq = ((df_gp.population.value_counts(normalize = True).reset_index().sort_values(by = 'index').population)*100)\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWkAAAD8CAYAAAC1p1UKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYv8F2WXrSKuxSVAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26984aa35c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.gridspec as gs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "freq =np.linspace(20,2,8) \n",
    "fig, ax =plt.subplots()\n",
    "features = ['0-10', '11-20', '21-30', '31-40', '41-50', '51-60','61-70','70+']\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.bar(features, freq, alpha=0.5, color ='#004c99')   \n",
    "ax.tick_params(axis='y', labelsize = 0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njY/dp0/vRgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x269fd6ca860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "sns.lineplot(x=['A','B','C','D'], \n",
    "             y=[4,2,5,3],\n",
    "             color='r',\n",
    "             ax=ax)\n",
    "sns.lineplot(x=['A','B','C','D'], \n",
    "             y=[1,6,2,4], \n",
    "             color='b',\n",
    "             ax=ax)    \n",
    "ax.legend(['alpha', 'beta'], facecolor='w')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44    1\n",
       "61    1\n",
       "Name: population, dtype: int64"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['population'].value_counts()[2:4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'age'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-202-a666b51fdd80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"age\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mgroupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, **kwargs)\u001b[0m\n\u001b[0;32m   4414\u001b[0m         return groupby(self, by=by, axis=axis, level=level, as_index=as_index,\n\u001b[0;32m   4415\u001b[0m                        \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msqueeze\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4416\u001b[1;33m                        **kwargs)\n\u001b[0m\u001b[0;32m   4417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4418\u001b[0m     def asfreq(self, freq, method=None, how=None, normalize=False,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby.py\u001b[0m in \u001b[0;36mgroupby\u001b[1;34m(obj, by, **kwds)\u001b[0m\n\u001b[0;32m   1697\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'invalid type: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1699\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1700\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, **kwargs)\u001b[0m\n\u001b[0;32m    390\u001b[0m                                                     \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m                                                     \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 392\u001b[1;33m                                                     mutated=self.mutated)\n\u001b[0m\u001b[0;32m    393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby.py\u001b[0m in \u001b[0;36m_get_grouper\u001b[1;34m(obj, key, axis, level, sort, mutated)\u001b[0m\n\u001b[0;32m   2688\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgpr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2689\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2690\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2691\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mgpr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2692\u001b[0m             \u001b[1;31m# Add key to exclusions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'age'"
     ]
    }
   ],
   "source": [
    "df.groupby(\"age\").size().plot.bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x  y\n",
       "0 -1 -1\n",
       "1  1  2\n",
       "2  2  3\n",
       "3  4  3\n",
       "4  6  5\n",
       "5  7  8"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = [[-1,-1],[1,2],[2,3],[4,3],[6,5],[7,8]]\n",
    "df1 = pd.DataFrame(data, columns=['x','y'])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 4]\n",
      " [ 6]\n",
      " [ 7]]\n",
      "[-1  2  3  3  5  8]\n"
     ]
    }
   ],
   "source": [
    "x= df1.iloc[:,:-1].values\n",
    "y=df1.iloc[:,1].values\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "Regressor=LinearRegression()\n",
    "Regressor.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89810550554741464"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Regressor.score(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3807829181494653"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Regressor.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.93238434])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Regressor.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Regressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-bfca455436b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mxtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mRegressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'Regressor' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "xtest = np.array([10]).reshape(1,1)\n",
    "Regressor.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.947684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>0.947684</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x         y\n",
       "x  1.000000  0.947684\n",
       "y  0.947684  1.000000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.55160142,  1.31316726,  2.2455516 ,  4.11032028,  5.97508897,\n",
       "        6.90747331])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2=Regressor.predict(x)\n",
    "y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/0twHAGpenIeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22d3d133128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x,y, color='r')\n",
    "plt.scatter(x, Regressor.predict(x), color=\"b\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>y_prd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.551601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.313167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2.245552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4.110320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5.975089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>6.907473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x  y     y_prd\n",
       "0 -1 -1 -0.551601\n",
       "1  1  2  1.313167\n",
       "2  2  3  2.245552\n",
       "3  4  3  4.110320\n",
       "4  6  5  5.975089\n",
       "5  7  8  6.907473"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['y_prd']=y2\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df2=pd.read_csv(\"Social_Network_Ads.csv\", delimiter= \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "#Alternative method to feature engineering solution\n",
    "t5=\"\"\n",
    "df2[\"Age Category\"] =\"\"\n",
    "for u1 in range(len(df2[\"Age\"])):\n",
    "    if 0<df2[\"Age\"][u1]<24:\n",
    "        t5=\"\".join(\"iGen\")\n",
    "        df2[\"Age Category\"][u1]=t5\n",
    "    elif 24<df2[\"Age\"][u1]<40:\n",
    "        t5=\"\".join(\"Millenials\")\n",
    "        df2[\"Age Category\"][u1]=t5\n",
    "    elif 39<df2[\"Age\"][u1]<56:\n",
    "        t5=\"\".join(\"GenX\")\n",
    "        df2[\"Age Category\"][u1]=t5\n",
    "    elif 55<df2[\"Age\"][u1]<74:\n",
    "        t5=\"\".join(\"BabyBoomers\")\n",
    "        df2[\"Age Category\"][u1]=t5\n",
    "    else:\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Purchased</th>\n",
       "      <th>Age Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15624510</td>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>19000</td>\n",
       "      <td>0</td>\n",
       "      <td>iGen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15810944</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>20000</td>\n",
       "      <td>0</td>\n",
       "      <td>Millenials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15668575</td>\n",
       "      <td>Female</td>\n",
       "      <td>26</td>\n",
       "      <td>43000</td>\n",
       "      <td>0</td>\n",
       "      <td>Millenials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15603246</td>\n",
       "      <td>Female</td>\n",
       "      <td>27</td>\n",
       "      <td>57000</td>\n",
       "      <td>0</td>\n",
       "      <td>Millenials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15804002</td>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>76000</td>\n",
       "      <td>0</td>\n",
       "      <td>iGen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15728773</td>\n",
       "      <td>Male</td>\n",
       "      <td>27</td>\n",
       "      <td>58000</td>\n",
       "      <td>0</td>\n",
       "      <td>Millenials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15598044</td>\n",
       "      <td>Female</td>\n",
       "      <td>27</td>\n",
       "      <td>84000</td>\n",
       "      <td>0</td>\n",
       "      <td>Millenials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15694829</td>\n",
       "      <td>Female</td>\n",
       "      <td>32</td>\n",
       "      <td>150000</td>\n",
       "      <td>1</td>\n",
       "      <td>Millenials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>15600575</td>\n",
       "      <td>Male</td>\n",
       "      <td>25</td>\n",
       "      <td>33000</td>\n",
       "      <td>0</td>\n",
       "      <td>Millenials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15727311</td>\n",
       "      <td>Female</td>\n",
       "      <td>35</td>\n",
       "      <td>65000</td>\n",
       "      <td>0</td>\n",
       "      <td>Millenials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15570769</td>\n",
       "      <td>Female</td>\n",
       "      <td>26</td>\n",
       "      <td>80000</td>\n",
       "      <td>0</td>\n",
       "      <td>Millenials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>15606274</td>\n",
       "      <td>Female</td>\n",
       "      <td>26</td>\n",
       "      <td>52000</td>\n",
       "      <td>0</td>\n",
       "      <td>Millenials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15746139</td>\n",
       "      <td>Male</td>\n",
       "      <td>20</td>\n",
       "      <td>86000</td>\n",
       "      <td>0</td>\n",
       "      <td>iGen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15704987</td>\n",
       "      <td>Male</td>\n",
       "      <td>32</td>\n",
       "      <td>18000</td>\n",
       "      <td>0</td>\n",
       "      <td>Millenials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15628972</td>\n",
       "      <td>Male</td>\n",
       "      <td>18</td>\n",
       "      <td>82000</td>\n",
       "      <td>0</td>\n",
       "      <td>iGen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15697686</td>\n",
       "      <td>Male</td>\n",
       "      <td>29</td>\n",
       "      <td>80000</td>\n",
       "      <td>0</td>\n",
       "      <td>Millenials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15733883</td>\n",
       "      <td>Male</td>\n",
       "      <td>47</td>\n",
       "      <td>25000</td>\n",
       "      <td>1</td>\n",
       "      <td>GenX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>15617482</td>\n",
       "      <td>Male</td>\n",
       "      <td>45</td>\n",
       "      <td>26000</td>\n",
       "      <td>1</td>\n",
       "      <td>GenX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>15704583</td>\n",
       "      <td>Male</td>\n",
       "      <td>46</td>\n",
       "      <td>28000</td>\n",
       "      <td>1</td>\n",
       "      <td>GenX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>15621083</td>\n",
       "      <td>Female</td>\n",
       "      <td>48</td>\n",
       "      <td>29000</td>\n",
       "      <td>1</td>\n",
       "      <td>GenX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>15649487</td>\n",
       "      <td>Male</td>\n",
       "      <td>45</td>\n",
       "      <td>22000</td>\n",
       "      <td>1</td>\n",
       "      <td>GenX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>15736760</td>\n",
       "      <td>Female</td>\n",
       "      <td>47</td>\n",
       "      <td>49000</td>\n",
       "      <td>1</td>\n",
       "      <td>GenX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15714658</td>\n",
       "      <td>Male</td>\n",
       "      <td>48</td>\n",
       "      <td>41000</td>\n",
       "      <td>1</td>\n",
       "      <td>GenX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>15599081</td>\n",
       "      <td>Female</td>\n",
       "      <td>45</td>\n",
       "      <td>22000</td>\n",
       "      <td>1</td>\n",
       "      <td>GenX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>15705113</td>\n",
       "      <td>Male</td>\n",
       "      <td>46</td>\n",
       "      <td>23000</td>\n",
       "      <td>1</td>\n",
       "      <td>GenX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>15631159</td>\n",
       "      <td>Male</td>\n",
       "      <td>47</td>\n",
       "      <td>20000</td>\n",
       "      <td>1</td>\n",
       "      <td>GenX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>15792818</td>\n",
       "      <td>Male</td>\n",
       "      <td>49</td>\n",
       "      <td>28000</td>\n",
       "      <td>1</td>\n",
       "      <td>GenX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>15633531</td>\n",
       "      <td>Female</td>\n",
       "      <td>47</td>\n",
       "      <td>30000</td>\n",
       "      <td>1</td>\n",
       "      <td>GenX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>15744529</td>\n",
       "      <td>Male</td>\n",
       "      <td>29</td>\n",
       "      <td>43000</td>\n",
       "      <td>0</td>\n",
       "      <td>Millenials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>15669656</td>\n",
       "      <td>Male</td>\n",
       "      <td>31</td>\n",
       "      <td>18000</td>\n",
       "      <td>0</td>\n",
       "      <td>Millenials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>15611430</td>\n",
       "      <td>Female</td>\n",
       "      <td>60</td>\n",
       "      <td>46000</td>\n",
       "      <td>1</td>\n",
       "      <td>BabyBoomers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>15774744</td>\n",
       "      <td>Male</td>\n",
       "      <td>60</td>\n",
       "      <td>83000</td>\n",
       "      <td>1</td>\n",
       "      <td>BabyBoomers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>15629885</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>73000</td>\n",
       "      <td>0</td>\n",
       "      <td>Millenials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>15708791</td>\n",
       "      <td>Male</td>\n",
       "      <td>59</td>\n",
       "      <td>130000</td>\n",
       "      <td>1</td>\n",
       "      <td>BabyBoomers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>15793890</td>\n",
       "      <td>Female</td>\n",
       "      <td>37</td>\n",
       "      <td>80000</td>\n",
       "      <td>0</td>\n",
       "      <td>Millenials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>15646091</td>\n",
       "      <td>Female</td>\n",
       "      <td>46</td>\n",
       "      <td>32000</td>\n",
       "      <td>1</td>\n",
       "      <td>GenX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>15596984</td>\n",
       "      <td>Female</td>\n",
       "      <td>46</td>\n",
       "      <td>74000</td>\n",
       "      <td>0</td>\n",
       "      <td>GenX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>15800215</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>53000</td>\n",
       "      <td>0</td>\n",
       "      <td>GenX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>15577806</td>\n",
       "      <td>Male</td>\n",
       "      <td>41</td>\n",
       "      <td>87000</td>\n",
       "      <td>1</td>\n",
       "      <td>GenX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>15749381</td>\n",
       "      <td>Female</td>\n",
       "      <td>58</td>\n",
       "      <td>23000</td>\n",
       "      <td>1</td>\n",
       "      <td>BabyBoomers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>15683758</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>64000</td>\n",
       "      <td>0</td>\n",
       "      <td>GenX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>15670615</td>\n",
       "      <td>Male</td>\n",
       "      <td>48</td>\n",
       "      <td>33000</td>\n",
       "      <td>1</td>\n",
       "      <td>GenX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>15715622</td>\n",
       "      <td>Female</td>\n",
       "      <td>44</td>\n",
       "      <td>139000</td>\n",
       "      <td>1</td>\n",
       "      <td>GenX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>15707634</td>\n",
       "      <td>Male</td>\n",
       "      <td>49</td>\n",
       "      <td>28000</td>\n",
       "      <td>1</td>\n",
       "      <td>GenX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>15806901</td>\n",
       "      <td>Female</td>\n",
       "      <td>57</td>\n",
       "      <td>33000</td>\n",
       "      <td>1</td>\n",
       "      <td>BabyBoomers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>15775335</td>\n",
       "      <td>Male</td>\n",
       "      <td>56</td>\n",
       "      <td>60000</td>\n",
       "      <td>1</td>\n",
       "      <td>BabyBoomers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>15724150</td>\n",
       "      <td>Female</td>\n",
       "      <td>49</td>\n",
       "      <td>39000</td>\n",
       "      <td>1</td>\n",
       "      <td>GenX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>15627220</td>\n",
       "      <td>Male</td>\n",
       "      <td>39</td>\n",
       "      <td>71000</td>\n",
       "      <td>0</td>\n",
       "      <td>Millenials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>15672330</td>\n",
       "      <td>Male</td>\n",
       "      <td>47</td>\n",
       "      <td>34000</td>\n",
       "      <td>1</td>\n",
       "      <td>GenX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>15668521</td>\n",
       "      <td>Female</td>\n",
       "      <td>48</td>\n",
       "      <td>35000</td>\n",
       "      <td>1</td>\n",
       "      <td>GenX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>15807837</td>\n",
       "      <td>Male</td>\n",
       "      <td>48</td>\n",
       "      <td>33000</td>\n",
       "      <td>1</td>\n",
       "      <td>GenX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>15592570</td>\n",
       "      <td>Male</td>\n",
       "      <td>47</td>\n",
       "      <td>23000</td>\n",
       "      <td>1</td>\n",
       "      <td>GenX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>15748589</td>\n",
       "      <td>Female</td>\n",
       "      <td>45</td>\n",
       "      <td>45000</td>\n",
       "      <td>1</td>\n",
       "      <td>GenX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>15635893</td>\n",
       "      <td>Male</td>\n",
       "      <td>60</td>\n",
       "      <td>42000</td>\n",
       "      <td>1</td>\n",
       "      <td>BabyBoomers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>15757632</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>59000</td>\n",
       "      <td>0</td>\n",
       "      <td>Millenials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>15691863</td>\n",
       "      <td>Female</td>\n",
       "      <td>46</td>\n",
       "      <td>41000</td>\n",
       "      <td>1</td>\n",
       "      <td>GenX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>15706071</td>\n",
       "      <td>Male</td>\n",
       "      <td>51</td>\n",
       "      <td>23000</td>\n",
       "      <td>1</td>\n",
       "      <td>GenX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>15654296</td>\n",
       "      <td>Female</td>\n",
       "      <td>50</td>\n",
       "      <td>20000</td>\n",
       "      <td>1</td>\n",
       "      <td>GenX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>15755018</td>\n",
       "      <td>Male</td>\n",
       "      <td>36</td>\n",
       "      <td>33000</td>\n",
       "      <td>0</td>\n",
       "      <td>Millenials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>15594041</td>\n",
       "      <td>Female</td>\n",
       "      <td>49</td>\n",
       "      <td>36000</td>\n",
       "      <td>1</td>\n",
       "      <td>GenX</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      User ID  Gender  Age  EstimatedSalary  Purchased Age Category\n",
       "0    15624510    Male   19            19000          0         iGen\n",
       "1    15810944    Male   35            20000          0   Millenials\n",
       "2    15668575  Female   26            43000          0   Millenials\n",
       "3    15603246  Female   27            57000          0   Millenials\n",
       "4    15804002    Male   19            76000          0         iGen\n",
       "5    15728773    Male   27            58000          0   Millenials\n",
       "6    15598044  Female   27            84000          0   Millenials\n",
       "7    15694829  Female   32           150000          1   Millenials\n",
       "8    15600575    Male   25            33000          0   Millenials\n",
       "9    15727311  Female   35            65000          0   Millenials\n",
       "10   15570769  Female   26            80000          0   Millenials\n",
       "11   15606274  Female   26            52000          0   Millenials\n",
       "12   15746139    Male   20            86000          0         iGen\n",
       "13   15704987    Male   32            18000          0   Millenials\n",
       "14   15628972    Male   18            82000          0         iGen\n",
       "15   15697686    Male   29            80000          0   Millenials\n",
       "16   15733883    Male   47            25000          1         GenX\n",
       "17   15617482    Male   45            26000          1         GenX\n",
       "18   15704583    Male   46            28000          1         GenX\n",
       "19   15621083  Female   48            29000          1         GenX\n",
       "20   15649487    Male   45            22000          1         GenX\n",
       "21   15736760  Female   47            49000          1         GenX\n",
       "22   15714658    Male   48            41000          1         GenX\n",
       "23   15599081  Female   45            22000          1         GenX\n",
       "24   15705113    Male   46            23000          1         GenX\n",
       "25   15631159    Male   47            20000          1         GenX\n",
       "26   15792818    Male   49            28000          1         GenX\n",
       "27   15633531  Female   47            30000          1         GenX\n",
       "28   15744529    Male   29            43000          0   Millenials\n",
       "29   15669656    Male   31            18000          0   Millenials\n",
       "..        ...     ...  ...              ...        ...          ...\n",
       "370  15611430  Female   60            46000          1  BabyBoomers\n",
       "371  15774744    Male   60            83000          1  BabyBoomers\n",
       "372  15629885  Female   39            73000          0   Millenials\n",
       "373  15708791    Male   59           130000          1  BabyBoomers\n",
       "374  15793890  Female   37            80000          0   Millenials\n",
       "375  15646091  Female   46            32000          1         GenX\n",
       "376  15596984  Female   46            74000          0         GenX\n",
       "377  15800215  Female   42            53000          0         GenX\n",
       "378  15577806    Male   41            87000          1         GenX\n",
       "379  15749381  Female   58            23000          1  BabyBoomers\n",
       "380  15683758    Male   42            64000          0         GenX\n",
       "381  15670615    Male   48            33000          1         GenX\n",
       "382  15715622  Female   44           139000          1         GenX\n",
       "383  15707634    Male   49            28000          1         GenX\n",
       "384  15806901  Female   57            33000          1  BabyBoomers\n",
       "385  15775335    Male   56            60000          1  BabyBoomers\n",
       "386  15724150  Female   49            39000          1         GenX\n",
       "387  15627220    Male   39            71000          0   Millenials\n",
       "388  15672330    Male   47            34000          1         GenX\n",
       "389  15668521  Female   48            35000          1         GenX\n",
       "390  15807837    Male   48            33000          1         GenX\n",
       "391  15592570    Male   47            23000          1         GenX\n",
       "392  15748589  Female   45            45000          1         GenX\n",
       "393  15635893    Male   60            42000          1  BabyBoomers\n",
       "394  15757632  Female   39            59000          0   Millenials\n",
       "395  15691863  Female   46            41000          1         GenX\n",
       "396  15706071    Male   51            23000          1         GenX\n",
       "397  15654296  Female   50            20000          1         GenX\n",
       "398  15755018    Male   36            33000          0   Millenials\n",
       "399  15594041  Female   49            36000          1         GenX\n",
       "\n",
       "[400 rows x 6 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Which of the following phones industry uses Unix OS?\n",
      " (a)iphone \n",
      " (b)Samsung \n",
      " (c)None \n",
      "\n",
      "\n",
      "Enter your answer here\n",
      "2\n",
      "2. Which of the following phones industry uses Android OS?\n",
      " (a)iphone \n",
      " (b)Samsung \n",
      " (c)None\n",
      "\n",
      "\n",
      "Enter your answer here\n",
      "2\n",
      "3. Which of the following phones industry uses Android OS?\n",
      " (a)iphone \n",
      " (b)Samsung\n",
      " (c)None\n",
      "\n",
      "\n",
      "Enter your answer here\n",
      "2\n",
      "You got  0 correctly\n"
     ]
    }
   ],
   "source": [
    "class phone_industry:\n",
    "    def __init__(self, prompt_question, answer):\n",
    "        self.prompt_question = prompt_question\n",
    "        self.answer= answer\n",
    "questions=[phone_industry((\"1. Which of the following phones industry uses Unix OS?\\n (a)iphone \\n (b)Samsung \\n (c)None \\n\\n\"), \"c\" ),\n",
    "          phone_industry((\"2. Which of the following phones industry uses Android OS?\\n (a)iphone \\n (b)Samsung \\n (c)None\\n\\n\"), \"b\"),\n",
    "          phone_industry((\"3. Which of the following phones industry uses Android OS?\\n (a)iphone \\n (b)Samsung\\n (c)None\\n\\n\"), \"b\")]\n",
    "\n",
    "def test(questions):\n",
    "    score=0\n",
    "    for i in range(len(questions)):\n",
    "        print(questions[i].prompt_question)\n",
    "        try:\n",
    "            ans=str(input(\"Enter your answer here\\n\"))\n",
    "        except ValueError as err:\n",
    "            print(err)\n",
    "            if questions[i].answer == ans: \n",
    "                score=score+1\n",
    "    print(\"You got \", score , \"correctly\")\n",
    "\n",
    "test(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "c=list()\n",
    "\n",
    "N=input(\"Enter an integer here: \")\n",
    "while N.isnumeric()==False:\n",
    "    print(\"\\nInvalid input\")\n",
    "    N=input(\"Re-enter an integer here: \")\n",
    "for i in range(int(N)):\n",
    "    N1=str(input(\"Enter the message here: \"))\n",
    "    for q in range((len(N1)-3)):\n",
    "        if N1[q]==N1[q+1]==N1[q+2]:\n",
    "            c.extend(N1[q]*(N1.count(N1[q])))\n",
    "            if (len(c)>=6 and len(c)<=10):\n",
    "                print(N1[(len(N1)-3):len(N1)] + \" Possible\")\n",
    "            elif (len(c)>=11 and len(c)<=40):\n",
    "                print(N1[(len(N1)-3):len(N1)] + \" Probable\")\n",
    "            elif (len(c)>=41 and len(c)<=150):\n",
    "                print(N1[(len(N1)-3):len(N1)] + \" Escalate\")\n",
    "            elif (len(c)>150):\n",
    "                print(N1[(len(N1)-3):len(N1)] + \" Ignore\")\n",
    "        \n",
    "    \n",
    "    \n",
    "#xxxayyySPY\n",
    "#xxxxxxbzzzzzzIVV\n",
    "#xxAyyDJI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 12, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "Input = [0,1,0,3,12]\n",
    "#Output: [1,3,12,0,0]\n",
    "for i in range(len(Input)):\n",
    "    if Input[i]==0:\n",
    "        Input.remove(Input[i])\n",
    "        Input.append(0)\n",
    "print(Input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 7, '2': 1, '3': 5, '4': 3, '5': 6, '6': 4}\n",
      "6\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-170-b0d656fa53fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m#else:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mmaxProfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-170-b0d656fa53fe>\u001b[0m in \u001b[0;36mmaxProfit\u001b[1;34m(k)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m#print(t2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mt2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m<\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mt1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mt1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mt2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m#else:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "def maxProfit(k):\n",
    "    day=list()\n",
    "    if k[-1]==min(k):\n",
    "        return 0\n",
    "    else:\n",
    "        for i in range(1,(len(k)+1)):\n",
    "            day.append(str(i))\n",
    "            transaction = {day1:k1 for day1,k1 in zip(day, k)}\n",
    "    print(transaction)\n",
    "    t1=max(\n",
    "    print(t1)\n",
    "    t2=min(transaction.items())\n",
    "    #print(t2)\n",
    "    if t2[0]<t1[0]:\n",
    "        if t1[1]==k[-1]:\n",
    "            return t1[1]-t2[1]\n",
    "        #else: \n",
    "    \n",
    "maxProfit([7,1,5,3,6,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "arr = [1,2,3]\n",
    "#arr = [1,1,3,3,5,5,7,7]\n",
    "#arr = [1,3,2,3,5,0]\n",
    "#arr = [1,1,2,2]\n",
    "arr.sort()\n",
    "print(arr)\n",
    "count=0\n",
    "for i in range(len(arr)):\n",
    "    if (arr[i]+1) in arr:\n",
    "        count=count+1\n",
    "print(count)\n",
    "    #elif (arr[i]==arr[i+2]):\n",
    "        #b=arr.count(arr[i])\n",
    "    #if (arr[i]==arr[i+3]):\n",
    "        #b=arr.count(arr[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def maxSubArray(nums):\n",
    "        if len(nums)==1:\n",
    "            return sum(nums)\n",
    "        elif len(nums)==2:\n",
    "            if sum(nums)>max(nums):\n",
    "                return sum(nums)\n",
    "            else:\n",
    "                return max(nums)\n",
    "        c3=[]\n",
    "        for b in range(len(nums)):\n",
    "            c=nums[b]\n",
    "            for c1 in nums:\n",
    "                if c!=c1:\n",
    "                    c2=c+c1\n",
    "                    c3.append(c2)\n",
    "                else:\n",
    "                    c3.append(c)\n",
    "        return max(c3)\n",
    "maxSubArray([1,-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c=list()\n",
    "def maxsubarray(arr):\n",
    "    c=list(); n=0; c1=list()\n",
    "    if len(arr)==1:\n",
    "        return sum(arr)\n",
    "    elif len(arr)==2:\n",
    "        if sum(arr)>max(arr):\n",
    "            return sum(arr)\n",
    "        else:\n",
    "            return max(arr)\n",
    "    #elif len(arr)==3:\n",
    "        #print(max(arr))\n",
    "       # if sum(arr[0:2])>sum(arr[1:3])>max(arr):\n",
    "           # return sum(arr[0:2])\n",
    "        #elif max(arr)>sum(arr[0:2]) or max(arr)>sum(arr[1:3]):\n",
    "            #return max(arr)\n",
    "        #elif sum(arr[1:3])>max(arr)>sum(arr[0:2]):\n",
    "            #return sum(arr[1:3])\n",
    "    else:\n",
    "        while n!=len(arr):\n",
    "            n=n+1\n",
    "            for i in range(len(arr)-1):\n",
    "            #for n in range(1,len(arr)):\n",
    "                t= arr[i:(i+1)+n]\n",
    "                t1=arr[i]\n",
    "                #print(t)\n",
    "                c.append(sum(t))\n",
    "                c1.append(t1)\n",
    "                #print(t1)\n",
    "    \n",
    "        return max(c) if max(c)>max(c1) else max(c1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k\n",
      "2\n",
      "5\n",
      "1\n",
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def maxSubArray(a):\n",
    "        max_so_far =a[0] #1,2,3\n",
    "        curr_max = a[0] \n",
    "      \n",
    "        for i in range(1,len(a)): \n",
    "            curr_max = max(a[i], curr_max + a[i])\n",
    "            print(curr_max)\n",
    "            max_so_far = max(max_so_far,curr_max)\n",
    "            print(max_so_far)\n",
    "          \n",
    "        return max_so_far\n",
    "\n",
    "maxSubArray([5,-3,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#S =\"a##c\"\n",
    "#T= \"#a#c\"\n",
    "class Solution:\n",
    "    def backspaceCompare(S: str, T: str):\n",
    "        c1=list(); c2=list(); v=list(); v2=list()\n",
    "        c1.extend(S)\n",
    "        c2.extend(T)\n",
    "\n",
    "        for i in range(len(c1)):\n",
    "            if c1[i]!=\"#\":\n",
    "                v.append(c1[i])\n",
    "            elif c1[i]==\"#\":\n",
    "                if len(v)==0:\n",
    "                    v.append(c1[i])\n",
    "                else:\n",
    "                    v.pop()\n",
    "\n",
    "        for ii in range(len(c2)):\n",
    "            if c2[ii]!=\"#\":\n",
    "                v2.append(c2[ii])\n",
    "            elif c2[ii]==\"#\":\n",
    "                if len(v2)==0:\n",
    "                    v2.append(c2[ii])\n",
    "                else:\n",
    "                    v2.pop()\n",
    "    print(v==v2)\n",
    "    \n",
    "    backspaceCompare(\"y#fo##f\", \"y#f#o##f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "1 2\n",
      "2 3\n",
      "<generator object <genexpr> at 0x000001A2EA236728>\n"
     ]
    }
   ],
   "source": [
    "a=[1,2,3]\n",
    "for i,c in enumerate(a):\n",
    "    print(i,c)\n",
    "y=((v,v1) for v in a for v1 in range(a))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4)]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'itertools' has no attribute 'permutation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-229-de8494bb77e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#tuple(mr.chunked(t, 1))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#print(sum(t[2]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'itertools' has no attribute 'permutation'"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "str_list = [1,2,3,4]\n",
    "#this works for iterating each element across allother elements\n",
    "t=list(itertools.combinations(str_list,2))\n",
    "print(t)\n",
    "#list(mr.chunked(t,1))\n",
    "#print(sum(t))\n",
    "#list(mr.flatten(t))\n",
    "#use chunk to remove the tuple\n",
    "#tuple(mr.chunked(t, 1))\n",
    "#print(sum(t[2]))\n",
    "list(itertools.permutation(str_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 2, 4, 8, 1)"
      ]
     },
     "execution_count": 788,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i= input()\n",
    "tuple(map(len, ['1', 'de', 'fghi', 'ababbaba',\"a\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 5, 7, 9, 11]"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(sum, zip([1,2,3,4,5],[2,3,4,5,6])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 5), (7, 8), (9, 3), (2, 3), (5,), (), (), ()]\n"
     ]
    }
   ],
   "source": [
    "def summ(arr, n):\n",
    "    m2=[tuple(num1[i*n:(i+1)*n]) for i in range(len(num1)-1)]\n",
    "    #c=[sum(t[i]) for i in range(len(num))]\n",
    "    #print(sum(sum(m2[i+1])))\n",
    "    print(m2)\n",
    "num1=[2,5,7,8,9,3,2,3,5]\n",
    "\n",
    "summ(num1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2, 3), (4, 0, 5)]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#iterating along the elements\n",
    "def better_grouper(inputs, n):\n",
    "    iters = [iter(inputs)]*3\n",
    "    return list(zip(*iters))\n",
    "better_grouper([1,2,3,4,0,5,3],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=(1,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tuple_iterator object at 0x000001A2EA223128>, <tuple_iterator object at 0x000001A2EA223128>, <tuple_iterator object at 0x000001A2EA223128>]\n"
     ]
    }
   ],
   "source": [
    "x1=[iter(x)]*3\n",
    "print(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)]"
      ]
     },
     "execution_count": 732,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(itertools.combinations_with_replacement(list(x),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alternating_ones = itertools.cycle([1, -1])\n",
    "list(alternating_ones)\n",
    "# 1, -1, 1, -1, 1, -1, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxProfit(price): \n",
    "  \n",
    "    # If the stocks can't be bought \n",
    "    if len(price)==0: \n",
    "        return 0\n",
    "    profit = 0 \n",
    "    \n",
    "    for i in range(len(price)-1):\n",
    "                \n",
    "        if (price[i+1] > price[i]):\n",
    "            t1 =( price[i+1] - price[i]) \n",
    "            #print(t1)\n",
    "    for ii in range(0, i-1):\n",
    "        #if (price[i+1] > price[i]):\n",
    "        t2= maxProfit(price)\n",
    "        print(t2)\n",
    "    for iii in range(i+2, len(price)-1):\n",
    "        if (price[i+1] > price[i]):\n",
    "            t3=maxProfit(price) \n",
    "    curr_profit=t1+t2+t3\n",
    "    profit = max(profit, curr_profit); \n",
    "  \n",
    "    return profit; \n",
    "    \n",
    "# Driver code \n",
    "if __name__ == '__main__': \n",
    "    price = [1,4,5,8,9]\n",
    "    n = len(price); \n",
    "  \n",
    "    print(maxProfit(price)); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'112384': ['129'], '104257': ['193', '102']}\n"
     ]
    }
   ],
   "source": [
    "dic=dict()\n",
    "neigbour= {(\"104257\",\"193\"),(\"104257\",\"102\"),(\"112384\",\"129\")}\n",
    "\n",
    "for i in neigbour:\n",
    "    if i[0] not in dic.keys():\n",
    "        dic[i[0]]=[i[1]]\n",
    "    else:\n",
    "        dic[i[0]].append(i[1])\n",
    "        \n",
    "print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['eat', 'tea', 'ate'], ['tan', 'nat'], ['bat']]\n"
     ]
    }
   ],
   "source": [
    "tab=[\"eat\", \"tea\", \"tan\", \"ate\", \"nat\",\"bat\"]\n",
    "u=list(); u1=list(); m=list(); j=list()\n",
    "for i in range(len(tab)+1):\n",
    "    for word in tab[i+1:]:\n",
    "        u.extend(tab[i]); u.sort()\n",
    "        u1.extend(word); u1.sort()\n",
    "        j= \"\".join(u)\n",
    "        #print(j)\n",
    "        if u==u1:\n",
    "            tab.remove(word)\n",
    "            u.clear()\n",
    "            u1.clear()\n",
    "            if len(m)!=0 and (tab[i] in m[0]):\n",
    "                if (tab[i] and word in m[0]):\n",
    "                    m[0]=m[0]\n",
    "                else:\n",
    "                    m[0].append(word)\n",
    "            else:\n",
    "                m.append([tab[i], word])\n",
    "        else:\n",
    "            u.clear()\n",
    "            u1.clear()\n",
    "m.append([tab[-1]])           \n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['eat', 'tea', 'ate'], ['tan', 'nat'], ['bat']]\n"
     ]
    }
   ],
   "source": [
    "class Solution:\n",
    "    def groupAnagrams(strs):\n",
    "        result = {}\n",
    "        for i in strs:\n",
    "            #print(sorted(i))\n",
    "\n",
    "            x = \"\".join(sorted(i))\n",
    "            #print(sorted(i))\n",
    "            if x in result:\n",
    "                result[x].append(i)\n",
    "            else:\n",
    "                result[x] = [i]\n",
    "        return list(result.values())\n",
    "    strs=[\"eat\", \"tea\", \"tan\", \"ate\", \"nat\",\"bat\"]\n",
    "    print(groupAnagrams(strs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "#t=[7,1,5,3,6,4]\n",
    "#t=[1,2,3,4,5]\n",
    "t=[7,8]\n",
    "a=t[1]-t[0]\n",
    "for i in range(1,len(t)-1):\n",
    "    t1=t[i+1]-t[i]\n",
    "    a=max(a, a+t1, t1)\n",
    "if a<0:\n",
    "    print(0)\n",
    "else:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "b=[[1,3,1],\n",
    "   [1,5,1],\n",
    "   [4,2,1]]\n",
    "a=0\n",
    "print(b[0][0])\n",
    "#for i in range(len(b)):\n",
    "    #print(sum(b[0:][0]))\n",
    "    #print(sum(b[2]))\n",
    "    #print(b[0][2]); print(b[1][2])\n",
    "    #a=min(b1, b1) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "class Solution:\n",
    "    def minPathSum(grid):\n",
    "        colLen = len(grid)\n",
    "        column = [-1] * colLen\n",
    "        column[0] = grid[0][0]\n",
    "        \n",
    "        for index in range(1, colLen):\n",
    "            column[index] = grid[index][0] + column[index-1]\n",
    "            \n",
    "        for colIndex in range(1, len(grid[0])):\n",
    "            column[0] += grid[0][colIndex]\n",
    "            \n",
    "            for rowIndex in range(1, colLen):\n",
    "                column[rowIndex] = min(column[rowIndex], column[rowIndex-1]) + grid[rowIndex][colIndex]\n",
    "        return column[-1]\n",
    "    grid=[[1,3,1],\n",
    "     [1,5,1],\n",
    "     [4,2,1]]\n",
    "    print(minPathSum(grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def serh(g, target):\n",
    "    if target in g:\n",
    "        return g.index(target)\n",
    "    else:\n",
    "        return -1\n",
    "g=[4,5,6,7,0,1,2]\n",
    "serh(g, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('c', 2), ('b', 2), ('t', 1), ('a', 1)]\n",
      "c 2\n",
      "b 2\n",
      "t 1\n"
     ]
    }
   ],
   "source": [
    "t= \"abbtcc\"\n",
    "v= sorted(t); m=list(); m2=dict()\n",
    "\n",
    "for i in range(len(v)):\n",
    "            m.append(v.count(v[i]))\n",
    "            m1={x:x1 for x,x1 in zip(v, m)}\n",
    "            m2={k: k1 for k, k1 in sorted(m1.items(), key=lambda item: item[1])}\n",
    "            t1=list(m2.items())\n",
    "            \n",
    "\n",
    "#m1={k: k1 for k, k1 in sorted(m1.items(), key=lambda item: item[1])}\n",
    "#t1=list(m1.items())\n",
    "\n",
    "t1.reverse()\n",
    "print(t1)\n",
    "\n",
    "\n",
    "\n",
    "for ii in range(3):\n",
    "    print(t1[ii][0], t1[ii][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "{'a': 419, 'b': 380, 'c': 396, 'd': 396, 'e': 367, 'f': 394, 'g': 375, 'h': 399, 'i': 382, 'j': 411, 'k': 370, 'l': 407, 'm': 383, 'n': 377, 'o': 374, 'p': 331, 'q': 355, 'r': 371, 's': 372, 't': 396, 'u': 383, 'v': 379, 'w': 416, 'x': 366, 'y': 415, 'z': 386}\n",
      "{'a': 419, 'b': 380, 'c': 396, 'd': 396, 'e': 367, 'f': 394, 'g': 375, 'h': 399, 'i': 382, 'j': 411, 'k': 370, 'l': 407, 'm': 383, 'n': 377, 'o': 374, 'p': 331, 'q': 355, 'r': 371, 's': 372, 't': 396, 'u': 383, 'v': 379, 'w': 416, 'x': 366, 'y': 415, 'z': 386}\n",
      "a 419\n",
      "b 380\n",
      "c 396\n"
     ]
    }
   ],
   "source": [
    "t= \"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\"\n",
    "v=sorted(t); m=list(); m1=dict()\n",
    "\n",
    "print(len(v))\n",
    "\n",
    "for i in range(len(v)):\n",
    "            m.append(v.count(v[i]))\n",
    "            m1={x:x1 for x,x1 in zip(v, m)}\n",
    "print(m1)\n",
    "\n",
    "for am, pm in list(m1.items()):\n",
    "    if max(m1.values())==pm:\n",
    "        if am==min(m1.keys()):\n",
    "            am     \n",
    "    else:\n",
    "        m1.pop(am,pm)\n",
    "        m1[am]= pm\n",
    "        \n",
    "\n",
    "t1=list(m1.items())\n",
    "print(m1)\n",
    "\n",
    "for ii in range(3):\n",
    "    print(t1[ii][0], t1[ii][1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n",
      "{'a': 6, 'b': 3, 'c': 4, 'd': 2, 'e': 5, 'f': 3, 'g': 4, 'h': 1, 'k': 5, 'l': 6, 'm': 4, 'n': 5, 'o': 4, 'p': 4, 'q': 4, 'r': 6, 's': 5, 't': 1, 'u': 3, 'v': 4, 'w': 3, 'x': 3, 'y': 3, 'z': 3}\n",
      "{'a': 6, 'l': 6, 'r': 6, 'b': 3, 'c': 4, 'd': 2, 'e': 5, 'f': 3, 'g': 4, 'h': 1, 'k': 5, 'm': 4, 'n': 5, 'o': 4, 'p': 4, 'q': 4, 's': 5, 't': 1, 'u': 3, 'v': 4, 'w': 3, 'x': 3, 'y': 3, 'z': 3}\n",
      "a 6\n",
      "l 6\n",
      "r 6\n"
     ]
    }
   ],
   "source": [
    "t=\"kwuzazrxreqpypvzdsnvtumobemekfskfshopevggqgoascrunynwaxolxcygnkllnqslcqrlmlbvrarepmfkdbacaw\"\n",
    "v=sorted(t); m=list(); m1=dict()\n",
    "\n",
    "print(len(v))\n",
    "\n",
    "for i in range(len(v)):\n",
    "            m.append(v.count(v[i]))\n",
    "            m1={x:x1 for x,x1 in zip(v, m)}\n",
    "print(m1)\n",
    "\n",
    "for am, pm in list(m1.items()):\n",
    "    if max(m1.values())==pm:\n",
    "        if am==min(m1.keys()):\n",
    "            m1=m1     \n",
    "    else:\n",
    "        m1.pop(am,pm)\n",
    "        m1[am]= pm\n",
    "        \n",
    "\n",
    "t1=list(m1.items())\n",
    "print(m1)\n",
    "\n",
    "for ii in range(3):\n",
    "    print(t1[ii][0], t1[ii][1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asi\n",
      "invalid email\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "b=input()\n",
    "b1=\"[a-zA-Z0-9]+@[a-zA-Z]+\\.(com|ng)\"\n",
    "if re.findall(b1, b):\n",
    "    print(\"Correct email\")\n",
    "else:\n",
    "    print(\"invalid email\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3\n",
      "['1', '2', '3']\n"
     ]
    }
   ],
   "source": [
    "bb=input()\n",
    "Bb=('\\d{1,2}')\n",
    "cc=re.findall(Bb, bb)\n",
    "print(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 6\n",
      "l 6\n",
      "r 6\n"
     ]
    }
   ],
   "source": [
    "from collection import Counter\n",
    "\n",
    "for letter, counts in sorted(Counter(m1).most_common(3),key = lambda x:(-x[1],x[0]))[:3]:\n",
    "    print (letter, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 670,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x19'"
      ]
     },
     "execution_count": 677,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "a 1\n",
      "b 0\n",
      "c 0\n"
     ]
    }
   ],
   "source": [
    "S = input()\n",
    "letters = [0]*26\n",
    "\n",
    "for letter in S:\n",
    "    #print(letter)\n",
    "    letters[ord(letter)-ord('a')] += 1\n",
    "    #print(letters)\n",
    "\n",
    "for _ in range(3):\n",
    "    \n",
    "    max_letter = max(letters)\n",
    "    \n",
    "    for index in range(26):\n",
    "        if max_letter == letters[index]:\n",
    "            print (chr(ord('a')+index),max_letter)\n",
    "            letters[index] = -1\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google\n",
      "6\n",
      "g 2\n",
      "o 2\n",
      "e 1\n"
     ]
    }
   ],
   "source": [
    "t=input(); alpha=list(); num=list()\n",
    "v=sorted(t); m=list(); m1=dict()\n",
    "\n",
    "print(len(v))\n",
    "\n",
    "for i in range(len(v)):\n",
    "            m.append(v.count(v[i]))\n",
    "            m1={x:x1 for x,x1 in zip(v, m)}\n",
    "            t1=list(m1.items())\n",
    "\n",
    "for y,yy in t1:\n",
    "    alpha.append(y)\n",
    "    num.append(yy)\n",
    "\n",
    "for ii in range(3):\n",
    "    for iii in range(len(num)):\n",
    "        max_num=max(num)\n",
    "        if num[iii]==max_num:\n",
    "            cc=alpha[iii]\n",
    "            print(cc,max_num)\n",
    "            num[iii]=num[iii]-num[iii]\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1222311\n",
      "(1, 1) (3, 2) (1, 3) (2, 1) "
     ]
    }
   ],
   "source": [
    "from itertools import groupby\n",
    "S=input(); v=list()\n",
    "t=[list(g) for k, g in groupby(S)]\n",
    "for i in t:\n",
    "    print((len(i), int(i[0])), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3430019387558"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=list(); \n",
    "t=int(input()); n=input().split()\n",
    "for ii in n:\n",
    "    if len(n)==t:\n",
    "        b.append(int(ii))\n",
    "    \n",
    "    else:\n",
    "        b==0\n",
    "        hash(tuple(b))==0\n",
    "hash(tuple(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<filter at 0x259af2b9a90>"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "had=[1,2,3,4,5]\n",
    "filter(2,had)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "[[0, 0, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0], [1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "#Use of List comprehension\n",
    "x=int(input()); x1=int(input()); x2=int(input()); xx=int(input())\n",
    "print([[i,j,z] for i in range(1+x) for j in range(1+x1) for z in range(1+x2) if ((i+j+z)!=xx)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "[[0, 0, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0], [1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "x=int(input())\n",
    "x1=int(input())\n",
    "x2=int(input())\n",
    "xx=int(input())\n",
    "p=[]\n",
    "for i in range(1+x):\n",
    "    for j in range(1+x1):\n",
    "        for z in range(1+x2):\n",
    "            if i+j+z!=xx:\n",
    "                p.append([i,j,z])\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "u=[i**2 for i in range(10) if i%2!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=int(input()\n",
    "\n",
    "x=input().split()\n",
    "      \n",
    "if ((len(r1)==r) and (1<r<11)):\n",
    "      h=[i for i in t if i==max(t)]\n",
    "      u=[ii for ii in t if ii not in h]\n",
    "      print(max(u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "2 3 6 6 5\n",
      "[2, 3, 5]\n"
     ]
    }
   ],
   "source": [
    "r = int(input())\n",
    "r1 = map(int, input().split())\n",
    "r1=list(r1)\n",
    "      \n",
    "if (1<r<11):\n",
    "    h=[i for i in r1 if i==max(r1)]\n",
    "    u=[ii for ii in r1 if ii not in h]\n",
    "    print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "terry\n",
      "37.21\n",
      "berry\n",
      "37.21\n",
      "anki\n",
      "37.2\n",
      "tank\n",
      "39\n",
      "harsh\n",
      "40\n",
      "berry\n",
      "terry\n"
     ]
    }
   ],
   "source": [
    "c=list(); m1=list()\n",
    "\n",
    "for i in range(int(input())):\n",
    "    name = input()\n",
    "    score=float(input())\n",
    "    c.append([score, name])\n",
    "\n",
    "for pp in range(len(c)):\n",
    "    min_score=min(c)\n",
    "    if c[pp][0]==min_score[0]:\n",
    "        m1.append(c[pp])\n",
    "        \n",
    "h=[c[u] for u in range(len(c)) if c[u] not in m1]\n",
    "mim_score=min(h)\n",
    "yy=[xc for xc in h if mim_score[0] in xc]\n",
    "\n",
    "yy.sort()\n",
    "for ij in yy:\n",
    "    print(ij[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Krishna 67 68 69\n",
      "Arjun 70 98 63\n",
      "Malika 52 56 60\n",
      "Malika\n",
      "56.00\n"
     ]
    }
   ],
   "source": [
    "student_marks = {}\n",
    "    \n",
    "for _ in range(int(input())):\n",
    "    name, *line = input().split()\n",
    "    scores = list(map(float, line))\n",
    "    student_marks[name] = scores\n",
    "\n",
    "query_name = input()\n",
    "if query_name in student_marks.keys():\n",
    "               x=sum(student_marks[query_name])/len(student_marks[query_name])\n",
    "               print(\"{:.2f}\".format(x)) #To 2 decimal places\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lara@hackerrank.com\n",
      "['lara@hackerrank.com']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "c=list()\n",
    "pattern=\"[a-zA-Z0-9]+@[a-zA-Z0-9]+\\.(com|ng|org)\"\n",
    "for i in range(int(input())):\n",
    "    email=input()\n",
    "    if re.search(pattern, email):\n",
    "        c.append(email)\n",
    "c.sort()\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "It must have the username@websitename.extension format type.\n",
    "The username can only contain letters, digits, dashes and underscores.\n",
    "The website name can only have letters and digits.\n",
    "The maximum length of the extension is 3\n",
    "\n",
    "l = list(range(10))\n",
    "l = list(map(lambda x:x*x, l))\n",
    "\n",
    "l = list(filter(lambda x: x > 10 and x < 80, l))\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "asimiakin@gmail.com\n",
      "asim\n",
      "['asimiakin@gmail.com']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def fun(emails):\n",
    "    pattern=\"[a-zA-Z0-9]+@[a-zA-Z0-9]+\\.(com|ng|org)\"\n",
    "    #a = re.findall('[a-zA-Z0-9_-]+@[a-zA-Z0-9]+\\.[a-zA-Z]{1,3}$',s)\n",
    "    if re.findall(pattern,emails):\n",
    "        return True\n",
    "    # return True if s is a valid email, else return False\n",
    "def filter_mail(emails):\n",
    "    return list(filter(fun, emails))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    n = int(input())\n",
    "    emails = []\n",
    "    for _ in range(n):\n",
    "        #a,b=input().split\n",
    "        emails.append(input())\n",
    "\n",
    "filtered_emails = filter_mail(emails)\n",
    "filtered_emails.sort()\n",
    "print(filtered_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_-\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "v=\"asimiakin_1!1-\"\n",
    "allowed_chars=string.ascii_letters + string.digits + '_-'\n",
    "print(allowed_chars)\n",
    "i=[ii for ii in v if ii not in allowed_chars]\n",
    "print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "harsh@gmail\n",
      "iota_98@hackerrank.com\n",
      "['iota_98@hackerrank.com']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "allowed_chars=string.ascii_letters + string.digits + '_-'\n",
    "def fun(emails):\n",
    "    if \"@\" in emails:\n",
    "        x,y=emails.split(\"@\")\n",
    "        p=[ii for ii in x if ii not in allowed_chars]\n",
    "        if len(p)==0:\n",
    "            if \".\" in y:\n",
    "                i,j=y.split(\".\")\n",
    "                if i.isalnum() and len(j)==3:\n",
    "                    return True\n",
    "    # return True if s is a valid email, else return False\n",
    "def filter_mail(emails):\n",
    "    return list(filter(fun, emails))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    n = int(input())\n",
    "    emails = []\n",
    "    for _ in range(n):\n",
    "        emails.append(input())\n",
    "\n",
    "filtered_emails = filter_mail(emails)\n",
    "filtered_emails.sort()\n",
    "print(filtered_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "asimi_ola-11@hotmail.com\n",
      "['asimi_ola-11@hotmail.com']\n"
     ]
    }
   ],
   "source": [
    "def fun(emails):\n",
    "    mail=list()\n",
    "    if \"@\" in emails:\n",
    "        x,y=emails.split(\"@\")\n",
    "        if (x.isidentifier() or (\"-\") in x) or (x.isidentifier() and \"-\" in x):\n",
    "            if \".\" in y:\n",
    "                i,j=y.split(\".\")\n",
    "                if i.isalnum() and len(j)==3:\n",
    "                    return True\n",
    "    # return True if s is a valid email, else return False\n",
    "def filter_mail(emails):\n",
    "    return list(filter(fun, emails))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    n = int(input())\n",
    "    emails = []\n",
    "    for _ in range(n):\n",
    "        emails.append(input())\n",
    "\n",
    "filtered_emails = filter_mail(emails)\n",
    "filtered_emails.sort()\n",
    "print(filtered_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fun(s):       \n",
    "    if re.search(r'[a-zA-Z0-9_-]+@[a-zA-Z0-9]+\\.[a-zA-Z]{1,3}$',s):\n",
    "        return True\n",
    "fun(\"asi--miakin-_@gmail.ng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0 3 4 \n",
      "\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 0, 3, 4, 7]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Singly Linked List\n",
    "\n",
    "class node:\n",
    "    def __init__(self, data=None):\n",
    "        self.data=data\n",
    "        self.next=None\n",
    "class Linked_List:\n",
    "    def __init__(self):\n",
    "        self.head=node()\n",
    "    \n",
    "    def append(self, data):\n",
    "        new_node=node(data)\n",
    "        curr=self.head\n",
    "        while curr.next!=None:\n",
    "            curr=curr.next\n",
    "        curr.next=new_node\n",
    "        print(curr.next.data, end=\" \")\n",
    "        \n",
    "    def length(self):\n",
    "        curr=self.head\n",
    "        count=0\n",
    "        while curr.next!=None:\n",
    "            count+=1\n",
    "            curr=curr.next\n",
    "        return count\n",
    "    \n",
    "    def NodeElement(self):\n",
    "        elem=[]\n",
    "        curr=self.head\n",
    "        while curr.next!=None:\n",
    "            curr=curr.next\n",
    "            elem.append(curr.data)\n",
    "        return elem\n",
    "        #print(len(elem)//2)\n",
    "        #print(elem[(len(elem)//2)])\n",
    "        #print(elem)\n",
    "\n",
    "    def eNode(self, index):\n",
    "        if index>self.length():\n",
    "            print(\"Man\")\n",
    "        curr=self.head\n",
    "        node_index=0\n",
    "        while curr.next!=None:\n",
    "            node_index+=1\n",
    "            curr=curr.next\n",
    "            if node_index==index:\n",
    "                print(\"\\n\")\n",
    "                print(curr.data)\n",
    "    \n",
    "    def Insert(self,previous, data):\n",
    "        new_node=node(data)\n",
    "        curr=self.head\n",
    "        if previous not in self.NodeElement():\n",
    "            print(\"not in the list\")\n",
    "            return\n",
    "        while curr.next!=None:\n",
    "            curr=curr.next\n",
    "            if curr.data==previous:\n",
    "                new_node.next=curr.next\n",
    "                curr.next=new_node\n",
    "                    \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "#my_list=node()\n",
    "my_list=Linked_List()\n",
    "my_list.append(1)\n",
    "my_list.append(0)\n",
    "my_list.append(3)\n",
    "my_list.append(4)\n",
    "my_list.length()\n",
    "\n",
    "#my_list.NodeElement()\n",
    "\n",
    "my_list.eNode(2)\n",
    "\n",
    "my_list.Insert(4,7)\n",
    "\n",
    "my_list.NodeElement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition for singly-linked list.\n",
    "class ListNode:\n",
    "    def __init__(self, val=0):\n",
    "        self.val = val\n",
    "        self.next = next\n",
    "class Solution:\n",
    "    def middleNode(self, head):\n",
    "        count=0\n",
    "        curr=head\n",
    "        while curr!=None:\n",
    "            curr=curr.next\n",
    "            count+=1\n",
    "        m=count//2\n",
    "        i=0\n",
    "        cur=head\n",
    "        while i!=m:\n",
    "            cur=cur.next\n",
    "            i+=1\n",
    "        return cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MinStack:\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        initialize your data structure here.\n",
    "        \"\"\"\n",
    "        self.items=[[]]\n",
    "        self.out=[]\n",
    "        \n",
    "\n",
    "    def push(self, x: int):\n",
    "        if len(self.out)==0:\n",
    "            self.out.append(x)\n",
    "        else:\n",
    "            for i in self.out:\n",
    "                if \n",
    "        self.items.append([x])\n",
    "     \n",
    "    def get_stack(self):\n",
    "        #print(self.items)\n",
    "        return self.items\n",
    "    \n",
    "   \n",
    "    def Pop(self) -> None:\n",
    "        self.items.pop()\n",
    "    \n",
    "    def top(self) -> int:\n",
    "        if len(self.items)==0:\n",
    "            return self.items\n",
    "        return self.items[0]\n",
    "\n",
    "    def getMin(self):\n",
    "        if len(self.items)==0:\n",
    "            return self.items\n",
    "        return min(self.get_stack())\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "# Your MinStack object will be instantiated and called as such:\n",
    "obj = MinStack()\n",
    "obj.push(-2)\n",
    "obj.push(0)\n",
    "obj.push(-3)\n",
    "\n",
    "\n",
    "#obj.get_stack()\n",
    "\n",
    "#obj.pop()\n",
    "param_3 = obj.top()\n",
    "param_4 = obj.getMin()\n",
    "print(param_3)\n",
    "obj.Pop()\n",
    "param_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TreeNode:\n",
    "    def __init__(self, val=0, left=None, right=None):\n",
    "        self.val = val\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "class Solution:\n",
    "    def diameterOfBinaryTree(self, root: TreeNode) -> int:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "9587456281\n",
      "YES\n"
     ]
    }
   ],
   "source": [
    "#Validating correctness of Phone Number HackerRank\n",
    "import re\n",
    "patern=\"[7-9]\"       #[789]+[0-9]{0,9}$ or [7-9]+\\d{9}\n",
    "path=\"[0-9]{0,10}$\"\n",
    "for i in range(int(input())):\n",
    "    x=input()\n",
    "    if len(x)==10 and re.match(path, x):\n",
    "        if re.findall(patern, x[0]):\n",
    "            print(\"YES\")\n",
    "        else:\n",
    "            print(\"NO\")\n",
    "    else:\n",
    "        print(\"NO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "this <is@valid.com>\n",
      "this <is@valid.com>\n"
     ]
    }
   ],
   "source": [
    "#Email verification HackerRank\n",
    "\n",
    "import re\n",
    "\n",
    "pattern=\"\\<[a-zA-Z]+[a-zA-Z0-9-._]+@[a-zA-Z]+\\.[a-zA-Z]{1,3}\\>\"\n",
    "\n",
    "for i in range(int(input())):\n",
    "    m,n=input().split(\" \")\n",
    "    \n",
    "    if re.match(pattern,n):\n",
    "        print(m,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "#FfFdF8\n",
      "#FfFdF8\n"
     ]
    }
   ],
   "source": [
    "#checking for CSS HEX color code HackerRank\n",
    "\n",
    "import re\n",
    "\n",
    "pat=\"\\#+([a-fA-F0-9]{6}|[a-fA-F0-9]{3})\"\n",
    "\n",
    "for i in range(int(input())):\n",
    "    n=input()\n",
    "    if not len(n) <=6:\n",
    "        y=re.findall(pat, n)\n",
    "        for ii in y:\n",
    "            print(\"#\"+ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1 2\n",
      "2 3\n",
      "1 3\n"
     ]
    }
   ],
   "source": [
    "from fractions import Fraction\n",
    "from functools import reduce\n",
    "\n",
    "def product(fracs):\n",
    "    t = reduce(lambda x,y: x*y , fracs)\n",
    "    return t.numerator, t.denominator\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    fracs = []\n",
    "    for _ in range(int(input())):\n",
    "        fracs.append(Fraction(*map(int, input().split())))  #Mind the use of fraction and reduce function here\n",
    "    result = product(fracs)\n",
    "    print(*result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "List: a a a a d e u o i\n",
      "3\n",
      "[(1, 2, 3), (1, 2, 4), (1, 2, 5), (1, 2, 6), (1, 2, 7), (1, 2, 8), (1, 2, 9), (1, 3, 4), (1, 3, 5), (1, 3, 6), (1, 3, 7), (1, 3, 8), (1, 3, 9), (1, 4, 5), (1, 4, 6), (1, 4, 7), (1, 4, 8), (1, 4, 9), (1, 5, 6), (1, 5, 7), (1, 5, 8), (1, 5, 9), (1, 6, 7), (1, 6, 8), (1, 6, 9), (1, 7, 8), (1, 7, 9), (1, 8, 9), (2, 3, 4), (2, 3, 5), (2, 3, 6), (2, 3, 7), (2, 3, 8), (2, 3, 9), (2, 4, 5), (2, 4, 6), (2, 4, 7), (2, 4, 8), (2, 4, 9), (2, 5, 6), (2, 5, 7), (2, 5, 8), (2, 5, 9), (2, 6, 7), (2, 6, 8), (2, 6, 9), (2, 7, 8), (2, 7, 9), (2, 8, 9), (3, 4, 5), (3, 4, 6), (3, 4, 7), (3, 4, 8), (3, 4, 9), (3, 5, 6), (3, 5, 7), (3, 5, 8), (3, 5, 9), (3, 6, 7), (3, 6, 8), (3, 6, 9), (3, 7, 8), (3, 7, 9), (3, 8, 9), (4, 5, 6), (4, 5, 7), (4, 5, 8), (4, 5, 9), (4, 6, 7), (4, 6, 8), (4, 6, 9), (4, 7, 8), (4, 7, 9), (4, 8, 9), (5, 6, 7), (5, 6, 8), (5, 6, 9), (5, 7, 8), (5, 7, 9), (5, 8, 9), (6, 7, 8), (6, 7, 9), (6, 8, 9), (7, 8, 9)]\n",
      "0.7619\n"
     ]
    }
   ],
   "source": [
    "#Iteratable and Iterator for HackerRank using Itertools\n",
    "import itertools\n",
    "\n",
    "N=int(input())\n",
    "\n",
    "def iterable(N, List):\n",
    "    c=int(input())\n",
    "    if N==len(List):\n",
    "        y=[i for i in range(1, len(List)+1)]\n",
    "        x=list(itertools.combinations(y, c))\n",
    "        print(x)\n",
    "\n",
    "    count=0\n",
    "    for ii in x:\n",
    "        for m in range(1, c+1):\n",
    "            if m==ii[0]:\n",
    "                count+=1\n",
    "                \n",
    "    \n",
    "    print(\"{:.4f}\".format(count/len(x)))\n",
    "    \n",
    "iterable(N, input(\"List: \").split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v=[\"a\",\"a\",\"c\"]\n",
    "v.count(v[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mysql.connector.connection_cext.CMySQLConnection object at 0x000001E4133EAE48>\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector as my\n",
    "mydata=my.connect(host=\"localhost\", user=\"root\", passwd=\"Mrasimi123!\", database=\"mydatabase\")\n",
    "print(mydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycurs=mydata.cursor()\n",
    "mycurs.execute(\"CREATE DATABASE mydatabase1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "List: a a b c\n",
      "2\n",
      "[('a', 'a'), ('a', 'b'), ('a', 'c'), ('a', 'b'), ('a', 'c')]\n",
      "0.8333\n",
      "0.833333\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "N=int(input())\n",
    "\n",
    "def iterable(N, List):\n",
    "    c=int(input())\n",
    "    if N==len(List):\n",
    "        #y=[i for i in range(1, len(List)+1)]\n",
    "        x=list(itertools.combinations(List, c))\n",
    "        #print(x)\n",
    "        F = list(filter(lambda k: 'a' in k, x))\n",
    "        print(F)          \n",
    "        print(\"{:.4f}\".format(len(F)/len(x)))\n",
    "        print(\"%f\" %(len(F)/len(x)))\n",
    "iterable(N, input(\"List: \").split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "1 2 2 3\n",
      "Counter({'2': 2, '1': 1, '3': 1})\n",
      "2\n",
      "2 34\n",
      "1 44\n",
      "78\n"
     ]
    }
   ],
   "source": [
    "#Counter funtion challenge HackerRank challenge\n",
    "\n",
    "import collections as c\n",
    "X=list()\n",
    "\n",
    "N = int(input())\n",
    "v=input().split(\" \")\n",
    "if N==len(v):\n",
    "    k=c.Counter(v)\n",
    "    print(k)\n",
    "    k.setdefault(\"n\", 1)\n",
    "    \n",
    "for i in range(int(input())):\n",
    "    a,b=input().split(\" \")\n",
    "    if a in k.keys():\n",
    "        if k[a]!=0:\n",
    "            k[a]=k[a]-1\n",
    "            X.append(int(b))\n",
    "print(sum(X))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 2\n",
      "a\n",
      "a\n",
      "b\n",
      "a\n",
      "b\n",
      "a\n",
      "1 2 4\n",
      "b\n",
      "3 5\n"
     ]
    }
   ],
   "source": [
    "#Defaultdict funtion challenge HackerRank challenge\n",
    "\n",
    "import collections as c\n",
    "\n",
    "x,y = input().split(\" \")\n",
    "d=c.defaultdict(list)\n",
    "\n",
    "for i in range(1, int(x)+1):\n",
    "    d[input()].append(i)\n",
    "#print(d.items())\n",
    "    \n",
    "for ii in range(int(y)):\n",
    "    t=d.setdefault(input(), [-1])\n",
    "    print(\" \".join(map(str, t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ALFRED'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_pets = ['alfred', 'tabitha', 'william', 'arla']\n",
    "my_pets[0].upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello John Doe. Your account balance is 53.44\n"
     ]
    }
   ],
   "source": [
    "data = (\"John\", \"Doe\", 53.44)\n",
    "format_string = \"Hello\"\n",
    "\n",
    "print(f\"{format_string} {data[0]} {data[1]}. Your account balance is {data[-1]}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3\n",
      "defaultdict(<class 'list'>, {})\n",
      "a\n",
      "a\n",
      "dict_items([('a', [1, 2])])\n",
      "6\n",
      "\n",
      "-1\n",
      "-1\n",
      "5\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "import collections as c\n",
    "\n",
    "x,y = input().split(\" \")\n",
    "d=c.defaultdict(list)\n",
    "print(d)\n",
    "for i in range(1, int(x)+1):\n",
    "    d[input()].append(i)\n",
    "print(d.items())\n",
    "    \n",
    "for ii in range(int(y)):\n",
    "    t=input()\n",
    "    if t in d.keys():\n",
    "        for j in d[t]:\n",
    "            k\n",
    "    else:\n",
    "        print(\"-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "Your running time is 0.0009589195251464844\n"
     ]
    }
   ],
   "source": [
    "#Binary Search Algorithms for both Iterative and Recursive\n",
    "\n",
    "start = time.time()\n",
    "#print(start)\n",
    "arr=[2,5,6,1,8,3,8,9]\n",
    "target=100\n",
    "\n",
    "def Binary_Search_iterative(arr, target):\n",
    "    low=0\n",
    "    high=len(arr)\n",
    "   \n",
    "    while low<=high:\n",
    "        mid=(low+high)//2\n",
    "        if target==arr[mid]:\n",
    "            return True\n",
    "        elif target<arr[mid]:\n",
    "            high=mid-1\n",
    "        else:\n",
    "            low=mid+1\n",
    "    return False\n",
    "\n",
    "         \n",
    "def Binary_Search_Recursive(arr, target, low, high):\n",
    "    if low>high:\n",
    "        return False\n",
    "    mid=(low+high)//2\n",
    "    if target == arr[mid]:\n",
    "        return True\n",
    "    elif target<arr[mid]:\n",
    "        return Binary_Search_Recursive(arr, target, low, mid-1)\n",
    "    elif target>arr[mid]:\n",
    "        return Binary_Search_Recursive(arr, target, mid+1, high)\n",
    "\n",
    "print(Binary_Search_Recursive(arr, target, 0, len(arr)-1))\n",
    "print(Binary_Search_iterative([1,3,4,5,30],  30))\n",
    "print(f\"Your running time is {time.time()-(start)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a', 'h', 'b', 'd', 'g', 'f'}\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "A = [\"a\",\"b\",\"d\",\"f\"]\n",
    "B = [\"g\",\"a\",\"d\",\"h\"]\n",
    "print(set(A) | set(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Welcome to Python3!\n",
      "\n",
      "             ____\n",
      "            / . .\\\n",
      "            \\  ---<\n",
      "             \\  /\n",
      "   __________/ /\n",
      "-=:___________/\n",
      "\n",
      "<3, Juno\n",
      "\n"
     ]
    }
   ],
   "source": [
    "how_many_snakes = 1\n",
    "snake_string = \"\"\"\n",
    "Welcome to Python3!\n",
    "\n",
    "             ____\n",
    "            / . .\\\\\n",
    "            \\  ---<\n",
    "             \\  /\n",
    "   __________/ /\n",
    "-=:___________/\n",
    "\n",
    "<3, Juno\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "print(snake_string * how_many_snakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 1000\n",
      "2 3 5\n",
      "5 6 7\n",
      "5 6 7\n",
      "[2, 3, 5, 5, 6, 7, 5, 6, 7]\n",
      "[25, 49, 49]\n",
      "123\n"
     ]
    }
   ],
   "source": [
    "# pytz package for world time zone, pyglet for game dev\n",
    "\n",
    "# Maximize it, HackerRank's challenge (Hard 50 mark)\n",
    "\n",
    "first_line = input().split(\" \")\n",
    "a, b = first_line\n",
    "max_list = list()\n",
    "max_reminder = list()\n",
    "\n",
    "for i in range(int(a)):\n",
    "    lists_input = map(int, input().split(\" \"))\n",
    "    for i in lists_input:\n",
    "        max_list.append(i%(int(b)))\n",
    "    max_reminder.append(max(max_list) ** 2)\n",
    "    \n",
    "print(max_list)   \n",
    "print(max_reminder)\n",
    "\n",
    "maximized_list = sum(max_reminder)\n",
    "print(maximized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 1000\n",
      "2 5 4\n",
      "3 7 8 9\n",
      "5 5 6 7 8 9 10\n",
      "[[5, 4], [7, 8, 9], [5, 6, 7, 8, 9, 10]]\n",
      "206\n"
     ]
    }
   ],
   "source": [
    "first_line = input().split(\" \")\n",
    "a, b = first_line\n",
    "list_n = list()\n",
    "\n",
    "for i in range(int(a)):\n",
    "    list_n.append(list(map(int, input().split(\" \")))[1:])\n",
    "print(list_n)\n",
    "\n",
    "t = max(list(map(lambda x: sum(i**2 for i in x) % int(b), product(*list_n))))\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(list_n)):\n",
    "    for i in list_n[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ABC',)\n"
     ]
    }
   ],
   "source": [
    "def bar(*a):\n",
    "    print(a)\n",
    "bar((\"ABC\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'c', 'e'),\n",
       " ('a', 'c', 'f'),\n",
       " ('a', 'd', 'e'),\n",
       " ('a', 'd', 'f'),\n",
       " ('b', 'c', 'e'),\n",
       " ('b', 'c', 'f'),\n",
       " ('b', 'd', 'e'),\n",
       " ('b', 'd', 'f')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "list(product(*[[\"a\",\"b\"],(\"c\",\"d\"),(\"e\",\"f\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 1000\n",
      "2 4 5\n",
      "3 7 8 9\n",
      "5 5 6 7 8 9 10\n",
      "206\n"
     ]
    }
   ],
   "source": [
    "# prettyPicture(clf, features_test, labels_test)\n",
    "# output_image(\"test.png\", \"png\", open(\"test.png\", \"rb\").read())\n",
    "# from class_vis import prettyPicture, output_image\n",
    "\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "K,M = map(int,input().split())\n",
    "N = (list(map(int, input().split()))[1:] for _ in range(K))\n",
    "results = map(lambda x: sum(i**2 for i in x)%M, product(*N))\n",
    "print(max(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.7071373 , -0.16395837,  0.60953767])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "y = np.random.randn(3)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls \"/content/drive/My drive/colab notebooks/hello.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-45acf5f5f725>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\DELL\\Anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Temp\\\\pip-install-y8jpxe5u\\\\torch\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Temp\\\\pip-install-y8jpxe5u\\\\torch\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\DELL\\AppData\\Local\\Temp\\pip-wheel-fhr0k3e5'\n",
      "       cwd: C:\\Users\\DELL\\AppData\\Local\\Temp\\pip-install-y8jpxe5u\\torch\\\n",
      "  Complete output (30 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_deps\n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 1, in <module>\n",
      "    File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\pip-install-y8jpxe5u\\torch\\setup.py\", line 265, in <module>\n",
      "      description=\"Tensors and Dynamic neural networks in Python with strong GPU acceleration\",\n",
      "    File \"C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\setuptools\\__init__.py\", line 144, in setup\n",
      "      return distutils.core.setup(**attrs)\n",
      "    File \"C:\\Users\\DELL\\Anaconda3\\lib\\distutils\\core.py\", line 148, in setup\n",
      "      dist.run_commands()\n",
      "    File \"C:\\Users\\DELL\\Anaconda3\\lib\\distutils\\dist.py\", line 955, in run_commands\n",
      "      self.run_command(cmd)\n",
      "    File \"C:\\Users\\DELL\\Anaconda3\\lib\\distutils\\dist.py\", line 974, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\wheel\\bdist_wheel.py\", line 223, in run\n",
      "      self.run_command('build')\n",
      "    File \"C:\\Users\\DELL\\Anaconda3\\lib\\distutils\\cmd.py\", line 313, in run_command\n",
      "      self.distribution.run_command(command)\n",
      "    File \"C:\\Users\\DELL\\Anaconda3\\lib\\distutils\\dist.py\", line 974, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"C:\\Users\\DELL\\Anaconda3\\lib\\distutils\\command\\build.py\", line 135, in run\n",
      "      self.run_command(cmd_name)\n",
      "    File \"C:\\Users\\DELL\\Anaconda3\\lib\\distutils\\cmd.py\", line 313, in run_command\n",
      "      self.distribution.run_command(command)\n",
      "    File \"C:\\Users\\DELL\\Anaconda3\\lib\\distutils\\dist.py\", line 974, in run_command"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading torch-0.1.2.post2.tar.gz (128 kB)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\dell\\anaconda3\\lib\\site-packages (from torch) (5.3.1)\n",
      "Building wheels for collected packages: torch\n",
      "  Building wheel for torch (setup.py): started\n",
      "  Building wheel for torch (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for torch\n",
      "Failed to build torch\n",
      "Installing collected packages: torch\n",
      "    Running setup.py install for torch: started\n",
      "    Running setup.py install for torch: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      cmd_obj.run()\n",
      "    File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\pip-install-y8jpxe5u\\torch\\setup.py\", line 51, in run\n",
      "      from tools.nnwrap import generate_wrappers as generate_nn_wrappers\n",
      "  ModuleNotFoundError: No module named 'tools.nnwrap'\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for torch\n",
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\DELL\\Anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Temp\\\\pip-install-y8jpxe5u\\\\torch\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Temp\\\\pip-install-y8jpxe5u\\\\torch\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' clean --all\n",
      "       cwd: C:\\Users\\DELL\\AppData\\Local\\Temp\\pip-install-y8jpxe5u\\torch\n",
      "  Complete output (2 lines):\n",
      "  running clean\n",
      "  error: [Errno 2] No such file or directory: '.gitignore'\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed cleaning build dir for torch\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\DELL\\Anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Temp\\\\pip-install-y8jpxe5u\\\\torch\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Temp\\\\pip-install-y8jpxe5u\\\\torch\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\DELL\\AppData\\Local\\Temp\\pip-record-o2isqsoe\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\DELL\\Anaconda3\\Include\\torch'\n",
      "         cwd: C:\\Users\\DELL\\AppData\\Local\\Temp\\pip-install-y8jpxe5u\\torch\\\n",
      "    Complete output (23 lines):\n",
      "    running install\n",
      "    running build_deps\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\pip-install-y8jpxe5u\\torch\\setup.py\", line 265, in <module>\n",
      "        description=\"Tensors and Dynamic neural networks in Python with strong GPU acceleration\",\n",
      "      File \"C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\setuptools\\__init__.py\", line 144, in setup\n",
      "        return distutils.core.setup(**attrs)\n",
      "      File \"C:\\Users\\DELL\\Anaconda3\\lib\\distutils\\core.py\", line 148, in setup\n",
      "        dist.run_commands()\n",
      "      File \"C:\\Users\\DELL\\Anaconda3\\lib\\distutils\\dist.py\", line 955, in run_commands\n",
      "        self.run_command(cmd)\n",
      "      File \"C:\\Users\\DELL\\Anaconda3\\lib\\distutils\\dist.py\", line 974, in run_command\n",
      "        cmd_obj.run()\n",
      "      File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\pip-install-y8jpxe5u\\torch\\setup.py\", line 99, in run\n",
      "        self.run_command('build_deps')\n",
      "      File \"C:\\Users\\DELL\\Anaconda3\\lib\\distutils\\cmd.py\", line 313, in run_command\n",
      "        self.distribution.run_command(command)\n",
      "      File \"C:\\Users\\DELL\\Anaconda3\\lib\\distutils\\dist.py\", line 974, in run_command\n",
      "        cmd_obj.run()\n",
      "      File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\pip-install-y8jpxe5u\\torch\\setup.py\", line 51, in run\n",
      "        from tools.nnwrap import generate_wrappers as generate_nn_wrappers\n",
      "    ModuleNotFoundError: No module named 'tools.nnwrap'\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'C:\\Users\\DELL\\Anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Temp\\\\pip-install-y8jpxe5u\\\\torch\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Temp\\\\pip-install-y8jpxe5u\\\\torch\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\DELL\\AppData\\Local\\Temp\\pip-record-o2isqsoe\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\DELL\\Anaconda3\\Include\\torch' Check the logs for full command output.\n"
     ]
    }
   ],
   "source": [
    "! pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: unknown command \"nltk\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! pip nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw = stopwords.words(\"english\")\n",
    "\n",
    "len(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "t= np.exp([2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [b for b in t/sum(t)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(7)\n",
    "features = torch.randn(1,5)\n",
    "target = torch.randn((features.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6014, -1.0122, -0.3023, -1.2277,  0.9198],\n",
      "        [-0.3485, -0.8692, -0.9582, -1.1920,  1.9050]])\n"
     ]
    }
   ],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.randn_like(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = torch.randn(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = target.view(5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0990]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2931]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def activation(x):\n",
    "    # Sigmoid function\n",
    "    result = 1/(1 + torch.exp(-x))\n",
    "    return result\n",
    "def plotline(features, target, bias):\n",
    "    # computing y value\n",
    "    x = torch.mm(features, target)\n",
    "    x1 = x + bias\n",
    "    return activation(x1)\n",
    "plotline(features, target, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = target.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "train_data = datasets.MNIST(root=\"data\", download=True, train=True, transform=transform) \n",
    "\n",
    "train_Loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data1\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd39d984bfe244c994e2ce93e6bf9826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data1\\MNIST\\raw\\train-images-idx3-ubyte.gz to data1\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data1\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc70a84bf3164d8b98b99e658947fbac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data1\\MNIST\\raw\\train-labels-idx1-ubyte.gz to data1\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data1\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa3d6d52b89f4b5c84f1055fdeae3601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data1\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to data1\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data1\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a98170407e4d38834641f10a1454b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data1\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to data1\\MNIST\\raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.ToTensor()\n",
    "test_data = datasets.MNIST(root=\"data1\", download=True, train=False, transform=transform) \n",
    "\n",
    "test_Loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABXEAAAD7CAYAAAAsAtcsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3dd7wU5dXA8fNQlWIUEBS5gJLciBWFIAgKKF2lSo+CJVIsoVgSUYpgIzQLKijBxquAiAqi2AAVQQMkICoQItIRkK60C/P+ccmT53lkl9lh987s3d/38+HznuPZnTl5mTs7+zBzrvI8TwAAAAAAAAAA0VQg7AYAAAAAAAAAALGxiAsAAAAAAAAAEcYiLgAAAAAAAABEGIu4AAAAAAAAABBhLOICAAAAAAAAQISxiAsAAAAAAAAAEcYiLgAAAAAAAABEWMYs4iql7lBKLVRKHVBKvRh2P0gPSqmqSqlPlFK7lFKrlFKtw+4J0aaUKqqUGq+UWqOU2qOU+qdSqlnYfSH6lFKvKqU2KaV2K6VWKqVuDbsnRJ9SqpRSappS6uej553OYfeE6FNKzVFK7VdK7T36Z0XYPSG6jOPkv38OK6WeCrsvRJ9SqrJSaqZSaodSarNS6mmlVKGw+0J0cb5BUJlybZMxi7gislFEhorI38NuBOnh6AXG2yIyQ0RKichtIvKqUio71MYQdYVEZJ2I1BOR34jIgyIyWSlVOcSekB4eFZHKnuedIiItRGSoUqp6yD0h+saIyEERKSciXUTkWaXU+eG2hDRxh+d5JY7++X3YzSC6jOOkhOSea/aJyJSQ20J6eEZEtojImSJSTXKvj3uF2hEijfMNTlC+v7bJmEVcz/Pe9DzvLRH5KexekDbOFZHyIjLK87zDnud9IiLzROSGcNtClHme97PneYM8z/vB87wjnufNEJHVIsJiHOLyPO8bz/MO/Dc9+qdKiC0h4pRSxUWkrYg86HneXs/zPheRd4TPKQCpc73kLsp9FnYjSAtni8hkz/P2e563WUTeFxH+oRF+cb4BHBmziAsEoGL8twvyuhGkL6VUORHJFpFvwu4F0aeUekYp9YuILBeRTSIyM+SWEG3ZInLY87yVxn9bInxBhj+PKqW2KaXmKaXqh90M0kZXEXnZ8zwv7EaQFp4QkY5KqWJKqbNEpJnkLuQCfnC+QaLy/bUNi7hAbMsl91/+7lFKFVZKNZbcR4CKhdsW0oVSqrCITBSRlzzPWx52P4g+z/N6iUhJEblCRN4UkQPx34EMV0JEdjn/bZfkHkNAPPeJyDkicpaIjBOR6Uop7vxHXEqpipJ7LfxS2L0gbcyV3H9Y3C0i60VkoYi8FWpHSAucbxBARlzbsIgLxOB53iERaSUi14jIZhHpJyKTJfcCBIhLKVVARF6R3FmVd4TcDtLI0fEtn4tIBRHpGXY/iLS9InKK899OEZE9IfSCNOJ53pee5+3xPO+A53kvSe64qOZh94XIu1FEPvc8b3XYjSD6jl4Lz5Lcf5QuLiJlROQ0EXk8zL6QNjjfICGZcm3DIi4Qh+d5Sz3Pq+d5XmnP85pI7r/sfBV2X4g2pZQSkfGSO4y/7dF/EAASVUiYiYv4VopIIaXU74z/drEwvgWJ8+TYY6QA043CXXHwr5SIZInI00cXVX4SkQmSDxdVkBKcb3Ci8uW1TcYs4iqlCimlThKRgiJSUCl1klKqUNh9IdqUUhcdPVaKKaXultzfrPpiyG0h+p4Vkaoicp3nefvCbgbRp5Qqq5TqqJQqoZQqqJRqIiKdROSTsHtDdHme97Pk3uH0kFKquFKqjoi0lNynAIBjUkqdqpRq8t9rYaVUFxG5UnLvmAOOSSl1ueQ+ospviYcvnudtk9xf7tvz6LnmVMmdcbok3M4QdZxvkKhMurbJmEVcEXlARPaJyF9E5I9H4wdC7Qjp4AbJ/eVCW0TkahFpZPz2eOBXlFKVRKS7iFQTkc1Kqb1H/3QJuTVEmye5oxPWi8gOERkuIr09z3s71K6QDnqJyMmS+zn1moj09DyPO3ERT2ERGSoiW0Vkm4jcKSKtPM9bEWpXiLquIvKm53mMa0Ei2ohIU8k936wSkRwR6RNqR0gHnG+QqIy5tlH8oj8AAAAAAAAAiK5MuhMXAAAAAAAAANIOi7gAAAAAAAAAEGEs4gIAAAAAAABAhLGICwAAAAAAAAARxiIuAAAAAAAAAERYoURerJTyUtUIErbN87zTw27CD46b6PA8T4Xdgx8cM5HCuQZBcNwgCI4bBMFxgyA4bhAExw0SxndwBBDzXMOduOlrTdgNAMgInGsQBMcNguC4QRAcNwiC4wZBcNwAyAsxzzUs4gIAAAAAAABAhLGICwAAAAAAAAARxiIuAAAAAAAAAEQYi7gAAAAAAAAAEGEs4gIAAAAAAABAhLGICwAAAAAAAAARxiIuAAAAAAAAAEQYi7gAAAAAAAAAEGEs4gIAAAAAAABAhLGICwAAAAAAAAARxiIuAAAAAAAAAEQYi7gAAAAAAAAAEGGFwm4garp166bjcePGWbWxY8fquG/fvlbt0KFDKe0LAJD/VahQwcoHDRpk5TfffHPM9yqldLxkyRKr9vjjj+v4jTfesGp8fgEQESlcuLCV33LLLTo+66yzrFqPHj10XKZMmUD7mz9/vpVfccUVOj58+HCgbQIAAORn3IkLAAAAAAAAABHGIi4AAAAAAAAARBiLuAAAAAAAAAAQYczEdTRv3lzHhQrZ/++5/fbbdfzoo49atY0bN6a2MQBppV27dknZzpQpU5KyHUTHqaeeauXmXMjTTjvNqp1++ulW7nmer31ceOGFVv7www/rePXq1VZtwYIFvraJ/KlYsWI6Pumkk3y/b/fu3TrOyclJak9InoIFC1q5+ffdpUsXq3b//fdbuTujOxa/5yVXrVq1rLxAgf/dW8JMXCD9jRgxQsfu75Nxr2/79eun43Xr1qW2MQBIY9yJCwAAAAAAAAARxiIuAAAAAAAAAERYxo9TuP766628TZs2MV/7zTff6Nh8jBCIp0GDBjru3LmzVbv11lt17D6OOHToUB0PGDAgRd3heNxH3+vUqaPjFi1aWLWWLVvqOOhj8K5GjRpZ+R133KHjgwcPBtom8p75M3znnXdatdKlS+s46HFyPJUqVdJxuXLlUrIPRIf7GHyNGjV0bI6NEhGpXr26jqtVq+Z7H2+//baO//KXv1i1lStX+t4OUuuee+6x8kceeSSkTn5txowZVn7kyJGQOgEQlDkyISsry6qZI1Pc8QnuOJXevXvr2BytgGgpU6aMlT/22GM6vummm6za999/r+OHHnrIqr311lsx97F3714dp+q6GDiW8uXL6/hPf/qTVbvuuut0bF47i4h8/fXXOh44cKBVmzNnjo537NiRjDa5ExcAAAAAAAAAooxFXAAAAAAAAACIMBZxAQAAAAAAACDCVCJzRpRS+W4oyXnnnWfly5Yti/na0aNH67hv374p68mnRZ7n1Tj+y8KXH4+bs846y8ovueQSHd98881WzZyTqpTyvY9Dhw7puHbt2lZt8eLFvrdj8jzPfwMhCvuYueyyy3Rszn0U+fUsqFjcv+ugM53c7ZhzdVq3bm3VUjSrm3ONT/Xr19fx8OHDrdrFF1+s4wIF7H8/Nf+O3eNkz549Vm4ej3Xr1rVqZ599dsztmDPdzVncIiJfffWVpADHTZJVrFjRys844wwdu59J48aNs/JSpUqlrjH59Xmyffv2Os7JyUlkUxw3SZCdna3j9957z6qZ54lk2bdvn5Xv3LlTx+a1s4g96/CFF16wauZ1T4I4biLkzDPP1HHjxo2t2uuvv67jAwcO5FlPMXDcBDB58mQrb9eunY7dzynTunXrAm/HfW/IMvq4cWfnL1y4MNm7kLFjx+o4kWsId87uF198oeOwzzd8Bw/PaaedZuW33HKLjhs2bGjVLrroIh0H/R0i+/fvt/KpU6fq+MYbb0xkUzHPNdyJCwAAAAAAAAARxiIuAAAAAAAAAERYobAbCFvPnj1j1sxHvkRExo8fn+p2EFH16tWz8oEDB1r5lVdeqWP3UWnTDz/8YOWzZs3Scdu2ba2a+di++6hs0HEK8OePf/yjjuONT9i4caOVm48Ub9++3apNmjQp5nbKly9v5a+99pqO3cegzWOxa9euVu2pp56KuQ+kXufOnXVsjlhJxMSJE638kUcesfIKFSrouEWLFr63++STT+o4ReMTkAKFCxfW8WeffWbVzGPhRHz77bc6vuuuu3y/7+6779axOTZIRKRVq1Y6fuONN06gO/jhPvJnjlBIZHyC+wjgjz/+qOMVK1ZYtSlTpuh41apVVm3u3Lm+9wl/ihYtauWFCsX+CmeOtzhy5IjvfRQrVkzH7ignc1SYW3Ovkc0RYObYFxGRkiVL6vjpp5/23RvCNWLECB2bYw9ERDp06KDjExl7MH/+fB337t3bqpljWSI2WiHjpGIkj6t79+46TmQ83e23327lEyZM0LF7feOOAUL+0aBBAyt/9dVXrdz9XEq2k046ycpLly6d9H1wJy4AAAAAAAAARBiLuAAAAAAAAAAQYSziAgAAAAAAAECEZdxM3KysLCt3Z+Kac1fceVMnn3xy6hpD6NyZfvfff7+Oq1evbtXcuberV6/W8YwZM6yaOTfuH//4h1U7cOCAjrdt22bV+vfv76dtpIA58zg7O9uqmbP/3NlLQZkzKUXs+chz5syJ+T53Xi7yljuH0vx5dmd4maZOnWrl7nw5U5MmTax82rRpOnZnLpn77Nevn1V74YUXYu4D0TFo0CArN2fLJjIDd8OGDVb+7LPP6njmzJlWbfPmzTo2Z6AeT9++fWPWmjZtqmNm4qbejTfeaOVBZxbec889Vj5mzJjAPSG5vvnmGys/55xzYr52+vTpOnZ/v0c85mdRvJm7J+Kdd95JyXaRWuacY3MGrojI5MmTA22zffv2MWvu3FPz+3u89yH1Fi1aZOXmvPTf//73ed1OXDfddJOOX3rpJav2+eef53U7OEHm75Bx1/BycnJ0fO+991o19/uSXzt37rRy89ravZb+/vvvY27H/B1IycKduAAAAAAAAAAQYSziAgAAAAAAAECEZdw4hT59+li5+1i8+fjGiBEjrNrChQtT1xhCUatWLR2/8sorVq1EiRIx3zdhwgQrf+CBB3S8adOmQL1MnDjRytevX6/jr7/+OtA2Ecz27dt17D7OnhfOP/98HbuPlJm5W0Pech897927t46ffvrpmO9bu3atlRcuXFjHvXr1smoPPfSQlRctWlTH7t//2LFjdcxj0OnDHKFgjvERESlYsGDM9+3YsUPHw4cPt2ru+Ax3XE8ylC1bNmZt69atSd8fbOedd56O3fOGX59++qmVB30sGqlXpUoVKzd//j/66COrZo59atSokVWrVKlSzH24j46a/v73v+vYHRfkjuEwuZ9FGzdujPlaRFe8cQqpYI6iE4k/dgp5y72GNY+Nyy+/3Kp16dJFx+5IS9Nll11m5eZ1cbLE2z+iqVu3blZujpqMN1LIZX5eitijCr/88kurZq73uWs6y5cv973PVONOXAAAAAAAAACIMBZxAQAAAAAAACDCWMQFAAAAAAAAgAjLiJm4JUuW1PGhQ4fivtac4/btt9+mrCeEw50p9uyzz+rYnYG7d+9eHZszfURE3nvvPSs/88wzdTx48GCrZs7/MmdWutw5K1Gau4LUqly5spXfe++9vt63f//+FHSDoA4fPqzjFStWWDXzc2j06NFWrWrVqjquU6eO7/2tXLnSynv27On7vchb5nw3c6aXm7tz+k3vv/9+zPypp5460RaPy+3tq6++0nGNGjWs2vz581PeT6br27evjuPNOXXNnTtXx+5sS2YZR1fr1q2t/JNPPtHx7t27Y77PnbOdDO61tDsT9+eff9bxY489ZtVycnKS3g/y1rx586y8YsWKSd+H+TtLEG3m+SfedUo8NWvWtPL69evr+KqrrrJqDRs2TLDDXJ07d7by1157LdB2kFqdOnXSsblOIyJSpEgRHR85csSqtWnTRsfu54z7e63yw7UOd+ICAAAAAAAAQISxiAsAAAAAAAAAEZYR4xRKlSql45tuuinua4sWLarjGTNmpKwnhKN79+5WfvHFF8d87RtvvKHjk08+2ap17drVygcMGKDjrKwsq2Y+Yr127Vqr5o5lQOYwRyi4j1e7x5DJfARk3LhxSe8LyXH++edb+YcffqjjcuXKWTWllI49z/O9j+zsbCv/+uuvdew+ertq1Srf28WJK1OmjJWbj5/+9re/jfm+X375xcpnzpypY/fR97xWr149K+/Vq1fM13722Wepbgc+meMTROzjaMuWLXndDgJ66623wm5Ba9y4cdy6+Sjrhg0bUt0O8oB53pg0aZJVM7/bTJkyxar169cv5jbda13zc9Kthf35h9QyxzO5uTk6RiT4OIW///3vgd6HvHXllVfq2ByfICJy8OBBHXfs2NGqTZ8+PbWNRQx34gIAAAAAAABAhLGICwAAAAAAAAARxiIuAAAAAAAAAERYvpyJW6CAvTbds2dPHbtz6lybNm3ScSKzCZEeTjrpJN+v7dat2zHjRBUsWFDHzZo1s2rMxM1cf/vb33Tszi817dixw8pbtmyp4507dya/MQRmnl+GDx9u1dw5uEGtXLlSx+5MXHMO77Rp06zahRdemJT9IzbzXH/77bdbtXhzcHfv3q3jdu3aWbWPPvooSd0FYx7T7du3j/k6d+6qO9sXJ878nQ0i/s8p7jFkzisFAD8mT54cs9a7d28d9+3b16qZn2l33323VXNn665bt07HFStWjFlDZhk4cGBStlOjRg0rN79DzZ49Oyn7QOIKFy5s5eb3XNfrr7+u47fffjtlPaUD7sQFAAAAAAAAgAhjERcAAAAAAAAAIiwjxim0atUq5mtXr15t5TVr1tTx3r17k9sYQhfG4+dbt27VcbzHkRBNlStXtvL69evruEWLFlbNfATEPQ8tXbrUyi+44IKY+zRHKJj7ExFZtmxZvHYREe6jzkqpmK/ds2ePjsePH2/V3EcTTddee62VT58+XcfmaAUR+1h95513Ym4Twd188806HjBgQMzXuT/D9957r47DHp/gatiwoY5vu+22mK+bM2eOlR84cCBVLWWsEiVKWHm1atV8vW/IkCFWXrduXR2PHDnSqv30009Wvnjx4kRaBERE5OWXXw67BaSQ+13GzGvVqmXV5s+fr+N44xNEROrUqROzhvTUoEEDHb/66quBtnHGGWdYedBxl/fdd5+V33XXXTq+6aabrNobb7wRaB9InDvi6YMPPtDxDTfcYNW2b9+eJz2lA+7EBQAAAAAAAIAIYxEXAAAAAAAAACKMRVwAAAAAAAAAiLB8ORO3dOnSVv673/0u5mv3799v5eZsQuQ/jz32mJV/8803Or7++uut2nXXXafjIkWK+N7Htm3brLxNmzY6njdvnu/tIDxPPvmkjrt06WLVfvOb38R8nzmn6ciRI1bNnVFqvnbJkiVWrUOHDjpetWqVj44RBebnSa9evazajTfeqOOTTjrJqg0aNEjHa9eu9b2/okWLWrl5TO3bt8+qJbJdBFOlSpWYNXMeu3tOidKc6379+lm5Oa/X9cILL+h42LBhKesJudx5tbNmzdKxOY/5eJo0aXLMWERk165dVj5jxgwdjx492qotWrTI9z6Rubp27Rqz5h5/WVlZvrY5c+ZMK3/00UcTbwwpsWHDBt+vdf++mYOb//Tv31/H7u+KCNvJJ5+s43jf7ZBa7ozj5cuX69hdl+vRo4eOJ0yYYNWidC2dF7gTFwAAAAAAAAAijEVcAAAAAAAAAIgwFnEBAAAAAAAAIMLyzUzc8uXL63j69OlWTSkV831vvfVWynpC9Pzyyy9WPmnSJB27M1nMmbjHs2bNGh03a9bMqpmzXRCe4sWLW3nr1q11bM5sEhHJzs7WsXtcpMJ5551n5XfccYeO//rXv1o1d9YpomnBggVx86AqVKig4/vvvz/m69y5uxUrVtTxv/71r6T0Aps799pk/v3v3r07L9rxrVq1ajpu2rSpVStTpoyOP/roI6s2Z84cHbu/XwCpZ86kbdSokVXzO1vU5c4FNOc3X3vttVatc+fOOn7vvfcC7Q/RYp4LChSw7/MxzwUdO3aMux3zGibedzDX0qVLdfzOO+9YNfN3WEyePNn3NpF65vnG/d0f8+fP17E7V9v8DiYi0r59ex3zd5w/fPHFFzpu0KBBoG245yL3d44kw9y5c5O+TQRj/v4i87uLiEj37t11bJ5bRETq1q2rY/d3zeRH3IkLAAAAAAAAABHGIi4AAAAAAAAARFi+GadQvXp1Hbu3XpsWLlxo5W+++WbKekL0/f73v9exefu+iEiRIkVivu/zzz+38ttuu03HjE+IDvPRwHHjxlm1c845R8fx/q6Dch/N+frrr63cPGYKFy5s1cxHEd1HtNu1a6fjnTt3nnCfSC8tW7bUsXl8u9zH29euXZuynpDrmmuu0fHq1autWtu2bXUctdEDn3zyiY7dx+nNR5hbtGhh1Q4cOJDaxhDXs88+q2P3EWZzrFjQ0Qou99h47bXXdHzxxRdbNXPEFKKlRIkSOh4wYIBV69u3r47dR5gTcejQIR2PHz/eqv3zn//UsfudbNmyZTrOyckJvH/kLfP8455v6tSpo+N169ZZtd69e1v58OHDdexesyRrJBXy1uzZs3Xcq1cvq3bqqaf62oY7PiHemDv3e1G8EXTm9/WtW7f66gV5689//rOVlyxZUsfmSCcRkebNm+vY/c6dihEcYeNOXAAAAAAAAACIMBZxAQAAAAAAACDCWMQFAAAAAAAAgAjLNzNx69atq+PSpUvHfN2SJUusfNGiRSnrCdHTqVMnKx85cqSOy5UrF/N9U6ZMsfI+ffpY+caNG5PQHU7Ub3/7Wyv/9NNPdVysWDGrZs6s3bJli1Vr3769r/25f+/m8eXOKHSZ83qee+65mK+76qqrrLxLly46HjNmTMz3uce6Ob8Q6eOWW26x8hEjRvh639SpU62cmbip9+WXX+q4SpUqVq1SpUo6XrFiRcp7cc935nzL7777zqqZs07dObdDhw6NWUN0LF261Movu+wyHZszUEXszwZzXqWISOPGjX3v85RTTtGxO9cd0bV3714dx5tlPGfOHKtmXjMVLFjQqrlzb805lLfffnvgXhFN7txbM+/QoYNVc+fgmtzXmtcp5nxmEf/X5YgWcyZujRo1rFr9+vV1bH4fF7E/X1yHDx+28qefflrHTz31lFX74Ycf/LaKCDLnq4vYv0+mYcOGVs28Xl2/fr1Ve+WVV1LQXbi4ExcAAAAAAAAAIoxFXAAAAAAAAACIMOV5nv8XK+X/xSlWq1YtK//kk090fNJJJ1m1rVu36th9rOeNN95IQXd5YpHneTWO/7LwhX3cZGdn63jWrFlWzXzE1TVjxgwdu4/8mI+KpRPP81TYPfgR9JhxHyk9//zzg+5fx+6jON26ddOxOa7hRNx5551WPnr0aB2bj0GLiBw5ckTHEyZMsGpXXnmljt3Hud3HHxOQceeac889V8fueB7zfDJz5kyr9uOPPwban/nY2B/+8Aer5j6mWrFixZjbWbVqlY7dx4xCGKeQcceNeT3173//26qZx02ymI+wn3nmmVbNvbapXr16zO2Yj7sOGDDAqr388ssn0mIQGXfcpFqjRo2s3Byh0KNHD6tWtmzZQPtwj2/zXJRHOG4CMK8ZRERWrlyp482bN8d8nzs+wx21Yn5u/elPfzqRFlON4yYAd5xC7dq1dTx58uTA2zXf265dO6tmXpdHAMdNkm3YsMHKzRGH7t/9M888Y+Xud6ioyu/fwfOaO/7pvffe07E7Ui6Nx7HEPNdwJy4AAAAAAAAARBiLuAAAAAAAAAAQYSziAgAAAAAAAECEFQq7gaD27t1r5e4cXNO3336r448//jhlPSEa6tata+WTJk3SsTs30DRy5Egrf+KJJ3ScrjNwM407A9fvzO8PPvjAyh9//HEdL1myxKrt3LkzYHexvfLKK1Z+3XXX6fjqq6+2aub/JnM+ryuEmYRpy507PGLECB03bdo05vvcGbhm/tprr/nef69evXTszppzmX//7rFoztPdtWuX7/0jOcy/myJFilg1c2aoOXfyeM444wwdd+rUyapdddVVOm7evHnc7ezYsUPH7nzkmjVr6jgnJ8d3b0gO81hZs2ZNzNft2bPHykeNGqXjVq1aWbWLLrpIx6eeeqpVK1q0aKA+47niiiusnM+f9BB0rr97XeJKxXUSosP8XiUiMn/+fB2fyEzcChUqBH4v0o95feNeM8WzYsWKVLSDNON+Pzevn9q2bWvVOnbsaOWvv/566hrLI9yJCwAAAAAAAAARxiIuAAAAAAAAAERYWo1TqFy5so6nT58e83VHjhyx8v79++vYfKQQ+dOf/vQnK483QuGdd97R8QMPPGDV9u/fn9zGkHLt27e3cnPMSqlSpaya+SiFe17I60eK3UcPW7ZsqWP3XOeOjDBNmDBBx4888kiSusv/atWqZeXxRiiYzEfBRETKlSunY/Nx5uNRSun4eCNAZs+erWP3eGeEQrjMMQXuWIwvv/xSxwsWLPC9zSpVqhwzdrl/94sXL7byIUOG6Hju3Lm+94/UM69Z169fb9WqV6+uY/P8IiIyZsyY1DZ2HJdffrmO//GPf4TYCfLa2WefbeXmZ5jIr88/yF9q165t5R06dEj6dtetW5eUbSK6br/9dh2fdtppIXaCdOSOtHv33Xd1bI6pExEpVCitljx94U5cAAAAAAAAAIgwFnEBAAAAAAAAIMJYxAUAAAAAAACACEurARFVq1bVcaVKlWK+rkABe23afN8XX3yR/MYQunvuuUfHnTp1ivk6d97cfffdp2Nm4Ka/qVOnht1CUuzbt0/HDRs2DLGTzPDdd99Z+d69e3VcokSJvG7HsmfPHis3j/Ht27fndTuIw/xZffLJJ61akyZNdNy4ceOk7M88Nm688UarNmPGjKTsA6lnzmD/5JNPrJo5EzcMr776qo4HDx5s1VavXs1TFRsAACAASURBVK1j93dRILO4s9yZiZu/ufNqR4wYoWN3Vn88kydPjlmrU6dO4o0hrSRyrACu+vXrW7nf32eSX3AnLgAAAAAAAABEGIu4AAAAAAAAABBhkR6nULx4cSs3H5mPZ8mSJVY+bdq0pPWEaChYsKCV9+jRQ8eFCtmHtTlCwX00feXKlSnoDkA62bFjh5VfdtllOn7xxRet2h/+8IdA+5g4caKODxw4EPN1y5Yts/JZs2ZZ+fLlywPtH6m3atUqHbds2dKqVaxYUcfuyJ8NGzbouGjRolbt/PPP1/Hnn39u1czxUO7jrUhPgwYNsvJ69erpuGbNmknZh3m8TZgwwaq99NJLVs7IBByLO+ZHKRVSJwiDO+rAHKfgjtaYMmWKjitUqGDVateuHfO1fKYhFve4efrpp0PqBH4VLlzYyocNG6ZjdxSiuW7TunVrq2ZeW9etW9eqmZ9D7vnj//7v/xLsOPq4ExcAAAAAAAAAIoxFXAAAAAAAAACIMBZxAQAAAAAAACDClDu7Ju6LlfL/YqTaIs/zaoTdhB+pOG6uu+46K3/77bdjvrZnz546Hjt2bLJbSSue56XF4DLONZGS0ecaBMZxgyA4bhAEx00ecucbunPezWv0d999N096CojjJsn69OkTM8/KyrJqI0eOtPJ+/fqlrrHk4rhJghUrVui4SpUqMV/nztz+85//bOXpMhM3k7+DZ2dnW/l3332X7F1YbrvtNisfP358SveXQjHPNdyJCwAAAAAAAAARxiIuAAAAAAAAAERYobAbAIKYPn26lX/88cc6vvrqq63avHnz8qQnAAAAIJNddNFFOo74OAUk2ahRo+LmwH917dpVxwMHDrRqjRo10vGHH35o1TJ9NGI62rlzp5XPmDFDx9dee63v7eTk5Oj4ueees2pTp07VcSas/XAnLgAAAAAAAABEGIu4AAAAAAAAABBhLOICAAAAAAAAQIQxExf5gjk7BwAAAEDyHT582Mpr1apl5V9//XVetgMgDS1YsEDHzZo1C7ETpNqWLVusvGXLliF1kn9wJy4AAAAAAAAARBiLuAAAAAAAAAAQYYxTAAAAAAAc15EjR6z8q6++CqkTAAAyD3fiAgAAAAAAAECEsYgLAAAAAAAAABHGIi4AAAAAAAAARFiiM3G3iciaVDSChFUKu4EEcNxEA8cMguC4QRAcNwiC4wZBcNwgCI4bBMFxg0RxzCCImMeN8jwvLxsBAAAAAAAAACSAcQoAAAAAAAAAEGEs4gIAAAAAAABAhLGICwAAAAAAAAARljGLuEqpO5RSC5VSB5RSL4bdD9KDUupVpdQmpdRupdRKpdStYfeE9KGU+p1Sar9S6tWwe0H0KaXmHD1e9h79syLsnpAelFIdlVLfKaV+Vkr9Ryl1Rdg9Ibq4JkYQXBMjCKVUKaXUtKOfT2uUUp3D7gnRppQqqpQaf/R42aOU+qdSqlnYfSH6lFKVlVIzlVI7lFKblVJPK6UKhd1XsmXMIq6IbBSRoSLy97AbQVp5VEQqe553ioi0EJGhSqnqIfeE9DFGRP4RdhNIK3d4nlfi6J/fh90Mok8p1UhEHheRm0SkpIhcKSLfh9oUoo5rYgTBNTGCGCMiB0WknIh0EZFnlVLnh9sSIq6QiKwTkXoi8hsReVBEJiulKofYE9LDMyKyRUTOFJFqknsM9Qq1oxTImEVcz/Pe9DzvLRH5KexekD48z/vG87wD/02P/qkSYktIE0qpjiKyU0Q+DrsXAPnaYBF5yPO8BZ7nHfE8b4PneRvCbgrRxTUxguCaGIlSShUXkbYi8qDneXs9z/tcRN4RkRvC7QxR5nnez57nDfI874ej1zUzRGS1iPCPRjies0Vksud5+z3P2ywi74tIvvtHo4xZxAWCUko9o5T6RUSWi8gmEZkZckuIOKXUKSLykIj0C7sXpJ1HlVLblFLzlFL1w24G0aaUKigiNUTkdKXUKqXU+qOPjp0cdm8A8h+uiZGgbBE57HneSuO/LZF8uKiC1FFKlZPcY+mbsHtB5D0hIh2VUsWUUmeJSDPJXcjNV1jEBY7D87xekvuI6hUi8qaIHIj/DkCGiMh4z/PWhd0I0sp9InKOiJwlIuNEZLpSirucEE85ESksItdL7mdUNRG5REQeCLMpAPkT18RIUAkR2eX8t12SewwBx6WUKiwiE0XkJc/zlofdDyJvruT+I9FuEVkvIgtF5K1QO0oBFnEBHzzPO3z0EaAKItIz7H4QXUqpaiLSUERGhd0L0ovneV96nrfH87wDnue9JCLzRKR52H0h0vYd/b9PeZ63yfO8bSIyUjhuAKQI18RIwF4ROcX5b6eIyJ4QekGaUUoVEJFXJHem8h0ht4OIO3q8zJLcf2AsLiJlROQ0yf29EfkKi7hAYgoJ878QX30RqSwia5VSm0XkbhFpq5RaHGZTSEueiKiwm0B0eZ63Q3LvNPDC7gVAxuGaGMezUkQKKaV+Z/y3i4XH4nEcSiklIuMl94mjtp7nHQq5JURfKRHJEpGnj94Q85OITJB8eGNDxiziKqUKKaVOEpGCIlJQKXWSUqpQ2H0hupRSZZVSHZVSJZRSBZVSTUSkk4h8EnZviLRxkvulptrRP8+JyLsi0iTMphBtSqlTlVJN/vvZpJTqIiJXSu6/KAPxTBCRO49+Zp0mIr1FZEbIPSHCuCZGorgmRhCe5/0suXfFPaSUKq6UqiMiLSX37kognmdFpKqIXOd53r7jvRg4+jTaahHpefQ651QR6Sq5c7jzlYxZxJXc+XD7ROQvIvLHozEz4xCPJ7mPia0XkR0iMlxEenue93aoXSHSPM/7xfO8zf/9I7mPku33PG9r2L0h0gqLyFAR2Soi20TkThFp5XneilC7QjoYIiL/kNw7nr4TkX+KyMOhdoSo45oYieKaGEH1EpGTRWSLiLwmIj09z+NOXMSklKokIt0l92aYzUqpvUf/dAm5NURfGxFpKrnfp1aJSI6I9Am1oxRQnscTeAAAAAAAAAAQVZl0Jy4AAAAAAAAApB0WcQEAAAAAAAAgwljEBQAAAAAAAIAIYxEXAAAAAAAAACKsUCIvVkrxW9CiY5vneaeH3YQfHDfR4XmeCrsHPzhmIoVzDYLguEEQHDcIguMGQXDcIAiOGySM7+AIIOa5hjtx09easBsAkBE41yAIjhsEwXGDIDhuEATHDYLguAGQF2Kea1jEBQAAAAAAAIAIYxEXAAAAAAAAACKMRVwAAAAAAAAAiDAWcQEAAAAAAAAgwljEBQAAAAAAAIAIYxEXAAAAAAAAACKMRVwAAAAAAAAAiDAWcQEAAAAAAAAgwljEBQAAAAAAAIAIKxR2AwAAAEhMVlaWjq+55hqr1rx5cyvftGmTjpVSVu2MM87Q8RtvvGHVXn755RPuEwAAwHT99dfruE6dOlbtnHPO0fG1115r1QoUsO9BPHLkiI7vv/9+q/a3v/3tmK8D0h134gIAAAAAAABAhLGICwAAAAAAAAARlvHjFMqVK2flH3zwgY6///57q9a6des86QkAAKB06dI67tixo1V78skndex5nu9tuuMUzPdedNFFVm3x4sU6XrZsme99IH+Id1w1aNBAx3PmzMmDbmAqWrSoji+//HKrZj5+XK9ePat2ySWX+Nr+6NGjY9Zee+01K//Xv/5l5Tk5Ob72gcw2aNCgQDWkpy5dulj5888/r+MiRYrEfJ/7OeSORTDrDz/8sFV76623dLxixQr/zQIRx524AAAAAAAAABBhLOICAAAAAAAAQISxiAsAAAAAAAAAEaYSnKPm/8URddZZZ1m5OStFRKRGjRo6PnjwoFUz509FwCLP82oc/2Xhi/JxU6JECR2fc845Vq1z586BttmoUSMrv/TSS3Xs/ry9/PLLOu7fv79V27BhQ6D9x+N5njr+q8IX5WMmA3GuQRAcN0lgfi4MHjzYqpmzbY93Lffuu+/qeOvWrVZt48aNOs7OzrZqM2fO1LH5eZVCHDcREu+4Mo/HCMyvzLjj5oILLtDx559/btXMa9t4M7DjSeR95nlCxD42Fi1a5Gt/Icm44yZKElmD4HwTTNjHTdmyZXU8b948q3b22WcH2mYi56bvvvtOxxdeeGGg/SUL38GP7eKLL7by7t27HzMWEZk6daqOx4wZY9Xmzp3re59TpkzRcZs2bazaY489pmN3bSYEMc813IkLAAAAAAAAABHGIi4AAAAAAAAARFjGjVNo3Lixlc+aNcvKf/zxRx3Xr1/fqi1fvjxlfQXAoxxJ8OKLL+r4hhtuCK8REXn++eetvEePHknfB49ynDjzvOCeI0xz5syJmyfD7Nmzrdzsp0GDBsnaP+eaJBsxYoSV9+3bN+Zr161bZ+WjRo06ZhxBHDcBPPvss1betWtXHRcpUsSqmY8Ujhs3zqo9/PDDVr5p0yYdHz58+IT7TCGOmwiJ9x3B/IxJxedbgjL6uBkyZIiVN2/eXMfuiBRztIpr8eLFOjZHgYmI1KpVS8cVKlSI28+hQ4d03KdPH6v23HPPxX1vHsvo4yavudfM7jVsUCGMWuC48al69eo6dscpFCpUSMc5OTlWbfv27To2v6uLiOzZs8fKzfPRrbfeatUOHDig47p161q1pUuXxms96fgObu1Dx+b1qYhImTJlfG1j9+7dVl6tWjUdr127Nu57zetg9zrn3//+t46rVq3qq5cUYpwCAAAAAAAAAKQjFnEBAAAAAAAAIMJYxAUAAAAAAACACCt0/JdkFnOOU8Rm4CJNbdy4Ucfly5eP+TpzPgyiy5zpNXDgQN/vS9bMQHPeVyIzeREuc55gvBm4rqysLCsfOXLkMWM379evX6ItIiTm/PPbbrst5uvMzxIRkVatWul40aJFyW8MGSeReZJ8xkTHgw8+aOXDhg3TcYkSJayaO3/Qr9KlS+v4pptusmoPPPCAlZcsWVLH7ufUV199pWNzBi/yJ3Pubbxr1hMR71o8j2bkIgbz2sT9ea9Zs6aOR48ebdX+8pe/+N7Hb37zGx27v/vo7LPP1vF5551n1fJ6Jm4mu/jii638kUce0bHfGbgu8+9dxF5jOd5M3KlTp+q4TZs2Vm3btm2B+slr3IkLAAAAAAAAABHGIi4AAAAAAAAARBjjFBzbt28PuwXkodWrV8esffnllzreuXOnVatcubKVm488t2/f3qrdeeedOo43TiFeL4iOevXq+Xpdsh41dR8/i/fY2ODBg5OyTyRfxYoVU74Pc0yDO7LBzEeNGpXyXuDfLbfcomPP86zaTz/9pOPq1atbtS1btqS2MWScREYEIbr27NlzzPhEmOei4cOHW7XDhw9buTlCoWjRolatSJEiSekH0WGOLDiRc4h5DeuOQXA/G2Nx929es7vXyIyEyVsrV6608gsvvFDH7tiVRHTu3FnH5vgEhMscE2mOTxARadKkSdL3N3HiRB1fc801Vs0dkdq2bVsdu+eWoUOHJr23VOBOXAAAAAAAAACIMBZxAQAAAAAAACDCWMQFAAAAAAAAgAjLuJm47mwm12OPPZZHnSAKHn/8cR2/9dZbVm3VqlU6/vnnn61anz59rHzYsGHHjEVEzjvvPF+9TJkyxdfrkLdmz55t5e6MWlODBg10nKxZW+7+43FniCG1atWqZeULFizQsTknW0Rk0qRJOnZnf5k/+7Vr17ZqFSpUsHKz7r42HnOfGzZssGqTJ0/2vR2cuOzsbCu/9NJLdezO5ho7dqyOmYGLMDFzHX4dOXJEx35nmSJ9JHJdbF4Lz50716olcs3qdzvxenP7NGd2IvV69Ohh5ffcc4+Ot27d6ns7119/vZWPGDEi5mt/+eUXHX/77be+94ET17RpUx0nMgP3/vvv17G7pmJyf9avuOIKHU+bNs2qucdefsCduAAAAAAAAAAQYSziAgAAAAAAAECEZdw4hSFDhoTdAiJk//79Ol6yZEng7RQo8L9/D4k3PsF9jLlNmzY6XrNmTeD9I7nMRzT8PiZ2rDwZ+4+Hx1vD5Y4hMMcptGvXLub71q9fH/N9Znw87du3t/LLLrtMx19++aVV6927t46HDx9u1dauXRto/wjGfczLtHjxYisfOnRoqttBBgv6ODMy29VXX23lN910U0idIFXca99416XxRh2YtRM5h5jjyuKdt8zXicS/nje3wziy1DO/cx8rN51yyik6btmypVUzxzCIiBQpUiTmdsaPH6/jpUuX+uoTeevll1+28lGjRvl6X6tWrax89OjROi5fvrxVe/PNN2Nuxx3lsW3bNl/7Dxt34gIAAAAAAABAhLGICwAAAAAAAAARxiIuAAAAAAAAAERYRszE7dSpk44vuuiiEDtBlJlzbUVEzj33XB23bdvWqpnzJY9n48aNOm7cuLFVW758eSItIkXcWVh+5+C6s7fyev/M8Mp75hzcrKwsq+bmpr59++rY77ynRHo5Vh6rNmLEiKTsH/5Vq1ZNxxUqVLBq5mfPsGHDrNrBgwdT2xgyWr169eLWkzXPEumnZMmSVn7rrbfq+M4777RqFStWjLkdd863myM6zGvPeDNw3d/HkNfXoomci8xe3WvrgQMH6pjr6XCZM3BFRCZOnKjjZs2aWTWllJV7nhdzu/37909Cdwhi4cKFOl60aJFVq169uo7d65CqVavqON4c4127dlm5OZv9t7/9rVWLt97y3nvvWbnba1RxJy4AAAAAAAAARBiLuAAAAAAAAAAQYRkxTsF8VNG9BR+ZpWDBglZ++umn6/j222+3avfff39S9vnwww/rePXq1UnZJk6c+ViV+UjV8biPkSXD8R5pNc2dOzfp+4d/7dq18/W6kSNHWnmyRigkQ79+/cJuIeOYI3iKFy9u1bZu3arjBQsW5FlPyEzmZ1+80T0ifN7kR0WLFtXxXXfdZdXOOOMMHbvXJZdcconvfZjftTZv3mzVzLF2//rXv6xaTk6O730g+fyOUAh79EAi4xTijYQ53vkPqTVu3DgdN23a1KqVL18+0Dbd7+779+8PtB2cOPPa9rnnnrNqzz//vI4rVapk1aZNm6bjF1980aoNGTIkKb19+umnOu7Tp09StpnXuBMXAAAAAAAAACKMRVwAAAAAAAAAiDAWcQEAAAAAAAAgwjJiJm68WRcrVqyw8vfffz/V7SDFSpUqZeVt2rTRccOGDa2a3/mWJ2LMmDE6bt26tVXr37+/jhcuXJjyXvA/fufgujNwE5nFZXJniPndv7u/sGeRwZ++fftaeVZWlo7debnMQc1/LrjgAitv1apVzNdOmDBBx+vXr09ZT7FkZ2fr2DxORezP0ylTpuRZT0gd5kBmFnMGrog9n/22226L+T73d4h4nhdo/82bN4+ZDx061KqZMzI3btwYaH/wL97faX659kxkBjiSz5yB/dhjj1m1Jk2aBNrmjh07rHzmzJk6HjZsWKBtIrXM61wRkT179uh40qRJVq1y5co6ds87Zu6u75nnM/ezzf08W7p0qY537doVu/EI405cAAAAAAAAAIgwFnEBAAAAAAAAIMIyYpzCmWeeGbP2xBNPWLl7iz7ST6dOnaz8ySefDKmTX3PHOZx33nk6rlmzplXbtGlTnvSUqfw+VhXvETJ3G2Zer169QPtzueMcEC5zTII7FiEec3TL/PnzrdrkyZN1bD7qeqwc6aFEiRJxc9PmzZtPeH/ly5e38g4dOsR8rfsIWvHixXV88sknW7UiRYro+KqrrrJqPXv2TLhPAHnLPfe44w1iWbJkiZWvXLnS9z6LFSvma38PPPCAlZctW1bHvXr18r0/+BfvWtQcodCgQYPUN5MH4o0uyy//G6OkWbNmVj527Fgdu9cpfke0fPHFF1berVs3K//+++8T6BBRMHXqVB2bI71EREaPHq1j93gyud+P4h1Pbi3oeKAo4U5cAAAAAAAAAIgwFnEBAAAAAAAAIMJYxAUAAAAAAACACMuXM3FLlixp5QULFtTx119/bdWmTZuWJz0h+rZv327l8+bN07F73Lz//vtWXrhwYR337t3bqlWvXl3H7jwgMzdniCH5gs6kDWNujjmXzIwRPnMG04YNG6zapEmTfG0j3ixdc3auyK/nl5r7Z15u+lBKnfA23M+Pd955R8eXXHJJ4F78nuO6d+9u5YcOHdLxXXfd5Xv/SB/xZsIjPfz0009Wbl6T1qlTx6otXrxYx7t27bJqe/bs8b3PQoX+9/XSnXvr5qZrrrnG9z4QTNBr4XThnrPM/73u9TTX18kxYMAAHT/44INWze+1j/s7iW644QYdf/LJJ1bt4MGDibaIiDGvO//zn/9YtZtvvlnHl156qVVr27atjlu0aGHVypQp43v/p556qo7da+uNGzf63k6YuBMXAAAAAAAAACKMRVwAAAAAAAAAiDAWcQEAAAAAAAAgwvLlTFxzloaISLly5XS8dOlSq7Z58+Y86Ql5Z/ny5Vaek5Oj43/+859W7bPPPtPx008/bdXWrFkTaP8LFy608g8++EDH7twV5J28nn3l7i+ROWQNGjRIbjNIicmTJ8fMa9WqZdX69u2r4woVKli12rVrHzM+FnOervva9u3bH6djhCXobO0LLrhAxx9++KFVK1u2bMztu3MwV65cqWPzc0/EnoPpMufbmb2IJDaHF9ExcODAmDVmROZ/5rnBnKudTOZ1tzuj9MiRIzp25+NmZWXp+Mknn7RqzN1Ojng//+nKvL6O979v8ODBedBN/vfXv/7Vys3rhETm/5vnny5duli1ffv2BewO6W7r1q06njVrllUzc/f3FSXye0LMmctXXHGFVTNns7trSlHCnbgAAAAAAAAAEGEs4gIAAAAAAABAhOXLcQoNGzYMuwWE6OOPP7Zy8/GsvXv3WrVffvklKfssXry4jv/zn/9YtTJlyiRlH0gu87GqevXqWbV4ow/Mx03nzp1r1dzHBk1BH6dGelqwYIGVxxt1YI5eMMcliPx69IJ5PmvXrp1VW7t2rY4rVqzov1kk3TfffBMzP//8861ahw4ddOyOTPj88891XKJECatmfn49//zzVu2ZZ56x8lWrVvlp+1fM42348OFW7cILL9SxO2ph2bJlgfaH5EtklA+PGyPVHnroIR3fcsstVs38vLvyyiutWsmSJXW8Z8+eFHWX2dxr2qhyz2mzZ8+O+VpzPBnjYpLjsssus3K/IxTcURcjRozQ8f79+0+8MbHHTInY11u33nprzPe5oxDnz5+v4++++86q7dq160RaRJK4x10iozwKFPjffayVK1e2auZ5okmTJlZtyZIl/htMMe7EBQAAAAAAAIAIYxEXAAAAAAAAACKMRVwAAAAAAAAAiLB8ORPXnV9hcucUIv/bsmVL0rfpzmMy5/zEm4F7+PBhK+/Xr5+Ov//+++Q0B1/iza/N6+0zhzCzmZ9Ll19+edzXmjPEateubdXM/IsvvrBqx9suksud2fjpp5/q2J2Je+mll+r48ccft2ruHFxTx44ddfzuu+8G6vN44s2QW7ly5TFjRIs7hzAeZkamp6uvvtrKS5UqpWN3hl+UflZff/11K7/77rt17M7ZNs+bfJcLzvwZT2RedtjMa+p457RE5mLCvx49eui4bt26gbZRrVo1K2/dunWg7dx1111Wbv7OkdNPP92qnX322b62af5uAtfMmTOt3DwWFy9e7Gv7SL7+/ftbebzfPfPtt99a+ahRo3T8xBNPWDVzHWfWrFlW7aqrroq5zbzGnbgAAAAAAAAAEGEs4gIAAAAAAABAhOWbcQrmY+mFCxeO+bo333wzL9pBErh/j2eeeWbM127evFnHBw8eDLS/okWLWrn5+HGbNm2sWrdu3ay8WLFiMbf7008/6fiRRx6xak899VSibSIf4hFW+GV+1tWqVcuqVaxYUceTJk3Ks55wfOa5vkuXLlbttNNO03HTpk19b7N69eo6TmScQlZWlpWbn60PPPCAVTv33HN17D6m+tFHH+k46OcuUi+dHpmGf+bP//Tp061akSJFdLxixQqr5o5zCVP58uXDbiHjzJ07V8fuuaFevXp53E1s8cbWudfMjCRLvTFjxug43mPr8bjjE4KOU3CvRYL241fz5s2t3Lxmu+KKK1K6b8Tmjq80jwP3mnTYsGFW/uqrr+rY/Lx0X+vuY8aMGTpu1aqVVVu6dKmftpOGO3EBAAAAAAAAIMJYxAUAAAAAAACACGMRFwAAAAAAAAAiLN/MxD311FNj1rZs2aLjvXv35kU7SII77rjDyocPHx7ztaNGjdLx6NGjrdrGjRt1fNFFF1m10qVL6/i+++6zaldffbXvXo8cOaLjefPmWbWuXbvqeM2aNb63ifRmzu86FnOmFzNxEUTt2rXj5qYRI0bo2Jyri7yxcuVKHd9www1W7bnnntNxvNnvrnvuuUfHicy5vP7666083jw5s/bhhx9atWeeecb3PpG3/M7B5bMnfW3atEnHq1atsmrm+aBq1apWbd26dTp2Z1Ka29m5c2fg3goV+t/XS/ec1rFjRx23aNHCqpmzLt2ZhszdTo5Bgwbp2L1ONc8bs2fPtmoNGjRIaS/H6ieWVPSC+Mzf71KqVKkQO0meXbt26bhEiRJWrWDBgjHfl52dnbKekBzueos5A9c1duxYKzc/l5o0aWLVKlWqpOOHH37Yql133XUJ93kiuBMXAAAAAAAAACKMRVwAAAAAAAAAiLB8M04hnpycHB2XKVPGqrmPICE63EfA4unTp4+O3UdFV6xYoeOGDRueeGNiP44mIjJ+/HgdDxkyJCn7QPpxHz+LZ/DgwSnsBJnAHCMjItKuXbuYr83Kykp1O/Dp3XfftfK6devq+JZbbrFq5mOj7riMYsWK6bht27aB+zEfk3TPYS+88IKOP/roo8D7QN7yO05h7ty5qW0EKWOOCmvevLlV69mzp44bN25s1S655BIdL1iwwKr9+9//1vHSpUutmnneMsfDHEv//v11k62lJgAABTRJREFU3KxZs7ivNR04cEDHvXv3tmqLFy/2vR34444lMM//7jnErCVy3vA7IsHlXiO7oxeQtyZOnKjjO++8M+X7+/HHH3W8bds2q7Z69WorX7t2rY7Nc5iISK1atXRsXs+I2OM2y5Yta9XuuuuumL3NmDEjZg15p0AB+15Uc7TlaaedZtXcY/app56Kud1u3brp+IMPPrBq1apV07H7udu0aVMdv//++zG3nyzciQsAAAAAAAAAEcYiLgAAAAAAAABEGIu4AAAAAAAAABBh+WYm7i+//BKzVr58eR3fd999Vq1169Yp6wnhcGc/JmMWpDsTxZzBKyKyfPnyE94H0p/fOYTua+fMmZP0XpD/tW/f3sorVKgQ87UjR45MdTsIaM2aNToeMGCAVTPn3pYqVcqqTZs2TcfmnEuRX8+sNOdZKqVibmfevHl+20Y+wGdP/rB+/XorN2fSPvTQQ1bN/Czo3r27Vfvd736n4+zsbKuWyNxt8xzjeZ7v902YMEHHY8eO9f0+BOP+/JtzaN1ZtuY1ayLXuvG4n0VcF0fXE088oeNEfqbN2bYuc5bt/Pnzrdq+ffuOGYuI7N692/f+480+NX3zzTdWnsjvOEE4li1bZuXm71JyfweW+ztEzBm58Y5ndzvm3F33fYn8XCQDd+ICAAAAAAAAQISxiAsAAAAAAAAAEZZvxik8+uijOq5Zs6ZVq1y5so7dR4eQ2Xbu3Klj81EREZGJEyfqeN26dVbt4MGDqW0M+R6PiuG/Jk+eHLPWrl27QNt0z1lIT+aoKHds1B/+8Ie8bgf5gPnINJ9D+d+BAwesvG/fvjpevXq1VWvevLmO69Wrl5T9m6NcROxRL+PHj7dqjCYL16BBg44ZHyv3yzzHHO98w/koun744Qcdm+cQICx/+9vfrNwcwVO4cOG4761SpYqOExmDYK4bzZ0716otWrTI93aSgTtxAQAAAAAAACDCWMQFAAAAAAAAgAhjERcAAAAAAAAAIkwlMgdCKeX/xUi1RZ7n1Qi7CT+CHjdnnXWWlX/00Uc6zs7OtmrffvutjmfMmGHVcnJydDxy5EirdvjwYR3v3r07SJtpxfM8FXYPfqTruSbB82kKO0mqfH+uyWtZWVlWPmLECB3Pnz/fqrnnLL86dOhg5fHm7qYIxw2C4LhBEBw3CILjBkFw3CBhfAdPrRtuuEHH9957r1WrWrWqlZvfwd3v7s8++6yOV6xYYdWWLl2q408//TR4s/7FPNdwJy4AAAAAAAAARBiLuAAAAAAAAAAQYYXCbgCIZcOGDVbu3goPRNmcOXOsfPDgweE0gshZt26dlbdv3z7maytUqGDl69ev13G8UQtr164N2B0AAAAApIdXXnnlmHF+xZ24AAAAAAAAABBhLOICAAAAAAAAQISxiAsAAAAAAAAAEcZMXABIEqVU2C0gn+nXr1/M2qhRo/KwEwAAAABAmLgTFwAAAAAAAAAijEVcAAAAAAAAAIgwFnEBAAAAAAAAIMJYxAUAAAAAAACACGMRFwAAAAAAAAAijEVcAAAAAAAAAIiwQgm+fpuIrElFI0hYpbAbSADHTTRwzCAIjhsEwXGDIDhuEATHDYLguEEQHDdIFMcMgoh53CjP8/KyEQAAAAAAAABAAhinAAAAAAAAAAARxiIuAAAAAAAAAEQYi7gAAAAAAAAAEGEs4gIAAAAAAABAhLGICwAAAAAAAAARxiIuAAAAAAAAAEQYi7gAAAAAAAAAEGEs4gIAAAAAAABAhLGICwAAAAAAAAAR9v8xqNfhaLi9ugAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1800x288 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "    \n",
    "# obtain one batch of training images\n",
    "dataiter = iter(train_Loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    # print out the correct label for each image\n",
    "    # .item() gets the value contained in a Tensor\n",
    "    ax.set_title(str(labels[idx].item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = np.squeeze(images[1])\n",
    "\n",
    "fig = plt.figure(figsize = (12,12)) \n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(img, cmap='gray')\n",
    "ax.set_title(str(labels[1].item()))\n",
    "width, height = img.shape\n",
    "thresh = img.max()/2.5\n",
    "for x in range(width):\n",
    "    for y in range(height):\n",
    "        val = round(img[x][y],2) if img[x][y] !=0 else 0\n",
    "        ax.annotate(str(val), xy=(y,x),\n",
    "                    horizontalalignment='center',\n",
    "                    verticalalignment='center',\n",
    "                    color='white' if img[x][y]<thresh else 'black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (fc3): Linear(in_features=512, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        hidden_1 = 512\n",
    "        hidden_2 = 512\n",
    "        self.fc1 = nn.Linear(28*28, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = nn.Linear(hidden_2, 10)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "model = Network()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000183\n",
      "Epoch: 2 \tTraining Loss: 0.000183\n",
      "Epoch: 3 \tTraining Loss: 0.000183\n",
      "Epoch: 4 \tTraining Loss: 0.000183\n",
      "Epoch: 5 \tTraining Loss: 0.000183\n",
      "Epoch: 6 \tTraining Loss: 0.000183\n",
      "Epoch: 7 \tTraining Loss: 0.000183\n",
      "Epoch: 8 \tTraining Loss: 0.000183\n",
      "Epoch: 9 \tTraining Loss: 0.000183\n",
      "Epoch: 10 \tTraining Loss: 0.000183\n",
      "Epoch: 11 \tTraining Loss: 0.000183\n",
      "Epoch: 12 \tTraining Loss: 0.000183\n",
      "Epoch: 13 \tTraining Loss: 0.000183\n",
      "Epoch: 14 \tTraining Loss: 0.000183\n",
      "Epoch: 15 \tTraining Loss: 0.000183\n",
      "Epoch: 16 \tTraining Loss: 0.000183\n",
      "Epoch: 17 \tTraining Loss: 0.000183\n",
      "Epoch: 18 \tTraining Loss: 0.000183\n",
      "Epoch: 1 \tTraining Loss: 0.000206\n",
      "Epoch: 2 \tTraining Loss: 0.000206\n",
      "Epoch: 3 \tTraining Loss: 0.000206\n",
      "Epoch: 4 \tTraining Loss: 0.000206\n",
      "Epoch: 5 \tTraining Loss: 0.000206\n",
      "Epoch: 6 \tTraining Loss: 0.000206\n",
      "Epoch: 7 \tTraining Loss: 0.000206\n",
      "Epoch: 8 \tTraining Loss: 0.000206\n",
      "Epoch: 9 \tTraining Loss: 0.000206\n",
      "Epoch: 10 \tTraining Loss: 0.000206\n",
      "Epoch: 11 \tTraining Loss: 0.000206\n",
      "Epoch: 12 \tTraining Loss: 0.000206\n",
      "Epoch: 13 \tTraining Loss: 0.000206\n",
      "Epoch: 14 \tTraining Loss: 0.000206\n",
      "Epoch: 15 \tTraining Loss: 0.000206\n",
      "Epoch: 16 \tTraining Loss: 0.000206\n",
      "Epoch: 17 \tTraining Loss: 0.000206\n",
      "Epoch: 18 \tTraining Loss: 0.000206\n",
      "Epoch: 1 \tTraining Loss: 0.000269\n",
      "Epoch: 2 \tTraining Loss: 0.000269\n",
      "Epoch: 3 \tTraining Loss: 0.000269\n",
      "Epoch: 4 \tTraining Loss: 0.000269\n",
      "Epoch: 5 \tTraining Loss: 0.000269\n",
      "Epoch: 6 \tTraining Loss: 0.000269\n",
      "Epoch: 7 \tTraining Loss: 0.000269\n",
      "Epoch: 8 \tTraining Loss: 0.000269\n",
      "Epoch: 9 \tTraining Loss: 0.000269\n",
      "Epoch: 10 \tTraining Loss: 0.000269\n",
      "Epoch: 11 \tTraining Loss: 0.000269\n",
      "Epoch: 12 \tTraining Loss: 0.000269\n",
      "Epoch: 13 \tTraining Loss: 0.000269\n",
      "Epoch: 14 \tTraining Loss: 0.000269\n",
      "Epoch: 15 \tTraining Loss: 0.000269\n",
      "Epoch: 16 \tTraining Loss: 0.000269\n",
      "Epoch: 17 \tTraining Loss: 0.000269\n",
      "Epoch: 18 \tTraining Loss: 0.000269\n",
      "Epoch: 1 \tTraining Loss: 0.000241\n",
      "Epoch: 2 \tTraining Loss: 0.000241\n",
      "Epoch: 3 \tTraining Loss: 0.000241\n",
      "Epoch: 4 \tTraining Loss: 0.000241\n",
      "Epoch: 5 \tTraining Loss: 0.000241\n",
      "Epoch: 6 \tTraining Loss: 0.000241\n",
      "Epoch: 7 \tTraining Loss: 0.000241\n",
      "Epoch: 8 \tTraining Loss: 0.000241\n",
      "Epoch: 9 \tTraining Loss: 0.000241\n",
      "Epoch: 10 \tTraining Loss: 0.000241\n",
      "Epoch: 11 \tTraining Loss: 0.000241\n",
      "Epoch: 12 \tTraining Loss: 0.000241\n",
      "Epoch: 13 \tTraining Loss: 0.000241\n",
      "Epoch: 14 \tTraining Loss: 0.000241\n",
      "Epoch: 15 \tTraining Loss: 0.000241\n",
      "Epoch: 16 \tTraining Loss: 0.000241\n",
      "Epoch: 17 \tTraining Loss: 0.000241\n",
      "Epoch: 18 \tTraining Loss: 0.000241\n",
      "Epoch: 1 \tTraining Loss: 0.000140\n",
      "Epoch: 2 \tTraining Loss: 0.000140\n",
      "Epoch: 3 \tTraining Loss: 0.000140\n",
      "Epoch: 4 \tTraining Loss: 0.000140\n",
      "Epoch: 5 \tTraining Loss: 0.000140\n",
      "Epoch: 6 \tTraining Loss: 0.000140\n",
      "Epoch: 7 \tTraining Loss: 0.000140\n",
      "Epoch: 8 \tTraining Loss: 0.000140\n",
      "Epoch: 9 \tTraining Loss: 0.000140\n",
      "Epoch: 10 \tTraining Loss: 0.000140\n",
      "Epoch: 11 \tTraining Loss: 0.000140\n",
      "Epoch: 12 \tTraining Loss: 0.000140\n",
      "Epoch: 13 \tTraining Loss: 0.000140\n",
      "Epoch: 14 \tTraining Loss: 0.000140\n",
      "Epoch: 15 \tTraining Loss: 0.000140\n",
      "Epoch: 16 \tTraining Loss: 0.000140\n",
      "Epoch: 17 \tTraining Loss: 0.000140\n",
      "Epoch: 18 \tTraining Loss: 0.000140\n",
      "Epoch: 1 \tTraining Loss: 0.000215\n",
      "Epoch: 2 \tTraining Loss: 0.000215\n",
      "Epoch: 3 \tTraining Loss: 0.000215\n",
      "Epoch: 4 \tTraining Loss: 0.000215\n",
      "Epoch: 5 \tTraining Loss: 0.000215\n",
      "Epoch: 6 \tTraining Loss: 0.000215\n",
      "Epoch: 7 \tTraining Loss: 0.000215\n",
      "Epoch: 8 \tTraining Loss: 0.000215\n",
      "Epoch: 9 \tTraining Loss: 0.000215\n",
      "Epoch: 10 \tTraining Loss: 0.000215\n",
      "Epoch: 11 \tTraining Loss: 0.000215\n",
      "Epoch: 12 \tTraining Loss: 0.000215\n",
      "Epoch: 13 \tTraining Loss: 0.000215\n",
      "Epoch: 14 \tTraining Loss: 0.000215\n",
      "Epoch: 15 \tTraining Loss: 0.000215\n",
      "Epoch: 16 \tTraining Loss: 0.000215\n",
      "Epoch: 17 \tTraining Loss: 0.000215\n",
      "Epoch: 18 \tTraining Loss: 0.000215\n",
      "Epoch: 1 \tTraining Loss: 0.000144\n",
      "Epoch: 2 \tTraining Loss: 0.000144\n",
      "Epoch: 3 \tTraining Loss: 0.000144\n",
      "Epoch: 4 \tTraining Loss: 0.000144\n",
      "Epoch: 5 \tTraining Loss: 0.000144\n",
      "Epoch: 6 \tTraining Loss: 0.000144\n",
      "Epoch: 7 \tTraining Loss: 0.000144\n",
      "Epoch: 8 \tTraining Loss: 0.000144\n",
      "Epoch: 9 \tTraining Loss: 0.000144\n",
      "Epoch: 10 \tTraining Loss: 0.000144\n",
      "Epoch: 11 \tTraining Loss: 0.000144\n",
      "Epoch: 12 \tTraining Loss: 0.000144\n",
      "Epoch: 13 \tTraining Loss: 0.000144\n",
      "Epoch: 14 \tTraining Loss: 0.000144\n",
      "Epoch: 15 \tTraining Loss: 0.000144\n",
      "Epoch: 16 \tTraining Loss: 0.000144\n",
      "Epoch: 17 \tTraining Loss: 0.000144\n",
      "Epoch: 18 \tTraining Loss: 0.000144\n",
      "Epoch: 1 \tTraining Loss: 0.000165\n",
      "Epoch: 2 \tTraining Loss: 0.000165\n",
      "Epoch: 3 \tTraining Loss: 0.000165\n",
      "Epoch: 4 \tTraining Loss: 0.000165\n",
      "Epoch: 5 \tTraining Loss: 0.000165\n",
      "Epoch: 6 \tTraining Loss: 0.000165\n",
      "Epoch: 7 \tTraining Loss: 0.000165\n",
      "Epoch: 8 \tTraining Loss: 0.000165\n",
      "Epoch: 9 \tTraining Loss: 0.000165\n",
      "Epoch: 10 \tTraining Loss: 0.000165\n",
      "Epoch: 11 \tTraining Loss: 0.000165\n",
      "Epoch: 12 \tTraining Loss: 0.000165\n",
      "Epoch: 13 \tTraining Loss: 0.000165\n",
      "Epoch: 14 \tTraining Loss: 0.000165\n",
      "Epoch: 15 \tTraining Loss: 0.000165\n",
      "Epoch: 16 \tTraining Loss: 0.000165\n",
      "Epoch: 17 \tTraining Loss: 0.000165\n",
      "Epoch: 18 \tTraining Loss: 0.000165\n",
      "Epoch: 1 \tTraining Loss: 0.000340\n",
      "Epoch: 2 \tTraining Loss: 0.000340\n",
      "Epoch: 3 \tTraining Loss: 0.000340\n",
      "Epoch: 4 \tTraining Loss: 0.000340\n",
      "Epoch: 5 \tTraining Loss: 0.000340\n",
      "Epoch: 6 \tTraining Loss: 0.000340\n",
      "Epoch: 7 \tTraining Loss: 0.000340\n",
      "Epoch: 8 \tTraining Loss: 0.000340\n",
      "Epoch: 9 \tTraining Loss: 0.000340\n",
      "Epoch: 10 \tTraining Loss: 0.000340\n",
      "Epoch: 11 \tTraining Loss: 0.000340\n",
      "Epoch: 12 \tTraining Loss: 0.000340\n",
      "Epoch: 13 \tTraining Loss: 0.000340\n",
      "Epoch: 14 \tTraining Loss: 0.000340\n",
      "Epoch: 15 \tTraining Loss: 0.000340\n",
      "Epoch: 16 \tTraining Loss: 0.000340\n",
      "Epoch: 17 \tTraining Loss: 0.000340\n",
      "Epoch: 18 \tTraining Loss: 0.000340\n",
      "Epoch: 1 \tTraining Loss: 0.000115\n",
      "Epoch: 2 \tTraining Loss: 0.000115\n",
      "Epoch: 3 \tTraining Loss: 0.000115\n",
      "Epoch: 4 \tTraining Loss: 0.000115\n",
      "Epoch: 5 \tTraining Loss: 0.000115\n",
      "Epoch: 6 \tTraining Loss: 0.000115\n",
      "Epoch: 7 \tTraining Loss: 0.000115\n",
      "Epoch: 8 \tTraining Loss: 0.000115\n",
      "Epoch: 9 \tTraining Loss: 0.000115\n",
      "Epoch: 10 \tTraining Loss: 0.000115\n",
      "Epoch: 11 \tTraining Loss: 0.000115\n",
      "Epoch: 12 \tTraining Loss: 0.000115\n",
      "Epoch: 13 \tTraining Loss: 0.000115\n",
      "Epoch: 14 \tTraining Loss: 0.000115\n",
      "Epoch: 15 \tTraining Loss: 0.000115\n",
      "Epoch: 16 \tTraining Loss: 0.000115\n",
      "Epoch: 17 \tTraining Loss: 0.000115\n",
      "Epoch: 18 \tTraining Loss: 0.000115\n",
      "Epoch: 1 \tTraining Loss: 0.000277\n",
      "Epoch: 2 \tTraining Loss: 0.000277\n",
      "Epoch: 3 \tTraining Loss: 0.000277\n",
      "Epoch: 4 \tTraining Loss: 0.000277\n",
      "Epoch: 5 \tTraining Loss: 0.000277\n",
      "Epoch: 6 \tTraining Loss: 0.000277\n",
      "Epoch: 7 \tTraining Loss: 0.000277\n",
      "Epoch: 8 \tTraining Loss: 0.000277\n",
      "Epoch: 9 \tTraining Loss: 0.000277\n",
      "Epoch: 10 \tTraining Loss: 0.000277\n",
      "Epoch: 11 \tTraining Loss: 0.000277\n",
      "Epoch: 12 \tTraining Loss: 0.000277\n",
      "Epoch: 13 \tTraining Loss: 0.000277\n",
      "Epoch: 14 \tTraining Loss: 0.000277\n",
      "Epoch: 15 \tTraining Loss: 0.000277\n",
      "Epoch: 16 \tTraining Loss: 0.000277\n",
      "Epoch: 17 \tTraining Loss: 0.000277\n",
      "Epoch: 18 \tTraining Loss: 0.000277\n",
      "Epoch: 1 \tTraining Loss: 0.000151\n",
      "Epoch: 2 \tTraining Loss: 0.000151\n",
      "Epoch: 3 \tTraining Loss: 0.000151\n",
      "Epoch: 4 \tTraining Loss: 0.000151\n",
      "Epoch: 5 \tTraining Loss: 0.000151\n",
      "Epoch: 6 \tTraining Loss: 0.000151\n",
      "Epoch: 7 \tTraining Loss: 0.000151\n",
      "Epoch: 8 \tTraining Loss: 0.000151\n",
      "Epoch: 9 \tTraining Loss: 0.000151\n",
      "Epoch: 10 \tTraining Loss: 0.000151\n",
      "Epoch: 11 \tTraining Loss: 0.000151\n",
      "Epoch: 12 \tTraining Loss: 0.000151\n",
      "Epoch: 13 \tTraining Loss: 0.000151\n",
      "Epoch: 14 \tTraining Loss: 0.000151\n",
      "Epoch: 15 \tTraining Loss: 0.000151\n",
      "Epoch: 16 \tTraining Loss: 0.000151\n",
      "Epoch: 17 \tTraining Loss: 0.000151\n",
      "Epoch: 18 \tTraining Loss: 0.000151\n",
      "Epoch: 1 \tTraining Loss: 0.000178\n",
      "Epoch: 2 \tTraining Loss: 0.000178\n",
      "Epoch: 3 \tTraining Loss: 0.000178\n",
      "Epoch: 4 \tTraining Loss: 0.000178\n",
      "Epoch: 5 \tTraining Loss: 0.000178\n",
      "Epoch: 6 \tTraining Loss: 0.000178\n",
      "Epoch: 7 \tTraining Loss: 0.000178\n",
      "Epoch: 8 \tTraining Loss: 0.000178\n",
      "Epoch: 9 \tTraining Loss: 0.000178\n",
      "Epoch: 10 \tTraining Loss: 0.000178\n",
      "Epoch: 11 \tTraining Loss: 0.000178\n",
      "Epoch: 12 \tTraining Loss: 0.000178\n",
      "Epoch: 13 \tTraining Loss: 0.000178\n",
      "Epoch: 14 \tTraining Loss: 0.000178\n",
      "Epoch: 15 \tTraining Loss: 0.000178\n",
      "Epoch: 16 \tTraining Loss: 0.000178\n",
      "Epoch: 17 \tTraining Loss: 0.000178\n",
      "Epoch: 18 \tTraining Loss: 0.000178\n",
      "Epoch: 1 \tTraining Loss: 0.000170\n",
      "Epoch: 2 \tTraining Loss: 0.000170\n",
      "Epoch: 3 \tTraining Loss: 0.000170\n",
      "Epoch: 4 \tTraining Loss: 0.000170\n",
      "Epoch: 5 \tTraining Loss: 0.000170\n",
      "Epoch: 6 \tTraining Loss: 0.000170\n",
      "Epoch: 7 \tTraining Loss: 0.000170\n",
      "Epoch: 8 \tTraining Loss: 0.000170\n",
      "Epoch: 9 \tTraining Loss: 0.000170\n",
      "Epoch: 10 \tTraining Loss: 0.000170\n",
      "Epoch: 11 \tTraining Loss: 0.000170\n",
      "Epoch: 12 \tTraining Loss: 0.000170\n",
      "Epoch: 13 \tTraining Loss: 0.000170\n",
      "Epoch: 14 \tTraining Loss: 0.000170\n",
      "Epoch: 15 \tTraining Loss: 0.000170\n",
      "Epoch: 16 \tTraining Loss: 0.000170\n",
      "Epoch: 17 \tTraining Loss: 0.000170\n",
      "Epoch: 18 \tTraining Loss: 0.000170\n",
      "Epoch: 1 \tTraining Loss: 0.000154\n",
      "Epoch: 2 \tTraining Loss: 0.000154\n",
      "Epoch: 3 \tTraining Loss: 0.000154\n",
      "Epoch: 4 \tTraining Loss: 0.000154\n",
      "Epoch: 5 \tTraining Loss: 0.000154\n",
      "Epoch: 6 \tTraining Loss: 0.000154\n",
      "Epoch: 7 \tTraining Loss: 0.000154\n",
      "Epoch: 8 \tTraining Loss: 0.000154\n",
      "Epoch: 9 \tTraining Loss: 0.000154\n",
      "Epoch: 10 \tTraining Loss: 0.000154\n",
      "Epoch: 11 \tTraining Loss: 0.000154\n",
      "Epoch: 12 \tTraining Loss: 0.000154\n",
      "Epoch: 13 \tTraining Loss: 0.000154\n",
      "Epoch: 14 \tTraining Loss: 0.000154\n",
      "Epoch: 15 \tTraining Loss: 0.000154\n",
      "Epoch: 16 \tTraining Loss: 0.000154\n",
      "Epoch: 17 \tTraining Loss: 0.000154\n",
      "Epoch: 18 \tTraining Loss: 0.000154\n",
      "Epoch: 1 \tTraining Loss: 0.000221\n",
      "Epoch: 2 \tTraining Loss: 0.000221\n",
      "Epoch: 3 \tTraining Loss: 0.000221\n",
      "Epoch: 4 \tTraining Loss: 0.000221\n",
      "Epoch: 5 \tTraining Loss: 0.000221\n",
      "Epoch: 6 \tTraining Loss: 0.000221\n",
      "Epoch: 7 \tTraining Loss: 0.000221\n",
      "Epoch: 8 \tTraining Loss: 0.000221\n",
      "Epoch: 9 \tTraining Loss: 0.000221\n",
      "Epoch: 10 \tTraining Loss: 0.000221\n",
      "Epoch: 11 \tTraining Loss: 0.000221\n",
      "Epoch: 12 \tTraining Loss: 0.000221\n",
      "Epoch: 13 \tTraining Loss: 0.000221\n",
      "Epoch: 14 \tTraining Loss: 0.000221\n",
      "Epoch: 15 \tTraining Loss: 0.000221\n",
      "Epoch: 16 \tTraining Loss: 0.000221\n",
      "Epoch: 17 \tTraining Loss: 0.000221\n",
      "Epoch: 18 \tTraining Loss: 0.000221\n",
      "Epoch: 1 \tTraining Loss: 0.000181\n",
      "Epoch: 2 \tTraining Loss: 0.000181\n",
      "Epoch: 3 \tTraining Loss: 0.000181\n",
      "Epoch: 4 \tTraining Loss: 0.000181\n",
      "Epoch: 5 \tTraining Loss: 0.000181\n",
      "Epoch: 6 \tTraining Loss: 0.000181\n",
      "Epoch: 7 \tTraining Loss: 0.000181\n",
      "Epoch: 8 \tTraining Loss: 0.000181\n",
      "Epoch: 9 \tTraining Loss: 0.000181\n",
      "Epoch: 10 \tTraining Loss: 0.000181\n",
      "Epoch: 11 \tTraining Loss: 0.000181\n",
      "Epoch: 12 \tTraining Loss: 0.000181\n",
      "Epoch: 13 \tTraining Loss: 0.000181\n",
      "Epoch: 14 \tTraining Loss: 0.000181\n",
      "Epoch: 15 \tTraining Loss: 0.000181\n",
      "Epoch: 16 \tTraining Loss: 0.000181\n",
      "Epoch: 17 \tTraining Loss: 0.000181\n",
      "Epoch: 18 \tTraining Loss: 0.000181\n",
      "Epoch: 1 \tTraining Loss: 0.000148\n",
      "Epoch: 2 \tTraining Loss: 0.000148\n",
      "Epoch: 3 \tTraining Loss: 0.000148\n",
      "Epoch: 4 \tTraining Loss: 0.000148\n",
      "Epoch: 5 \tTraining Loss: 0.000148\n",
      "Epoch: 6 \tTraining Loss: 0.000148\n",
      "Epoch: 7 \tTraining Loss: 0.000148\n",
      "Epoch: 8 \tTraining Loss: 0.000148\n",
      "Epoch: 9 \tTraining Loss: 0.000148\n",
      "Epoch: 10 \tTraining Loss: 0.000148\n",
      "Epoch: 11 \tTraining Loss: 0.000148\n",
      "Epoch: 12 \tTraining Loss: 0.000148\n",
      "Epoch: 13 \tTraining Loss: 0.000148\n",
      "Epoch: 14 \tTraining Loss: 0.000148\n",
      "Epoch: 15 \tTraining Loss: 0.000148\n",
      "Epoch: 16 \tTraining Loss: 0.000148\n",
      "Epoch: 17 \tTraining Loss: 0.000148\n",
      "Epoch: 18 \tTraining Loss: 0.000148\n",
      "Epoch: 1 \tTraining Loss: 0.000193\n",
      "Epoch: 2 \tTraining Loss: 0.000193\n",
      "Epoch: 3 \tTraining Loss: 0.000193\n",
      "Epoch: 4 \tTraining Loss: 0.000193\n",
      "Epoch: 5 \tTraining Loss: 0.000193\n",
      "Epoch: 6 \tTraining Loss: 0.000193\n",
      "Epoch: 7 \tTraining Loss: 0.000193\n",
      "Epoch: 8 \tTraining Loss: 0.000193\n",
      "Epoch: 9 \tTraining Loss: 0.000193\n",
      "Epoch: 10 \tTraining Loss: 0.000193\n",
      "Epoch: 11 \tTraining Loss: 0.000193\n",
      "Epoch: 12 \tTraining Loss: 0.000193\n",
      "Epoch: 13 \tTraining Loss: 0.000193\n",
      "Epoch: 14 \tTraining Loss: 0.000193\n",
      "Epoch: 15 \tTraining Loss: 0.000193\n",
      "Epoch: 16 \tTraining Loss: 0.000193\n",
      "Epoch: 17 \tTraining Loss: 0.000193\n",
      "Epoch: 18 \tTraining Loss: 0.000193\n",
      "Epoch: 1 \tTraining Loss: 0.000138\n",
      "Epoch: 2 \tTraining Loss: 0.000138\n",
      "Epoch: 3 \tTraining Loss: 0.000138\n",
      "Epoch: 4 \tTraining Loss: 0.000138\n",
      "Epoch: 5 \tTraining Loss: 0.000138\n",
      "Epoch: 6 \tTraining Loss: 0.000138\n",
      "Epoch: 7 \tTraining Loss: 0.000138\n",
      "Epoch: 8 \tTraining Loss: 0.000138\n",
      "Epoch: 9 \tTraining Loss: 0.000138\n",
      "Epoch: 10 \tTraining Loss: 0.000138\n",
      "Epoch: 11 \tTraining Loss: 0.000138\n",
      "Epoch: 12 \tTraining Loss: 0.000138\n",
      "Epoch: 13 \tTraining Loss: 0.000138\n",
      "Epoch: 14 \tTraining Loss: 0.000138\n",
      "Epoch: 15 \tTraining Loss: 0.000138\n",
      "Epoch: 16 \tTraining Loss: 0.000138\n",
      "Epoch: 17 \tTraining Loss: 0.000138\n",
      "Epoch: 18 \tTraining Loss: 0.000138\n",
      "Epoch: 1 \tTraining Loss: 0.000145\n",
      "Epoch: 2 \tTraining Loss: 0.000145\n",
      "Epoch: 3 \tTraining Loss: 0.000145\n",
      "Epoch: 4 \tTraining Loss: 0.000145\n",
      "Epoch: 5 \tTraining Loss: 0.000145\n",
      "Epoch: 6 \tTraining Loss: 0.000145\n",
      "Epoch: 7 \tTraining Loss: 0.000145\n",
      "Epoch: 8 \tTraining Loss: 0.000145\n",
      "Epoch: 9 \tTraining Loss: 0.000145\n",
      "Epoch: 10 \tTraining Loss: 0.000145\n",
      "Epoch: 11 \tTraining Loss: 0.000145\n",
      "Epoch: 12 \tTraining Loss: 0.000145\n",
      "Epoch: 13 \tTraining Loss: 0.000145\n",
      "Epoch: 14 \tTraining Loss: 0.000145\n",
      "Epoch: 15 \tTraining Loss: 0.000145\n",
      "Epoch: 16 \tTraining Loss: 0.000145\n",
      "Epoch: 17 \tTraining Loss: 0.000145\n",
      "Epoch: 18 \tTraining Loss: 0.000145\n",
      "Epoch: 1 \tTraining Loss: 0.000311\n",
      "Epoch: 2 \tTraining Loss: 0.000311\n",
      "Epoch: 3 \tTraining Loss: 0.000311\n",
      "Epoch: 4 \tTraining Loss: 0.000311\n",
      "Epoch: 5 \tTraining Loss: 0.000311\n",
      "Epoch: 6 \tTraining Loss: 0.000311\n",
      "Epoch: 7 \tTraining Loss: 0.000311\n",
      "Epoch: 8 \tTraining Loss: 0.000311\n",
      "Epoch: 9 \tTraining Loss: 0.000311\n",
      "Epoch: 10 \tTraining Loss: 0.000311\n",
      "Epoch: 11 \tTraining Loss: 0.000311\n",
      "Epoch: 12 \tTraining Loss: 0.000311\n",
      "Epoch: 13 \tTraining Loss: 0.000311\n",
      "Epoch: 14 \tTraining Loss: 0.000311\n",
      "Epoch: 15 \tTraining Loss: 0.000311\n",
      "Epoch: 16 \tTraining Loss: 0.000311\n",
      "Epoch: 17 \tTraining Loss: 0.000311\n",
      "Epoch: 18 \tTraining Loss: 0.000311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000191\n",
      "Epoch: 2 \tTraining Loss: 0.000191\n",
      "Epoch: 3 \tTraining Loss: 0.000191\n",
      "Epoch: 4 \tTraining Loss: 0.000191\n",
      "Epoch: 5 \tTraining Loss: 0.000191\n",
      "Epoch: 6 \tTraining Loss: 0.000191\n",
      "Epoch: 7 \tTraining Loss: 0.000191\n",
      "Epoch: 8 \tTraining Loss: 0.000191\n",
      "Epoch: 9 \tTraining Loss: 0.000191\n",
      "Epoch: 10 \tTraining Loss: 0.000191\n",
      "Epoch: 11 \tTraining Loss: 0.000191\n",
      "Epoch: 12 \tTraining Loss: 0.000191\n",
      "Epoch: 13 \tTraining Loss: 0.000191\n",
      "Epoch: 14 \tTraining Loss: 0.000191\n",
      "Epoch: 15 \tTraining Loss: 0.000191\n",
      "Epoch: 16 \tTraining Loss: 0.000191\n",
      "Epoch: 17 \tTraining Loss: 0.000191\n",
      "Epoch: 18 \tTraining Loss: 0.000191\n",
      "Epoch: 1 \tTraining Loss: 0.000270\n",
      "Epoch: 2 \tTraining Loss: 0.000270\n",
      "Epoch: 3 \tTraining Loss: 0.000270\n",
      "Epoch: 4 \tTraining Loss: 0.000270\n",
      "Epoch: 5 \tTraining Loss: 0.000270\n",
      "Epoch: 6 \tTraining Loss: 0.000270\n",
      "Epoch: 7 \tTraining Loss: 0.000270\n",
      "Epoch: 8 \tTraining Loss: 0.000270\n",
      "Epoch: 9 \tTraining Loss: 0.000270\n",
      "Epoch: 10 \tTraining Loss: 0.000270\n",
      "Epoch: 11 \tTraining Loss: 0.000270\n",
      "Epoch: 12 \tTraining Loss: 0.000270\n",
      "Epoch: 13 \tTraining Loss: 0.000270\n",
      "Epoch: 14 \tTraining Loss: 0.000270\n",
      "Epoch: 15 \tTraining Loss: 0.000270\n",
      "Epoch: 16 \tTraining Loss: 0.000270\n",
      "Epoch: 17 \tTraining Loss: 0.000270\n",
      "Epoch: 18 \tTraining Loss: 0.000270\n",
      "Epoch: 1 \tTraining Loss: 0.000190\n",
      "Epoch: 2 \tTraining Loss: 0.000190\n",
      "Epoch: 3 \tTraining Loss: 0.000190\n",
      "Epoch: 4 \tTraining Loss: 0.000190\n",
      "Epoch: 5 \tTraining Loss: 0.000190\n",
      "Epoch: 6 \tTraining Loss: 0.000190\n",
      "Epoch: 7 \tTraining Loss: 0.000190\n",
      "Epoch: 8 \tTraining Loss: 0.000190\n",
      "Epoch: 9 \tTraining Loss: 0.000190\n",
      "Epoch: 10 \tTraining Loss: 0.000190\n",
      "Epoch: 11 \tTraining Loss: 0.000190\n",
      "Epoch: 12 \tTraining Loss: 0.000190\n",
      "Epoch: 13 \tTraining Loss: 0.000190\n",
      "Epoch: 14 \tTraining Loss: 0.000190\n",
      "Epoch: 15 \tTraining Loss: 0.000190\n",
      "Epoch: 16 \tTraining Loss: 0.000190\n",
      "Epoch: 17 \tTraining Loss: 0.000190\n",
      "Epoch: 18 \tTraining Loss: 0.000190\n",
      "Epoch: 1 \tTraining Loss: 0.000168\n",
      "Epoch: 2 \tTraining Loss: 0.000168\n",
      "Epoch: 3 \tTraining Loss: 0.000168\n",
      "Epoch: 4 \tTraining Loss: 0.000168\n",
      "Epoch: 5 \tTraining Loss: 0.000168\n",
      "Epoch: 6 \tTraining Loss: 0.000168\n",
      "Epoch: 7 \tTraining Loss: 0.000168\n",
      "Epoch: 8 \tTraining Loss: 0.000168\n",
      "Epoch: 9 \tTraining Loss: 0.000168\n",
      "Epoch: 10 \tTraining Loss: 0.000168\n",
      "Epoch: 11 \tTraining Loss: 0.000168\n",
      "Epoch: 12 \tTraining Loss: 0.000168\n",
      "Epoch: 13 \tTraining Loss: 0.000168\n",
      "Epoch: 14 \tTraining Loss: 0.000168\n",
      "Epoch: 15 \tTraining Loss: 0.000168\n",
      "Epoch: 16 \tTraining Loss: 0.000168\n",
      "Epoch: 17 \tTraining Loss: 0.000168\n",
      "Epoch: 18 \tTraining Loss: 0.000168\n",
      "Epoch: 1 \tTraining Loss: 0.000349\n",
      "Epoch: 2 \tTraining Loss: 0.000349\n",
      "Epoch: 3 \tTraining Loss: 0.000349\n",
      "Epoch: 4 \tTraining Loss: 0.000349\n",
      "Epoch: 5 \tTraining Loss: 0.000349\n",
      "Epoch: 6 \tTraining Loss: 0.000349\n",
      "Epoch: 7 \tTraining Loss: 0.000349\n",
      "Epoch: 8 \tTraining Loss: 0.000349\n",
      "Epoch: 9 \tTraining Loss: 0.000349\n",
      "Epoch: 10 \tTraining Loss: 0.000349\n",
      "Epoch: 11 \tTraining Loss: 0.000349\n",
      "Epoch: 12 \tTraining Loss: 0.000349\n",
      "Epoch: 13 \tTraining Loss: 0.000349\n",
      "Epoch: 14 \tTraining Loss: 0.000349\n",
      "Epoch: 15 \tTraining Loss: 0.000349\n",
      "Epoch: 16 \tTraining Loss: 0.000349\n",
      "Epoch: 17 \tTraining Loss: 0.000349\n",
      "Epoch: 18 \tTraining Loss: 0.000349\n",
      "Epoch: 1 \tTraining Loss: 0.000230\n",
      "Epoch: 2 \tTraining Loss: 0.000230\n",
      "Epoch: 3 \tTraining Loss: 0.000230\n",
      "Epoch: 4 \tTraining Loss: 0.000230\n",
      "Epoch: 5 \tTraining Loss: 0.000230\n",
      "Epoch: 6 \tTraining Loss: 0.000230\n",
      "Epoch: 7 \tTraining Loss: 0.000230\n",
      "Epoch: 8 \tTraining Loss: 0.000230\n",
      "Epoch: 9 \tTraining Loss: 0.000230\n",
      "Epoch: 10 \tTraining Loss: 0.000230\n",
      "Epoch: 11 \tTraining Loss: 0.000230\n",
      "Epoch: 12 \tTraining Loss: 0.000230\n",
      "Epoch: 13 \tTraining Loss: 0.000230\n",
      "Epoch: 14 \tTraining Loss: 0.000230\n",
      "Epoch: 15 \tTraining Loss: 0.000230\n",
      "Epoch: 16 \tTraining Loss: 0.000230\n",
      "Epoch: 17 \tTraining Loss: 0.000230\n",
      "Epoch: 18 \tTraining Loss: 0.000230\n",
      "Epoch: 1 \tTraining Loss: 0.000156\n",
      "Epoch: 2 \tTraining Loss: 0.000156\n",
      "Epoch: 3 \tTraining Loss: 0.000156\n",
      "Epoch: 4 \tTraining Loss: 0.000156\n",
      "Epoch: 5 \tTraining Loss: 0.000156\n",
      "Epoch: 6 \tTraining Loss: 0.000156\n",
      "Epoch: 7 \tTraining Loss: 0.000156\n",
      "Epoch: 8 \tTraining Loss: 0.000156\n",
      "Epoch: 9 \tTraining Loss: 0.000156\n",
      "Epoch: 10 \tTraining Loss: 0.000156\n",
      "Epoch: 11 \tTraining Loss: 0.000156\n",
      "Epoch: 12 \tTraining Loss: 0.000156\n",
      "Epoch: 13 \tTraining Loss: 0.000156\n",
      "Epoch: 14 \tTraining Loss: 0.000156\n",
      "Epoch: 15 \tTraining Loss: 0.000156\n",
      "Epoch: 16 \tTraining Loss: 0.000156\n",
      "Epoch: 17 \tTraining Loss: 0.000156\n",
      "Epoch: 18 \tTraining Loss: 0.000156\n",
      "Epoch: 1 \tTraining Loss: 0.000267\n",
      "Epoch: 2 \tTraining Loss: 0.000267\n",
      "Epoch: 3 \tTraining Loss: 0.000267\n",
      "Epoch: 4 \tTraining Loss: 0.000267\n",
      "Epoch: 5 \tTraining Loss: 0.000267\n",
      "Epoch: 6 \tTraining Loss: 0.000267\n",
      "Epoch: 7 \tTraining Loss: 0.000267\n",
      "Epoch: 8 \tTraining Loss: 0.000267\n",
      "Epoch: 9 \tTraining Loss: 0.000267\n",
      "Epoch: 10 \tTraining Loss: 0.000267\n",
      "Epoch: 11 \tTraining Loss: 0.000267\n",
      "Epoch: 12 \tTraining Loss: 0.000267\n",
      "Epoch: 13 \tTraining Loss: 0.000267\n",
      "Epoch: 14 \tTraining Loss: 0.000267\n",
      "Epoch: 15 \tTraining Loss: 0.000267\n",
      "Epoch: 16 \tTraining Loss: 0.000267\n",
      "Epoch: 17 \tTraining Loss: 0.000267\n",
      "Epoch: 18 \tTraining Loss: 0.000267\n",
      "Epoch: 1 \tTraining Loss: 0.000250\n",
      "Epoch: 2 \tTraining Loss: 0.000250\n",
      "Epoch: 3 \tTraining Loss: 0.000250\n",
      "Epoch: 4 \tTraining Loss: 0.000250\n",
      "Epoch: 5 \tTraining Loss: 0.000250\n",
      "Epoch: 6 \tTraining Loss: 0.000250\n",
      "Epoch: 7 \tTraining Loss: 0.000250\n",
      "Epoch: 8 \tTraining Loss: 0.000250\n",
      "Epoch: 9 \tTraining Loss: 0.000250\n",
      "Epoch: 10 \tTraining Loss: 0.000250\n",
      "Epoch: 11 \tTraining Loss: 0.000250\n",
      "Epoch: 12 \tTraining Loss: 0.000250\n",
      "Epoch: 13 \tTraining Loss: 0.000250\n",
      "Epoch: 14 \tTraining Loss: 0.000250\n",
      "Epoch: 15 \tTraining Loss: 0.000250\n",
      "Epoch: 16 \tTraining Loss: 0.000250\n",
      "Epoch: 17 \tTraining Loss: 0.000250\n",
      "Epoch: 18 \tTraining Loss: 0.000250\n",
      "Epoch: 1 \tTraining Loss: 0.000301\n",
      "Epoch: 2 \tTraining Loss: 0.000301\n",
      "Epoch: 3 \tTraining Loss: 0.000301\n",
      "Epoch: 4 \tTraining Loss: 0.000301\n",
      "Epoch: 5 \tTraining Loss: 0.000301\n",
      "Epoch: 6 \tTraining Loss: 0.000301\n",
      "Epoch: 7 \tTraining Loss: 0.000301\n",
      "Epoch: 8 \tTraining Loss: 0.000301\n",
      "Epoch: 9 \tTraining Loss: 0.000301\n",
      "Epoch: 10 \tTraining Loss: 0.000301\n",
      "Epoch: 11 \tTraining Loss: 0.000301\n",
      "Epoch: 12 \tTraining Loss: 0.000301\n",
      "Epoch: 13 \tTraining Loss: 0.000301\n",
      "Epoch: 14 \tTraining Loss: 0.000301\n",
      "Epoch: 15 \tTraining Loss: 0.000301\n",
      "Epoch: 16 \tTraining Loss: 0.000301\n",
      "Epoch: 17 \tTraining Loss: 0.000301\n",
      "Epoch: 18 \tTraining Loss: 0.000301\n",
      "Epoch: 1 \tTraining Loss: 0.000180\n",
      "Epoch: 2 \tTraining Loss: 0.000180\n",
      "Epoch: 3 \tTraining Loss: 0.000180\n",
      "Epoch: 4 \tTraining Loss: 0.000180\n",
      "Epoch: 5 \tTraining Loss: 0.000180\n",
      "Epoch: 6 \tTraining Loss: 0.000180\n",
      "Epoch: 7 \tTraining Loss: 0.000180\n",
      "Epoch: 8 \tTraining Loss: 0.000180\n",
      "Epoch: 9 \tTraining Loss: 0.000180\n",
      "Epoch: 10 \tTraining Loss: 0.000180\n",
      "Epoch: 11 \tTraining Loss: 0.000180\n",
      "Epoch: 12 \tTraining Loss: 0.000180\n",
      "Epoch: 13 \tTraining Loss: 0.000180\n",
      "Epoch: 14 \tTraining Loss: 0.000180\n",
      "Epoch: 15 \tTraining Loss: 0.000180\n",
      "Epoch: 16 \tTraining Loss: 0.000180\n",
      "Epoch: 17 \tTraining Loss: 0.000180\n",
      "Epoch: 18 \tTraining Loss: 0.000180\n",
      "Epoch: 1 \tTraining Loss: 0.000273\n",
      "Epoch: 2 \tTraining Loss: 0.000273\n",
      "Epoch: 3 \tTraining Loss: 0.000273\n",
      "Epoch: 4 \tTraining Loss: 0.000273\n",
      "Epoch: 5 \tTraining Loss: 0.000273\n",
      "Epoch: 6 \tTraining Loss: 0.000273\n",
      "Epoch: 7 \tTraining Loss: 0.000273\n",
      "Epoch: 8 \tTraining Loss: 0.000273\n",
      "Epoch: 9 \tTraining Loss: 0.000273\n",
      "Epoch: 10 \tTraining Loss: 0.000273\n",
      "Epoch: 11 \tTraining Loss: 0.000273\n",
      "Epoch: 12 \tTraining Loss: 0.000273\n",
      "Epoch: 13 \tTraining Loss: 0.000273\n",
      "Epoch: 14 \tTraining Loss: 0.000273\n",
      "Epoch: 15 \tTraining Loss: 0.000273\n",
      "Epoch: 16 \tTraining Loss: 0.000273\n",
      "Epoch: 17 \tTraining Loss: 0.000273\n",
      "Epoch: 18 \tTraining Loss: 0.000273\n",
      "Epoch: 1 \tTraining Loss: 0.000191\n",
      "Epoch: 2 \tTraining Loss: 0.000191\n",
      "Epoch: 3 \tTraining Loss: 0.000191\n",
      "Epoch: 4 \tTraining Loss: 0.000191\n",
      "Epoch: 5 \tTraining Loss: 0.000191\n",
      "Epoch: 6 \tTraining Loss: 0.000191\n",
      "Epoch: 7 \tTraining Loss: 0.000191\n",
      "Epoch: 8 \tTraining Loss: 0.000191\n",
      "Epoch: 9 \tTraining Loss: 0.000191\n",
      "Epoch: 10 \tTraining Loss: 0.000191\n",
      "Epoch: 11 \tTraining Loss: 0.000191\n",
      "Epoch: 12 \tTraining Loss: 0.000191\n",
      "Epoch: 13 \tTraining Loss: 0.000191\n",
      "Epoch: 14 \tTraining Loss: 0.000191\n",
      "Epoch: 15 \tTraining Loss: 0.000191\n",
      "Epoch: 16 \tTraining Loss: 0.000191\n",
      "Epoch: 17 \tTraining Loss: 0.000191\n",
      "Epoch: 18 \tTraining Loss: 0.000191\n",
      "Epoch: 1 \tTraining Loss: 0.000099\n",
      "Epoch: 2 \tTraining Loss: 0.000099\n",
      "Epoch: 3 \tTraining Loss: 0.000099\n",
      "Epoch: 4 \tTraining Loss: 0.000099\n",
      "Epoch: 5 \tTraining Loss: 0.000099\n",
      "Epoch: 6 \tTraining Loss: 0.000099\n",
      "Epoch: 7 \tTraining Loss: 0.000099\n",
      "Epoch: 8 \tTraining Loss: 0.000099\n",
      "Epoch: 9 \tTraining Loss: 0.000099\n",
      "Epoch: 10 \tTraining Loss: 0.000099\n",
      "Epoch: 11 \tTraining Loss: 0.000099\n",
      "Epoch: 12 \tTraining Loss: 0.000099\n",
      "Epoch: 13 \tTraining Loss: 0.000099\n",
      "Epoch: 14 \tTraining Loss: 0.000099\n",
      "Epoch: 15 \tTraining Loss: 0.000099\n",
      "Epoch: 16 \tTraining Loss: 0.000099\n",
      "Epoch: 17 \tTraining Loss: 0.000099\n",
      "Epoch: 18 \tTraining Loss: 0.000099\n",
      "Epoch: 1 \tTraining Loss: 0.000239\n",
      "Epoch: 2 \tTraining Loss: 0.000239\n",
      "Epoch: 3 \tTraining Loss: 0.000239\n",
      "Epoch: 4 \tTraining Loss: 0.000239\n",
      "Epoch: 5 \tTraining Loss: 0.000239\n",
      "Epoch: 6 \tTraining Loss: 0.000239\n",
      "Epoch: 7 \tTraining Loss: 0.000239\n",
      "Epoch: 8 \tTraining Loss: 0.000239\n",
      "Epoch: 9 \tTraining Loss: 0.000239\n",
      "Epoch: 10 \tTraining Loss: 0.000239\n",
      "Epoch: 11 \tTraining Loss: 0.000239\n",
      "Epoch: 12 \tTraining Loss: 0.000239\n",
      "Epoch: 13 \tTraining Loss: 0.000239\n",
      "Epoch: 14 \tTraining Loss: 0.000239\n",
      "Epoch: 15 \tTraining Loss: 0.000239\n",
      "Epoch: 16 \tTraining Loss: 0.000239\n",
      "Epoch: 17 \tTraining Loss: 0.000239\n",
      "Epoch: 18 \tTraining Loss: 0.000239\n",
      "Epoch: 1 \tTraining Loss: 0.000359\n",
      "Epoch: 2 \tTraining Loss: 0.000359\n",
      "Epoch: 3 \tTraining Loss: 0.000359\n",
      "Epoch: 4 \tTraining Loss: 0.000359\n",
      "Epoch: 5 \tTraining Loss: 0.000359\n",
      "Epoch: 6 \tTraining Loss: 0.000359\n",
      "Epoch: 7 \tTraining Loss: 0.000359\n",
      "Epoch: 8 \tTraining Loss: 0.000359\n",
      "Epoch: 9 \tTraining Loss: 0.000359\n",
      "Epoch: 10 \tTraining Loss: 0.000359\n",
      "Epoch: 11 \tTraining Loss: 0.000359\n",
      "Epoch: 12 \tTraining Loss: 0.000359\n",
      "Epoch: 13 \tTraining Loss: 0.000359\n",
      "Epoch: 14 \tTraining Loss: 0.000359\n",
      "Epoch: 15 \tTraining Loss: 0.000359\n",
      "Epoch: 16 \tTraining Loss: 0.000359\n",
      "Epoch: 17 \tTraining Loss: 0.000359\n",
      "Epoch: 18 \tTraining Loss: 0.000359\n",
      "Epoch: 1 \tTraining Loss: 0.000288\n",
      "Epoch: 2 \tTraining Loss: 0.000288\n",
      "Epoch: 3 \tTraining Loss: 0.000288\n",
      "Epoch: 4 \tTraining Loss: 0.000288\n",
      "Epoch: 5 \tTraining Loss: 0.000288\n",
      "Epoch: 6 \tTraining Loss: 0.000288\n",
      "Epoch: 7 \tTraining Loss: 0.000288\n",
      "Epoch: 8 \tTraining Loss: 0.000288\n",
      "Epoch: 9 \tTraining Loss: 0.000288\n",
      "Epoch: 10 \tTraining Loss: 0.000288\n",
      "Epoch: 11 \tTraining Loss: 0.000288\n",
      "Epoch: 12 \tTraining Loss: 0.000288\n",
      "Epoch: 13 \tTraining Loss: 0.000288\n",
      "Epoch: 14 \tTraining Loss: 0.000288\n",
      "Epoch: 15 \tTraining Loss: 0.000288\n",
      "Epoch: 16 \tTraining Loss: 0.000288\n",
      "Epoch: 17 \tTraining Loss: 0.000288\n",
      "Epoch: 18 \tTraining Loss: 0.000288\n",
      "Epoch: 1 \tTraining Loss: 0.000345\n",
      "Epoch: 2 \tTraining Loss: 0.000345\n",
      "Epoch: 3 \tTraining Loss: 0.000345\n",
      "Epoch: 4 \tTraining Loss: 0.000345\n",
      "Epoch: 5 \tTraining Loss: 0.000345\n",
      "Epoch: 6 \tTraining Loss: 0.000345\n",
      "Epoch: 7 \tTraining Loss: 0.000345\n",
      "Epoch: 8 \tTraining Loss: 0.000345\n",
      "Epoch: 9 \tTraining Loss: 0.000345\n",
      "Epoch: 10 \tTraining Loss: 0.000345\n",
      "Epoch: 11 \tTraining Loss: 0.000345\n",
      "Epoch: 12 \tTraining Loss: 0.000345\n",
      "Epoch: 13 \tTraining Loss: 0.000345\n",
      "Epoch: 14 \tTraining Loss: 0.000345\n",
      "Epoch: 15 \tTraining Loss: 0.000345\n",
      "Epoch: 16 \tTraining Loss: 0.000345\n",
      "Epoch: 17 \tTraining Loss: 0.000345\n",
      "Epoch: 18 \tTraining Loss: 0.000345\n",
      "Epoch: 1 \tTraining Loss: 0.000216\n",
      "Epoch: 2 \tTraining Loss: 0.000216\n",
      "Epoch: 3 \tTraining Loss: 0.000216\n",
      "Epoch: 4 \tTraining Loss: 0.000216\n",
      "Epoch: 5 \tTraining Loss: 0.000216\n",
      "Epoch: 6 \tTraining Loss: 0.000216\n",
      "Epoch: 7 \tTraining Loss: 0.000216\n",
      "Epoch: 8 \tTraining Loss: 0.000216\n",
      "Epoch: 9 \tTraining Loss: 0.000216\n",
      "Epoch: 10 \tTraining Loss: 0.000216\n",
      "Epoch: 11 \tTraining Loss: 0.000216\n",
      "Epoch: 12 \tTraining Loss: 0.000216\n",
      "Epoch: 13 \tTraining Loss: 0.000216\n",
      "Epoch: 14 \tTraining Loss: 0.000216\n",
      "Epoch: 15 \tTraining Loss: 0.000216\n",
      "Epoch: 16 \tTraining Loss: 0.000216\n",
      "Epoch: 17 \tTraining Loss: 0.000216\n",
      "Epoch: 18 \tTraining Loss: 0.000216\n",
      "Epoch: 1 \tTraining Loss: 0.000216\n",
      "Epoch: 2 \tTraining Loss: 0.000216\n",
      "Epoch: 3 \tTraining Loss: 0.000216\n",
      "Epoch: 4 \tTraining Loss: 0.000216\n",
      "Epoch: 5 \tTraining Loss: 0.000216\n",
      "Epoch: 6 \tTraining Loss: 0.000216\n",
      "Epoch: 7 \tTraining Loss: 0.000216\n",
      "Epoch: 8 \tTraining Loss: 0.000216\n",
      "Epoch: 9 \tTraining Loss: 0.000216\n",
      "Epoch: 10 \tTraining Loss: 0.000216\n",
      "Epoch: 11 \tTraining Loss: 0.000216\n",
      "Epoch: 12 \tTraining Loss: 0.000216\n",
      "Epoch: 13 \tTraining Loss: 0.000216\n",
      "Epoch: 14 \tTraining Loss: 0.000216\n",
      "Epoch: 15 \tTraining Loss: 0.000216\n",
      "Epoch: 16 \tTraining Loss: 0.000216\n",
      "Epoch: 17 \tTraining Loss: 0.000216\n",
      "Epoch: 18 \tTraining Loss: 0.000216\n",
      "Epoch: 1 \tTraining Loss: 0.000166\n",
      "Epoch: 2 \tTraining Loss: 0.000166\n",
      "Epoch: 3 \tTraining Loss: 0.000166\n",
      "Epoch: 4 \tTraining Loss: 0.000166\n",
      "Epoch: 5 \tTraining Loss: 0.000166\n",
      "Epoch: 6 \tTraining Loss: 0.000166\n",
      "Epoch: 7 \tTraining Loss: 0.000166\n",
      "Epoch: 8 \tTraining Loss: 0.000166\n",
      "Epoch: 9 \tTraining Loss: 0.000166\n",
      "Epoch: 10 \tTraining Loss: 0.000166\n",
      "Epoch: 11 \tTraining Loss: 0.000166\n",
      "Epoch: 12 \tTraining Loss: 0.000166\n",
      "Epoch: 13 \tTraining Loss: 0.000166\n",
      "Epoch: 14 \tTraining Loss: 0.000166\n",
      "Epoch: 15 \tTraining Loss: 0.000166\n",
      "Epoch: 16 \tTraining Loss: 0.000166\n",
      "Epoch: 17 \tTraining Loss: 0.000166\n",
      "Epoch: 18 \tTraining Loss: 0.000166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000166\n",
      "Epoch: 2 \tTraining Loss: 0.000166\n",
      "Epoch: 3 \tTraining Loss: 0.000166\n",
      "Epoch: 4 \tTraining Loss: 0.000166\n",
      "Epoch: 5 \tTraining Loss: 0.000166\n",
      "Epoch: 6 \tTraining Loss: 0.000166\n",
      "Epoch: 7 \tTraining Loss: 0.000166\n",
      "Epoch: 8 \tTraining Loss: 0.000166\n",
      "Epoch: 9 \tTraining Loss: 0.000166\n",
      "Epoch: 10 \tTraining Loss: 0.000166\n",
      "Epoch: 11 \tTraining Loss: 0.000166\n",
      "Epoch: 12 \tTraining Loss: 0.000166\n",
      "Epoch: 13 \tTraining Loss: 0.000166\n",
      "Epoch: 14 \tTraining Loss: 0.000166\n",
      "Epoch: 15 \tTraining Loss: 0.000166\n",
      "Epoch: 16 \tTraining Loss: 0.000166\n",
      "Epoch: 17 \tTraining Loss: 0.000166\n",
      "Epoch: 18 \tTraining Loss: 0.000166\n",
      "Epoch: 1 \tTraining Loss: 0.000155\n",
      "Epoch: 2 \tTraining Loss: 0.000155\n",
      "Epoch: 3 \tTraining Loss: 0.000155\n",
      "Epoch: 4 \tTraining Loss: 0.000155\n",
      "Epoch: 5 \tTraining Loss: 0.000155\n",
      "Epoch: 6 \tTraining Loss: 0.000155\n",
      "Epoch: 7 \tTraining Loss: 0.000155\n",
      "Epoch: 8 \tTraining Loss: 0.000155\n",
      "Epoch: 9 \tTraining Loss: 0.000155\n",
      "Epoch: 10 \tTraining Loss: 0.000155\n",
      "Epoch: 11 \tTraining Loss: 0.000155\n",
      "Epoch: 12 \tTraining Loss: 0.000155\n",
      "Epoch: 13 \tTraining Loss: 0.000155\n",
      "Epoch: 14 \tTraining Loss: 0.000155\n",
      "Epoch: 15 \tTraining Loss: 0.000155\n",
      "Epoch: 16 \tTraining Loss: 0.000155\n",
      "Epoch: 17 \tTraining Loss: 0.000155\n",
      "Epoch: 18 \tTraining Loss: 0.000155\n",
      "Epoch: 1 \tTraining Loss: 0.000474\n",
      "Epoch: 2 \tTraining Loss: 0.000474\n",
      "Epoch: 3 \tTraining Loss: 0.000474\n",
      "Epoch: 4 \tTraining Loss: 0.000474\n",
      "Epoch: 5 \tTraining Loss: 0.000474\n",
      "Epoch: 6 \tTraining Loss: 0.000474\n",
      "Epoch: 7 \tTraining Loss: 0.000474\n",
      "Epoch: 8 \tTraining Loss: 0.000474\n",
      "Epoch: 9 \tTraining Loss: 0.000474\n",
      "Epoch: 10 \tTraining Loss: 0.000474\n",
      "Epoch: 11 \tTraining Loss: 0.000474\n",
      "Epoch: 12 \tTraining Loss: 0.000474\n",
      "Epoch: 13 \tTraining Loss: 0.000474\n",
      "Epoch: 14 \tTraining Loss: 0.000474\n",
      "Epoch: 15 \tTraining Loss: 0.000474\n",
      "Epoch: 16 \tTraining Loss: 0.000474\n",
      "Epoch: 17 \tTraining Loss: 0.000474\n",
      "Epoch: 18 \tTraining Loss: 0.000474\n",
      "Epoch: 1 \tTraining Loss: 0.000219\n",
      "Epoch: 2 \tTraining Loss: 0.000219\n",
      "Epoch: 3 \tTraining Loss: 0.000219\n",
      "Epoch: 4 \tTraining Loss: 0.000219\n",
      "Epoch: 5 \tTraining Loss: 0.000219\n",
      "Epoch: 6 \tTraining Loss: 0.000219\n",
      "Epoch: 7 \tTraining Loss: 0.000219\n",
      "Epoch: 8 \tTraining Loss: 0.000219\n",
      "Epoch: 9 \tTraining Loss: 0.000219\n",
      "Epoch: 10 \tTraining Loss: 0.000219\n",
      "Epoch: 11 \tTraining Loss: 0.000219\n",
      "Epoch: 12 \tTraining Loss: 0.000219\n",
      "Epoch: 13 \tTraining Loss: 0.000219\n",
      "Epoch: 14 \tTraining Loss: 0.000219\n",
      "Epoch: 15 \tTraining Loss: 0.000219\n",
      "Epoch: 16 \tTraining Loss: 0.000219\n",
      "Epoch: 17 \tTraining Loss: 0.000219\n",
      "Epoch: 18 \tTraining Loss: 0.000219\n",
      "Epoch: 1 \tTraining Loss: 0.000171\n",
      "Epoch: 2 \tTraining Loss: 0.000171\n",
      "Epoch: 3 \tTraining Loss: 0.000171\n",
      "Epoch: 4 \tTraining Loss: 0.000171\n",
      "Epoch: 5 \tTraining Loss: 0.000171\n",
      "Epoch: 6 \tTraining Loss: 0.000171\n",
      "Epoch: 7 \tTraining Loss: 0.000171\n",
      "Epoch: 8 \tTraining Loss: 0.000171\n",
      "Epoch: 9 \tTraining Loss: 0.000171\n",
      "Epoch: 10 \tTraining Loss: 0.000171\n",
      "Epoch: 11 \tTraining Loss: 0.000171\n",
      "Epoch: 12 \tTraining Loss: 0.000171\n",
      "Epoch: 13 \tTraining Loss: 0.000171\n",
      "Epoch: 14 \tTraining Loss: 0.000171\n",
      "Epoch: 15 \tTraining Loss: 0.000171\n",
      "Epoch: 16 \tTraining Loss: 0.000171\n",
      "Epoch: 17 \tTraining Loss: 0.000171\n",
      "Epoch: 18 \tTraining Loss: 0.000171\n",
      "Epoch: 1 \tTraining Loss: 0.000198\n",
      "Epoch: 2 \tTraining Loss: 0.000198\n",
      "Epoch: 3 \tTraining Loss: 0.000198\n",
      "Epoch: 4 \tTraining Loss: 0.000198\n",
      "Epoch: 5 \tTraining Loss: 0.000198\n",
      "Epoch: 6 \tTraining Loss: 0.000198\n",
      "Epoch: 7 \tTraining Loss: 0.000198\n",
      "Epoch: 8 \tTraining Loss: 0.000198\n",
      "Epoch: 9 \tTraining Loss: 0.000198\n",
      "Epoch: 10 \tTraining Loss: 0.000198\n",
      "Epoch: 11 \tTraining Loss: 0.000198\n",
      "Epoch: 12 \tTraining Loss: 0.000198\n",
      "Epoch: 13 \tTraining Loss: 0.000198\n",
      "Epoch: 14 \tTraining Loss: 0.000198\n",
      "Epoch: 15 \tTraining Loss: 0.000198\n",
      "Epoch: 16 \tTraining Loss: 0.000198\n",
      "Epoch: 17 \tTraining Loss: 0.000198\n",
      "Epoch: 18 \tTraining Loss: 0.000198\n",
      "Epoch: 1 \tTraining Loss: 0.000199\n",
      "Epoch: 2 \tTraining Loss: 0.000199\n",
      "Epoch: 3 \tTraining Loss: 0.000199\n",
      "Epoch: 4 \tTraining Loss: 0.000199\n",
      "Epoch: 5 \tTraining Loss: 0.000199\n",
      "Epoch: 6 \tTraining Loss: 0.000199\n",
      "Epoch: 7 \tTraining Loss: 0.000199\n",
      "Epoch: 8 \tTraining Loss: 0.000199\n",
      "Epoch: 9 \tTraining Loss: 0.000199\n",
      "Epoch: 10 \tTraining Loss: 0.000199\n",
      "Epoch: 11 \tTraining Loss: 0.000199\n",
      "Epoch: 12 \tTraining Loss: 0.000199\n",
      "Epoch: 13 \tTraining Loss: 0.000199\n",
      "Epoch: 14 \tTraining Loss: 0.000199\n",
      "Epoch: 15 \tTraining Loss: 0.000199\n",
      "Epoch: 16 \tTraining Loss: 0.000199\n",
      "Epoch: 17 \tTraining Loss: 0.000199\n",
      "Epoch: 18 \tTraining Loss: 0.000199\n",
      "Epoch: 1 \tTraining Loss: 0.000242\n",
      "Epoch: 2 \tTraining Loss: 0.000242\n",
      "Epoch: 3 \tTraining Loss: 0.000242\n",
      "Epoch: 4 \tTraining Loss: 0.000242\n",
      "Epoch: 5 \tTraining Loss: 0.000242\n",
      "Epoch: 6 \tTraining Loss: 0.000242\n",
      "Epoch: 7 \tTraining Loss: 0.000242\n",
      "Epoch: 8 \tTraining Loss: 0.000242\n",
      "Epoch: 9 \tTraining Loss: 0.000242\n",
      "Epoch: 10 \tTraining Loss: 0.000242\n",
      "Epoch: 11 \tTraining Loss: 0.000242\n",
      "Epoch: 12 \tTraining Loss: 0.000242\n",
      "Epoch: 13 \tTraining Loss: 0.000242\n",
      "Epoch: 14 \tTraining Loss: 0.000242\n",
      "Epoch: 15 \tTraining Loss: 0.000242\n",
      "Epoch: 16 \tTraining Loss: 0.000242\n",
      "Epoch: 17 \tTraining Loss: 0.000242\n",
      "Epoch: 18 \tTraining Loss: 0.000242\n",
      "Epoch: 1 \tTraining Loss: 0.000142\n",
      "Epoch: 2 \tTraining Loss: 0.000142\n",
      "Epoch: 3 \tTraining Loss: 0.000142\n",
      "Epoch: 4 \tTraining Loss: 0.000142\n",
      "Epoch: 5 \tTraining Loss: 0.000142\n",
      "Epoch: 6 \tTraining Loss: 0.000142\n",
      "Epoch: 7 \tTraining Loss: 0.000142\n",
      "Epoch: 8 \tTraining Loss: 0.000142\n",
      "Epoch: 9 \tTraining Loss: 0.000142\n",
      "Epoch: 10 \tTraining Loss: 0.000142\n",
      "Epoch: 11 \tTraining Loss: 0.000142\n",
      "Epoch: 12 \tTraining Loss: 0.000142\n",
      "Epoch: 13 \tTraining Loss: 0.000142\n",
      "Epoch: 14 \tTraining Loss: 0.000142\n",
      "Epoch: 15 \tTraining Loss: 0.000142\n",
      "Epoch: 16 \tTraining Loss: 0.000142\n",
      "Epoch: 17 \tTraining Loss: 0.000142\n",
      "Epoch: 18 \tTraining Loss: 0.000142\n",
      "Epoch: 1 \tTraining Loss: 0.000095\n",
      "Epoch: 2 \tTraining Loss: 0.000095\n",
      "Epoch: 3 \tTraining Loss: 0.000095\n",
      "Epoch: 4 \tTraining Loss: 0.000095\n",
      "Epoch: 5 \tTraining Loss: 0.000095\n",
      "Epoch: 6 \tTraining Loss: 0.000095\n",
      "Epoch: 7 \tTraining Loss: 0.000095\n",
      "Epoch: 8 \tTraining Loss: 0.000095\n",
      "Epoch: 9 \tTraining Loss: 0.000095\n",
      "Epoch: 10 \tTraining Loss: 0.000095\n",
      "Epoch: 11 \tTraining Loss: 0.000095\n",
      "Epoch: 12 \tTraining Loss: 0.000095\n",
      "Epoch: 13 \tTraining Loss: 0.000095\n",
      "Epoch: 14 \tTraining Loss: 0.000095\n",
      "Epoch: 15 \tTraining Loss: 0.000095\n",
      "Epoch: 16 \tTraining Loss: 0.000095\n",
      "Epoch: 17 \tTraining Loss: 0.000095\n",
      "Epoch: 18 \tTraining Loss: 0.000095\n",
      "Epoch: 1 \tTraining Loss: 0.000147\n",
      "Epoch: 2 \tTraining Loss: 0.000147\n",
      "Epoch: 3 \tTraining Loss: 0.000147\n",
      "Epoch: 4 \tTraining Loss: 0.000147\n",
      "Epoch: 5 \tTraining Loss: 0.000147\n",
      "Epoch: 6 \tTraining Loss: 0.000147\n",
      "Epoch: 7 \tTraining Loss: 0.000147\n",
      "Epoch: 8 \tTraining Loss: 0.000147\n",
      "Epoch: 9 \tTraining Loss: 0.000147\n",
      "Epoch: 10 \tTraining Loss: 0.000147\n",
      "Epoch: 11 \tTraining Loss: 0.000147\n",
      "Epoch: 12 \tTraining Loss: 0.000147\n",
      "Epoch: 13 \tTraining Loss: 0.000147\n",
      "Epoch: 14 \tTraining Loss: 0.000147\n",
      "Epoch: 15 \tTraining Loss: 0.000147\n",
      "Epoch: 16 \tTraining Loss: 0.000147\n",
      "Epoch: 17 \tTraining Loss: 0.000147\n",
      "Epoch: 18 \tTraining Loss: 0.000147\n",
      "Epoch: 1 \tTraining Loss: 0.000273\n",
      "Epoch: 2 \tTraining Loss: 0.000273\n",
      "Epoch: 3 \tTraining Loss: 0.000273\n",
      "Epoch: 4 \tTraining Loss: 0.000273\n",
      "Epoch: 5 \tTraining Loss: 0.000273\n",
      "Epoch: 6 \tTraining Loss: 0.000273\n",
      "Epoch: 7 \tTraining Loss: 0.000273\n",
      "Epoch: 8 \tTraining Loss: 0.000273\n",
      "Epoch: 9 \tTraining Loss: 0.000273\n",
      "Epoch: 10 \tTraining Loss: 0.000273\n",
      "Epoch: 11 \tTraining Loss: 0.000273\n",
      "Epoch: 12 \tTraining Loss: 0.000273\n",
      "Epoch: 13 \tTraining Loss: 0.000273\n",
      "Epoch: 14 \tTraining Loss: 0.000273\n",
      "Epoch: 15 \tTraining Loss: 0.000273\n",
      "Epoch: 16 \tTraining Loss: 0.000273\n",
      "Epoch: 17 \tTraining Loss: 0.000273\n",
      "Epoch: 18 \tTraining Loss: 0.000273\n",
      "Epoch: 1 \tTraining Loss: 0.000224\n",
      "Epoch: 2 \tTraining Loss: 0.000224\n",
      "Epoch: 3 \tTraining Loss: 0.000224\n",
      "Epoch: 4 \tTraining Loss: 0.000224\n",
      "Epoch: 5 \tTraining Loss: 0.000224\n",
      "Epoch: 6 \tTraining Loss: 0.000224\n",
      "Epoch: 7 \tTraining Loss: 0.000224\n",
      "Epoch: 8 \tTraining Loss: 0.000224\n",
      "Epoch: 9 \tTraining Loss: 0.000224\n",
      "Epoch: 10 \tTraining Loss: 0.000224\n",
      "Epoch: 11 \tTraining Loss: 0.000224\n",
      "Epoch: 12 \tTraining Loss: 0.000224\n",
      "Epoch: 13 \tTraining Loss: 0.000224\n",
      "Epoch: 14 \tTraining Loss: 0.000224\n",
      "Epoch: 15 \tTraining Loss: 0.000224\n",
      "Epoch: 16 \tTraining Loss: 0.000224\n",
      "Epoch: 17 \tTraining Loss: 0.000224\n",
      "Epoch: 18 \tTraining Loss: 0.000224\n",
      "Epoch: 1 \tTraining Loss: 0.000298\n",
      "Epoch: 2 \tTraining Loss: 0.000298\n",
      "Epoch: 3 \tTraining Loss: 0.000298\n",
      "Epoch: 4 \tTraining Loss: 0.000298\n",
      "Epoch: 5 \tTraining Loss: 0.000298\n",
      "Epoch: 6 \tTraining Loss: 0.000298\n",
      "Epoch: 7 \tTraining Loss: 0.000298\n",
      "Epoch: 8 \tTraining Loss: 0.000298\n",
      "Epoch: 9 \tTraining Loss: 0.000298\n",
      "Epoch: 10 \tTraining Loss: 0.000298\n",
      "Epoch: 11 \tTraining Loss: 0.000298\n",
      "Epoch: 12 \tTraining Loss: 0.000298\n",
      "Epoch: 13 \tTraining Loss: 0.000298\n",
      "Epoch: 14 \tTraining Loss: 0.000298\n",
      "Epoch: 15 \tTraining Loss: 0.000298\n",
      "Epoch: 16 \tTraining Loss: 0.000298\n",
      "Epoch: 17 \tTraining Loss: 0.000298\n",
      "Epoch: 18 \tTraining Loss: 0.000298\n",
      "Epoch: 1 \tTraining Loss: 0.000408\n",
      "Epoch: 2 \tTraining Loss: 0.000408\n",
      "Epoch: 3 \tTraining Loss: 0.000408\n",
      "Epoch: 4 \tTraining Loss: 0.000408\n",
      "Epoch: 5 \tTraining Loss: 0.000408\n",
      "Epoch: 6 \tTraining Loss: 0.000408\n",
      "Epoch: 7 \tTraining Loss: 0.000408\n",
      "Epoch: 8 \tTraining Loss: 0.000408\n",
      "Epoch: 9 \tTraining Loss: 0.000408\n",
      "Epoch: 10 \tTraining Loss: 0.000408\n",
      "Epoch: 11 \tTraining Loss: 0.000408\n",
      "Epoch: 12 \tTraining Loss: 0.000408\n",
      "Epoch: 13 \tTraining Loss: 0.000408\n",
      "Epoch: 14 \tTraining Loss: 0.000408\n",
      "Epoch: 15 \tTraining Loss: 0.000408\n",
      "Epoch: 16 \tTraining Loss: 0.000408\n",
      "Epoch: 17 \tTraining Loss: 0.000408\n",
      "Epoch: 18 \tTraining Loss: 0.000408\n",
      "Epoch: 1 \tTraining Loss: 0.000368\n",
      "Epoch: 2 \tTraining Loss: 0.000368\n",
      "Epoch: 3 \tTraining Loss: 0.000368\n",
      "Epoch: 4 \tTraining Loss: 0.000368\n",
      "Epoch: 5 \tTraining Loss: 0.000368\n",
      "Epoch: 6 \tTraining Loss: 0.000368\n",
      "Epoch: 7 \tTraining Loss: 0.000368\n",
      "Epoch: 8 \tTraining Loss: 0.000368\n",
      "Epoch: 9 \tTraining Loss: 0.000368\n",
      "Epoch: 10 \tTraining Loss: 0.000368\n",
      "Epoch: 11 \tTraining Loss: 0.000368\n",
      "Epoch: 12 \tTraining Loss: 0.000368\n",
      "Epoch: 13 \tTraining Loss: 0.000368\n",
      "Epoch: 14 \tTraining Loss: 0.000368\n",
      "Epoch: 15 \tTraining Loss: 0.000368\n",
      "Epoch: 16 \tTraining Loss: 0.000368\n",
      "Epoch: 17 \tTraining Loss: 0.000368\n",
      "Epoch: 18 \tTraining Loss: 0.000368\n",
      "Epoch: 1 \tTraining Loss: 0.000103\n",
      "Epoch: 2 \tTraining Loss: 0.000103\n",
      "Epoch: 3 \tTraining Loss: 0.000103\n",
      "Epoch: 4 \tTraining Loss: 0.000103\n",
      "Epoch: 5 \tTraining Loss: 0.000103\n",
      "Epoch: 6 \tTraining Loss: 0.000103\n",
      "Epoch: 7 \tTraining Loss: 0.000103\n",
      "Epoch: 8 \tTraining Loss: 0.000103\n",
      "Epoch: 9 \tTraining Loss: 0.000103\n",
      "Epoch: 10 \tTraining Loss: 0.000103\n",
      "Epoch: 11 \tTraining Loss: 0.000103\n",
      "Epoch: 12 \tTraining Loss: 0.000103\n",
      "Epoch: 13 \tTraining Loss: 0.000103\n",
      "Epoch: 14 \tTraining Loss: 0.000103\n",
      "Epoch: 15 \tTraining Loss: 0.000103\n",
      "Epoch: 16 \tTraining Loss: 0.000103\n",
      "Epoch: 17 \tTraining Loss: 0.000103\n",
      "Epoch: 18 \tTraining Loss: 0.000103\n",
      "Epoch: 1 \tTraining Loss: 0.000199\n",
      "Epoch: 2 \tTraining Loss: 0.000199\n",
      "Epoch: 3 \tTraining Loss: 0.000199\n",
      "Epoch: 4 \tTraining Loss: 0.000199\n",
      "Epoch: 5 \tTraining Loss: 0.000199\n",
      "Epoch: 6 \tTraining Loss: 0.000199\n",
      "Epoch: 7 \tTraining Loss: 0.000199\n",
      "Epoch: 8 \tTraining Loss: 0.000199\n",
      "Epoch: 9 \tTraining Loss: 0.000199\n",
      "Epoch: 10 \tTraining Loss: 0.000199\n",
      "Epoch: 11 \tTraining Loss: 0.000199\n",
      "Epoch: 12 \tTraining Loss: 0.000199\n",
      "Epoch: 13 \tTraining Loss: 0.000199\n",
      "Epoch: 14 \tTraining Loss: 0.000199\n",
      "Epoch: 15 \tTraining Loss: 0.000199\n",
      "Epoch: 16 \tTraining Loss: 0.000199\n",
      "Epoch: 17 \tTraining Loss: 0.000199\n",
      "Epoch: 18 \tTraining Loss: 0.000199\n",
      "Epoch: 1 \tTraining Loss: 0.000247\n",
      "Epoch: 2 \tTraining Loss: 0.000247\n",
      "Epoch: 3 \tTraining Loss: 0.000247\n",
      "Epoch: 4 \tTraining Loss: 0.000247\n",
      "Epoch: 5 \tTraining Loss: 0.000247\n",
      "Epoch: 6 \tTraining Loss: 0.000247\n",
      "Epoch: 7 \tTraining Loss: 0.000247\n",
      "Epoch: 8 \tTraining Loss: 0.000247\n",
      "Epoch: 9 \tTraining Loss: 0.000247\n",
      "Epoch: 10 \tTraining Loss: 0.000247\n",
      "Epoch: 11 \tTraining Loss: 0.000247\n",
      "Epoch: 12 \tTraining Loss: 0.000247\n",
      "Epoch: 13 \tTraining Loss: 0.000247\n",
      "Epoch: 14 \tTraining Loss: 0.000247\n",
      "Epoch: 15 \tTraining Loss: 0.000247\n",
      "Epoch: 16 \tTraining Loss: 0.000247\n",
      "Epoch: 17 \tTraining Loss: 0.000247\n",
      "Epoch: 18 \tTraining Loss: 0.000247\n",
      "Epoch: 1 \tTraining Loss: 0.000166\n",
      "Epoch: 2 \tTraining Loss: 0.000166\n",
      "Epoch: 3 \tTraining Loss: 0.000166\n",
      "Epoch: 4 \tTraining Loss: 0.000166\n",
      "Epoch: 5 \tTraining Loss: 0.000166\n",
      "Epoch: 6 \tTraining Loss: 0.000166\n",
      "Epoch: 7 \tTraining Loss: 0.000166\n",
      "Epoch: 8 \tTraining Loss: 0.000166\n",
      "Epoch: 9 \tTraining Loss: 0.000166\n",
      "Epoch: 10 \tTraining Loss: 0.000166\n",
      "Epoch: 11 \tTraining Loss: 0.000166\n",
      "Epoch: 12 \tTraining Loss: 0.000166\n",
      "Epoch: 13 \tTraining Loss: 0.000166\n",
      "Epoch: 14 \tTraining Loss: 0.000166\n",
      "Epoch: 15 \tTraining Loss: 0.000166\n",
      "Epoch: 16 \tTraining Loss: 0.000166\n",
      "Epoch: 17 \tTraining Loss: 0.000166\n",
      "Epoch: 18 \tTraining Loss: 0.000166\n",
      "Epoch: 1 \tTraining Loss: 0.000300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \tTraining Loss: 0.000300\n",
      "Epoch: 3 \tTraining Loss: 0.000300\n",
      "Epoch: 4 \tTraining Loss: 0.000300\n",
      "Epoch: 5 \tTraining Loss: 0.000300\n",
      "Epoch: 6 \tTraining Loss: 0.000300\n",
      "Epoch: 7 \tTraining Loss: 0.000300\n",
      "Epoch: 8 \tTraining Loss: 0.000300\n",
      "Epoch: 9 \tTraining Loss: 0.000300\n",
      "Epoch: 10 \tTraining Loss: 0.000300\n",
      "Epoch: 11 \tTraining Loss: 0.000300\n",
      "Epoch: 12 \tTraining Loss: 0.000300\n",
      "Epoch: 13 \tTraining Loss: 0.000300\n",
      "Epoch: 14 \tTraining Loss: 0.000300\n",
      "Epoch: 15 \tTraining Loss: 0.000300\n",
      "Epoch: 16 \tTraining Loss: 0.000300\n",
      "Epoch: 17 \tTraining Loss: 0.000300\n",
      "Epoch: 18 \tTraining Loss: 0.000300\n",
      "Epoch: 1 \tTraining Loss: 0.000176\n",
      "Epoch: 2 \tTraining Loss: 0.000176\n",
      "Epoch: 3 \tTraining Loss: 0.000176\n",
      "Epoch: 4 \tTraining Loss: 0.000176\n",
      "Epoch: 5 \tTraining Loss: 0.000176\n",
      "Epoch: 6 \tTraining Loss: 0.000176\n",
      "Epoch: 7 \tTraining Loss: 0.000176\n",
      "Epoch: 8 \tTraining Loss: 0.000176\n",
      "Epoch: 9 \tTraining Loss: 0.000176\n",
      "Epoch: 10 \tTraining Loss: 0.000176\n",
      "Epoch: 11 \tTraining Loss: 0.000176\n",
      "Epoch: 12 \tTraining Loss: 0.000176\n",
      "Epoch: 13 \tTraining Loss: 0.000176\n",
      "Epoch: 14 \tTraining Loss: 0.000176\n",
      "Epoch: 15 \tTraining Loss: 0.000176\n",
      "Epoch: 16 \tTraining Loss: 0.000176\n",
      "Epoch: 17 \tTraining Loss: 0.000176\n",
      "Epoch: 18 \tTraining Loss: 0.000176\n",
      "Epoch: 1 \tTraining Loss: 0.000300\n",
      "Epoch: 2 \tTraining Loss: 0.000300\n",
      "Epoch: 3 \tTraining Loss: 0.000300\n",
      "Epoch: 4 \tTraining Loss: 0.000300\n",
      "Epoch: 5 \tTraining Loss: 0.000300\n",
      "Epoch: 6 \tTraining Loss: 0.000300\n",
      "Epoch: 7 \tTraining Loss: 0.000300\n",
      "Epoch: 8 \tTraining Loss: 0.000300\n",
      "Epoch: 9 \tTraining Loss: 0.000300\n",
      "Epoch: 10 \tTraining Loss: 0.000300\n",
      "Epoch: 11 \tTraining Loss: 0.000300\n",
      "Epoch: 12 \tTraining Loss: 0.000300\n",
      "Epoch: 13 \tTraining Loss: 0.000300\n",
      "Epoch: 14 \tTraining Loss: 0.000300\n",
      "Epoch: 15 \tTraining Loss: 0.000300\n",
      "Epoch: 16 \tTraining Loss: 0.000300\n",
      "Epoch: 17 \tTraining Loss: 0.000300\n",
      "Epoch: 18 \tTraining Loss: 0.000300\n",
      "Epoch: 1 \tTraining Loss: 0.000310\n",
      "Epoch: 2 \tTraining Loss: 0.000310\n",
      "Epoch: 3 \tTraining Loss: 0.000310\n",
      "Epoch: 4 \tTraining Loss: 0.000310\n",
      "Epoch: 5 \tTraining Loss: 0.000310\n",
      "Epoch: 6 \tTraining Loss: 0.000310\n",
      "Epoch: 7 \tTraining Loss: 0.000310\n",
      "Epoch: 8 \tTraining Loss: 0.000310\n",
      "Epoch: 9 \tTraining Loss: 0.000310\n",
      "Epoch: 10 \tTraining Loss: 0.000310\n",
      "Epoch: 11 \tTraining Loss: 0.000310\n",
      "Epoch: 12 \tTraining Loss: 0.000310\n",
      "Epoch: 13 \tTraining Loss: 0.000310\n",
      "Epoch: 14 \tTraining Loss: 0.000310\n",
      "Epoch: 15 \tTraining Loss: 0.000310\n",
      "Epoch: 16 \tTraining Loss: 0.000310\n",
      "Epoch: 17 \tTraining Loss: 0.000310\n",
      "Epoch: 18 \tTraining Loss: 0.000310\n",
      "Epoch: 1 \tTraining Loss: 0.000348\n",
      "Epoch: 2 \tTraining Loss: 0.000348\n",
      "Epoch: 3 \tTraining Loss: 0.000348\n",
      "Epoch: 4 \tTraining Loss: 0.000348\n",
      "Epoch: 5 \tTraining Loss: 0.000348\n",
      "Epoch: 6 \tTraining Loss: 0.000348\n",
      "Epoch: 7 \tTraining Loss: 0.000348\n",
      "Epoch: 8 \tTraining Loss: 0.000348\n",
      "Epoch: 9 \tTraining Loss: 0.000348\n",
      "Epoch: 10 \tTraining Loss: 0.000348\n",
      "Epoch: 11 \tTraining Loss: 0.000348\n",
      "Epoch: 12 \tTraining Loss: 0.000348\n",
      "Epoch: 13 \tTraining Loss: 0.000348\n",
      "Epoch: 14 \tTraining Loss: 0.000348\n",
      "Epoch: 15 \tTraining Loss: 0.000348\n",
      "Epoch: 16 \tTraining Loss: 0.000348\n",
      "Epoch: 17 \tTraining Loss: 0.000348\n",
      "Epoch: 18 \tTraining Loss: 0.000348\n",
      "Epoch: 1 \tTraining Loss: 0.000264\n",
      "Epoch: 2 \tTraining Loss: 0.000264\n",
      "Epoch: 3 \tTraining Loss: 0.000264\n",
      "Epoch: 4 \tTraining Loss: 0.000264\n",
      "Epoch: 5 \tTraining Loss: 0.000264\n",
      "Epoch: 6 \tTraining Loss: 0.000264\n",
      "Epoch: 7 \tTraining Loss: 0.000264\n",
      "Epoch: 8 \tTraining Loss: 0.000264\n",
      "Epoch: 9 \tTraining Loss: 0.000264\n",
      "Epoch: 10 \tTraining Loss: 0.000264\n",
      "Epoch: 11 \tTraining Loss: 0.000264\n",
      "Epoch: 12 \tTraining Loss: 0.000264\n",
      "Epoch: 13 \tTraining Loss: 0.000264\n",
      "Epoch: 14 \tTraining Loss: 0.000264\n",
      "Epoch: 15 \tTraining Loss: 0.000264\n",
      "Epoch: 16 \tTraining Loss: 0.000264\n",
      "Epoch: 17 \tTraining Loss: 0.000264\n",
      "Epoch: 18 \tTraining Loss: 0.000264\n",
      "Epoch: 1 \tTraining Loss: 0.000263\n",
      "Epoch: 2 \tTraining Loss: 0.000263\n",
      "Epoch: 3 \tTraining Loss: 0.000263\n",
      "Epoch: 4 \tTraining Loss: 0.000263\n",
      "Epoch: 5 \tTraining Loss: 0.000263\n",
      "Epoch: 6 \tTraining Loss: 0.000263\n",
      "Epoch: 7 \tTraining Loss: 0.000263\n",
      "Epoch: 8 \tTraining Loss: 0.000263\n",
      "Epoch: 9 \tTraining Loss: 0.000263\n",
      "Epoch: 10 \tTraining Loss: 0.000263\n",
      "Epoch: 11 \tTraining Loss: 0.000263\n",
      "Epoch: 12 \tTraining Loss: 0.000263\n",
      "Epoch: 13 \tTraining Loss: 0.000263\n",
      "Epoch: 14 \tTraining Loss: 0.000263\n",
      "Epoch: 15 \tTraining Loss: 0.000263\n",
      "Epoch: 16 \tTraining Loss: 0.000263\n",
      "Epoch: 17 \tTraining Loss: 0.000263\n",
      "Epoch: 18 \tTraining Loss: 0.000263\n",
      "Epoch: 1 \tTraining Loss: 0.000216\n",
      "Epoch: 2 \tTraining Loss: 0.000216\n",
      "Epoch: 3 \tTraining Loss: 0.000216\n",
      "Epoch: 4 \tTraining Loss: 0.000216\n",
      "Epoch: 5 \tTraining Loss: 0.000216\n",
      "Epoch: 6 \tTraining Loss: 0.000216\n",
      "Epoch: 7 \tTraining Loss: 0.000216\n",
      "Epoch: 8 \tTraining Loss: 0.000216\n",
      "Epoch: 9 \tTraining Loss: 0.000216\n",
      "Epoch: 10 \tTraining Loss: 0.000216\n",
      "Epoch: 11 \tTraining Loss: 0.000216\n",
      "Epoch: 12 \tTraining Loss: 0.000216\n",
      "Epoch: 13 \tTraining Loss: 0.000216\n",
      "Epoch: 14 \tTraining Loss: 0.000216\n",
      "Epoch: 15 \tTraining Loss: 0.000216\n",
      "Epoch: 16 \tTraining Loss: 0.000216\n",
      "Epoch: 17 \tTraining Loss: 0.000216\n",
      "Epoch: 18 \tTraining Loss: 0.000216\n",
      "Epoch: 1 \tTraining Loss: 0.000295\n",
      "Epoch: 2 \tTraining Loss: 0.000295\n",
      "Epoch: 3 \tTraining Loss: 0.000295\n",
      "Epoch: 4 \tTraining Loss: 0.000295\n",
      "Epoch: 5 \tTraining Loss: 0.000295\n",
      "Epoch: 6 \tTraining Loss: 0.000295\n",
      "Epoch: 7 \tTraining Loss: 0.000295\n",
      "Epoch: 8 \tTraining Loss: 0.000295\n",
      "Epoch: 9 \tTraining Loss: 0.000295\n",
      "Epoch: 10 \tTraining Loss: 0.000295\n",
      "Epoch: 11 \tTraining Loss: 0.000295\n",
      "Epoch: 12 \tTraining Loss: 0.000295\n",
      "Epoch: 13 \tTraining Loss: 0.000295\n",
      "Epoch: 14 \tTraining Loss: 0.000295\n",
      "Epoch: 15 \tTraining Loss: 0.000295\n",
      "Epoch: 16 \tTraining Loss: 0.000295\n",
      "Epoch: 17 \tTraining Loss: 0.000295\n",
      "Epoch: 18 \tTraining Loss: 0.000295\n",
      "Epoch: 1 \tTraining Loss: 0.000326\n",
      "Epoch: 2 \tTraining Loss: 0.000326\n",
      "Epoch: 3 \tTraining Loss: 0.000326\n",
      "Epoch: 4 \tTraining Loss: 0.000326\n",
      "Epoch: 5 \tTraining Loss: 0.000326\n",
      "Epoch: 6 \tTraining Loss: 0.000326\n",
      "Epoch: 7 \tTraining Loss: 0.000326\n",
      "Epoch: 8 \tTraining Loss: 0.000326\n",
      "Epoch: 9 \tTraining Loss: 0.000326\n",
      "Epoch: 10 \tTraining Loss: 0.000326\n",
      "Epoch: 11 \tTraining Loss: 0.000326\n",
      "Epoch: 12 \tTraining Loss: 0.000326\n",
      "Epoch: 13 \tTraining Loss: 0.000326\n",
      "Epoch: 14 \tTraining Loss: 0.000326\n",
      "Epoch: 15 \tTraining Loss: 0.000326\n",
      "Epoch: 16 \tTraining Loss: 0.000326\n",
      "Epoch: 17 \tTraining Loss: 0.000326\n",
      "Epoch: 18 \tTraining Loss: 0.000326\n",
      "Epoch: 1 \tTraining Loss: 0.000141\n",
      "Epoch: 2 \tTraining Loss: 0.000141\n",
      "Epoch: 3 \tTraining Loss: 0.000141\n",
      "Epoch: 4 \tTraining Loss: 0.000141\n",
      "Epoch: 5 \tTraining Loss: 0.000141\n",
      "Epoch: 6 \tTraining Loss: 0.000141\n",
      "Epoch: 7 \tTraining Loss: 0.000141\n",
      "Epoch: 8 \tTraining Loss: 0.000141\n",
      "Epoch: 9 \tTraining Loss: 0.000141\n",
      "Epoch: 10 \tTraining Loss: 0.000141\n",
      "Epoch: 11 \tTraining Loss: 0.000141\n",
      "Epoch: 12 \tTraining Loss: 0.000141\n",
      "Epoch: 13 \tTraining Loss: 0.000141\n",
      "Epoch: 14 \tTraining Loss: 0.000141\n",
      "Epoch: 15 \tTraining Loss: 0.000141\n",
      "Epoch: 16 \tTraining Loss: 0.000141\n",
      "Epoch: 17 \tTraining Loss: 0.000141\n",
      "Epoch: 18 \tTraining Loss: 0.000141\n",
      "Epoch: 1 \tTraining Loss: 0.000271\n",
      "Epoch: 2 \tTraining Loss: 0.000271\n",
      "Epoch: 3 \tTraining Loss: 0.000271\n",
      "Epoch: 4 \tTraining Loss: 0.000271\n",
      "Epoch: 5 \tTraining Loss: 0.000271\n",
      "Epoch: 6 \tTraining Loss: 0.000271\n",
      "Epoch: 7 \tTraining Loss: 0.000271\n",
      "Epoch: 8 \tTraining Loss: 0.000271\n",
      "Epoch: 9 \tTraining Loss: 0.000271\n",
      "Epoch: 10 \tTraining Loss: 0.000271\n",
      "Epoch: 11 \tTraining Loss: 0.000271\n",
      "Epoch: 12 \tTraining Loss: 0.000271\n",
      "Epoch: 13 \tTraining Loss: 0.000271\n",
      "Epoch: 14 \tTraining Loss: 0.000271\n",
      "Epoch: 15 \tTraining Loss: 0.000271\n",
      "Epoch: 16 \tTraining Loss: 0.000271\n",
      "Epoch: 17 \tTraining Loss: 0.000271\n",
      "Epoch: 18 \tTraining Loss: 0.000271\n",
      "Epoch: 1 \tTraining Loss: 0.000079\n",
      "Epoch: 2 \tTraining Loss: 0.000079\n",
      "Epoch: 3 \tTraining Loss: 0.000079\n",
      "Epoch: 4 \tTraining Loss: 0.000079\n",
      "Epoch: 5 \tTraining Loss: 0.000079\n",
      "Epoch: 6 \tTraining Loss: 0.000079\n",
      "Epoch: 7 \tTraining Loss: 0.000079\n",
      "Epoch: 8 \tTraining Loss: 0.000079\n",
      "Epoch: 9 \tTraining Loss: 0.000079\n",
      "Epoch: 10 \tTraining Loss: 0.000079\n",
      "Epoch: 11 \tTraining Loss: 0.000079\n",
      "Epoch: 12 \tTraining Loss: 0.000079\n",
      "Epoch: 13 \tTraining Loss: 0.000079\n",
      "Epoch: 14 \tTraining Loss: 0.000079\n",
      "Epoch: 15 \tTraining Loss: 0.000079\n",
      "Epoch: 16 \tTraining Loss: 0.000079\n",
      "Epoch: 17 \tTraining Loss: 0.000079\n",
      "Epoch: 18 \tTraining Loss: 0.000079\n",
      "Epoch: 1 \tTraining Loss: 0.000239\n",
      "Epoch: 2 \tTraining Loss: 0.000239\n",
      "Epoch: 3 \tTraining Loss: 0.000239\n",
      "Epoch: 4 \tTraining Loss: 0.000239\n",
      "Epoch: 5 \tTraining Loss: 0.000239\n",
      "Epoch: 6 \tTraining Loss: 0.000239\n",
      "Epoch: 7 \tTraining Loss: 0.000239\n",
      "Epoch: 8 \tTraining Loss: 0.000239\n",
      "Epoch: 9 \tTraining Loss: 0.000239\n",
      "Epoch: 10 \tTraining Loss: 0.000239\n",
      "Epoch: 11 \tTraining Loss: 0.000239\n",
      "Epoch: 12 \tTraining Loss: 0.000239\n",
      "Epoch: 13 \tTraining Loss: 0.000239\n",
      "Epoch: 14 \tTraining Loss: 0.000239\n",
      "Epoch: 15 \tTraining Loss: 0.000239\n",
      "Epoch: 16 \tTraining Loss: 0.000239\n",
      "Epoch: 17 \tTraining Loss: 0.000239\n",
      "Epoch: 18 \tTraining Loss: 0.000239\n",
      "Epoch: 1 \tTraining Loss: 0.000206\n",
      "Epoch: 2 \tTraining Loss: 0.000206\n",
      "Epoch: 3 \tTraining Loss: 0.000206\n",
      "Epoch: 4 \tTraining Loss: 0.000206\n",
      "Epoch: 5 \tTraining Loss: 0.000206\n",
      "Epoch: 6 \tTraining Loss: 0.000206\n",
      "Epoch: 7 \tTraining Loss: 0.000206\n",
      "Epoch: 8 \tTraining Loss: 0.000206\n",
      "Epoch: 9 \tTraining Loss: 0.000206\n",
      "Epoch: 10 \tTraining Loss: 0.000206\n",
      "Epoch: 11 \tTraining Loss: 0.000206\n",
      "Epoch: 12 \tTraining Loss: 0.000206\n",
      "Epoch: 13 \tTraining Loss: 0.000206\n",
      "Epoch: 14 \tTraining Loss: 0.000206\n",
      "Epoch: 15 \tTraining Loss: 0.000206\n",
      "Epoch: 16 \tTraining Loss: 0.000206\n",
      "Epoch: 17 \tTraining Loss: 0.000206\n",
      "Epoch: 18 \tTraining Loss: 0.000206\n",
      "Epoch: 1 \tTraining Loss: 0.000429\n",
      "Epoch: 2 \tTraining Loss: 0.000429\n",
      "Epoch: 3 \tTraining Loss: 0.000429\n",
      "Epoch: 4 \tTraining Loss: 0.000429\n",
      "Epoch: 5 \tTraining Loss: 0.000429\n",
      "Epoch: 6 \tTraining Loss: 0.000429\n",
      "Epoch: 7 \tTraining Loss: 0.000429\n",
      "Epoch: 8 \tTraining Loss: 0.000429\n",
      "Epoch: 9 \tTraining Loss: 0.000429\n",
      "Epoch: 10 \tTraining Loss: 0.000429\n",
      "Epoch: 11 \tTraining Loss: 0.000429\n",
      "Epoch: 12 \tTraining Loss: 0.000429\n",
      "Epoch: 13 \tTraining Loss: 0.000429\n",
      "Epoch: 14 \tTraining Loss: 0.000429\n",
      "Epoch: 15 \tTraining Loss: 0.000429\n",
      "Epoch: 16 \tTraining Loss: 0.000429\n",
      "Epoch: 17 \tTraining Loss: 0.000429\n",
      "Epoch: 18 \tTraining Loss: 0.000429\n",
      "Epoch: 1 \tTraining Loss: 0.000204\n",
      "Epoch: 2 \tTraining Loss: 0.000204\n",
      "Epoch: 3 \tTraining Loss: 0.000204\n",
      "Epoch: 4 \tTraining Loss: 0.000204\n",
      "Epoch: 5 \tTraining Loss: 0.000204\n",
      "Epoch: 6 \tTraining Loss: 0.000204\n",
      "Epoch: 7 \tTraining Loss: 0.000204\n",
      "Epoch: 8 \tTraining Loss: 0.000204\n",
      "Epoch: 9 \tTraining Loss: 0.000204\n",
      "Epoch: 10 \tTraining Loss: 0.000204\n",
      "Epoch: 11 \tTraining Loss: 0.000204\n",
      "Epoch: 12 \tTraining Loss: 0.000204\n",
      "Epoch: 13 \tTraining Loss: 0.000204\n",
      "Epoch: 14 \tTraining Loss: 0.000204\n",
      "Epoch: 15 \tTraining Loss: 0.000204\n",
      "Epoch: 16 \tTraining Loss: 0.000204\n",
      "Epoch: 17 \tTraining Loss: 0.000204\n",
      "Epoch: 18 \tTraining Loss: 0.000204\n",
      "Epoch: 1 \tTraining Loss: 0.000168\n",
      "Epoch: 2 \tTraining Loss: 0.000168\n",
      "Epoch: 3 \tTraining Loss: 0.000168\n",
      "Epoch: 4 \tTraining Loss: 0.000168\n",
      "Epoch: 5 \tTraining Loss: 0.000168\n",
      "Epoch: 6 \tTraining Loss: 0.000168\n",
      "Epoch: 7 \tTraining Loss: 0.000168\n",
      "Epoch: 8 \tTraining Loss: 0.000168\n",
      "Epoch: 9 \tTraining Loss: 0.000168\n",
      "Epoch: 10 \tTraining Loss: 0.000168\n",
      "Epoch: 11 \tTraining Loss: 0.000168\n",
      "Epoch: 12 \tTraining Loss: 0.000168\n",
      "Epoch: 13 \tTraining Loss: 0.000168\n",
      "Epoch: 14 \tTraining Loss: 0.000168\n",
      "Epoch: 15 \tTraining Loss: 0.000168\n",
      "Epoch: 16 \tTraining Loss: 0.000168\n",
      "Epoch: 17 \tTraining Loss: 0.000168\n",
      "Epoch: 18 \tTraining Loss: 0.000168\n",
      "Epoch: 1 \tTraining Loss: 0.000371\n",
      "Epoch: 2 \tTraining Loss: 0.000371\n",
      "Epoch: 3 \tTraining Loss: 0.000371\n",
      "Epoch: 4 \tTraining Loss: 0.000371\n",
      "Epoch: 5 \tTraining Loss: 0.000371\n",
      "Epoch: 6 \tTraining Loss: 0.000371\n",
      "Epoch: 7 \tTraining Loss: 0.000371\n",
      "Epoch: 8 \tTraining Loss: 0.000371\n",
      "Epoch: 9 \tTraining Loss: 0.000371\n",
      "Epoch: 10 \tTraining Loss: 0.000371\n",
      "Epoch: 11 \tTraining Loss: 0.000371\n",
      "Epoch: 12 \tTraining Loss: 0.000371\n",
      "Epoch: 13 \tTraining Loss: 0.000371\n",
      "Epoch: 14 \tTraining Loss: 0.000371\n",
      "Epoch: 15 \tTraining Loss: 0.000371\n",
      "Epoch: 16 \tTraining Loss: 0.000371\n",
      "Epoch: 17 \tTraining Loss: 0.000371\n",
      "Epoch: 18 \tTraining Loss: 0.000371\n",
      "Epoch: 1 \tTraining Loss: 0.000374\n",
      "Epoch: 2 \tTraining Loss: 0.000374\n",
      "Epoch: 3 \tTraining Loss: 0.000374\n",
      "Epoch: 4 \tTraining Loss: 0.000374\n",
      "Epoch: 5 \tTraining Loss: 0.000374\n",
      "Epoch: 6 \tTraining Loss: 0.000374\n",
      "Epoch: 7 \tTraining Loss: 0.000374\n",
      "Epoch: 8 \tTraining Loss: 0.000374\n",
      "Epoch: 9 \tTraining Loss: 0.000374\n",
      "Epoch: 10 \tTraining Loss: 0.000374\n",
      "Epoch: 11 \tTraining Loss: 0.000374\n",
      "Epoch: 12 \tTraining Loss: 0.000374\n",
      "Epoch: 13 \tTraining Loss: 0.000374\n",
      "Epoch: 14 \tTraining Loss: 0.000374\n",
      "Epoch: 15 \tTraining Loss: 0.000374\n",
      "Epoch: 16 \tTraining Loss: 0.000374\n",
      "Epoch: 17 \tTraining Loss: 0.000374\n",
      "Epoch: 18 \tTraining Loss: 0.000374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000141\n",
      "Epoch: 2 \tTraining Loss: 0.000141\n",
      "Epoch: 3 \tTraining Loss: 0.000141\n",
      "Epoch: 4 \tTraining Loss: 0.000141\n",
      "Epoch: 5 \tTraining Loss: 0.000141\n",
      "Epoch: 6 \tTraining Loss: 0.000141\n",
      "Epoch: 7 \tTraining Loss: 0.000141\n",
      "Epoch: 8 \tTraining Loss: 0.000141\n",
      "Epoch: 9 \tTraining Loss: 0.000141\n",
      "Epoch: 10 \tTraining Loss: 0.000141\n",
      "Epoch: 11 \tTraining Loss: 0.000141\n",
      "Epoch: 12 \tTraining Loss: 0.000141\n",
      "Epoch: 13 \tTraining Loss: 0.000141\n",
      "Epoch: 14 \tTraining Loss: 0.000141\n",
      "Epoch: 15 \tTraining Loss: 0.000141\n",
      "Epoch: 16 \tTraining Loss: 0.000141\n",
      "Epoch: 17 \tTraining Loss: 0.000141\n",
      "Epoch: 18 \tTraining Loss: 0.000141\n",
      "Epoch: 1 \tTraining Loss: 0.000155\n",
      "Epoch: 2 \tTraining Loss: 0.000155\n",
      "Epoch: 3 \tTraining Loss: 0.000155\n",
      "Epoch: 4 \tTraining Loss: 0.000155\n",
      "Epoch: 5 \tTraining Loss: 0.000155\n",
      "Epoch: 6 \tTraining Loss: 0.000155\n",
      "Epoch: 7 \tTraining Loss: 0.000155\n",
      "Epoch: 8 \tTraining Loss: 0.000155\n",
      "Epoch: 9 \tTraining Loss: 0.000155\n",
      "Epoch: 10 \tTraining Loss: 0.000155\n",
      "Epoch: 11 \tTraining Loss: 0.000155\n",
      "Epoch: 12 \tTraining Loss: 0.000155\n",
      "Epoch: 13 \tTraining Loss: 0.000155\n",
      "Epoch: 14 \tTraining Loss: 0.000155\n",
      "Epoch: 15 \tTraining Loss: 0.000155\n",
      "Epoch: 16 \tTraining Loss: 0.000155\n",
      "Epoch: 17 \tTraining Loss: 0.000155\n",
      "Epoch: 18 \tTraining Loss: 0.000155\n",
      "Epoch: 1 \tTraining Loss: 0.000255\n",
      "Epoch: 2 \tTraining Loss: 0.000255\n",
      "Epoch: 3 \tTraining Loss: 0.000255\n",
      "Epoch: 4 \tTraining Loss: 0.000255\n",
      "Epoch: 5 \tTraining Loss: 0.000255\n",
      "Epoch: 6 \tTraining Loss: 0.000255\n",
      "Epoch: 7 \tTraining Loss: 0.000255\n",
      "Epoch: 8 \tTraining Loss: 0.000255\n",
      "Epoch: 9 \tTraining Loss: 0.000255\n",
      "Epoch: 10 \tTraining Loss: 0.000255\n",
      "Epoch: 11 \tTraining Loss: 0.000255\n",
      "Epoch: 12 \tTraining Loss: 0.000255\n",
      "Epoch: 13 \tTraining Loss: 0.000255\n",
      "Epoch: 14 \tTraining Loss: 0.000255\n",
      "Epoch: 15 \tTraining Loss: 0.000255\n",
      "Epoch: 16 \tTraining Loss: 0.000255\n",
      "Epoch: 17 \tTraining Loss: 0.000255\n",
      "Epoch: 18 \tTraining Loss: 0.000255\n",
      "Epoch: 1 \tTraining Loss: 0.000434\n",
      "Epoch: 2 \tTraining Loss: 0.000434\n",
      "Epoch: 3 \tTraining Loss: 0.000434\n",
      "Epoch: 4 \tTraining Loss: 0.000434\n",
      "Epoch: 5 \tTraining Loss: 0.000434\n",
      "Epoch: 6 \tTraining Loss: 0.000434\n",
      "Epoch: 7 \tTraining Loss: 0.000434\n",
      "Epoch: 8 \tTraining Loss: 0.000434\n",
      "Epoch: 9 \tTraining Loss: 0.000434\n",
      "Epoch: 10 \tTraining Loss: 0.000434\n",
      "Epoch: 11 \tTraining Loss: 0.000434\n",
      "Epoch: 12 \tTraining Loss: 0.000434\n",
      "Epoch: 13 \tTraining Loss: 0.000434\n",
      "Epoch: 14 \tTraining Loss: 0.000434\n",
      "Epoch: 15 \tTraining Loss: 0.000434\n",
      "Epoch: 16 \tTraining Loss: 0.000434\n",
      "Epoch: 17 \tTraining Loss: 0.000434\n",
      "Epoch: 18 \tTraining Loss: 0.000434\n",
      "Epoch: 1 \tTraining Loss: 0.000180\n",
      "Epoch: 2 \tTraining Loss: 0.000180\n",
      "Epoch: 3 \tTraining Loss: 0.000180\n",
      "Epoch: 4 \tTraining Loss: 0.000180\n",
      "Epoch: 5 \tTraining Loss: 0.000180\n",
      "Epoch: 6 \tTraining Loss: 0.000180\n",
      "Epoch: 7 \tTraining Loss: 0.000180\n",
      "Epoch: 8 \tTraining Loss: 0.000180\n",
      "Epoch: 9 \tTraining Loss: 0.000180\n",
      "Epoch: 10 \tTraining Loss: 0.000180\n",
      "Epoch: 11 \tTraining Loss: 0.000180\n",
      "Epoch: 12 \tTraining Loss: 0.000180\n",
      "Epoch: 13 \tTraining Loss: 0.000180\n",
      "Epoch: 14 \tTraining Loss: 0.000180\n",
      "Epoch: 15 \tTraining Loss: 0.000180\n",
      "Epoch: 16 \tTraining Loss: 0.000180\n",
      "Epoch: 17 \tTraining Loss: 0.000180\n",
      "Epoch: 18 \tTraining Loss: 0.000180\n",
      "Epoch: 1 \tTraining Loss: 0.000207\n",
      "Epoch: 2 \tTraining Loss: 0.000207\n",
      "Epoch: 3 \tTraining Loss: 0.000207\n",
      "Epoch: 4 \tTraining Loss: 0.000207\n",
      "Epoch: 5 \tTraining Loss: 0.000207\n",
      "Epoch: 6 \tTraining Loss: 0.000207\n",
      "Epoch: 7 \tTraining Loss: 0.000207\n",
      "Epoch: 8 \tTraining Loss: 0.000207\n",
      "Epoch: 9 \tTraining Loss: 0.000207\n",
      "Epoch: 10 \tTraining Loss: 0.000207\n",
      "Epoch: 11 \tTraining Loss: 0.000207\n",
      "Epoch: 12 \tTraining Loss: 0.000207\n",
      "Epoch: 13 \tTraining Loss: 0.000207\n",
      "Epoch: 14 \tTraining Loss: 0.000207\n",
      "Epoch: 15 \tTraining Loss: 0.000207\n",
      "Epoch: 16 \tTraining Loss: 0.000207\n",
      "Epoch: 17 \tTraining Loss: 0.000207\n",
      "Epoch: 18 \tTraining Loss: 0.000207\n",
      "Epoch: 1 \tTraining Loss: 0.000313\n",
      "Epoch: 2 \tTraining Loss: 0.000313\n",
      "Epoch: 3 \tTraining Loss: 0.000313\n",
      "Epoch: 4 \tTraining Loss: 0.000313\n",
      "Epoch: 5 \tTraining Loss: 0.000313\n",
      "Epoch: 6 \tTraining Loss: 0.000313\n",
      "Epoch: 7 \tTraining Loss: 0.000313\n",
      "Epoch: 8 \tTraining Loss: 0.000313\n",
      "Epoch: 9 \tTraining Loss: 0.000313\n",
      "Epoch: 10 \tTraining Loss: 0.000313\n",
      "Epoch: 11 \tTraining Loss: 0.000313\n",
      "Epoch: 12 \tTraining Loss: 0.000313\n",
      "Epoch: 13 \tTraining Loss: 0.000313\n",
      "Epoch: 14 \tTraining Loss: 0.000313\n",
      "Epoch: 15 \tTraining Loss: 0.000313\n",
      "Epoch: 16 \tTraining Loss: 0.000313\n",
      "Epoch: 17 \tTraining Loss: 0.000313\n",
      "Epoch: 18 \tTraining Loss: 0.000313\n",
      "Epoch: 1 \tTraining Loss: 0.000169\n",
      "Epoch: 2 \tTraining Loss: 0.000169\n",
      "Epoch: 3 \tTraining Loss: 0.000169\n",
      "Epoch: 4 \tTraining Loss: 0.000169\n",
      "Epoch: 5 \tTraining Loss: 0.000169\n",
      "Epoch: 6 \tTraining Loss: 0.000169\n",
      "Epoch: 7 \tTraining Loss: 0.000169\n",
      "Epoch: 8 \tTraining Loss: 0.000169\n",
      "Epoch: 9 \tTraining Loss: 0.000169\n",
      "Epoch: 10 \tTraining Loss: 0.000169\n",
      "Epoch: 11 \tTraining Loss: 0.000169\n",
      "Epoch: 12 \tTraining Loss: 0.000169\n",
      "Epoch: 13 \tTraining Loss: 0.000169\n",
      "Epoch: 14 \tTraining Loss: 0.000169\n",
      "Epoch: 15 \tTraining Loss: 0.000169\n",
      "Epoch: 16 \tTraining Loss: 0.000169\n",
      "Epoch: 17 \tTraining Loss: 0.000169\n",
      "Epoch: 18 \tTraining Loss: 0.000169\n",
      "Epoch: 1 \tTraining Loss: 0.000179\n",
      "Epoch: 2 \tTraining Loss: 0.000179\n",
      "Epoch: 3 \tTraining Loss: 0.000179\n",
      "Epoch: 4 \tTraining Loss: 0.000179\n",
      "Epoch: 5 \tTraining Loss: 0.000179\n",
      "Epoch: 6 \tTraining Loss: 0.000179\n",
      "Epoch: 7 \tTraining Loss: 0.000179\n",
      "Epoch: 8 \tTraining Loss: 0.000179\n",
      "Epoch: 9 \tTraining Loss: 0.000179\n",
      "Epoch: 10 \tTraining Loss: 0.000179\n",
      "Epoch: 11 \tTraining Loss: 0.000179\n",
      "Epoch: 12 \tTraining Loss: 0.000179\n",
      "Epoch: 13 \tTraining Loss: 0.000179\n",
      "Epoch: 14 \tTraining Loss: 0.000179\n",
      "Epoch: 15 \tTraining Loss: 0.000179\n",
      "Epoch: 16 \tTraining Loss: 0.000179\n",
      "Epoch: 17 \tTraining Loss: 0.000179\n",
      "Epoch: 18 \tTraining Loss: 0.000179\n",
      "Epoch: 1 \tTraining Loss: 0.000267\n",
      "Epoch: 2 \tTraining Loss: 0.000267\n",
      "Epoch: 3 \tTraining Loss: 0.000267\n",
      "Epoch: 4 \tTraining Loss: 0.000267\n",
      "Epoch: 5 \tTraining Loss: 0.000267\n",
      "Epoch: 6 \tTraining Loss: 0.000267\n",
      "Epoch: 7 \tTraining Loss: 0.000267\n",
      "Epoch: 8 \tTraining Loss: 0.000267\n",
      "Epoch: 9 \tTraining Loss: 0.000267\n",
      "Epoch: 10 \tTraining Loss: 0.000267\n",
      "Epoch: 11 \tTraining Loss: 0.000267\n",
      "Epoch: 12 \tTraining Loss: 0.000267\n",
      "Epoch: 13 \tTraining Loss: 0.000267\n",
      "Epoch: 14 \tTraining Loss: 0.000267\n",
      "Epoch: 15 \tTraining Loss: 0.000267\n",
      "Epoch: 16 \tTraining Loss: 0.000267\n",
      "Epoch: 17 \tTraining Loss: 0.000267\n",
      "Epoch: 18 \tTraining Loss: 0.000267\n",
      "Epoch: 1 \tTraining Loss: 0.000195\n",
      "Epoch: 2 \tTraining Loss: 0.000195\n",
      "Epoch: 3 \tTraining Loss: 0.000195\n",
      "Epoch: 4 \tTraining Loss: 0.000195\n",
      "Epoch: 5 \tTraining Loss: 0.000195\n",
      "Epoch: 6 \tTraining Loss: 0.000195\n",
      "Epoch: 7 \tTraining Loss: 0.000195\n",
      "Epoch: 8 \tTraining Loss: 0.000195\n",
      "Epoch: 9 \tTraining Loss: 0.000195\n",
      "Epoch: 10 \tTraining Loss: 0.000195\n",
      "Epoch: 11 \tTraining Loss: 0.000195\n",
      "Epoch: 12 \tTraining Loss: 0.000195\n",
      "Epoch: 13 \tTraining Loss: 0.000195\n",
      "Epoch: 14 \tTraining Loss: 0.000195\n",
      "Epoch: 15 \tTraining Loss: 0.000195\n",
      "Epoch: 16 \tTraining Loss: 0.000195\n",
      "Epoch: 17 \tTraining Loss: 0.000195\n",
      "Epoch: 18 \tTraining Loss: 0.000195\n",
      "Epoch: 1 \tTraining Loss: 0.000269\n",
      "Epoch: 2 \tTraining Loss: 0.000269\n",
      "Epoch: 3 \tTraining Loss: 0.000269\n",
      "Epoch: 4 \tTraining Loss: 0.000269\n",
      "Epoch: 5 \tTraining Loss: 0.000269\n",
      "Epoch: 6 \tTraining Loss: 0.000269\n",
      "Epoch: 7 \tTraining Loss: 0.000269\n",
      "Epoch: 8 \tTraining Loss: 0.000269\n",
      "Epoch: 9 \tTraining Loss: 0.000269\n",
      "Epoch: 10 \tTraining Loss: 0.000269\n",
      "Epoch: 11 \tTraining Loss: 0.000269\n",
      "Epoch: 12 \tTraining Loss: 0.000269\n",
      "Epoch: 13 \tTraining Loss: 0.000269\n",
      "Epoch: 14 \tTraining Loss: 0.000269\n",
      "Epoch: 15 \tTraining Loss: 0.000269\n",
      "Epoch: 16 \tTraining Loss: 0.000269\n",
      "Epoch: 17 \tTraining Loss: 0.000269\n",
      "Epoch: 18 \tTraining Loss: 0.000269\n",
      "Epoch: 1 \tTraining Loss: 0.000265\n",
      "Epoch: 2 \tTraining Loss: 0.000265\n",
      "Epoch: 3 \tTraining Loss: 0.000265\n",
      "Epoch: 4 \tTraining Loss: 0.000265\n",
      "Epoch: 5 \tTraining Loss: 0.000265\n",
      "Epoch: 6 \tTraining Loss: 0.000265\n",
      "Epoch: 7 \tTraining Loss: 0.000265\n",
      "Epoch: 8 \tTraining Loss: 0.000265\n",
      "Epoch: 9 \tTraining Loss: 0.000265\n",
      "Epoch: 10 \tTraining Loss: 0.000265\n",
      "Epoch: 11 \tTraining Loss: 0.000265\n",
      "Epoch: 12 \tTraining Loss: 0.000265\n",
      "Epoch: 13 \tTraining Loss: 0.000265\n",
      "Epoch: 14 \tTraining Loss: 0.000265\n",
      "Epoch: 15 \tTraining Loss: 0.000265\n",
      "Epoch: 16 \tTraining Loss: 0.000265\n",
      "Epoch: 17 \tTraining Loss: 0.000265\n",
      "Epoch: 18 \tTraining Loss: 0.000265\n",
      "Epoch: 1 \tTraining Loss: 0.000314\n",
      "Epoch: 2 \tTraining Loss: 0.000314\n",
      "Epoch: 3 \tTraining Loss: 0.000314\n",
      "Epoch: 4 \tTraining Loss: 0.000314\n",
      "Epoch: 5 \tTraining Loss: 0.000314\n",
      "Epoch: 6 \tTraining Loss: 0.000314\n",
      "Epoch: 7 \tTraining Loss: 0.000314\n",
      "Epoch: 8 \tTraining Loss: 0.000314\n",
      "Epoch: 9 \tTraining Loss: 0.000314\n",
      "Epoch: 10 \tTraining Loss: 0.000314\n",
      "Epoch: 11 \tTraining Loss: 0.000314\n",
      "Epoch: 12 \tTraining Loss: 0.000314\n",
      "Epoch: 13 \tTraining Loss: 0.000314\n",
      "Epoch: 14 \tTraining Loss: 0.000314\n",
      "Epoch: 15 \tTraining Loss: 0.000314\n",
      "Epoch: 16 \tTraining Loss: 0.000314\n",
      "Epoch: 17 \tTraining Loss: 0.000314\n",
      "Epoch: 18 \tTraining Loss: 0.000314\n",
      "Epoch: 1 \tTraining Loss: 0.000217\n",
      "Epoch: 2 \tTraining Loss: 0.000217\n",
      "Epoch: 3 \tTraining Loss: 0.000217\n",
      "Epoch: 4 \tTraining Loss: 0.000217\n",
      "Epoch: 5 \tTraining Loss: 0.000217\n",
      "Epoch: 6 \tTraining Loss: 0.000217\n",
      "Epoch: 7 \tTraining Loss: 0.000217\n",
      "Epoch: 8 \tTraining Loss: 0.000217\n",
      "Epoch: 9 \tTraining Loss: 0.000217\n",
      "Epoch: 10 \tTraining Loss: 0.000217\n",
      "Epoch: 11 \tTraining Loss: 0.000217\n",
      "Epoch: 12 \tTraining Loss: 0.000217\n",
      "Epoch: 13 \tTraining Loss: 0.000217\n",
      "Epoch: 14 \tTraining Loss: 0.000217\n",
      "Epoch: 15 \tTraining Loss: 0.000217\n",
      "Epoch: 16 \tTraining Loss: 0.000217\n",
      "Epoch: 17 \tTraining Loss: 0.000217\n",
      "Epoch: 18 \tTraining Loss: 0.000217\n",
      "Epoch: 1 \tTraining Loss: 0.000128\n",
      "Epoch: 2 \tTraining Loss: 0.000128\n",
      "Epoch: 3 \tTraining Loss: 0.000128\n",
      "Epoch: 4 \tTraining Loss: 0.000128\n",
      "Epoch: 5 \tTraining Loss: 0.000128\n",
      "Epoch: 6 \tTraining Loss: 0.000128\n",
      "Epoch: 7 \tTraining Loss: 0.000128\n",
      "Epoch: 8 \tTraining Loss: 0.000128\n",
      "Epoch: 9 \tTraining Loss: 0.000128\n",
      "Epoch: 10 \tTraining Loss: 0.000128\n",
      "Epoch: 11 \tTraining Loss: 0.000128\n",
      "Epoch: 12 \tTraining Loss: 0.000128\n",
      "Epoch: 13 \tTraining Loss: 0.000128\n",
      "Epoch: 14 \tTraining Loss: 0.000128\n",
      "Epoch: 15 \tTraining Loss: 0.000128\n",
      "Epoch: 16 \tTraining Loss: 0.000128\n",
      "Epoch: 17 \tTraining Loss: 0.000128\n",
      "Epoch: 18 \tTraining Loss: 0.000128\n",
      "Epoch: 1 \tTraining Loss: 0.000248\n",
      "Epoch: 2 \tTraining Loss: 0.000248\n",
      "Epoch: 3 \tTraining Loss: 0.000248\n",
      "Epoch: 4 \tTraining Loss: 0.000248\n",
      "Epoch: 5 \tTraining Loss: 0.000248\n",
      "Epoch: 6 \tTraining Loss: 0.000248\n",
      "Epoch: 7 \tTraining Loss: 0.000248\n",
      "Epoch: 8 \tTraining Loss: 0.000248\n",
      "Epoch: 9 \tTraining Loss: 0.000248\n",
      "Epoch: 10 \tTraining Loss: 0.000248\n",
      "Epoch: 11 \tTraining Loss: 0.000248\n",
      "Epoch: 12 \tTraining Loss: 0.000248\n",
      "Epoch: 13 \tTraining Loss: 0.000248\n",
      "Epoch: 14 \tTraining Loss: 0.000248\n",
      "Epoch: 15 \tTraining Loss: 0.000248\n",
      "Epoch: 16 \tTraining Loss: 0.000248\n",
      "Epoch: 17 \tTraining Loss: 0.000248\n",
      "Epoch: 18 \tTraining Loss: 0.000248\n",
      "Epoch: 1 \tTraining Loss: 0.000243\n",
      "Epoch: 2 \tTraining Loss: 0.000243\n",
      "Epoch: 3 \tTraining Loss: 0.000243\n",
      "Epoch: 4 \tTraining Loss: 0.000243\n",
      "Epoch: 5 \tTraining Loss: 0.000243\n",
      "Epoch: 6 \tTraining Loss: 0.000243\n",
      "Epoch: 7 \tTraining Loss: 0.000243\n",
      "Epoch: 8 \tTraining Loss: 0.000243\n",
      "Epoch: 9 \tTraining Loss: 0.000243\n",
      "Epoch: 10 \tTraining Loss: 0.000243\n",
      "Epoch: 11 \tTraining Loss: 0.000243\n",
      "Epoch: 12 \tTraining Loss: 0.000243\n",
      "Epoch: 13 \tTraining Loss: 0.000243\n",
      "Epoch: 14 \tTraining Loss: 0.000243\n",
      "Epoch: 15 \tTraining Loss: 0.000243\n",
      "Epoch: 16 \tTraining Loss: 0.000243\n",
      "Epoch: 17 \tTraining Loss: 0.000243\n",
      "Epoch: 18 \tTraining Loss: 0.000243\n",
      "Epoch: 1 \tTraining Loss: 0.000277\n",
      "Epoch: 2 \tTraining Loss: 0.000277\n",
      "Epoch: 3 \tTraining Loss: 0.000277\n",
      "Epoch: 4 \tTraining Loss: 0.000277\n",
      "Epoch: 5 \tTraining Loss: 0.000277\n",
      "Epoch: 6 \tTraining Loss: 0.000277\n",
      "Epoch: 7 \tTraining Loss: 0.000277\n",
      "Epoch: 8 \tTraining Loss: 0.000277\n",
      "Epoch: 9 \tTraining Loss: 0.000277\n",
      "Epoch: 10 \tTraining Loss: 0.000277\n",
      "Epoch: 11 \tTraining Loss: 0.000277\n",
      "Epoch: 12 \tTraining Loss: 0.000277\n",
      "Epoch: 13 \tTraining Loss: 0.000277\n",
      "Epoch: 14 \tTraining Loss: 0.000277\n",
      "Epoch: 15 \tTraining Loss: 0.000277\n",
      "Epoch: 16 \tTraining Loss: 0.000277\n",
      "Epoch: 17 \tTraining Loss: 0.000277\n",
      "Epoch: 18 \tTraining Loss: 0.000277\n",
      "Epoch: 1 \tTraining Loss: 0.000119\n",
      "Epoch: 2 \tTraining Loss: 0.000119\n",
      "Epoch: 3 \tTraining Loss: 0.000119\n",
      "Epoch: 4 \tTraining Loss: 0.000119\n",
      "Epoch: 5 \tTraining Loss: 0.000119\n",
      "Epoch: 6 \tTraining Loss: 0.000119\n",
      "Epoch: 7 \tTraining Loss: 0.000119\n",
      "Epoch: 8 \tTraining Loss: 0.000119\n",
      "Epoch: 9 \tTraining Loss: 0.000119\n",
      "Epoch: 10 \tTraining Loss: 0.000119\n",
      "Epoch: 11 \tTraining Loss: 0.000119\n",
      "Epoch: 12 \tTraining Loss: 0.000119\n",
      "Epoch: 13 \tTraining Loss: 0.000119\n",
      "Epoch: 14 \tTraining Loss: 0.000119\n",
      "Epoch: 15 \tTraining Loss: 0.000119\n",
      "Epoch: 16 \tTraining Loss: 0.000119\n",
      "Epoch: 17 \tTraining Loss: 0.000119\n",
      "Epoch: 18 \tTraining Loss: 0.000119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000246\n",
      "Epoch: 2 \tTraining Loss: 0.000246\n",
      "Epoch: 3 \tTraining Loss: 0.000246\n",
      "Epoch: 4 \tTraining Loss: 0.000246\n",
      "Epoch: 5 \tTraining Loss: 0.000246\n",
      "Epoch: 6 \tTraining Loss: 0.000246\n",
      "Epoch: 7 \tTraining Loss: 0.000246\n",
      "Epoch: 8 \tTraining Loss: 0.000246\n",
      "Epoch: 9 \tTraining Loss: 0.000246\n",
      "Epoch: 10 \tTraining Loss: 0.000246\n",
      "Epoch: 11 \tTraining Loss: 0.000246\n",
      "Epoch: 12 \tTraining Loss: 0.000246\n",
      "Epoch: 13 \tTraining Loss: 0.000246\n",
      "Epoch: 14 \tTraining Loss: 0.000246\n",
      "Epoch: 15 \tTraining Loss: 0.000246\n",
      "Epoch: 16 \tTraining Loss: 0.000246\n",
      "Epoch: 17 \tTraining Loss: 0.000246\n",
      "Epoch: 18 \tTraining Loss: 0.000246\n",
      "Epoch: 1 \tTraining Loss: 0.000155\n",
      "Epoch: 2 \tTraining Loss: 0.000155\n",
      "Epoch: 3 \tTraining Loss: 0.000155\n",
      "Epoch: 4 \tTraining Loss: 0.000155\n",
      "Epoch: 5 \tTraining Loss: 0.000155\n",
      "Epoch: 6 \tTraining Loss: 0.000155\n",
      "Epoch: 7 \tTraining Loss: 0.000155\n",
      "Epoch: 8 \tTraining Loss: 0.000155\n",
      "Epoch: 9 \tTraining Loss: 0.000155\n",
      "Epoch: 10 \tTraining Loss: 0.000155\n",
      "Epoch: 11 \tTraining Loss: 0.000155\n",
      "Epoch: 12 \tTraining Loss: 0.000155\n",
      "Epoch: 13 \tTraining Loss: 0.000155\n",
      "Epoch: 14 \tTraining Loss: 0.000155\n",
      "Epoch: 15 \tTraining Loss: 0.000155\n",
      "Epoch: 16 \tTraining Loss: 0.000155\n",
      "Epoch: 17 \tTraining Loss: 0.000155\n",
      "Epoch: 18 \tTraining Loss: 0.000155\n",
      "Epoch: 1 \tTraining Loss: 0.000588\n",
      "Epoch: 2 \tTraining Loss: 0.000588\n",
      "Epoch: 3 \tTraining Loss: 0.000588\n",
      "Epoch: 4 \tTraining Loss: 0.000588\n",
      "Epoch: 5 \tTraining Loss: 0.000588\n",
      "Epoch: 6 \tTraining Loss: 0.000588\n",
      "Epoch: 7 \tTraining Loss: 0.000588\n",
      "Epoch: 8 \tTraining Loss: 0.000588\n",
      "Epoch: 9 \tTraining Loss: 0.000588\n",
      "Epoch: 10 \tTraining Loss: 0.000588\n",
      "Epoch: 11 \tTraining Loss: 0.000588\n",
      "Epoch: 12 \tTraining Loss: 0.000588\n",
      "Epoch: 13 \tTraining Loss: 0.000588\n",
      "Epoch: 14 \tTraining Loss: 0.000588\n",
      "Epoch: 15 \tTraining Loss: 0.000588\n",
      "Epoch: 16 \tTraining Loss: 0.000588\n",
      "Epoch: 17 \tTraining Loss: 0.000588\n",
      "Epoch: 18 \tTraining Loss: 0.000588\n",
      "Epoch: 1 \tTraining Loss: 0.000245\n",
      "Epoch: 2 \tTraining Loss: 0.000245\n",
      "Epoch: 3 \tTraining Loss: 0.000245\n",
      "Epoch: 4 \tTraining Loss: 0.000245\n",
      "Epoch: 5 \tTraining Loss: 0.000245\n",
      "Epoch: 6 \tTraining Loss: 0.000245\n",
      "Epoch: 7 \tTraining Loss: 0.000245\n",
      "Epoch: 8 \tTraining Loss: 0.000245\n",
      "Epoch: 9 \tTraining Loss: 0.000245\n",
      "Epoch: 10 \tTraining Loss: 0.000245\n",
      "Epoch: 11 \tTraining Loss: 0.000245\n",
      "Epoch: 12 \tTraining Loss: 0.000245\n",
      "Epoch: 13 \tTraining Loss: 0.000245\n",
      "Epoch: 14 \tTraining Loss: 0.000245\n",
      "Epoch: 15 \tTraining Loss: 0.000245\n",
      "Epoch: 16 \tTraining Loss: 0.000245\n",
      "Epoch: 17 \tTraining Loss: 0.000245\n",
      "Epoch: 18 \tTraining Loss: 0.000245\n",
      "Epoch: 1 \tTraining Loss: 0.000300\n",
      "Epoch: 2 \tTraining Loss: 0.000300\n",
      "Epoch: 3 \tTraining Loss: 0.000300\n",
      "Epoch: 4 \tTraining Loss: 0.000300\n",
      "Epoch: 5 \tTraining Loss: 0.000300\n",
      "Epoch: 6 \tTraining Loss: 0.000300\n",
      "Epoch: 7 \tTraining Loss: 0.000300\n",
      "Epoch: 8 \tTraining Loss: 0.000300\n",
      "Epoch: 9 \tTraining Loss: 0.000300\n",
      "Epoch: 10 \tTraining Loss: 0.000300\n",
      "Epoch: 11 \tTraining Loss: 0.000300\n",
      "Epoch: 12 \tTraining Loss: 0.000300\n",
      "Epoch: 13 \tTraining Loss: 0.000300\n",
      "Epoch: 14 \tTraining Loss: 0.000300\n",
      "Epoch: 15 \tTraining Loss: 0.000300\n",
      "Epoch: 16 \tTraining Loss: 0.000300\n",
      "Epoch: 17 \tTraining Loss: 0.000300\n",
      "Epoch: 18 \tTraining Loss: 0.000300\n",
      "Epoch: 1 \tTraining Loss: 0.000206\n",
      "Epoch: 2 \tTraining Loss: 0.000206\n",
      "Epoch: 3 \tTraining Loss: 0.000206\n",
      "Epoch: 4 \tTraining Loss: 0.000206\n",
      "Epoch: 5 \tTraining Loss: 0.000206\n",
      "Epoch: 6 \tTraining Loss: 0.000206\n",
      "Epoch: 7 \tTraining Loss: 0.000206\n",
      "Epoch: 8 \tTraining Loss: 0.000206\n",
      "Epoch: 9 \tTraining Loss: 0.000206\n",
      "Epoch: 10 \tTraining Loss: 0.000206\n",
      "Epoch: 11 \tTraining Loss: 0.000206\n",
      "Epoch: 12 \tTraining Loss: 0.000206\n",
      "Epoch: 13 \tTraining Loss: 0.000206\n",
      "Epoch: 14 \tTraining Loss: 0.000206\n",
      "Epoch: 15 \tTraining Loss: 0.000206\n",
      "Epoch: 16 \tTraining Loss: 0.000206\n",
      "Epoch: 17 \tTraining Loss: 0.000206\n",
      "Epoch: 18 \tTraining Loss: 0.000206\n",
      "Epoch: 1 \tTraining Loss: 0.000181\n",
      "Epoch: 2 \tTraining Loss: 0.000181\n",
      "Epoch: 3 \tTraining Loss: 0.000181\n",
      "Epoch: 4 \tTraining Loss: 0.000181\n",
      "Epoch: 5 \tTraining Loss: 0.000181\n",
      "Epoch: 6 \tTraining Loss: 0.000181\n",
      "Epoch: 7 \tTraining Loss: 0.000181\n",
      "Epoch: 8 \tTraining Loss: 0.000181\n",
      "Epoch: 9 \tTraining Loss: 0.000181\n",
      "Epoch: 10 \tTraining Loss: 0.000181\n",
      "Epoch: 11 \tTraining Loss: 0.000181\n",
      "Epoch: 12 \tTraining Loss: 0.000181\n",
      "Epoch: 13 \tTraining Loss: 0.000181\n",
      "Epoch: 14 \tTraining Loss: 0.000181\n",
      "Epoch: 15 \tTraining Loss: 0.000181\n",
      "Epoch: 16 \tTraining Loss: 0.000181\n",
      "Epoch: 17 \tTraining Loss: 0.000181\n",
      "Epoch: 18 \tTraining Loss: 0.000181\n",
      "Epoch: 1 \tTraining Loss: 0.000209\n",
      "Epoch: 2 \tTraining Loss: 0.000209\n",
      "Epoch: 3 \tTraining Loss: 0.000209\n",
      "Epoch: 4 \tTraining Loss: 0.000209\n",
      "Epoch: 5 \tTraining Loss: 0.000209\n",
      "Epoch: 6 \tTraining Loss: 0.000209\n",
      "Epoch: 7 \tTraining Loss: 0.000209\n",
      "Epoch: 8 \tTraining Loss: 0.000209\n",
      "Epoch: 9 \tTraining Loss: 0.000209\n",
      "Epoch: 10 \tTraining Loss: 0.000209\n",
      "Epoch: 11 \tTraining Loss: 0.000209\n",
      "Epoch: 12 \tTraining Loss: 0.000209\n",
      "Epoch: 13 \tTraining Loss: 0.000209\n",
      "Epoch: 14 \tTraining Loss: 0.000209\n",
      "Epoch: 15 \tTraining Loss: 0.000209\n",
      "Epoch: 16 \tTraining Loss: 0.000209\n",
      "Epoch: 17 \tTraining Loss: 0.000209\n",
      "Epoch: 18 \tTraining Loss: 0.000209\n",
      "Epoch: 1 \tTraining Loss: 0.000243\n",
      "Epoch: 2 \tTraining Loss: 0.000243\n",
      "Epoch: 3 \tTraining Loss: 0.000243\n",
      "Epoch: 4 \tTraining Loss: 0.000243\n",
      "Epoch: 5 \tTraining Loss: 0.000243\n",
      "Epoch: 6 \tTraining Loss: 0.000243\n",
      "Epoch: 7 \tTraining Loss: 0.000243\n",
      "Epoch: 8 \tTraining Loss: 0.000243\n",
      "Epoch: 9 \tTraining Loss: 0.000243\n",
      "Epoch: 10 \tTraining Loss: 0.000243\n",
      "Epoch: 11 \tTraining Loss: 0.000243\n",
      "Epoch: 12 \tTraining Loss: 0.000243\n",
      "Epoch: 13 \tTraining Loss: 0.000243\n",
      "Epoch: 14 \tTraining Loss: 0.000243\n",
      "Epoch: 15 \tTraining Loss: 0.000243\n",
      "Epoch: 16 \tTraining Loss: 0.000243\n",
      "Epoch: 17 \tTraining Loss: 0.000243\n",
      "Epoch: 18 \tTraining Loss: 0.000243\n",
      "Epoch: 1 \tTraining Loss: 0.000368\n",
      "Epoch: 2 \tTraining Loss: 0.000368\n",
      "Epoch: 3 \tTraining Loss: 0.000368\n",
      "Epoch: 4 \tTraining Loss: 0.000368\n",
      "Epoch: 5 \tTraining Loss: 0.000368\n",
      "Epoch: 6 \tTraining Loss: 0.000368\n",
      "Epoch: 7 \tTraining Loss: 0.000368\n",
      "Epoch: 8 \tTraining Loss: 0.000368\n",
      "Epoch: 9 \tTraining Loss: 0.000368\n",
      "Epoch: 10 \tTraining Loss: 0.000368\n",
      "Epoch: 11 \tTraining Loss: 0.000368\n",
      "Epoch: 12 \tTraining Loss: 0.000368\n",
      "Epoch: 13 \tTraining Loss: 0.000368\n",
      "Epoch: 14 \tTraining Loss: 0.000368\n",
      "Epoch: 15 \tTraining Loss: 0.000368\n",
      "Epoch: 16 \tTraining Loss: 0.000368\n",
      "Epoch: 17 \tTraining Loss: 0.000368\n",
      "Epoch: 18 \tTraining Loss: 0.000368\n",
      "Epoch: 1 \tTraining Loss: 0.000216\n",
      "Epoch: 2 \tTraining Loss: 0.000216\n",
      "Epoch: 3 \tTraining Loss: 0.000216\n",
      "Epoch: 4 \tTraining Loss: 0.000216\n",
      "Epoch: 5 \tTraining Loss: 0.000216\n",
      "Epoch: 6 \tTraining Loss: 0.000216\n",
      "Epoch: 7 \tTraining Loss: 0.000216\n",
      "Epoch: 8 \tTraining Loss: 0.000216\n",
      "Epoch: 9 \tTraining Loss: 0.000216\n",
      "Epoch: 10 \tTraining Loss: 0.000216\n",
      "Epoch: 11 \tTraining Loss: 0.000216\n",
      "Epoch: 12 \tTraining Loss: 0.000216\n",
      "Epoch: 13 \tTraining Loss: 0.000216\n",
      "Epoch: 14 \tTraining Loss: 0.000216\n",
      "Epoch: 15 \tTraining Loss: 0.000216\n",
      "Epoch: 16 \tTraining Loss: 0.000216\n",
      "Epoch: 17 \tTraining Loss: 0.000216\n",
      "Epoch: 18 \tTraining Loss: 0.000216\n",
      "Epoch: 1 \tTraining Loss: 0.000322\n",
      "Epoch: 2 \tTraining Loss: 0.000322\n",
      "Epoch: 3 \tTraining Loss: 0.000322\n",
      "Epoch: 4 \tTraining Loss: 0.000322\n",
      "Epoch: 5 \tTraining Loss: 0.000322\n",
      "Epoch: 6 \tTraining Loss: 0.000322\n",
      "Epoch: 7 \tTraining Loss: 0.000322\n",
      "Epoch: 8 \tTraining Loss: 0.000322\n",
      "Epoch: 9 \tTraining Loss: 0.000322\n",
      "Epoch: 10 \tTraining Loss: 0.000322\n",
      "Epoch: 11 \tTraining Loss: 0.000322\n",
      "Epoch: 12 \tTraining Loss: 0.000322\n",
      "Epoch: 13 \tTraining Loss: 0.000322\n",
      "Epoch: 14 \tTraining Loss: 0.000322\n",
      "Epoch: 15 \tTraining Loss: 0.000322\n",
      "Epoch: 16 \tTraining Loss: 0.000322\n",
      "Epoch: 17 \tTraining Loss: 0.000322\n",
      "Epoch: 18 \tTraining Loss: 0.000322\n",
      "Epoch: 1 \tTraining Loss: 0.000306\n",
      "Epoch: 2 \tTraining Loss: 0.000306\n",
      "Epoch: 3 \tTraining Loss: 0.000306\n",
      "Epoch: 4 \tTraining Loss: 0.000306\n",
      "Epoch: 5 \tTraining Loss: 0.000306\n",
      "Epoch: 6 \tTraining Loss: 0.000306\n",
      "Epoch: 7 \tTraining Loss: 0.000306\n",
      "Epoch: 8 \tTraining Loss: 0.000306\n",
      "Epoch: 9 \tTraining Loss: 0.000306\n",
      "Epoch: 10 \tTraining Loss: 0.000306\n",
      "Epoch: 11 \tTraining Loss: 0.000306\n",
      "Epoch: 12 \tTraining Loss: 0.000306\n",
      "Epoch: 13 \tTraining Loss: 0.000306\n",
      "Epoch: 14 \tTraining Loss: 0.000306\n",
      "Epoch: 15 \tTraining Loss: 0.000306\n",
      "Epoch: 16 \tTraining Loss: 0.000306\n",
      "Epoch: 17 \tTraining Loss: 0.000306\n",
      "Epoch: 18 \tTraining Loss: 0.000306\n",
      "Epoch: 1 \tTraining Loss: 0.000263\n",
      "Epoch: 2 \tTraining Loss: 0.000263\n",
      "Epoch: 3 \tTraining Loss: 0.000263\n",
      "Epoch: 4 \tTraining Loss: 0.000263\n",
      "Epoch: 5 \tTraining Loss: 0.000263\n",
      "Epoch: 6 \tTraining Loss: 0.000263\n",
      "Epoch: 7 \tTraining Loss: 0.000263\n",
      "Epoch: 8 \tTraining Loss: 0.000263\n",
      "Epoch: 9 \tTraining Loss: 0.000263\n",
      "Epoch: 10 \tTraining Loss: 0.000263\n",
      "Epoch: 11 \tTraining Loss: 0.000263\n",
      "Epoch: 12 \tTraining Loss: 0.000263\n",
      "Epoch: 13 \tTraining Loss: 0.000263\n",
      "Epoch: 14 \tTraining Loss: 0.000263\n",
      "Epoch: 15 \tTraining Loss: 0.000263\n",
      "Epoch: 16 \tTraining Loss: 0.000263\n",
      "Epoch: 17 \tTraining Loss: 0.000263\n",
      "Epoch: 18 \tTraining Loss: 0.000263\n",
      "Epoch: 1 \tTraining Loss: 0.000126\n",
      "Epoch: 2 \tTraining Loss: 0.000126\n",
      "Epoch: 3 \tTraining Loss: 0.000126\n",
      "Epoch: 4 \tTraining Loss: 0.000126\n",
      "Epoch: 5 \tTraining Loss: 0.000126\n",
      "Epoch: 6 \tTraining Loss: 0.000126\n",
      "Epoch: 7 \tTraining Loss: 0.000126\n",
      "Epoch: 8 \tTraining Loss: 0.000126\n",
      "Epoch: 9 \tTraining Loss: 0.000126\n",
      "Epoch: 10 \tTraining Loss: 0.000126\n",
      "Epoch: 11 \tTraining Loss: 0.000126\n",
      "Epoch: 12 \tTraining Loss: 0.000126\n",
      "Epoch: 13 \tTraining Loss: 0.000126\n",
      "Epoch: 14 \tTraining Loss: 0.000126\n",
      "Epoch: 15 \tTraining Loss: 0.000126\n",
      "Epoch: 16 \tTraining Loss: 0.000126\n",
      "Epoch: 17 \tTraining Loss: 0.000126\n",
      "Epoch: 18 \tTraining Loss: 0.000126\n",
      "Epoch: 1 \tTraining Loss: 0.000206\n",
      "Epoch: 2 \tTraining Loss: 0.000206\n",
      "Epoch: 3 \tTraining Loss: 0.000206\n",
      "Epoch: 4 \tTraining Loss: 0.000206\n",
      "Epoch: 5 \tTraining Loss: 0.000206\n",
      "Epoch: 6 \tTraining Loss: 0.000206\n",
      "Epoch: 7 \tTraining Loss: 0.000206\n",
      "Epoch: 8 \tTraining Loss: 0.000206\n",
      "Epoch: 9 \tTraining Loss: 0.000206\n",
      "Epoch: 10 \tTraining Loss: 0.000206\n",
      "Epoch: 11 \tTraining Loss: 0.000206\n",
      "Epoch: 12 \tTraining Loss: 0.000206\n",
      "Epoch: 13 \tTraining Loss: 0.000206\n",
      "Epoch: 14 \tTraining Loss: 0.000206\n",
      "Epoch: 15 \tTraining Loss: 0.000206\n",
      "Epoch: 16 \tTraining Loss: 0.000206\n",
      "Epoch: 17 \tTraining Loss: 0.000206\n",
      "Epoch: 18 \tTraining Loss: 0.000206\n",
      "Epoch: 1 \tTraining Loss: 0.000314\n",
      "Epoch: 2 \tTraining Loss: 0.000314\n",
      "Epoch: 3 \tTraining Loss: 0.000314\n",
      "Epoch: 4 \tTraining Loss: 0.000314\n",
      "Epoch: 5 \tTraining Loss: 0.000314\n",
      "Epoch: 6 \tTraining Loss: 0.000314\n",
      "Epoch: 7 \tTraining Loss: 0.000314\n",
      "Epoch: 8 \tTraining Loss: 0.000314\n",
      "Epoch: 9 \tTraining Loss: 0.000314\n",
      "Epoch: 10 \tTraining Loss: 0.000314\n",
      "Epoch: 11 \tTraining Loss: 0.000314\n",
      "Epoch: 12 \tTraining Loss: 0.000314\n",
      "Epoch: 13 \tTraining Loss: 0.000314\n",
      "Epoch: 14 \tTraining Loss: 0.000314\n",
      "Epoch: 15 \tTraining Loss: 0.000314\n",
      "Epoch: 16 \tTraining Loss: 0.000314\n",
      "Epoch: 17 \tTraining Loss: 0.000314\n",
      "Epoch: 18 \tTraining Loss: 0.000314\n",
      "Epoch: 1 \tTraining Loss: 0.000289\n",
      "Epoch: 2 \tTraining Loss: 0.000289\n",
      "Epoch: 3 \tTraining Loss: 0.000289\n",
      "Epoch: 4 \tTraining Loss: 0.000289\n",
      "Epoch: 5 \tTraining Loss: 0.000289\n",
      "Epoch: 6 \tTraining Loss: 0.000289\n",
      "Epoch: 7 \tTraining Loss: 0.000289\n",
      "Epoch: 8 \tTraining Loss: 0.000289\n",
      "Epoch: 9 \tTraining Loss: 0.000289\n",
      "Epoch: 10 \tTraining Loss: 0.000289\n",
      "Epoch: 11 \tTraining Loss: 0.000289\n",
      "Epoch: 12 \tTraining Loss: 0.000289\n",
      "Epoch: 13 \tTraining Loss: 0.000289\n",
      "Epoch: 14 \tTraining Loss: 0.000289\n",
      "Epoch: 15 \tTraining Loss: 0.000289\n",
      "Epoch: 16 \tTraining Loss: 0.000289\n",
      "Epoch: 17 \tTraining Loss: 0.000289\n",
      "Epoch: 18 \tTraining Loss: 0.000289\n",
      "Epoch: 1 \tTraining Loss: 0.000281\n",
      "Epoch: 2 \tTraining Loss: 0.000281\n",
      "Epoch: 3 \tTraining Loss: 0.000281\n",
      "Epoch: 4 \tTraining Loss: 0.000281\n",
      "Epoch: 5 \tTraining Loss: 0.000281\n",
      "Epoch: 6 \tTraining Loss: 0.000281\n",
      "Epoch: 7 \tTraining Loss: 0.000281\n",
      "Epoch: 8 \tTraining Loss: 0.000281\n",
      "Epoch: 9 \tTraining Loss: 0.000281\n",
      "Epoch: 10 \tTraining Loss: 0.000281\n",
      "Epoch: 11 \tTraining Loss: 0.000281\n",
      "Epoch: 12 \tTraining Loss: 0.000281\n",
      "Epoch: 13 \tTraining Loss: 0.000281\n",
      "Epoch: 14 \tTraining Loss: 0.000281\n",
      "Epoch: 15 \tTraining Loss: 0.000281\n",
      "Epoch: 16 \tTraining Loss: 0.000281\n",
      "Epoch: 17 \tTraining Loss: 0.000281\n",
      "Epoch: 18 \tTraining Loss: 0.000281\n",
      "Epoch: 1 \tTraining Loss: 0.000131\n",
      "Epoch: 2 \tTraining Loss: 0.000131\n",
      "Epoch: 3 \tTraining Loss: 0.000131\n",
      "Epoch: 4 \tTraining Loss: 0.000131\n",
      "Epoch: 5 \tTraining Loss: 0.000131\n",
      "Epoch: 6 \tTraining Loss: 0.000131\n",
      "Epoch: 7 \tTraining Loss: 0.000131\n",
      "Epoch: 8 \tTraining Loss: 0.000131\n",
      "Epoch: 9 \tTraining Loss: 0.000131\n",
      "Epoch: 10 \tTraining Loss: 0.000131\n",
      "Epoch: 11 \tTraining Loss: 0.000131\n",
      "Epoch: 12 \tTraining Loss: 0.000131\n",
      "Epoch: 13 \tTraining Loss: 0.000131\n",
      "Epoch: 14 \tTraining Loss: 0.000131\n",
      "Epoch: 15 \tTraining Loss: 0.000131\n",
      "Epoch: 16 \tTraining Loss: 0.000131\n",
      "Epoch: 17 \tTraining Loss: 0.000131\n",
      "Epoch: 18 \tTraining Loss: 0.000131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000117\n",
      "Epoch: 2 \tTraining Loss: 0.000117\n",
      "Epoch: 3 \tTraining Loss: 0.000117\n",
      "Epoch: 4 \tTraining Loss: 0.000117\n",
      "Epoch: 5 \tTraining Loss: 0.000117\n",
      "Epoch: 6 \tTraining Loss: 0.000117\n",
      "Epoch: 7 \tTraining Loss: 0.000117\n",
      "Epoch: 8 \tTraining Loss: 0.000117\n",
      "Epoch: 9 \tTraining Loss: 0.000117\n",
      "Epoch: 10 \tTraining Loss: 0.000117\n",
      "Epoch: 11 \tTraining Loss: 0.000117\n",
      "Epoch: 12 \tTraining Loss: 0.000117\n",
      "Epoch: 13 \tTraining Loss: 0.000117\n",
      "Epoch: 14 \tTraining Loss: 0.000117\n",
      "Epoch: 15 \tTraining Loss: 0.000117\n",
      "Epoch: 16 \tTraining Loss: 0.000117\n",
      "Epoch: 17 \tTraining Loss: 0.000117\n",
      "Epoch: 18 \tTraining Loss: 0.000117\n",
      "Epoch: 1 \tTraining Loss: 0.000104\n",
      "Epoch: 2 \tTraining Loss: 0.000104\n",
      "Epoch: 3 \tTraining Loss: 0.000104\n",
      "Epoch: 4 \tTraining Loss: 0.000104\n",
      "Epoch: 5 \tTraining Loss: 0.000104\n",
      "Epoch: 6 \tTraining Loss: 0.000104\n",
      "Epoch: 7 \tTraining Loss: 0.000104\n",
      "Epoch: 8 \tTraining Loss: 0.000104\n",
      "Epoch: 9 \tTraining Loss: 0.000104\n",
      "Epoch: 10 \tTraining Loss: 0.000104\n",
      "Epoch: 11 \tTraining Loss: 0.000104\n",
      "Epoch: 12 \tTraining Loss: 0.000104\n",
      "Epoch: 13 \tTraining Loss: 0.000104\n",
      "Epoch: 14 \tTraining Loss: 0.000104\n",
      "Epoch: 15 \tTraining Loss: 0.000104\n",
      "Epoch: 16 \tTraining Loss: 0.000104\n",
      "Epoch: 17 \tTraining Loss: 0.000104\n",
      "Epoch: 18 \tTraining Loss: 0.000104\n",
      "Epoch: 1 \tTraining Loss: 0.000263\n",
      "Epoch: 2 \tTraining Loss: 0.000263\n",
      "Epoch: 3 \tTraining Loss: 0.000263\n",
      "Epoch: 4 \tTraining Loss: 0.000263\n",
      "Epoch: 5 \tTraining Loss: 0.000263\n",
      "Epoch: 6 \tTraining Loss: 0.000263\n",
      "Epoch: 7 \tTraining Loss: 0.000263\n",
      "Epoch: 8 \tTraining Loss: 0.000263\n",
      "Epoch: 9 \tTraining Loss: 0.000263\n",
      "Epoch: 10 \tTraining Loss: 0.000263\n",
      "Epoch: 11 \tTraining Loss: 0.000263\n",
      "Epoch: 12 \tTraining Loss: 0.000263\n",
      "Epoch: 13 \tTraining Loss: 0.000263\n",
      "Epoch: 14 \tTraining Loss: 0.000263\n",
      "Epoch: 15 \tTraining Loss: 0.000263\n",
      "Epoch: 16 \tTraining Loss: 0.000263\n",
      "Epoch: 17 \tTraining Loss: 0.000263\n",
      "Epoch: 18 \tTraining Loss: 0.000263\n",
      "Epoch: 1 \tTraining Loss: 0.000166\n",
      "Epoch: 2 \tTraining Loss: 0.000166\n",
      "Epoch: 3 \tTraining Loss: 0.000166\n",
      "Epoch: 4 \tTraining Loss: 0.000166\n",
      "Epoch: 5 \tTraining Loss: 0.000166\n",
      "Epoch: 6 \tTraining Loss: 0.000166\n",
      "Epoch: 7 \tTraining Loss: 0.000166\n",
      "Epoch: 8 \tTraining Loss: 0.000166\n",
      "Epoch: 9 \tTraining Loss: 0.000166\n",
      "Epoch: 10 \tTraining Loss: 0.000166\n",
      "Epoch: 11 \tTraining Loss: 0.000166\n",
      "Epoch: 12 \tTraining Loss: 0.000166\n",
      "Epoch: 13 \tTraining Loss: 0.000166\n",
      "Epoch: 14 \tTraining Loss: 0.000166\n",
      "Epoch: 15 \tTraining Loss: 0.000166\n",
      "Epoch: 16 \tTraining Loss: 0.000166\n",
      "Epoch: 17 \tTraining Loss: 0.000166\n",
      "Epoch: 18 \tTraining Loss: 0.000166\n",
      "Epoch: 1 \tTraining Loss: 0.000181\n",
      "Epoch: 2 \tTraining Loss: 0.000181\n",
      "Epoch: 3 \tTraining Loss: 0.000181\n",
      "Epoch: 4 \tTraining Loss: 0.000181\n",
      "Epoch: 5 \tTraining Loss: 0.000181\n",
      "Epoch: 6 \tTraining Loss: 0.000181\n",
      "Epoch: 7 \tTraining Loss: 0.000181\n",
      "Epoch: 8 \tTraining Loss: 0.000181\n",
      "Epoch: 9 \tTraining Loss: 0.000181\n",
      "Epoch: 10 \tTraining Loss: 0.000181\n",
      "Epoch: 11 \tTraining Loss: 0.000181\n",
      "Epoch: 12 \tTraining Loss: 0.000181\n",
      "Epoch: 13 \tTraining Loss: 0.000181\n",
      "Epoch: 14 \tTraining Loss: 0.000181\n",
      "Epoch: 15 \tTraining Loss: 0.000181\n",
      "Epoch: 16 \tTraining Loss: 0.000181\n",
      "Epoch: 17 \tTraining Loss: 0.000181\n",
      "Epoch: 18 \tTraining Loss: 0.000181\n",
      "Epoch: 1 \tTraining Loss: 0.000250\n",
      "Epoch: 2 \tTraining Loss: 0.000250\n",
      "Epoch: 3 \tTraining Loss: 0.000250\n",
      "Epoch: 4 \tTraining Loss: 0.000250\n",
      "Epoch: 5 \tTraining Loss: 0.000250\n",
      "Epoch: 6 \tTraining Loss: 0.000250\n",
      "Epoch: 7 \tTraining Loss: 0.000250\n",
      "Epoch: 8 \tTraining Loss: 0.000250\n",
      "Epoch: 9 \tTraining Loss: 0.000250\n",
      "Epoch: 10 \tTraining Loss: 0.000250\n",
      "Epoch: 11 \tTraining Loss: 0.000250\n",
      "Epoch: 12 \tTraining Loss: 0.000250\n",
      "Epoch: 13 \tTraining Loss: 0.000250\n",
      "Epoch: 14 \tTraining Loss: 0.000250\n",
      "Epoch: 15 \tTraining Loss: 0.000250\n",
      "Epoch: 16 \tTraining Loss: 0.000250\n",
      "Epoch: 17 \tTraining Loss: 0.000250\n",
      "Epoch: 18 \tTraining Loss: 0.000250\n",
      "Epoch: 1 \tTraining Loss: 0.000236\n",
      "Epoch: 2 \tTraining Loss: 0.000236\n",
      "Epoch: 3 \tTraining Loss: 0.000236\n",
      "Epoch: 4 \tTraining Loss: 0.000236\n",
      "Epoch: 5 \tTraining Loss: 0.000236\n",
      "Epoch: 6 \tTraining Loss: 0.000236\n",
      "Epoch: 7 \tTraining Loss: 0.000236\n",
      "Epoch: 8 \tTraining Loss: 0.000236\n",
      "Epoch: 9 \tTraining Loss: 0.000236\n",
      "Epoch: 10 \tTraining Loss: 0.000236\n",
      "Epoch: 11 \tTraining Loss: 0.000236\n",
      "Epoch: 12 \tTraining Loss: 0.000236\n",
      "Epoch: 13 \tTraining Loss: 0.000236\n",
      "Epoch: 14 \tTraining Loss: 0.000236\n",
      "Epoch: 15 \tTraining Loss: 0.000236\n",
      "Epoch: 16 \tTraining Loss: 0.000236\n",
      "Epoch: 17 \tTraining Loss: 0.000236\n",
      "Epoch: 18 \tTraining Loss: 0.000236\n",
      "Epoch: 1 \tTraining Loss: 0.000388\n",
      "Epoch: 2 \tTraining Loss: 0.000388\n",
      "Epoch: 3 \tTraining Loss: 0.000388\n",
      "Epoch: 4 \tTraining Loss: 0.000388\n",
      "Epoch: 5 \tTraining Loss: 0.000388\n",
      "Epoch: 6 \tTraining Loss: 0.000388\n",
      "Epoch: 7 \tTraining Loss: 0.000388\n",
      "Epoch: 8 \tTraining Loss: 0.000388\n",
      "Epoch: 9 \tTraining Loss: 0.000388\n",
      "Epoch: 10 \tTraining Loss: 0.000388\n",
      "Epoch: 11 \tTraining Loss: 0.000388\n",
      "Epoch: 12 \tTraining Loss: 0.000388\n",
      "Epoch: 13 \tTraining Loss: 0.000388\n",
      "Epoch: 14 \tTraining Loss: 0.000388\n",
      "Epoch: 15 \tTraining Loss: 0.000388\n",
      "Epoch: 16 \tTraining Loss: 0.000388\n",
      "Epoch: 17 \tTraining Loss: 0.000388\n",
      "Epoch: 18 \tTraining Loss: 0.000388\n",
      "Epoch: 1 \tTraining Loss: 0.000392\n",
      "Epoch: 2 \tTraining Loss: 0.000392\n",
      "Epoch: 3 \tTraining Loss: 0.000392\n",
      "Epoch: 4 \tTraining Loss: 0.000392\n",
      "Epoch: 5 \tTraining Loss: 0.000392\n",
      "Epoch: 6 \tTraining Loss: 0.000392\n",
      "Epoch: 7 \tTraining Loss: 0.000392\n",
      "Epoch: 8 \tTraining Loss: 0.000392\n",
      "Epoch: 9 \tTraining Loss: 0.000392\n",
      "Epoch: 10 \tTraining Loss: 0.000392\n",
      "Epoch: 11 \tTraining Loss: 0.000392\n",
      "Epoch: 12 \tTraining Loss: 0.000392\n",
      "Epoch: 13 \tTraining Loss: 0.000392\n",
      "Epoch: 14 \tTraining Loss: 0.000392\n",
      "Epoch: 15 \tTraining Loss: 0.000392\n",
      "Epoch: 16 \tTraining Loss: 0.000392\n",
      "Epoch: 17 \tTraining Loss: 0.000392\n",
      "Epoch: 18 \tTraining Loss: 0.000392\n",
      "Epoch: 1 \tTraining Loss: 0.000202\n",
      "Epoch: 2 \tTraining Loss: 0.000202\n",
      "Epoch: 3 \tTraining Loss: 0.000202\n",
      "Epoch: 4 \tTraining Loss: 0.000202\n",
      "Epoch: 5 \tTraining Loss: 0.000202\n",
      "Epoch: 6 \tTraining Loss: 0.000202\n",
      "Epoch: 7 \tTraining Loss: 0.000202\n",
      "Epoch: 8 \tTraining Loss: 0.000202\n",
      "Epoch: 9 \tTraining Loss: 0.000202\n",
      "Epoch: 10 \tTraining Loss: 0.000202\n",
      "Epoch: 11 \tTraining Loss: 0.000202\n",
      "Epoch: 12 \tTraining Loss: 0.000202\n",
      "Epoch: 13 \tTraining Loss: 0.000202\n",
      "Epoch: 14 \tTraining Loss: 0.000202\n",
      "Epoch: 15 \tTraining Loss: 0.000202\n",
      "Epoch: 16 \tTraining Loss: 0.000202\n",
      "Epoch: 17 \tTraining Loss: 0.000202\n",
      "Epoch: 18 \tTraining Loss: 0.000202\n",
      "Epoch: 1 \tTraining Loss: 0.000137\n",
      "Epoch: 2 \tTraining Loss: 0.000137\n",
      "Epoch: 3 \tTraining Loss: 0.000137\n",
      "Epoch: 4 \tTraining Loss: 0.000137\n",
      "Epoch: 5 \tTraining Loss: 0.000137\n",
      "Epoch: 6 \tTraining Loss: 0.000137\n",
      "Epoch: 7 \tTraining Loss: 0.000137\n",
      "Epoch: 8 \tTraining Loss: 0.000137\n",
      "Epoch: 9 \tTraining Loss: 0.000137\n",
      "Epoch: 10 \tTraining Loss: 0.000137\n",
      "Epoch: 11 \tTraining Loss: 0.000137\n",
      "Epoch: 12 \tTraining Loss: 0.000137\n",
      "Epoch: 13 \tTraining Loss: 0.000137\n",
      "Epoch: 14 \tTraining Loss: 0.000137\n",
      "Epoch: 15 \tTraining Loss: 0.000137\n",
      "Epoch: 16 \tTraining Loss: 0.000137\n",
      "Epoch: 17 \tTraining Loss: 0.000137\n",
      "Epoch: 18 \tTraining Loss: 0.000137\n",
      "Epoch: 1 \tTraining Loss: 0.000199\n",
      "Epoch: 2 \tTraining Loss: 0.000199\n",
      "Epoch: 3 \tTraining Loss: 0.000199\n",
      "Epoch: 4 \tTraining Loss: 0.000199\n",
      "Epoch: 5 \tTraining Loss: 0.000199\n",
      "Epoch: 6 \tTraining Loss: 0.000199\n",
      "Epoch: 7 \tTraining Loss: 0.000199\n",
      "Epoch: 8 \tTraining Loss: 0.000199\n",
      "Epoch: 9 \tTraining Loss: 0.000199\n",
      "Epoch: 10 \tTraining Loss: 0.000199\n",
      "Epoch: 11 \tTraining Loss: 0.000199\n",
      "Epoch: 12 \tTraining Loss: 0.000199\n",
      "Epoch: 13 \tTraining Loss: 0.000199\n",
      "Epoch: 14 \tTraining Loss: 0.000199\n",
      "Epoch: 15 \tTraining Loss: 0.000199\n",
      "Epoch: 16 \tTraining Loss: 0.000199\n",
      "Epoch: 17 \tTraining Loss: 0.000199\n",
      "Epoch: 18 \tTraining Loss: 0.000199\n",
      "Epoch: 1 \tTraining Loss: 0.000269\n",
      "Epoch: 2 \tTraining Loss: 0.000269\n",
      "Epoch: 3 \tTraining Loss: 0.000269\n",
      "Epoch: 4 \tTraining Loss: 0.000269\n",
      "Epoch: 5 \tTraining Loss: 0.000269\n",
      "Epoch: 6 \tTraining Loss: 0.000269\n",
      "Epoch: 7 \tTraining Loss: 0.000269\n",
      "Epoch: 8 \tTraining Loss: 0.000269\n",
      "Epoch: 9 \tTraining Loss: 0.000269\n",
      "Epoch: 10 \tTraining Loss: 0.000269\n",
      "Epoch: 11 \tTraining Loss: 0.000269\n",
      "Epoch: 12 \tTraining Loss: 0.000269\n",
      "Epoch: 13 \tTraining Loss: 0.000269\n",
      "Epoch: 14 \tTraining Loss: 0.000269\n",
      "Epoch: 15 \tTraining Loss: 0.000269\n",
      "Epoch: 16 \tTraining Loss: 0.000269\n",
      "Epoch: 17 \tTraining Loss: 0.000269\n",
      "Epoch: 18 \tTraining Loss: 0.000269\n",
      "Epoch: 1 \tTraining Loss: 0.000311\n",
      "Epoch: 2 \tTraining Loss: 0.000311\n",
      "Epoch: 3 \tTraining Loss: 0.000311\n",
      "Epoch: 4 \tTraining Loss: 0.000311\n",
      "Epoch: 5 \tTraining Loss: 0.000311\n",
      "Epoch: 6 \tTraining Loss: 0.000311\n",
      "Epoch: 7 \tTraining Loss: 0.000311\n",
      "Epoch: 8 \tTraining Loss: 0.000311\n",
      "Epoch: 9 \tTraining Loss: 0.000311\n",
      "Epoch: 10 \tTraining Loss: 0.000311\n",
      "Epoch: 11 \tTraining Loss: 0.000311\n",
      "Epoch: 12 \tTraining Loss: 0.000311\n",
      "Epoch: 13 \tTraining Loss: 0.000311\n",
      "Epoch: 14 \tTraining Loss: 0.000311\n",
      "Epoch: 15 \tTraining Loss: 0.000311\n",
      "Epoch: 16 \tTraining Loss: 0.000311\n",
      "Epoch: 17 \tTraining Loss: 0.000311\n",
      "Epoch: 18 \tTraining Loss: 0.000311\n",
      "Epoch: 1 \tTraining Loss: 0.000118\n",
      "Epoch: 2 \tTraining Loss: 0.000118\n",
      "Epoch: 3 \tTraining Loss: 0.000118\n",
      "Epoch: 4 \tTraining Loss: 0.000118\n",
      "Epoch: 5 \tTraining Loss: 0.000118\n",
      "Epoch: 6 \tTraining Loss: 0.000118\n",
      "Epoch: 7 \tTraining Loss: 0.000118\n",
      "Epoch: 8 \tTraining Loss: 0.000118\n",
      "Epoch: 9 \tTraining Loss: 0.000118\n",
      "Epoch: 10 \tTraining Loss: 0.000118\n",
      "Epoch: 11 \tTraining Loss: 0.000118\n",
      "Epoch: 12 \tTraining Loss: 0.000118\n",
      "Epoch: 13 \tTraining Loss: 0.000118\n",
      "Epoch: 14 \tTraining Loss: 0.000118\n",
      "Epoch: 15 \tTraining Loss: 0.000118\n",
      "Epoch: 16 \tTraining Loss: 0.000118\n",
      "Epoch: 17 \tTraining Loss: 0.000118\n",
      "Epoch: 18 \tTraining Loss: 0.000118\n",
      "Epoch: 1 \tTraining Loss: 0.000323\n",
      "Epoch: 2 \tTraining Loss: 0.000323\n",
      "Epoch: 3 \tTraining Loss: 0.000323\n",
      "Epoch: 4 \tTraining Loss: 0.000323\n",
      "Epoch: 5 \tTraining Loss: 0.000323\n",
      "Epoch: 6 \tTraining Loss: 0.000323\n",
      "Epoch: 7 \tTraining Loss: 0.000323\n",
      "Epoch: 8 \tTraining Loss: 0.000323\n",
      "Epoch: 9 \tTraining Loss: 0.000323\n",
      "Epoch: 10 \tTraining Loss: 0.000323\n",
      "Epoch: 11 \tTraining Loss: 0.000323\n",
      "Epoch: 12 \tTraining Loss: 0.000323\n",
      "Epoch: 13 \tTraining Loss: 0.000323\n",
      "Epoch: 14 \tTraining Loss: 0.000323\n",
      "Epoch: 15 \tTraining Loss: 0.000323\n",
      "Epoch: 16 \tTraining Loss: 0.000323\n",
      "Epoch: 17 \tTraining Loss: 0.000323\n",
      "Epoch: 18 \tTraining Loss: 0.000323\n",
      "Epoch: 1 \tTraining Loss: 0.000215\n",
      "Epoch: 2 \tTraining Loss: 0.000215\n",
      "Epoch: 3 \tTraining Loss: 0.000215\n",
      "Epoch: 4 \tTraining Loss: 0.000215\n",
      "Epoch: 5 \tTraining Loss: 0.000215\n",
      "Epoch: 6 \tTraining Loss: 0.000215\n",
      "Epoch: 7 \tTraining Loss: 0.000215\n",
      "Epoch: 8 \tTraining Loss: 0.000215\n",
      "Epoch: 9 \tTraining Loss: 0.000215\n",
      "Epoch: 10 \tTraining Loss: 0.000215\n",
      "Epoch: 11 \tTraining Loss: 0.000215\n",
      "Epoch: 12 \tTraining Loss: 0.000215\n",
      "Epoch: 13 \tTraining Loss: 0.000215\n",
      "Epoch: 14 \tTraining Loss: 0.000215\n",
      "Epoch: 15 \tTraining Loss: 0.000215\n",
      "Epoch: 16 \tTraining Loss: 0.000215\n",
      "Epoch: 17 \tTraining Loss: 0.000215\n",
      "Epoch: 18 \tTraining Loss: 0.000215\n",
      "Epoch: 1 \tTraining Loss: 0.000207\n",
      "Epoch: 2 \tTraining Loss: 0.000207\n",
      "Epoch: 3 \tTraining Loss: 0.000207\n",
      "Epoch: 4 \tTraining Loss: 0.000207\n",
      "Epoch: 5 \tTraining Loss: 0.000207\n",
      "Epoch: 6 \tTraining Loss: 0.000207\n",
      "Epoch: 7 \tTraining Loss: 0.000207\n",
      "Epoch: 8 \tTraining Loss: 0.000207\n",
      "Epoch: 9 \tTraining Loss: 0.000207\n",
      "Epoch: 10 \tTraining Loss: 0.000207\n",
      "Epoch: 11 \tTraining Loss: 0.000207\n",
      "Epoch: 12 \tTraining Loss: 0.000207\n",
      "Epoch: 13 \tTraining Loss: 0.000207\n",
      "Epoch: 14 \tTraining Loss: 0.000207\n",
      "Epoch: 15 \tTraining Loss: 0.000207\n",
      "Epoch: 16 \tTraining Loss: 0.000207\n",
      "Epoch: 17 \tTraining Loss: 0.000207\n",
      "Epoch: 18 \tTraining Loss: 0.000207\n",
      "Epoch: 1 \tTraining Loss: 0.000143\n",
      "Epoch: 2 \tTraining Loss: 0.000143\n",
      "Epoch: 3 \tTraining Loss: 0.000143\n",
      "Epoch: 4 \tTraining Loss: 0.000143\n",
      "Epoch: 5 \tTraining Loss: 0.000143\n",
      "Epoch: 6 \tTraining Loss: 0.000143\n",
      "Epoch: 7 \tTraining Loss: 0.000143\n",
      "Epoch: 8 \tTraining Loss: 0.000143\n",
      "Epoch: 9 \tTraining Loss: 0.000143\n",
      "Epoch: 10 \tTraining Loss: 0.000143\n",
      "Epoch: 11 \tTraining Loss: 0.000143\n",
      "Epoch: 12 \tTraining Loss: 0.000143\n",
      "Epoch: 13 \tTraining Loss: 0.000143\n",
      "Epoch: 14 \tTraining Loss: 0.000143\n",
      "Epoch: 15 \tTraining Loss: 0.000143\n",
      "Epoch: 16 \tTraining Loss: 0.000143\n",
      "Epoch: 17 \tTraining Loss: 0.000143\n",
      "Epoch: 18 \tTraining Loss: 0.000143\n",
      "Epoch: 1 \tTraining Loss: 0.000295\n",
      "Epoch: 2 \tTraining Loss: 0.000295\n",
      "Epoch: 3 \tTraining Loss: 0.000295\n",
      "Epoch: 4 \tTraining Loss: 0.000295\n",
      "Epoch: 5 \tTraining Loss: 0.000295\n",
      "Epoch: 6 \tTraining Loss: 0.000295\n",
      "Epoch: 7 \tTraining Loss: 0.000295\n",
      "Epoch: 8 \tTraining Loss: 0.000295\n",
      "Epoch: 9 \tTraining Loss: 0.000295\n",
      "Epoch: 10 \tTraining Loss: 0.000295\n",
      "Epoch: 11 \tTraining Loss: 0.000295\n",
      "Epoch: 12 \tTraining Loss: 0.000295\n",
      "Epoch: 13 \tTraining Loss: 0.000295\n",
      "Epoch: 14 \tTraining Loss: 0.000295\n",
      "Epoch: 15 \tTraining Loss: 0.000295\n",
      "Epoch: 16 \tTraining Loss: 0.000295\n",
      "Epoch: 17 \tTraining Loss: 0.000295\n",
      "Epoch: 18 \tTraining Loss: 0.000295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000197\n",
      "Epoch: 2 \tTraining Loss: 0.000197\n",
      "Epoch: 3 \tTraining Loss: 0.000197\n",
      "Epoch: 4 \tTraining Loss: 0.000197\n",
      "Epoch: 5 \tTraining Loss: 0.000197\n",
      "Epoch: 6 \tTraining Loss: 0.000197\n",
      "Epoch: 7 \tTraining Loss: 0.000197\n",
      "Epoch: 8 \tTraining Loss: 0.000197\n",
      "Epoch: 9 \tTraining Loss: 0.000197\n",
      "Epoch: 10 \tTraining Loss: 0.000197\n",
      "Epoch: 11 \tTraining Loss: 0.000197\n",
      "Epoch: 12 \tTraining Loss: 0.000197\n",
      "Epoch: 13 \tTraining Loss: 0.000197\n",
      "Epoch: 14 \tTraining Loss: 0.000197\n",
      "Epoch: 15 \tTraining Loss: 0.000197\n",
      "Epoch: 16 \tTraining Loss: 0.000197\n",
      "Epoch: 17 \tTraining Loss: 0.000197\n",
      "Epoch: 18 \tTraining Loss: 0.000197\n",
      "Epoch: 1 \tTraining Loss: 0.000181\n",
      "Epoch: 2 \tTraining Loss: 0.000181\n",
      "Epoch: 3 \tTraining Loss: 0.000181\n",
      "Epoch: 4 \tTraining Loss: 0.000181\n",
      "Epoch: 5 \tTraining Loss: 0.000181\n",
      "Epoch: 6 \tTraining Loss: 0.000181\n",
      "Epoch: 7 \tTraining Loss: 0.000181\n",
      "Epoch: 8 \tTraining Loss: 0.000181\n",
      "Epoch: 9 \tTraining Loss: 0.000181\n",
      "Epoch: 10 \tTraining Loss: 0.000181\n",
      "Epoch: 11 \tTraining Loss: 0.000181\n",
      "Epoch: 12 \tTraining Loss: 0.000181\n",
      "Epoch: 13 \tTraining Loss: 0.000181\n",
      "Epoch: 14 \tTraining Loss: 0.000181\n",
      "Epoch: 15 \tTraining Loss: 0.000181\n",
      "Epoch: 16 \tTraining Loss: 0.000181\n",
      "Epoch: 17 \tTraining Loss: 0.000181\n",
      "Epoch: 18 \tTraining Loss: 0.000181\n",
      "Epoch: 1 \tTraining Loss: 0.000163\n",
      "Epoch: 2 \tTraining Loss: 0.000163\n",
      "Epoch: 3 \tTraining Loss: 0.000163\n",
      "Epoch: 4 \tTraining Loss: 0.000163\n",
      "Epoch: 5 \tTraining Loss: 0.000163\n",
      "Epoch: 6 \tTraining Loss: 0.000163\n",
      "Epoch: 7 \tTraining Loss: 0.000163\n",
      "Epoch: 8 \tTraining Loss: 0.000163\n",
      "Epoch: 9 \tTraining Loss: 0.000163\n",
      "Epoch: 10 \tTraining Loss: 0.000163\n",
      "Epoch: 11 \tTraining Loss: 0.000163\n",
      "Epoch: 12 \tTraining Loss: 0.000163\n",
      "Epoch: 13 \tTraining Loss: 0.000163\n",
      "Epoch: 14 \tTraining Loss: 0.000163\n",
      "Epoch: 15 \tTraining Loss: 0.000163\n",
      "Epoch: 16 \tTraining Loss: 0.000163\n",
      "Epoch: 17 \tTraining Loss: 0.000163\n",
      "Epoch: 18 \tTraining Loss: 0.000163\n",
      "Epoch: 1 \tTraining Loss: 0.000175\n",
      "Epoch: 2 \tTraining Loss: 0.000175\n",
      "Epoch: 3 \tTraining Loss: 0.000175\n",
      "Epoch: 4 \tTraining Loss: 0.000175\n",
      "Epoch: 5 \tTraining Loss: 0.000175\n",
      "Epoch: 6 \tTraining Loss: 0.000175\n",
      "Epoch: 7 \tTraining Loss: 0.000175\n",
      "Epoch: 8 \tTraining Loss: 0.000175\n",
      "Epoch: 9 \tTraining Loss: 0.000175\n",
      "Epoch: 10 \tTraining Loss: 0.000175\n",
      "Epoch: 11 \tTraining Loss: 0.000175\n",
      "Epoch: 12 \tTraining Loss: 0.000175\n",
      "Epoch: 13 \tTraining Loss: 0.000175\n",
      "Epoch: 14 \tTraining Loss: 0.000175\n",
      "Epoch: 15 \tTraining Loss: 0.000175\n",
      "Epoch: 16 \tTraining Loss: 0.000175\n",
      "Epoch: 17 \tTraining Loss: 0.000175\n",
      "Epoch: 18 \tTraining Loss: 0.000175\n",
      "Epoch: 1 \tTraining Loss: 0.000275\n",
      "Epoch: 2 \tTraining Loss: 0.000275\n",
      "Epoch: 3 \tTraining Loss: 0.000275\n",
      "Epoch: 4 \tTraining Loss: 0.000275\n",
      "Epoch: 5 \tTraining Loss: 0.000275\n",
      "Epoch: 6 \tTraining Loss: 0.000275\n",
      "Epoch: 7 \tTraining Loss: 0.000275\n",
      "Epoch: 8 \tTraining Loss: 0.000275\n",
      "Epoch: 9 \tTraining Loss: 0.000275\n",
      "Epoch: 10 \tTraining Loss: 0.000275\n",
      "Epoch: 11 \tTraining Loss: 0.000275\n",
      "Epoch: 12 \tTraining Loss: 0.000275\n",
      "Epoch: 13 \tTraining Loss: 0.000275\n",
      "Epoch: 14 \tTraining Loss: 0.000275\n",
      "Epoch: 15 \tTraining Loss: 0.000275\n",
      "Epoch: 16 \tTraining Loss: 0.000275\n",
      "Epoch: 17 \tTraining Loss: 0.000275\n",
      "Epoch: 18 \tTraining Loss: 0.000275\n",
      "Epoch: 1 \tTraining Loss: 0.000116\n",
      "Epoch: 2 \tTraining Loss: 0.000116\n",
      "Epoch: 3 \tTraining Loss: 0.000116\n",
      "Epoch: 4 \tTraining Loss: 0.000116\n",
      "Epoch: 5 \tTraining Loss: 0.000116\n",
      "Epoch: 6 \tTraining Loss: 0.000116\n",
      "Epoch: 7 \tTraining Loss: 0.000116\n",
      "Epoch: 8 \tTraining Loss: 0.000116\n",
      "Epoch: 9 \tTraining Loss: 0.000116\n",
      "Epoch: 10 \tTraining Loss: 0.000116\n",
      "Epoch: 11 \tTraining Loss: 0.000116\n",
      "Epoch: 12 \tTraining Loss: 0.000116\n",
      "Epoch: 13 \tTraining Loss: 0.000116\n",
      "Epoch: 14 \tTraining Loss: 0.000116\n",
      "Epoch: 15 \tTraining Loss: 0.000116\n",
      "Epoch: 16 \tTraining Loss: 0.000116\n",
      "Epoch: 17 \tTraining Loss: 0.000116\n",
      "Epoch: 18 \tTraining Loss: 0.000116\n",
      "Epoch: 1 \tTraining Loss: 0.000173\n",
      "Epoch: 2 \tTraining Loss: 0.000173\n",
      "Epoch: 3 \tTraining Loss: 0.000173\n",
      "Epoch: 4 \tTraining Loss: 0.000173\n",
      "Epoch: 5 \tTraining Loss: 0.000173\n",
      "Epoch: 6 \tTraining Loss: 0.000173\n",
      "Epoch: 7 \tTraining Loss: 0.000173\n",
      "Epoch: 8 \tTraining Loss: 0.000173\n",
      "Epoch: 9 \tTraining Loss: 0.000173\n",
      "Epoch: 10 \tTraining Loss: 0.000173\n",
      "Epoch: 11 \tTraining Loss: 0.000173\n",
      "Epoch: 12 \tTraining Loss: 0.000173\n",
      "Epoch: 13 \tTraining Loss: 0.000173\n",
      "Epoch: 14 \tTraining Loss: 0.000173\n",
      "Epoch: 15 \tTraining Loss: 0.000173\n",
      "Epoch: 16 \tTraining Loss: 0.000173\n",
      "Epoch: 17 \tTraining Loss: 0.000173\n",
      "Epoch: 18 \tTraining Loss: 0.000173\n",
      "Epoch: 1 \tTraining Loss: 0.000156\n",
      "Epoch: 2 \tTraining Loss: 0.000156\n",
      "Epoch: 3 \tTraining Loss: 0.000156\n",
      "Epoch: 4 \tTraining Loss: 0.000156\n",
      "Epoch: 5 \tTraining Loss: 0.000156\n",
      "Epoch: 6 \tTraining Loss: 0.000156\n",
      "Epoch: 7 \tTraining Loss: 0.000156\n",
      "Epoch: 8 \tTraining Loss: 0.000156\n",
      "Epoch: 9 \tTraining Loss: 0.000156\n",
      "Epoch: 10 \tTraining Loss: 0.000156\n",
      "Epoch: 11 \tTraining Loss: 0.000156\n",
      "Epoch: 12 \tTraining Loss: 0.000156\n",
      "Epoch: 13 \tTraining Loss: 0.000156\n",
      "Epoch: 14 \tTraining Loss: 0.000156\n",
      "Epoch: 15 \tTraining Loss: 0.000156\n",
      "Epoch: 16 \tTraining Loss: 0.000156\n",
      "Epoch: 17 \tTraining Loss: 0.000156\n",
      "Epoch: 18 \tTraining Loss: 0.000156\n",
      "Epoch: 1 \tTraining Loss: 0.000133\n",
      "Epoch: 2 \tTraining Loss: 0.000133\n",
      "Epoch: 3 \tTraining Loss: 0.000133\n",
      "Epoch: 4 \tTraining Loss: 0.000133\n",
      "Epoch: 5 \tTraining Loss: 0.000133\n",
      "Epoch: 6 \tTraining Loss: 0.000133\n",
      "Epoch: 7 \tTraining Loss: 0.000133\n",
      "Epoch: 8 \tTraining Loss: 0.000133\n",
      "Epoch: 9 \tTraining Loss: 0.000133\n",
      "Epoch: 10 \tTraining Loss: 0.000133\n",
      "Epoch: 11 \tTraining Loss: 0.000133\n",
      "Epoch: 12 \tTraining Loss: 0.000133\n",
      "Epoch: 13 \tTraining Loss: 0.000133\n",
      "Epoch: 14 \tTraining Loss: 0.000133\n",
      "Epoch: 15 \tTraining Loss: 0.000133\n",
      "Epoch: 16 \tTraining Loss: 0.000133\n",
      "Epoch: 17 \tTraining Loss: 0.000133\n",
      "Epoch: 18 \tTraining Loss: 0.000133\n",
      "Epoch: 1 \tTraining Loss: 0.000156\n",
      "Epoch: 2 \tTraining Loss: 0.000156\n",
      "Epoch: 3 \tTraining Loss: 0.000156\n",
      "Epoch: 4 \tTraining Loss: 0.000156\n",
      "Epoch: 5 \tTraining Loss: 0.000156\n",
      "Epoch: 6 \tTraining Loss: 0.000156\n",
      "Epoch: 7 \tTraining Loss: 0.000156\n",
      "Epoch: 8 \tTraining Loss: 0.000156\n",
      "Epoch: 9 \tTraining Loss: 0.000156\n",
      "Epoch: 10 \tTraining Loss: 0.000156\n",
      "Epoch: 11 \tTraining Loss: 0.000156\n",
      "Epoch: 12 \tTraining Loss: 0.000156\n",
      "Epoch: 13 \tTraining Loss: 0.000156\n",
      "Epoch: 14 \tTraining Loss: 0.000156\n",
      "Epoch: 15 \tTraining Loss: 0.000156\n",
      "Epoch: 16 \tTraining Loss: 0.000156\n",
      "Epoch: 17 \tTraining Loss: 0.000156\n",
      "Epoch: 18 \tTraining Loss: 0.000156\n",
      "Epoch: 1 \tTraining Loss: 0.000162\n",
      "Epoch: 2 \tTraining Loss: 0.000162\n",
      "Epoch: 3 \tTraining Loss: 0.000162\n",
      "Epoch: 4 \tTraining Loss: 0.000162\n",
      "Epoch: 5 \tTraining Loss: 0.000162\n",
      "Epoch: 6 \tTraining Loss: 0.000162\n",
      "Epoch: 7 \tTraining Loss: 0.000162\n",
      "Epoch: 8 \tTraining Loss: 0.000162\n",
      "Epoch: 9 \tTraining Loss: 0.000162\n",
      "Epoch: 10 \tTraining Loss: 0.000162\n",
      "Epoch: 11 \tTraining Loss: 0.000162\n",
      "Epoch: 12 \tTraining Loss: 0.000162\n",
      "Epoch: 13 \tTraining Loss: 0.000162\n",
      "Epoch: 14 \tTraining Loss: 0.000162\n",
      "Epoch: 15 \tTraining Loss: 0.000162\n",
      "Epoch: 16 \tTraining Loss: 0.000162\n",
      "Epoch: 17 \tTraining Loss: 0.000162\n",
      "Epoch: 18 \tTraining Loss: 0.000162\n",
      "Epoch: 1 \tTraining Loss: 0.000256\n",
      "Epoch: 2 \tTraining Loss: 0.000256\n",
      "Epoch: 3 \tTraining Loss: 0.000256\n",
      "Epoch: 4 \tTraining Loss: 0.000256\n",
      "Epoch: 5 \tTraining Loss: 0.000256\n",
      "Epoch: 6 \tTraining Loss: 0.000256\n",
      "Epoch: 7 \tTraining Loss: 0.000256\n",
      "Epoch: 8 \tTraining Loss: 0.000256\n",
      "Epoch: 9 \tTraining Loss: 0.000256\n",
      "Epoch: 10 \tTraining Loss: 0.000256\n",
      "Epoch: 11 \tTraining Loss: 0.000256\n",
      "Epoch: 12 \tTraining Loss: 0.000256\n",
      "Epoch: 13 \tTraining Loss: 0.000256\n",
      "Epoch: 14 \tTraining Loss: 0.000256\n",
      "Epoch: 15 \tTraining Loss: 0.000256\n",
      "Epoch: 16 \tTraining Loss: 0.000256\n",
      "Epoch: 17 \tTraining Loss: 0.000256\n",
      "Epoch: 18 \tTraining Loss: 0.000256\n",
      "Epoch: 1 \tTraining Loss: 0.000190\n",
      "Epoch: 2 \tTraining Loss: 0.000190\n",
      "Epoch: 3 \tTraining Loss: 0.000190\n",
      "Epoch: 4 \tTraining Loss: 0.000190\n",
      "Epoch: 5 \tTraining Loss: 0.000190\n",
      "Epoch: 6 \tTraining Loss: 0.000190\n",
      "Epoch: 7 \tTraining Loss: 0.000190\n",
      "Epoch: 8 \tTraining Loss: 0.000190\n",
      "Epoch: 9 \tTraining Loss: 0.000190\n",
      "Epoch: 10 \tTraining Loss: 0.000190\n",
      "Epoch: 11 \tTraining Loss: 0.000190\n",
      "Epoch: 12 \tTraining Loss: 0.000190\n",
      "Epoch: 13 \tTraining Loss: 0.000190\n",
      "Epoch: 14 \tTraining Loss: 0.000190\n",
      "Epoch: 15 \tTraining Loss: 0.000190\n",
      "Epoch: 16 \tTraining Loss: 0.000190\n",
      "Epoch: 17 \tTraining Loss: 0.000190\n",
      "Epoch: 18 \tTraining Loss: 0.000190\n",
      "Epoch: 1 \tTraining Loss: 0.000116\n",
      "Epoch: 2 \tTraining Loss: 0.000116\n",
      "Epoch: 3 \tTraining Loss: 0.000116\n",
      "Epoch: 4 \tTraining Loss: 0.000116\n",
      "Epoch: 5 \tTraining Loss: 0.000116\n",
      "Epoch: 6 \tTraining Loss: 0.000116\n",
      "Epoch: 7 \tTraining Loss: 0.000116\n",
      "Epoch: 8 \tTraining Loss: 0.000116\n",
      "Epoch: 9 \tTraining Loss: 0.000116\n",
      "Epoch: 10 \tTraining Loss: 0.000116\n",
      "Epoch: 11 \tTraining Loss: 0.000116\n",
      "Epoch: 12 \tTraining Loss: 0.000116\n",
      "Epoch: 13 \tTraining Loss: 0.000116\n",
      "Epoch: 14 \tTraining Loss: 0.000116\n",
      "Epoch: 15 \tTraining Loss: 0.000116\n",
      "Epoch: 16 \tTraining Loss: 0.000116\n",
      "Epoch: 17 \tTraining Loss: 0.000116\n",
      "Epoch: 18 \tTraining Loss: 0.000116\n",
      "Epoch: 1 \tTraining Loss: 0.000277\n",
      "Epoch: 2 \tTraining Loss: 0.000277\n",
      "Epoch: 3 \tTraining Loss: 0.000277\n",
      "Epoch: 4 \tTraining Loss: 0.000277\n",
      "Epoch: 5 \tTraining Loss: 0.000277\n",
      "Epoch: 6 \tTraining Loss: 0.000277\n",
      "Epoch: 7 \tTraining Loss: 0.000277\n",
      "Epoch: 8 \tTraining Loss: 0.000277\n",
      "Epoch: 9 \tTraining Loss: 0.000277\n",
      "Epoch: 10 \tTraining Loss: 0.000277\n",
      "Epoch: 11 \tTraining Loss: 0.000277\n",
      "Epoch: 12 \tTraining Loss: 0.000277\n",
      "Epoch: 13 \tTraining Loss: 0.000277\n",
      "Epoch: 14 \tTraining Loss: 0.000277\n",
      "Epoch: 15 \tTraining Loss: 0.000277\n",
      "Epoch: 16 \tTraining Loss: 0.000277\n",
      "Epoch: 17 \tTraining Loss: 0.000277\n",
      "Epoch: 18 \tTraining Loss: 0.000277\n",
      "Epoch: 1 \tTraining Loss: 0.000123\n",
      "Epoch: 2 \tTraining Loss: 0.000123\n",
      "Epoch: 3 \tTraining Loss: 0.000123\n",
      "Epoch: 4 \tTraining Loss: 0.000123\n",
      "Epoch: 5 \tTraining Loss: 0.000123\n",
      "Epoch: 6 \tTraining Loss: 0.000123\n",
      "Epoch: 7 \tTraining Loss: 0.000123\n",
      "Epoch: 8 \tTraining Loss: 0.000123\n",
      "Epoch: 9 \tTraining Loss: 0.000123\n",
      "Epoch: 10 \tTraining Loss: 0.000123\n",
      "Epoch: 11 \tTraining Loss: 0.000123\n",
      "Epoch: 12 \tTraining Loss: 0.000123\n",
      "Epoch: 13 \tTraining Loss: 0.000123\n",
      "Epoch: 14 \tTraining Loss: 0.000123\n",
      "Epoch: 15 \tTraining Loss: 0.000123\n",
      "Epoch: 16 \tTraining Loss: 0.000123\n",
      "Epoch: 17 \tTraining Loss: 0.000123\n",
      "Epoch: 18 \tTraining Loss: 0.000123\n",
      "Epoch: 1 \tTraining Loss: 0.000161\n",
      "Epoch: 2 \tTraining Loss: 0.000161\n",
      "Epoch: 3 \tTraining Loss: 0.000161\n",
      "Epoch: 4 \tTraining Loss: 0.000161\n",
      "Epoch: 5 \tTraining Loss: 0.000161\n",
      "Epoch: 6 \tTraining Loss: 0.000161\n",
      "Epoch: 7 \tTraining Loss: 0.000161\n",
      "Epoch: 8 \tTraining Loss: 0.000161\n",
      "Epoch: 9 \tTraining Loss: 0.000161\n",
      "Epoch: 10 \tTraining Loss: 0.000161\n",
      "Epoch: 11 \tTraining Loss: 0.000161\n",
      "Epoch: 12 \tTraining Loss: 0.000161\n",
      "Epoch: 13 \tTraining Loss: 0.000161\n",
      "Epoch: 14 \tTraining Loss: 0.000161\n",
      "Epoch: 15 \tTraining Loss: 0.000161\n",
      "Epoch: 16 \tTraining Loss: 0.000161\n",
      "Epoch: 17 \tTraining Loss: 0.000161\n",
      "Epoch: 18 \tTraining Loss: 0.000161\n",
      "Epoch: 1 \tTraining Loss: 0.000348\n",
      "Epoch: 2 \tTraining Loss: 0.000348\n",
      "Epoch: 3 \tTraining Loss: 0.000348\n",
      "Epoch: 4 \tTraining Loss: 0.000348\n",
      "Epoch: 5 \tTraining Loss: 0.000348\n",
      "Epoch: 6 \tTraining Loss: 0.000348\n",
      "Epoch: 7 \tTraining Loss: 0.000348\n",
      "Epoch: 8 \tTraining Loss: 0.000348\n",
      "Epoch: 9 \tTraining Loss: 0.000348\n",
      "Epoch: 10 \tTraining Loss: 0.000348\n",
      "Epoch: 11 \tTraining Loss: 0.000348\n",
      "Epoch: 12 \tTraining Loss: 0.000348\n",
      "Epoch: 13 \tTraining Loss: 0.000348\n",
      "Epoch: 14 \tTraining Loss: 0.000348\n",
      "Epoch: 15 \tTraining Loss: 0.000348\n",
      "Epoch: 16 \tTraining Loss: 0.000348\n",
      "Epoch: 17 \tTraining Loss: 0.000348\n",
      "Epoch: 18 \tTraining Loss: 0.000348\n",
      "Epoch: 1 \tTraining Loss: 0.000239\n",
      "Epoch: 2 \tTraining Loss: 0.000239\n",
      "Epoch: 3 \tTraining Loss: 0.000239\n",
      "Epoch: 4 \tTraining Loss: 0.000239\n",
      "Epoch: 5 \tTraining Loss: 0.000239\n",
      "Epoch: 6 \tTraining Loss: 0.000239\n",
      "Epoch: 7 \tTraining Loss: 0.000239\n",
      "Epoch: 8 \tTraining Loss: 0.000239\n",
      "Epoch: 9 \tTraining Loss: 0.000239\n",
      "Epoch: 10 \tTraining Loss: 0.000239\n",
      "Epoch: 11 \tTraining Loss: 0.000239\n",
      "Epoch: 12 \tTraining Loss: 0.000239\n",
      "Epoch: 13 \tTraining Loss: 0.000239\n",
      "Epoch: 14 \tTraining Loss: 0.000239\n",
      "Epoch: 15 \tTraining Loss: 0.000239\n",
      "Epoch: 16 \tTraining Loss: 0.000239\n",
      "Epoch: 17 \tTraining Loss: 0.000239\n",
      "Epoch: 18 \tTraining Loss: 0.000239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000215\n",
      "Epoch: 2 \tTraining Loss: 0.000215\n",
      "Epoch: 3 \tTraining Loss: 0.000215\n",
      "Epoch: 4 \tTraining Loss: 0.000215\n",
      "Epoch: 5 \tTraining Loss: 0.000215\n",
      "Epoch: 6 \tTraining Loss: 0.000215\n",
      "Epoch: 7 \tTraining Loss: 0.000215\n",
      "Epoch: 8 \tTraining Loss: 0.000215\n",
      "Epoch: 9 \tTraining Loss: 0.000215\n",
      "Epoch: 10 \tTraining Loss: 0.000215\n",
      "Epoch: 11 \tTraining Loss: 0.000215\n",
      "Epoch: 12 \tTraining Loss: 0.000215\n",
      "Epoch: 13 \tTraining Loss: 0.000215\n",
      "Epoch: 14 \tTraining Loss: 0.000215\n",
      "Epoch: 15 \tTraining Loss: 0.000215\n",
      "Epoch: 16 \tTraining Loss: 0.000215\n",
      "Epoch: 17 \tTraining Loss: 0.000215\n",
      "Epoch: 18 \tTraining Loss: 0.000215\n",
      "Epoch: 1 \tTraining Loss: 0.000278\n",
      "Epoch: 2 \tTraining Loss: 0.000278\n",
      "Epoch: 3 \tTraining Loss: 0.000278\n",
      "Epoch: 4 \tTraining Loss: 0.000278\n",
      "Epoch: 5 \tTraining Loss: 0.000278\n",
      "Epoch: 6 \tTraining Loss: 0.000278\n",
      "Epoch: 7 \tTraining Loss: 0.000278\n",
      "Epoch: 8 \tTraining Loss: 0.000278\n",
      "Epoch: 9 \tTraining Loss: 0.000278\n",
      "Epoch: 10 \tTraining Loss: 0.000278\n",
      "Epoch: 11 \tTraining Loss: 0.000278\n",
      "Epoch: 12 \tTraining Loss: 0.000278\n",
      "Epoch: 13 \tTraining Loss: 0.000278\n",
      "Epoch: 14 \tTraining Loss: 0.000278\n",
      "Epoch: 15 \tTraining Loss: 0.000278\n",
      "Epoch: 16 \tTraining Loss: 0.000278\n",
      "Epoch: 17 \tTraining Loss: 0.000278\n",
      "Epoch: 18 \tTraining Loss: 0.000278\n",
      "Epoch: 1 \tTraining Loss: 0.000278\n",
      "Epoch: 2 \tTraining Loss: 0.000278\n",
      "Epoch: 3 \tTraining Loss: 0.000278\n",
      "Epoch: 4 \tTraining Loss: 0.000278\n",
      "Epoch: 5 \tTraining Loss: 0.000278\n",
      "Epoch: 6 \tTraining Loss: 0.000278\n",
      "Epoch: 7 \tTraining Loss: 0.000278\n",
      "Epoch: 8 \tTraining Loss: 0.000278\n",
      "Epoch: 9 \tTraining Loss: 0.000278\n",
      "Epoch: 10 \tTraining Loss: 0.000278\n",
      "Epoch: 11 \tTraining Loss: 0.000278\n",
      "Epoch: 12 \tTraining Loss: 0.000278\n",
      "Epoch: 13 \tTraining Loss: 0.000278\n",
      "Epoch: 14 \tTraining Loss: 0.000278\n",
      "Epoch: 15 \tTraining Loss: 0.000278\n",
      "Epoch: 16 \tTraining Loss: 0.000278\n",
      "Epoch: 17 \tTraining Loss: 0.000278\n",
      "Epoch: 18 \tTraining Loss: 0.000278\n",
      "Epoch: 1 \tTraining Loss: 0.000233\n",
      "Epoch: 2 \tTraining Loss: 0.000233\n",
      "Epoch: 3 \tTraining Loss: 0.000233\n",
      "Epoch: 4 \tTraining Loss: 0.000233\n",
      "Epoch: 5 \tTraining Loss: 0.000233\n",
      "Epoch: 6 \tTraining Loss: 0.000233\n",
      "Epoch: 7 \tTraining Loss: 0.000233\n",
      "Epoch: 8 \tTraining Loss: 0.000233\n",
      "Epoch: 9 \tTraining Loss: 0.000233\n",
      "Epoch: 10 \tTraining Loss: 0.000233\n",
      "Epoch: 11 \tTraining Loss: 0.000233\n",
      "Epoch: 12 \tTraining Loss: 0.000233\n",
      "Epoch: 13 \tTraining Loss: 0.000233\n",
      "Epoch: 14 \tTraining Loss: 0.000233\n",
      "Epoch: 15 \tTraining Loss: 0.000233\n",
      "Epoch: 16 \tTraining Loss: 0.000233\n",
      "Epoch: 17 \tTraining Loss: 0.000233\n",
      "Epoch: 18 \tTraining Loss: 0.000233\n",
      "Epoch: 1 \tTraining Loss: 0.000294\n",
      "Epoch: 2 \tTraining Loss: 0.000294\n",
      "Epoch: 3 \tTraining Loss: 0.000294\n",
      "Epoch: 4 \tTraining Loss: 0.000294\n",
      "Epoch: 5 \tTraining Loss: 0.000294\n",
      "Epoch: 6 \tTraining Loss: 0.000294\n",
      "Epoch: 7 \tTraining Loss: 0.000294\n",
      "Epoch: 8 \tTraining Loss: 0.000294\n",
      "Epoch: 9 \tTraining Loss: 0.000294\n",
      "Epoch: 10 \tTraining Loss: 0.000294\n",
      "Epoch: 11 \tTraining Loss: 0.000294\n",
      "Epoch: 12 \tTraining Loss: 0.000294\n",
      "Epoch: 13 \tTraining Loss: 0.000294\n",
      "Epoch: 14 \tTraining Loss: 0.000294\n",
      "Epoch: 15 \tTraining Loss: 0.000294\n",
      "Epoch: 16 \tTraining Loss: 0.000294\n",
      "Epoch: 17 \tTraining Loss: 0.000294\n",
      "Epoch: 18 \tTraining Loss: 0.000294\n",
      "Epoch: 1 \tTraining Loss: 0.000227\n",
      "Epoch: 2 \tTraining Loss: 0.000227\n",
      "Epoch: 3 \tTraining Loss: 0.000227\n",
      "Epoch: 4 \tTraining Loss: 0.000227\n",
      "Epoch: 5 \tTraining Loss: 0.000227\n",
      "Epoch: 6 \tTraining Loss: 0.000227\n",
      "Epoch: 7 \tTraining Loss: 0.000227\n",
      "Epoch: 8 \tTraining Loss: 0.000227\n",
      "Epoch: 9 \tTraining Loss: 0.000227\n",
      "Epoch: 10 \tTraining Loss: 0.000227\n",
      "Epoch: 11 \tTraining Loss: 0.000227\n",
      "Epoch: 12 \tTraining Loss: 0.000227\n",
      "Epoch: 13 \tTraining Loss: 0.000227\n",
      "Epoch: 14 \tTraining Loss: 0.000227\n",
      "Epoch: 15 \tTraining Loss: 0.000227\n",
      "Epoch: 16 \tTraining Loss: 0.000227\n",
      "Epoch: 17 \tTraining Loss: 0.000227\n",
      "Epoch: 18 \tTraining Loss: 0.000227\n",
      "Epoch: 1 \tTraining Loss: 0.000198\n",
      "Epoch: 2 \tTraining Loss: 0.000198\n",
      "Epoch: 3 \tTraining Loss: 0.000198\n",
      "Epoch: 4 \tTraining Loss: 0.000198\n",
      "Epoch: 5 \tTraining Loss: 0.000198\n",
      "Epoch: 6 \tTraining Loss: 0.000198\n",
      "Epoch: 7 \tTraining Loss: 0.000198\n",
      "Epoch: 8 \tTraining Loss: 0.000198\n",
      "Epoch: 9 \tTraining Loss: 0.000198\n",
      "Epoch: 10 \tTraining Loss: 0.000198\n",
      "Epoch: 11 \tTraining Loss: 0.000198\n",
      "Epoch: 12 \tTraining Loss: 0.000198\n",
      "Epoch: 13 \tTraining Loss: 0.000198\n",
      "Epoch: 14 \tTraining Loss: 0.000198\n",
      "Epoch: 15 \tTraining Loss: 0.000198\n",
      "Epoch: 16 \tTraining Loss: 0.000198\n",
      "Epoch: 17 \tTraining Loss: 0.000198\n",
      "Epoch: 18 \tTraining Loss: 0.000198\n",
      "Epoch: 1 \tTraining Loss: 0.000148\n",
      "Epoch: 2 \tTraining Loss: 0.000148\n",
      "Epoch: 3 \tTraining Loss: 0.000148\n",
      "Epoch: 4 \tTraining Loss: 0.000148\n",
      "Epoch: 5 \tTraining Loss: 0.000148\n",
      "Epoch: 6 \tTraining Loss: 0.000148\n",
      "Epoch: 7 \tTraining Loss: 0.000148\n",
      "Epoch: 8 \tTraining Loss: 0.000148\n",
      "Epoch: 9 \tTraining Loss: 0.000148\n",
      "Epoch: 10 \tTraining Loss: 0.000148\n",
      "Epoch: 11 \tTraining Loss: 0.000148\n",
      "Epoch: 12 \tTraining Loss: 0.000148\n",
      "Epoch: 13 \tTraining Loss: 0.000148\n",
      "Epoch: 14 \tTraining Loss: 0.000148\n",
      "Epoch: 15 \tTraining Loss: 0.000148\n",
      "Epoch: 16 \tTraining Loss: 0.000148\n",
      "Epoch: 17 \tTraining Loss: 0.000148\n",
      "Epoch: 18 \tTraining Loss: 0.000148\n",
      "Epoch: 1 \tTraining Loss: 0.000254\n",
      "Epoch: 2 \tTraining Loss: 0.000254\n",
      "Epoch: 3 \tTraining Loss: 0.000254\n",
      "Epoch: 4 \tTraining Loss: 0.000254\n",
      "Epoch: 5 \tTraining Loss: 0.000254\n",
      "Epoch: 6 \tTraining Loss: 0.000254\n",
      "Epoch: 7 \tTraining Loss: 0.000254\n",
      "Epoch: 8 \tTraining Loss: 0.000254\n",
      "Epoch: 9 \tTraining Loss: 0.000254\n",
      "Epoch: 10 \tTraining Loss: 0.000254\n",
      "Epoch: 11 \tTraining Loss: 0.000254\n",
      "Epoch: 12 \tTraining Loss: 0.000254\n",
      "Epoch: 13 \tTraining Loss: 0.000254\n",
      "Epoch: 14 \tTraining Loss: 0.000254\n",
      "Epoch: 15 \tTraining Loss: 0.000254\n",
      "Epoch: 16 \tTraining Loss: 0.000254\n",
      "Epoch: 17 \tTraining Loss: 0.000254\n",
      "Epoch: 18 \tTraining Loss: 0.000254\n",
      "Epoch: 1 \tTraining Loss: 0.000175\n",
      "Epoch: 2 \tTraining Loss: 0.000175\n",
      "Epoch: 3 \tTraining Loss: 0.000175\n",
      "Epoch: 4 \tTraining Loss: 0.000175\n",
      "Epoch: 5 \tTraining Loss: 0.000175\n",
      "Epoch: 6 \tTraining Loss: 0.000175\n",
      "Epoch: 7 \tTraining Loss: 0.000175\n",
      "Epoch: 8 \tTraining Loss: 0.000175\n",
      "Epoch: 9 \tTraining Loss: 0.000175\n",
      "Epoch: 10 \tTraining Loss: 0.000175\n",
      "Epoch: 11 \tTraining Loss: 0.000175\n",
      "Epoch: 12 \tTraining Loss: 0.000175\n",
      "Epoch: 13 \tTraining Loss: 0.000175\n",
      "Epoch: 14 \tTraining Loss: 0.000175\n",
      "Epoch: 15 \tTraining Loss: 0.000175\n",
      "Epoch: 16 \tTraining Loss: 0.000175\n",
      "Epoch: 17 \tTraining Loss: 0.000175\n",
      "Epoch: 18 \tTraining Loss: 0.000175\n",
      "Epoch: 1 \tTraining Loss: 0.000288\n",
      "Epoch: 2 \tTraining Loss: 0.000288\n",
      "Epoch: 3 \tTraining Loss: 0.000288\n",
      "Epoch: 4 \tTraining Loss: 0.000288\n",
      "Epoch: 5 \tTraining Loss: 0.000288\n",
      "Epoch: 6 \tTraining Loss: 0.000288\n",
      "Epoch: 7 \tTraining Loss: 0.000288\n",
      "Epoch: 8 \tTraining Loss: 0.000288\n",
      "Epoch: 9 \tTraining Loss: 0.000288\n",
      "Epoch: 10 \tTraining Loss: 0.000288\n",
      "Epoch: 11 \tTraining Loss: 0.000288\n",
      "Epoch: 12 \tTraining Loss: 0.000288\n",
      "Epoch: 13 \tTraining Loss: 0.000288\n",
      "Epoch: 14 \tTraining Loss: 0.000288\n",
      "Epoch: 15 \tTraining Loss: 0.000288\n",
      "Epoch: 16 \tTraining Loss: 0.000288\n",
      "Epoch: 17 \tTraining Loss: 0.000288\n",
      "Epoch: 18 \tTraining Loss: 0.000288\n",
      "Epoch: 1 \tTraining Loss: 0.000434\n",
      "Epoch: 2 \tTraining Loss: 0.000434\n",
      "Epoch: 3 \tTraining Loss: 0.000434\n",
      "Epoch: 4 \tTraining Loss: 0.000434\n",
      "Epoch: 5 \tTraining Loss: 0.000434\n",
      "Epoch: 6 \tTraining Loss: 0.000434\n",
      "Epoch: 7 \tTraining Loss: 0.000434\n",
      "Epoch: 8 \tTraining Loss: 0.000434\n",
      "Epoch: 9 \tTraining Loss: 0.000434\n",
      "Epoch: 10 \tTraining Loss: 0.000434\n",
      "Epoch: 11 \tTraining Loss: 0.000434\n",
      "Epoch: 12 \tTraining Loss: 0.000434\n",
      "Epoch: 13 \tTraining Loss: 0.000434\n",
      "Epoch: 14 \tTraining Loss: 0.000434\n",
      "Epoch: 15 \tTraining Loss: 0.000434\n",
      "Epoch: 16 \tTraining Loss: 0.000434\n",
      "Epoch: 17 \tTraining Loss: 0.000434\n",
      "Epoch: 18 \tTraining Loss: 0.000434\n",
      "Epoch: 1 \tTraining Loss: 0.000311\n",
      "Epoch: 2 \tTraining Loss: 0.000311\n",
      "Epoch: 3 \tTraining Loss: 0.000311\n",
      "Epoch: 4 \tTraining Loss: 0.000311\n",
      "Epoch: 5 \tTraining Loss: 0.000311\n",
      "Epoch: 6 \tTraining Loss: 0.000311\n",
      "Epoch: 7 \tTraining Loss: 0.000311\n",
      "Epoch: 8 \tTraining Loss: 0.000311\n",
      "Epoch: 9 \tTraining Loss: 0.000311\n",
      "Epoch: 10 \tTraining Loss: 0.000311\n",
      "Epoch: 11 \tTraining Loss: 0.000311\n",
      "Epoch: 12 \tTraining Loss: 0.000311\n",
      "Epoch: 13 \tTraining Loss: 0.000311\n",
      "Epoch: 14 \tTraining Loss: 0.000311\n",
      "Epoch: 15 \tTraining Loss: 0.000311\n",
      "Epoch: 16 \tTraining Loss: 0.000311\n",
      "Epoch: 17 \tTraining Loss: 0.000311\n",
      "Epoch: 18 \tTraining Loss: 0.000311\n",
      "Epoch: 1 \tTraining Loss: 0.000156\n",
      "Epoch: 2 \tTraining Loss: 0.000156\n",
      "Epoch: 3 \tTraining Loss: 0.000156\n",
      "Epoch: 4 \tTraining Loss: 0.000156\n",
      "Epoch: 5 \tTraining Loss: 0.000156\n",
      "Epoch: 6 \tTraining Loss: 0.000156\n",
      "Epoch: 7 \tTraining Loss: 0.000156\n",
      "Epoch: 8 \tTraining Loss: 0.000156\n",
      "Epoch: 9 \tTraining Loss: 0.000156\n",
      "Epoch: 10 \tTraining Loss: 0.000156\n",
      "Epoch: 11 \tTraining Loss: 0.000156\n",
      "Epoch: 12 \tTraining Loss: 0.000156\n",
      "Epoch: 13 \tTraining Loss: 0.000156\n",
      "Epoch: 14 \tTraining Loss: 0.000156\n",
      "Epoch: 15 \tTraining Loss: 0.000156\n",
      "Epoch: 16 \tTraining Loss: 0.000156\n",
      "Epoch: 17 \tTraining Loss: 0.000156\n",
      "Epoch: 18 \tTraining Loss: 0.000156\n",
      "Epoch: 1 \tTraining Loss: 0.000130\n",
      "Epoch: 2 \tTraining Loss: 0.000130\n",
      "Epoch: 3 \tTraining Loss: 0.000130\n",
      "Epoch: 4 \tTraining Loss: 0.000130\n",
      "Epoch: 5 \tTraining Loss: 0.000130\n",
      "Epoch: 6 \tTraining Loss: 0.000130\n",
      "Epoch: 7 \tTraining Loss: 0.000130\n",
      "Epoch: 8 \tTraining Loss: 0.000130\n",
      "Epoch: 9 \tTraining Loss: 0.000130\n",
      "Epoch: 10 \tTraining Loss: 0.000130\n",
      "Epoch: 11 \tTraining Loss: 0.000130\n",
      "Epoch: 12 \tTraining Loss: 0.000130\n",
      "Epoch: 13 \tTraining Loss: 0.000130\n",
      "Epoch: 14 \tTraining Loss: 0.000130\n",
      "Epoch: 15 \tTraining Loss: 0.000130\n",
      "Epoch: 16 \tTraining Loss: 0.000130\n",
      "Epoch: 17 \tTraining Loss: 0.000130\n",
      "Epoch: 18 \tTraining Loss: 0.000130\n",
      "Epoch: 1 \tTraining Loss: 0.000111\n",
      "Epoch: 2 \tTraining Loss: 0.000111\n",
      "Epoch: 3 \tTraining Loss: 0.000111\n",
      "Epoch: 4 \tTraining Loss: 0.000111\n",
      "Epoch: 5 \tTraining Loss: 0.000111\n",
      "Epoch: 6 \tTraining Loss: 0.000111\n",
      "Epoch: 7 \tTraining Loss: 0.000111\n",
      "Epoch: 8 \tTraining Loss: 0.000111\n",
      "Epoch: 9 \tTraining Loss: 0.000111\n",
      "Epoch: 10 \tTraining Loss: 0.000111\n",
      "Epoch: 11 \tTraining Loss: 0.000111\n",
      "Epoch: 12 \tTraining Loss: 0.000111\n",
      "Epoch: 13 \tTraining Loss: 0.000111\n",
      "Epoch: 14 \tTraining Loss: 0.000111\n",
      "Epoch: 15 \tTraining Loss: 0.000111\n",
      "Epoch: 16 \tTraining Loss: 0.000111\n",
      "Epoch: 17 \tTraining Loss: 0.000111\n",
      "Epoch: 18 \tTraining Loss: 0.000111\n",
      "Epoch: 1 \tTraining Loss: 0.000244\n",
      "Epoch: 2 \tTraining Loss: 0.000244\n",
      "Epoch: 3 \tTraining Loss: 0.000244\n",
      "Epoch: 4 \tTraining Loss: 0.000244\n",
      "Epoch: 5 \tTraining Loss: 0.000244\n",
      "Epoch: 6 \tTraining Loss: 0.000244\n",
      "Epoch: 7 \tTraining Loss: 0.000244\n",
      "Epoch: 8 \tTraining Loss: 0.000244\n",
      "Epoch: 9 \tTraining Loss: 0.000244\n",
      "Epoch: 10 \tTraining Loss: 0.000244\n",
      "Epoch: 11 \tTraining Loss: 0.000244\n",
      "Epoch: 12 \tTraining Loss: 0.000244\n",
      "Epoch: 13 \tTraining Loss: 0.000244\n",
      "Epoch: 14 \tTraining Loss: 0.000244\n",
      "Epoch: 15 \tTraining Loss: 0.000244\n",
      "Epoch: 16 \tTraining Loss: 0.000244\n",
      "Epoch: 17 \tTraining Loss: 0.000244\n",
      "Epoch: 18 \tTraining Loss: 0.000244\n",
      "Epoch: 1 \tTraining Loss: 0.000176\n",
      "Epoch: 2 \tTraining Loss: 0.000176\n",
      "Epoch: 3 \tTraining Loss: 0.000176\n",
      "Epoch: 4 \tTraining Loss: 0.000176\n",
      "Epoch: 5 \tTraining Loss: 0.000176\n",
      "Epoch: 6 \tTraining Loss: 0.000176\n",
      "Epoch: 7 \tTraining Loss: 0.000176\n",
      "Epoch: 8 \tTraining Loss: 0.000176\n",
      "Epoch: 9 \tTraining Loss: 0.000176\n",
      "Epoch: 10 \tTraining Loss: 0.000176\n",
      "Epoch: 11 \tTraining Loss: 0.000176\n",
      "Epoch: 12 \tTraining Loss: 0.000176\n",
      "Epoch: 13 \tTraining Loss: 0.000176\n",
      "Epoch: 14 \tTraining Loss: 0.000176\n",
      "Epoch: 15 \tTraining Loss: 0.000176\n",
      "Epoch: 16 \tTraining Loss: 0.000176\n",
      "Epoch: 17 \tTraining Loss: 0.000176\n",
      "Epoch: 18 \tTraining Loss: 0.000176\n",
      "Epoch: 1 \tTraining Loss: 0.000248\n",
      "Epoch: 2 \tTraining Loss: 0.000248\n",
      "Epoch: 3 \tTraining Loss: 0.000248\n",
      "Epoch: 4 \tTraining Loss: 0.000248\n",
      "Epoch: 5 \tTraining Loss: 0.000248\n",
      "Epoch: 6 \tTraining Loss: 0.000248\n",
      "Epoch: 7 \tTraining Loss: 0.000248\n",
      "Epoch: 8 \tTraining Loss: 0.000248\n",
      "Epoch: 9 \tTraining Loss: 0.000248\n",
      "Epoch: 10 \tTraining Loss: 0.000248\n",
      "Epoch: 11 \tTraining Loss: 0.000248\n",
      "Epoch: 12 \tTraining Loss: 0.000248\n",
      "Epoch: 13 \tTraining Loss: 0.000248\n",
      "Epoch: 14 \tTraining Loss: 0.000248\n",
      "Epoch: 15 \tTraining Loss: 0.000248\n",
      "Epoch: 16 \tTraining Loss: 0.000248\n",
      "Epoch: 17 \tTraining Loss: 0.000248\n",
      "Epoch: 18 \tTraining Loss: 0.000248\n",
      "Epoch: 1 \tTraining Loss: 0.000281\n",
      "Epoch: 2 \tTraining Loss: 0.000281\n",
      "Epoch: 3 \tTraining Loss: 0.000281\n",
      "Epoch: 4 \tTraining Loss: 0.000281\n",
      "Epoch: 5 \tTraining Loss: 0.000281\n",
      "Epoch: 6 \tTraining Loss: 0.000281\n",
      "Epoch: 7 \tTraining Loss: 0.000281\n",
      "Epoch: 8 \tTraining Loss: 0.000281\n",
      "Epoch: 9 \tTraining Loss: 0.000281\n",
      "Epoch: 10 \tTraining Loss: 0.000281\n",
      "Epoch: 11 \tTraining Loss: 0.000281\n",
      "Epoch: 12 \tTraining Loss: 0.000281\n",
      "Epoch: 13 \tTraining Loss: 0.000281\n",
      "Epoch: 14 \tTraining Loss: 0.000281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 \tTraining Loss: 0.000281\n",
      "Epoch: 16 \tTraining Loss: 0.000281\n",
      "Epoch: 17 \tTraining Loss: 0.000281\n",
      "Epoch: 18 \tTraining Loss: 0.000281\n",
      "Epoch: 1 \tTraining Loss: 0.000295\n",
      "Epoch: 2 \tTraining Loss: 0.000295\n",
      "Epoch: 3 \tTraining Loss: 0.000295\n",
      "Epoch: 4 \tTraining Loss: 0.000295\n",
      "Epoch: 5 \tTraining Loss: 0.000295\n",
      "Epoch: 6 \tTraining Loss: 0.000295\n",
      "Epoch: 7 \tTraining Loss: 0.000295\n",
      "Epoch: 8 \tTraining Loss: 0.000295\n",
      "Epoch: 9 \tTraining Loss: 0.000295\n",
      "Epoch: 10 \tTraining Loss: 0.000295\n",
      "Epoch: 11 \tTraining Loss: 0.000295\n",
      "Epoch: 12 \tTraining Loss: 0.000295\n",
      "Epoch: 13 \tTraining Loss: 0.000295\n",
      "Epoch: 14 \tTraining Loss: 0.000295\n",
      "Epoch: 15 \tTraining Loss: 0.000295\n",
      "Epoch: 16 \tTraining Loss: 0.000295\n",
      "Epoch: 17 \tTraining Loss: 0.000295\n",
      "Epoch: 18 \tTraining Loss: 0.000295\n",
      "Epoch: 1 \tTraining Loss: 0.000340\n",
      "Epoch: 2 \tTraining Loss: 0.000340\n",
      "Epoch: 3 \tTraining Loss: 0.000340\n",
      "Epoch: 4 \tTraining Loss: 0.000340\n",
      "Epoch: 5 \tTraining Loss: 0.000340\n",
      "Epoch: 6 \tTraining Loss: 0.000340\n",
      "Epoch: 7 \tTraining Loss: 0.000340\n",
      "Epoch: 8 \tTraining Loss: 0.000340\n",
      "Epoch: 9 \tTraining Loss: 0.000340\n",
      "Epoch: 10 \tTraining Loss: 0.000340\n",
      "Epoch: 11 \tTraining Loss: 0.000340\n",
      "Epoch: 12 \tTraining Loss: 0.000340\n",
      "Epoch: 13 \tTraining Loss: 0.000340\n",
      "Epoch: 14 \tTraining Loss: 0.000340\n",
      "Epoch: 15 \tTraining Loss: 0.000340\n",
      "Epoch: 16 \tTraining Loss: 0.000340\n",
      "Epoch: 17 \tTraining Loss: 0.000340\n",
      "Epoch: 18 \tTraining Loss: 0.000340\n",
      "Epoch: 1 \tTraining Loss: 0.000229\n",
      "Epoch: 2 \tTraining Loss: 0.000229\n",
      "Epoch: 3 \tTraining Loss: 0.000229\n",
      "Epoch: 4 \tTraining Loss: 0.000229\n",
      "Epoch: 5 \tTraining Loss: 0.000229\n",
      "Epoch: 6 \tTraining Loss: 0.000229\n",
      "Epoch: 7 \tTraining Loss: 0.000229\n",
      "Epoch: 8 \tTraining Loss: 0.000229\n",
      "Epoch: 9 \tTraining Loss: 0.000229\n",
      "Epoch: 10 \tTraining Loss: 0.000229\n",
      "Epoch: 11 \tTraining Loss: 0.000229\n",
      "Epoch: 12 \tTraining Loss: 0.000229\n",
      "Epoch: 13 \tTraining Loss: 0.000229\n",
      "Epoch: 14 \tTraining Loss: 0.000229\n",
      "Epoch: 15 \tTraining Loss: 0.000229\n",
      "Epoch: 16 \tTraining Loss: 0.000229\n",
      "Epoch: 17 \tTraining Loss: 0.000229\n",
      "Epoch: 18 \tTraining Loss: 0.000229\n",
      "Epoch: 1 \tTraining Loss: 0.000244\n",
      "Epoch: 2 \tTraining Loss: 0.000244\n",
      "Epoch: 3 \tTraining Loss: 0.000244\n",
      "Epoch: 4 \tTraining Loss: 0.000244\n",
      "Epoch: 5 \tTraining Loss: 0.000244\n",
      "Epoch: 6 \tTraining Loss: 0.000244\n",
      "Epoch: 7 \tTraining Loss: 0.000244\n",
      "Epoch: 8 \tTraining Loss: 0.000244\n",
      "Epoch: 9 \tTraining Loss: 0.000244\n",
      "Epoch: 10 \tTraining Loss: 0.000244\n",
      "Epoch: 11 \tTraining Loss: 0.000244\n",
      "Epoch: 12 \tTraining Loss: 0.000244\n",
      "Epoch: 13 \tTraining Loss: 0.000244\n",
      "Epoch: 14 \tTraining Loss: 0.000244\n",
      "Epoch: 15 \tTraining Loss: 0.000244\n",
      "Epoch: 16 \tTraining Loss: 0.000244\n",
      "Epoch: 17 \tTraining Loss: 0.000244\n",
      "Epoch: 18 \tTraining Loss: 0.000244\n",
      "Epoch: 1 \tTraining Loss: 0.000294\n",
      "Epoch: 2 \tTraining Loss: 0.000294\n",
      "Epoch: 3 \tTraining Loss: 0.000294\n",
      "Epoch: 4 \tTraining Loss: 0.000294\n",
      "Epoch: 5 \tTraining Loss: 0.000294\n",
      "Epoch: 6 \tTraining Loss: 0.000294\n",
      "Epoch: 7 \tTraining Loss: 0.000294\n",
      "Epoch: 8 \tTraining Loss: 0.000294\n",
      "Epoch: 9 \tTraining Loss: 0.000294\n",
      "Epoch: 10 \tTraining Loss: 0.000294\n",
      "Epoch: 11 \tTraining Loss: 0.000294\n",
      "Epoch: 12 \tTraining Loss: 0.000294\n",
      "Epoch: 13 \tTraining Loss: 0.000294\n",
      "Epoch: 14 \tTraining Loss: 0.000294\n",
      "Epoch: 15 \tTraining Loss: 0.000294\n",
      "Epoch: 16 \tTraining Loss: 0.000294\n",
      "Epoch: 17 \tTraining Loss: 0.000294\n",
      "Epoch: 18 \tTraining Loss: 0.000294\n",
      "Epoch: 1 \tTraining Loss: 0.000146\n",
      "Epoch: 2 \tTraining Loss: 0.000146\n",
      "Epoch: 3 \tTraining Loss: 0.000146\n",
      "Epoch: 4 \tTraining Loss: 0.000146\n",
      "Epoch: 5 \tTraining Loss: 0.000146\n",
      "Epoch: 6 \tTraining Loss: 0.000146\n",
      "Epoch: 7 \tTraining Loss: 0.000146\n",
      "Epoch: 8 \tTraining Loss: 0.000146\n",
      "Epoch: 9 \tTraining Loss: 0.000146\n",
      "Epoch: 10 \tTraining Loss: 0.000146\n",
      "Epoch: 11 \tTraining Loss: 0.000146\n",
      "Epoch: 12 \tTraining Loss: 0.000146\n",
      "Epoch: 13 \tTraining Loss: 0.000146\n",
      "Epoch: 14 \tTraining Loss: 0.000146\n",
      "Epoch: 15 \tTraining Loss: 0.000146\n",
      "Epoch: 16 \tTraining Loss: 0.000146\n",
      "Epoch: 17 \tTraining Loss: 0.000146\n",
      "Epoch: 18 \tTraining Loss: 0.000146\n",
      "Epoch: 1 \tTraining Loss: 0.000205\n",
      "Epoch: 2 \tTraining Loss: 0.000205\n",
      "Epoch: 3 \tTraining Loss: 0.000205\n",
      "Epoch: 4 \tTraining Loss: 0.000205\n",
      "Epoch: 5 \tTraining Loss: 0.000205\n",
      "Epoch: 6 \tTraining Loss: 0.000205\n",
      "Epoch: 7 \tTraining Loss: 0.000205\n",
      "Epoch: 8 \tTraining Loss: 0.000205\n",
      "Epoch: 9 \tTraining Loss: 0.000205\n",
      "Epoch: 10 \tTraining Loss: 0.000205\n",
      "Epoch: 11 \tTraining Loss: 0.000205\n",
      "Epoch: 12 \tTraining Loss: 0.000205\n",
      "Epoch: 13 \tTraining Loss: 0.000205\n",
      "Epoch: 14 \tTraining Loss: 0.000205\n",
      "Epoch: 15 \tTraining Loss: 0.000205\n",
      "Epoch: 16 \tTraining Loss: 0.000205\n",
      "Epoch: 17 \tTraining Loss: 0.000205\n",
      "Epoch: 18 \tTraining Loss: 0.000205\n",
      "Epoch: 1 \tTraining Loss: 0.000226\n",
      "Epoch: 2 \tTraining Loss: 0.000226\n",
      "Epoch: 3 \tTraining Loss: 0.000226\n",
      "Epoch: 4 \tTraining Loss: 0.000226\n",
      "Epoch: 5 \tTraining Loss: 0.000226\n",
      "Epoch: 6 \tTraining Loss: 0.000226\n",
      "Epoch: 7 \tTraining Loss: 0.000226\n",
      "Epoch: 8 \tTraining Loss: 0.000226\n",
      "Epoch: 9 \tTraining Loss: 0.000226\n",
      "Epoch: 10 \tTraining Loss: 0.000226\n",
      "Epoch: 11 \tTraining Loss: 0.000226\n",
      "Epoch: 12 \tTraining Loss: 0.000226\n",
      "Epoch: 13 \tTraining Loss: 0.000226\n",
      "Epoch: 14 \tTraining Loss: 0.000226\n",
      "Epoch: 15 \tTraining Loss: 0.000226\n",
      "Epoch: 16 \tTraining Loss: 0.000226\n",
      "Epoch: 17 \tTraining Loss: 0.000226\n",
      "Epoch: 18 \tTraining Loss: 0.000226\n",
      "Epoch: 1 \tTraining Loss: 0.000201\n",
      "Epoch: 2 \tTraining Loss: 0.000201\n",
      "Epoch: 3 \tTraining Loss: 0.000201\n",
      "Epoch: 4 \tTraining Loss: 0.000201\n",
      "Epoch: 5 \tTraining Loss: 0.000201\n",
      "Epoch: 6 \tTraining Loss: 0.000201\n",
      "Epoch: 7 \tTraining Loss: 0.000201\n",
      "Epoch: 8 \tTraining Loss: 0.000201\n",
      "Epoch: 9 \tTraining Loss: 0.000201\n",
      "Epoch: 10 \tTraining Loss: 0.000201\n",
      "Epoch: 11 \tTraining Loss: 0.000201\n",
      "Epoch: 12 \tTraining Loss: 0.000201\n",
      "Epoch: 13 \tTraining Loss: 0.000201\n",
      "Epoch: 14 \tTraining Loss: 0.000201\n",
      "Epoch: 15 \tTraining Loss: 0.000201\n",
      "Epoch: 16 \tTraining Loss: 0.000201\n",
      "Epoch: 17 \tTraining Loss: 0.000201\n",
      "Epoch: 18 \tTraining Loss: 0.000201\n",
      "Epoch: 1 \tTraining Loss: 0.000268\n",
      "Epoch: 2 \tTraining Loss: 0.000268\n",
      "Epoch: 3 \tTraining Loss: 0.000268\n",
      "Epoch: 4 \tTraining Loss: 0.000268\n",
      "Epoch: 5 \tTraining Loss: 0.000268\n",
      "Epoch: 6 \tTraining Loss: 0.000268\n",
      "Epoch: 7 \tTraining Loss: 0.000268\n",
      "Epoch: 8 \tTraining Loss: 0.000268\n",
      "Epoch: 9 \tTraining Loss: 0.000268\n",
      "Epoch: 10 \tTraining Loss: 0.000268\n",
      "Epoch: 11 \tTraining Loss: 0.000268\n",
      "Epoch: 12 \tTraining Loss: 0.000268\n",
      "Epoch: 13 \tTraining Loss: 0.000268\n",
      "Epoch: 14 \tTraining Loss: 0.000268\n",
      "Epoch: 15 \tTraining Loss: 0.000268\n",
      "Epoch: 16 \tTraining Loss: 0.000268\n",
      "Epoch: 17 \tTraining Loss: 0.000268\n",
      "Epoch: 18 \tTraining Loss: 0.000268\n",
      "Epoch: 1 \tTraining Loss: 0.000256\n",
      "Epoch: 2 \tTraining Loss: 0.000256\n",
      "Epoch: 3 \tTraining Loss: 0.000256\n",
      "Epoch: 4 \tTraining Loss: 0.000256\n",
      "Epoch: 5 \tTraining Loss: 0.000256\n",
      "Epoch: 6 \tTraining Loss: 0.000256\n",
      "Epoch: 7 \tTraining Loss: 0.000256\n",
      "Epoch: 8 \tTraining Loss: 0.000256\n",
      "Epoch: 9 \tTraining Loss: 0.000256\n",
      "Epoch: 10 \tTraining Loss: 0.000256\n",
      "Epoch: 11 \tTraining Loss: 0.000256\n",
      "Epoch: 12 \tTraining Loss: 0.000256\n",
      "Epoch: 13 \tTraining Loss: 0.000256\n",
      "Epoch: 14 \tTraining Loss: 0.000256\n",
      "Epoch: 15 \tTraining Loss: 0.000256\n",
      "Epoch: 16 \tTraining Loss: 0.000256\n",
      "Epoch: 17 \tTraining Loss: 0.000256\n",
      "Epoch: 18 \tTraining Loss: 0.000256\n",
      "Epoch: 1 \tTraining Loss: 0.000269\n",
      "Epoch: 2 \tTraining Loss: 0.000269\n",
      "Epoch: 3 \tTraining Loss: 0.000269\n",
      "Epoch: 4 \tTraining Loss: 0.000269\n",
      "Epoch: 5 \tTraining Loss: 0.000269\n",
      "Epoch: 6 \tTraining Loss: 0.000269\n",
      "Epoch: 7 \tTraining Loss: 0.000269\n",
      "Epoch: 8 \tTraining Loss: 0.000269\n",
      "Epoch: 9 \tTraining Loss: 0.000269\n",
      "Epoch: 10 \tTraining Loss: 0.000269\n",
      "Epoch: 11 \tTraining Loss: 0.000269\n",
      "Epoch: 12 \tTraining Loss: 0.000269\n",
      "Epoch: 13 \tTraining Loss: 0.000269\n",
      "Epoch: 14 \tTraining Loss: 0.000269\n",
      "Epoch: 15 \tTraining Loss: 0.000269\n",
      "Epoch: 16 \tTraining Loss: 0.000269\n",
      "Epoch: 17 \tTraining Loss: 0.000269\n",
      "Epoch: 18 \tTraining Loss: 0.000269\n",
      "Epoch: 1 \tTraining Loss: 0.000292\n",
      "Epoch: 2 \tTraining Loss: 0.000292\n",
      "Epoch: 3 \tTraining Loss: 0.000292\n",
      "Epoch: 4 \tTraining Loss: 0.000292\n",
      "Epoch: 5 \tTraining Loss: 0.000292\n",
      "Epoch: 6 \tTraining Loss: 0.000292\n",
      "Epoch: 7 \tTraining Loss: 0.000292\n",
      "Epoch: 8 \tTraining Loss: 0.000292\n",
      "Epoch: 9 \tTraining Loss: 0.000292\n",
      "Epoch: 10 \tTraining Loss: 0.000292\n",
      "Epoch: 11 \tTraining Loss: 0.000292\n",
      "Epoch: 12 \tTraining Loss: 0.000292\n",
      "Epoch: 13 \tTraining Loss: 0.000292\n",
      "Epoch: 14 \tTraining Loss: 0.000292\n",
      "Epoch: 15 \tTraining Loss: 0.000292\n",
      "Epoch: 16 \tTraining Loss: 0.000292\n",
      "Epoch: 17 \tTraining Loss: 0.000292\n",
      "Epoch: 18 \tTraining Loss: 0.000292\n",
      "Epoch: 1 \tTraining Loss: 0.000257\n",
      "Epoch: 2 \tTraining Loss: 0.000257\n",
      "Epoch: 3 \tTraining Loss: 0.000257\n",
      "Epoch: 4 \tTraining Loss: 0.000257\n",
      "Epoch: 5 \tTraining Loss: 0.000257\n",
      "Epoch: 6 \tTraining Loss: 0.000257\n",
      "Epoch: 7 \tTraining Loss: 0.000257\n",
      "Epoch: 8 \tTraining Loss: 0.000257\n",
      "Epoch: 9 \tTraining Loss: 0.000257\n",
      "Epoch: 10 \tTraining Loss: 0.000257\n",
      "Epoch: 11 \tTraining Loss: 0.000257\n",
      "Epoch: 12 \tTraining Loss: 0.000257\n",
      "Epoch: 13 \tTraining Loss: 0.000257\n",
      "Epoch: 14 \tTraining Loss: 0.000257\n",
      "Epoch: 15 \tTraining Loss: 0.000257\n",
      "Epoch: 16 \tTraining Loss: 0.000257\n",
      "Epoch: 17 \tTraining Loss: 0.000257\n",
      "Epoch: 18 \tTraining Loss: 0.000257\n",
      "Epoch: 1 \tTraining Loss: 0.000242\n",
      "Epoch: 2 \tTraining Loss: 0.000242\n",
      "Epoch: 3 \tTraining Loss: 0.000242\n",
      "Epoch: 4 \tTraining Loss: 0.000242\n",
      "Epoch: 5 \tTraining Loss: 0.000242\n",
      "Epoch: 6 \tTraining Loss: 0.000242\n",
      "Epoch: 7 \tTraining Loss: 0.000242\n",
      "Epoch: 8 \tTraining Loss: 0.000242\n",
      "Epoch: 9 \tTraining Loss: 0.000242\n",
      "Epoch: 10 \tTraining Loss: 0.000242\n",
      "Epoch: 11 \tTraining Loss: 0.000242\n",
      "Epoch: 12 \tTraining Loss: 0.000242\n",
      "Epoch: 13 \tTraining Loss: 0.000242\n",
      "Epoch: 14 \tTraining Loss: 0.000242\n",
      "Epoch: 15 \tTraining Loss: 0.000242\n",
      "Epoch: 16 \tTraining Loss: 0.000242\n",
      "Epoch: 17 \tTraining Loss: 0.000242\n",
      "Epoch: 18 \tTraining Loss: 0.000242\n",
      "Epoch: 1 \tTraining Loss: 0.000232\n",
      "Epoch: 2 \tTraining Loss: 0.000232\n",
      "Epoch: 3 \tTraining Loss: 0.000232\n",
      "Epoch: 4 \tTraining Loss: 0.000232\n",
      "Epoch: 5 \tTraining Loss: 0.000232\n",
      "Epoch: 6 \tTraining Loss: 0.000232\n",
      "Epoch: 7 \tTraining Loss: 0.000232\n",
      "Epoch: 8 \tTraining Loss: 0.000232\n",
      "Epoch: 9 \tTraining Loss: 0.000232\n",
      "Epoch: 10 \tTraining Loss: 0.000232\n",
      "Epoch: 11 \tTraining Loss: 0.000232\n",
      "Epoch: 12 \tTraining Loss: 0.000232\n",
      "Epoch: 13 \tTraining Loss: 0.000232\n",
      "Epoch: 14 \tTraining Loss: 0.000232\n",
      "Epoch: 15 \tTraining Loss: 0.000232\n",
      "Epoch: 16 \tTraining Loss: 0.000232\n",
      "Epoch: 17 \tTraining Loss: 0.000232\n",
      "Epoch: 18 \tTraining Loss: 0.000232\n",
      "Epoch: 1 \tTraining Loss: 0.000190\n",
      "Epoch: 2 \tTraining Loss: 0.000190\n",
      "Epoch: 3 \tTraining Loss: 0.000190\n",
      "Epoch: 4 \tTraining Loss: 0.000190\n",
      "Epoch: 5 \tTraining Loss: 0.000190\n",
      "Epoch: 6 \tTraining Loss: 0.000190\n",
      "Epoch: 7 \tTraining Loss: 0.000190\n",
      "Epoch: 8 \tTraining Loss: 0.000190\n",
      "Epoch: 9 \tTraining Loss: 0.000190\n",
      "Epoch: 10 \tTraining Loss: 0.000190\n",
      "Epoch: 11 \tTraining Loss: 0.000190\n",
      "Epoch: 12 \tTraining Loss: 0.000190\n",
      "Epoch: 13 \tTraining Loss: 0.000190\n",
      "Epoch: 14 \tTraining Loss: 0.000190\n",
      "Epoch: 15 \tTraining Loss: 0.000190\n",
      "Epoch: 16 \tTraining Loss: 0.000190\n",
      "Epoch: 17 \tTraining Loss: 0.000190\n",
      "Epoch: 18 \tTraining Loss: 0.000190\n",
      "Epoch: 1 \tTraining Loss: 0.000326\n",
      "Epoch: 2 \tTraining Loss: 0.000326\n",
      "Epoch: 3 \tTraining Loss: 0.000326\n",
      "Epoch: 4 \tTraining Loss: 0.000326\n",
      "Epoch: 5 \tTraining Loss: 0.000326\n",
      "Epoch: 6 \tTraining Loss: 0.000326\n",
      "Epoch: 7 \tTraining Loss: 0.000326\n",
      "Epoch: 8 \tTraining Loss: 0.000326\n",
      "Epoch: 9 \tTraining Loss: 0.000326\n",
      "Epoch: 10 \tTraining Loss: 0.000326\n",
      "Epoch: 11 \tTraining Loss: 0.000326\n",
      "Epoch: 12 \tTraining Loss: 0.000326\n",
      "Epoch: 13 \tTraining Loss: 0.000326\n",
      "Epoch: 14 \tTraining Loss: 0.000326\n",
      "Epoch: 15 \tTraining Loss: 0.000326\n",
      "Epoch: 16 \tTraining Loss: 0.000326\n",
      "Epoch: 17 \tTraining Loss: 0.000326\n",
      "Epoch: 18 \tTraining Loss: 0.000326\n",
      "Epoch: 1 \tTraining Loss: 0.000266\n",
      "Epoch: 2 \tTraining Loss: 0.000266\n",
      "Epoch: 3 \tTraining Loss: 0.000266\n",
      "Epoch: 4 \tTraining Loss: 0.000266\n",
      "Epoch: 5 \tTraining Loss: 0.000266\n",
      "Epoch: 6 \tTraining Loss: 0.000266\n",
      "Epoch: 7 \tTraining Loss: 0.000266\n",
      "Epoch: 8 \tTraining Loss: 0.000266\n",
      "Epoch: 9 \tTraining Loss: 0.000266\n",
      "Epoch: 10 \tTraining Loss: 0.000266\n",
      "Epoch: 11 \tTraining Loss: 0.000266\n",
      "Epoch: 12 \tTraining Loss: 0.000266\n",
      "Epoch: 13 \tTraining Loss: 0.000266\n",
      "Epoch: 14 \tTraining Loss: 0.000266\n",
      "Epoch: 15 \tTraining Loss: 0.000266\n",
      "Epoch: 16 \tTraining Loss: 0.000266\n",
      "Epoch: 17 \tTraining Loss: 0.000266\n",
      "Epoch: 18 \tTraining Loss: 0.000266\n",
      "Epoch: 1 \tTraining Loss: 0.000151\n",
      "Epoch: 2 \tTraining Loss: 0.000151\n",
      "Epoch: 3 \tTraining Loss: 0.000151\n",
      "Epoch: 4 \tTraining Loss: 0.000151\n",
      "Epoch: 5 \tTraining Loss: 0.000151\n",
      "Epoch: 6 \tTraining Loss: 0.000151\n",
      "Epoch: 7 \tTraining Loss: 0.000151\n",
      "Epoch: 8 \tTraining Loss: 0.000151\n",
      "Epoch: 9 \tTraining Loss: 0.000151\n",
      "Epoch: 10 \tTraining Loss: 0.000151\n",
      "Epoch: 11 \tTraining Loss: 0.000151\n",
      "Epoch: 12 \tTraining Loss: 0.000151\n",
      "Epoch: 13 \tTraining Loss: 0.000151\n",
      "Epoch: 14 \tTraining Loss: 0.000151\n",
      "Epoch: 15 \tTraining Loss: 0.000151\n",
      "Epoch: 16 \tTraining Loss: 0.000151\n",
      "Epoch: 17 \tTraining Loss: 0.000151\n",
      "Epoch: 18 \tTraining Loss: 0.000151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000432\n",
      "Epoch: 2 \tTraining Loss: 0.000432\n",
      "Epoch: 3 \tTraining Loss: 0.000432\n",
      "Epoch: 4 \tTraining Loss: 0.000432\n",
      "Epoch: 5 \tTraining Loss: 0.000432\n",
      "Epoch: 6 \tTraining Loss: 0.000432\n",
      "Epoch: 7 \tTraining Loss: 0.000432\n",
      "Epoch: 8 \tTraining Loss: 0.000432\n",
      "Epoch: 9 \tTraining Loss: 0.000432\n",
      "Epoch: 10 \tTraining Loss: 0.000432\n",
      "Epoch: 11 \tTraining Loss: 0.000432\n",
      "Epoch: 12 \tTraining Loss: 0.000432\n",
      "Epoch: 13 \tTraining Loss: 0.000432\n",
      "Epoch: 14 \tTraining Loss: 0.000432\n",
      "Epoch: 15 \tTraining Loss: 0.000432\n",
      "Epoch: 16 \tTraining Loss: 0.000432\n",
      "Epoch: 17 \tTraining Loss: 0.000432\n",
      "Epoch: 18 \tTraining Loss: 0.000432\n",
      "Epoch: 1 \tTraining Loss: 0.000357\n",
      "Epoch: 2 \tTraining Loss: 0.000357\n",
      "Epoch: 3 \tTraining Loss: 0.000357\n",
      "Epoch: 4 \tTraining Loss: 0.000357\n",
      "Epoch: 5 \tTraining Loss: 0.000357\n",
      "Epoch: 6 \tTraining Loss: 0.000357\n",
      "Epoch: 7 \tTraining Loss: 0.000357\n",
      "Epoch: 8 \tTraining Loss: 0.000357\n",
      "Epoch: 9 \tTraining Loss: 0.000357\n",
      "Epoch: 10 \tTraining Loss: 0.000357\n",
      "Epoch: 11 \tTraining Loss: 0.000357\n",
      "Epoch: 12 \tTraining Loss: 0.000357\n",
      "Epoch: 13 \tTraining Loss: 0.000357\n",
      "Epoch: 14 \tTraining Loss: 0.000357\n",
      "Epoch: 15 \tTraining Loss: 0.000357\n",
      "Epoch: 16 \tTraining Loss: 0.000357\n",
      "Epoch: 17 \tTraining Loss: 0.000357\n",
      "Epoch: 18 \tTraining Loss: 0.000357\n",
      "Epoch: 1 \tTraining Loss: 0.000090\n",
      "Epoch: 2 \tTraining Loss: 0.000090\n",
      "Epoch: 3 \tTraining Loss: 0.000090\n",
      "Epoch: 4 \tTraining Loss: 0.000090\n",
      "Epoch: 5 \tTraining Loss: 0.000090\n",
      "Epoch: 6 \tTraining Loss: 0.000090\n",
      "Epoch: 7 \tTraining Loss: 0.000090\n",
      "Epoch: 8 \tTraining Loss: 0.000090\n",
      "Epoch: 9 \tTraining Loss: 0.000090\n",
      "Epoch: 10 \tTraining Loss: 0.000090\n",
      "Epoch: 11 \tTraining Loss: 0.000090\n",
      "Epoch: 12 \tTraining Loss: 0.000090\n",
      "Epoch: 13 \tTraining Loss: 0.000090\n",
      "Epoch: 14 \tTraining Loss: 0.000090\n",
      "Epoch: 15 \tTraining Loss: 0.000090\n",
      "Epoch: 16 \tTraining Loss: 0.000090\n",
      "Epoch: 17 \tTraining Loss: 0.000090\n",
      "Epoch: 18 \tTraining Loss: 0.000090\n",
      "Epoch: 1 \tTraining Loss: 0.000129\n",
      "Epoch: 2 \tTraining Loss: 0.000129\n",
      "Epoch: 3 \tTraining Loss: 0.000129\n",
      "Epoch: 4 \tTraining Loss: 0.000129\n",
      "Epoch: 5 \tTraining Loss: 0.000129\n",
      "Epoch: 6 \tTraining Loss: 0.000129\n",
      "Epoch: 7 \tTraining Loss: 0.000129\n",
      "Epoch: 8 \tTraining Loss: 0.000129\n",
      "Epoch: 9 \tTraining Loss: 0.000129\n",
      "Epoch: 10 \tTraining Loss: 0.000129\n",
      "Epoch: 11 \tTraining Loss: 0.000129\n",
      "Epoch: 12 \tTraining Loss: 0.000129\n",
      "Epoch: 13 \tTraining Loss: 0.000129\n",
      "Epoch: 14 \tTraining Loss: 0.000129\n",
      "Epoch: 15 \tTraining Loss: 0.000129\n",
      "Epoch: 16 \tTraining Loss: 0.000129\n",
      "Epoch: 17 \tTraining Loss: 0.000129\n",
      "Epoch: 18 \tTraining Loss: 0.000129\n",
      "Epoch: 1 \tTraining Loss: 0.000318\n",
      "Epoch: 2 \tTraining Loss: 0.000318\n",
      "Epoch: 3 \tTraining Loss: 0.000318\n",
      "Epoch: 4 \tTraining Loss: 0.000318\n",
      "Epoch: 5 \tTraining Loss: 0.000318\n",
      "Epoch: 6 \tTraining Loss: 0.000318\n",
      "Epoch: 7 \tTraining Loss: 0.000318\n",
      "Epoch: 8 \tTraining Loss: 0.000318\n",
      "Epoch: 9 \tTraining Loss: 0.000318\n",
      "Epoch: 10 \tTraining Loss: 0.000318\n",
      "Epoch: 11 \tTraining Loss: 0.000318\n",
      "Epoch: 12 \tTraining Loss: 0.000318\n",
      "Epoch: 13 \tTraining Loss: 0.000318\n",
      "Epoch: 14 \tTraining Loss: 0.000318\n",
      "Epoch: 15 \tTraining Loss: 0.000318\n",
      "Epoch: 16 \tTraining Loss: 0.000318\n",
      "Epoch: 17 \tTraining Loss: 0.000318\n",
      "Epoch: 18 \tTraining Loss: 0.000318\n",
      "Epoch: 1 \tTraining Loss: 0.000178\n",
      "Epoch: 2 \tTraining Loss: 0.000178\n",
      "Epoch: 3 \tTraining Loss: 0.000178\n",
      "Epoch: 4 \tTraining Loss: 0.000178\n",
      "Epoch: 5 \tTraining Loss: 0.000178\n",
      "Epoch: 6 \tTraining Loss: 0.000178\n",
      "Epoch: 7 \tTraining Loss: 0.000178\n",
      "Epoch: 8 \tTraining Loss: 0.000178\n",
      "Epoch: 9 \tTraining Loss: 0.000178\n",
      "Epoch: 10 \tTraining Loss: 0.000178\n",
      "Epoch: 11 \tTraining Loss: 0.000178\n",
      "Epoch: 12 \tTraining Loss: 0.000178\n",
      "Epoch: 13 \tTraining Loss: 0.000178\n",
      "Epoch: 14 \tTraining Loss: 0.000178\n",
      "Epoch: 15 \tTraining Loss: 0.000178\n",
      "Epoch: 16 \tTraining Loss: 0.000178\n",
      "Epoch: 17 \tTraining Loss: 0.000178\n",
      "Epoch: 18 \tTraining Loss: 0.000178\n",
      "Epoch: 1 \tTraining Loss: 0.000325\n",
      "Epoch: 2 \tTraining Loss: 0.000325\n",
      "Epoch: 3 \tTraining Loss: 0.000325\n",
      "Epoch: 4 \tTraining Loss: 0.000325\n",
      "Epoch: 5 \tTraining Loss: 0.000325\n",
      "Epoch: 6 \tTraining Loss: 0.000325\n",
      "Epoch: 7 \tTraining Loss: 0.000325\n",
      "Epoch: 8 \tTraining Loss: 0.000325\n",
      "Epoch: 9 \tTraining Loss: 0.000325\n",
      "Epoch: 10 \tTraining Loss: 0.000325\n",
      "Epoch: 11 \tTraining Loss: 0.000325\n",
      "Epoch: 12 \tTraining Loss: 0.000325\n",
      "Epoch: 13 \tTraining Loss: 0.000325\n",
      "Epoch: 14 \tTraining Loss: 0.000325\n",
      "Epoch: 15 \tTraining Loss: 0.000325\n",
      "Epoch: 16 \tTraining Loss: 0.000325\n",
      "Epoch: 17 \tTraining Loss: 0.000325\n",
      "Epoch: 18 \tTraining Loss: 0.000325\n",
      "Epoch: 1 \tTraining Loss: 0.000266\n",
      "Epoch: 2 \tTraining Loss: 0.000266\n",
      "Epoch: 3 \tTraining Loss: 0.000266\n",
      "Epoch: 4 \tTraining Loss: 0.000266\n",
      "Epoch: 5 \tTraining Loss: 0.000266\n",
      "Epoch: 6 \tTraining Loss: 0.000266\n",
      "Epoch: 7 \tTraining Loss: 0.000266\n",
      "Epoch: 8 \tTraining Loss: 0.000266\n",
      "Epoch: 9 \tTraining Loss: 0.000266\n",
      "Epoch: 10 \tTraining Loss: 0.000266\n",
      "Epoch: 11 \tTraining Loss: 0.000266\n",
      "Epoch: 12 \tTraining Loss: 0.000266\n",
      "Epoch: 13 \tTraining Loss: 0.000266\n",
      "Epoch: 14 \tTraining Loss: 0.000266\n",
      "Epoch: 15 \tTraining Loss: 0.000266\n",
      "Epoch: 16 \tTraining Loss: 0.000266\n",
      "Epoch: 17 \tTraining Loss: 0.000266\n",
      "Epoch: 18 \tTraining Loss: 0.000266\n",
      "Epoch: 1 \tTraining Loss: 0.000149\n",
      "Epoch: 2 \tTraining Loss: 0.000149\n",
      "Epoch: 3 \tTraining Loss: 0.000149\n",
      "Epoch: 4 \tTraining Loss: 0.000149\n",
      "Epoch: 5 \tTraining Loss: 0.000149\n",
      "Epoch: 6 \tTraining Loss: 0.000149\n",
      "Epoch: 7 \tTraining Loss: 0.000149\n",
      "Epoch: 8 \tTraining Loss: 0.000149\n",
      "Epoch: 9 \tTraining Loss: 0.000149\n",
      "Epoch: 10 \tTraining Loss: 0.000149\n",
      "Epoch: 11 \tTraining Loss: 0.000149\n",
      "Epoch: 12 \tTraining Loss: 0.000149\n",
      "Epoch: 13 \tTraining Loss: 0.000149\n",
      "Epoch: 14 \tTraining Loss: 0.000149\n",
      "Epoch: 15 \tTraining Loss: 0.000149\n",
      "Epoch: 16 \tTraining Loss: 0.000149\n",
      "Epoch: 17 \tTraining Loss: 0.000149\n",
      "Epoch: 18 \tTraining Loss: 0.000149\n",
      "Epoch: 1 \tTraining Loss: 0.000463\n",
      "Epoch: 2 \tTraining Loss: 0.000463\n",
      "Epoch: 3 \tTraining Loss: 0.000463\n",
      "Epoch: 4 \tTraining Loss: 0.000463\n",
      "Epoch: 5 \tTraining Loss: 0.000463\n",
      "Epoch: 6 \tTraining Loss: 0.000463\n",
      "Epoch: 7 \tTraining Loss: 0.000463\n",
      "Epoch: 8 \tTraining Loss: 0.000463\n",
      "Epoch: 9 \tTraining Loss: 0.000463\n",
      "Epoch: 10 \tTraining Loss: 0.000463\n",
      "Epoch: 11 \tTraining Loss: 0.000463\n",
      "Epoch: 12 \tTraining Loss: 0.000463\n",
      "Epoch: 13 \tTraining Loss: 0.000463\n",
      "Epoch: 14 \tTraining Loss: 0.000463\n",
      "Epoch: 15 \tTraining Loss: 0.000463\n",
      "Epoch: 16 \tTraining Loss: 0.000463\n",
      "Epoch: 17 \tTraining Loss: 0.000463\n",
      "Epoch: 18 \tTraining Loss: 0.000463\n",
      "Epoch: 1 \tTraining Loss: 0.000466\n",
      "Epoch: 2 \tTraining Loss: 0.000466\n",
      "Epoch: 3 \tTraining Loss: 0.000466\n",
      "Epoch: 4 \tTraining Loss: 0.000466\n",
      "Epoch: 5 \tTraining Loss: 0.000466\n",
      "Epoch: 6 \tTraining Loss: 0.000466\n",
      "Epoch: 7 \tTraining Loss: 0.000466\n",
      "Epoch: 8 \tTraining Loss: 0.000466\n",
      "Epoch: 9 \tTraining Loss: 0.000466\n",
      "Epoch: 10 \tTraining Loss: 0.000466\n",
      "Epoch: 11 \tTraining Loss: 0.000466\n",
      "Epoch: 12 \tTraining Loss: 0.000466\n",
      "Epoch: 13 \tTraining Loss: 0.000466\n",
      "Epoch: 14 \tTraining Loss: 0.000466\n",
      "Epoch: 15 \tTraining Loss: 0.000466\n",
      "Epoch: 16 \tTraining Loss: 0.000466\n",
      "Epoch: 17 \tTraining Loss: 0.000466\n",
      "Epoch: 18 \tTraining Loss: 0.000466\n",
      "Epoch: 1 \tTraining Loss: 0.000129\n",
      "Epoch: 2 \tTraining Loss: 0.000129\n",
      "Epoch: 3 \tTraining Loss: 0.000129\n",
      "Epoch: 4 \tTraining Loss: 0.000129\n",
      "Epoch: 5 \tTraining Loss: 0.000129\n",
      "Epoch: 6 \tTraining Loss: 0.000129\n",
      "Epoch: 7 \tTraining Loss: 0.000129\n",
      "Epoch: 8 \tTraining Loss: 0.000129\n",
      "Epoch: 9 \tTraining Loss: 0.000129\n",
      "Epoch: 10 \tTraining Loss: 0.000129\n",
      "Epoch: 11 \tTraining Loss: 0.000129\n",
      "Epoch: 12 \tTraining Loss: 0.000129\n",
      "Epoch: 13 \tTraining Loss: 0.000129\n",
      "Epoch: 14 \tTraining Loss: 0.000129\n",
      "Epoch: 15 \tTraining Loss: 0.000129\n",
      "Epoch: 16 \tTraining Loss: 0.000129\n",
      "Epoch: 17 \tTraining Loss: 0.000129\n",
      "Epoch: 18 \tTraining Loss: 0.000129\n",
      "Epoch: 1 \tTraining Loss: 0.000128\n",
      "Epoch: 2 \tTraining Loss: 0.000128\n",
      "Epoch: 3 \tTraining Loss: 0.000128\n",
      "Epoch: 4 \tTraining Loss: 0.000128\n",
      "Epoch: 5 \tTraining Loss: 0.000128\n",
      "Epoch: 6 \tTraining Loss: 0.000128\n",
      "Epoch: 7 \tTraining Loss: 0.000128\n",
      "Epoch: 8 \tTraining Loss: 0.000128\n",
      "Epoch: 9 \tTraining Loss: 0.000128\n",
      "Epoch: 10 \tTraining Loss: 0.000128\n",
      "Epoch: 11 \tTraining Loss: 0.000128\n",
      "Epoch: 12 \tTraining Loss: 0.000128\n",
      "Epoch: 13 \tTraining Loss: 0.000128\n",
      "Epoch: 14 \tTraining Loss: 0.000128\n",
      "Epoch: 15 \tTraining Loss: 0.000128\n",
      "Epoch: 16 \tTraining Loss: 0.000128\n",
      "Epoch: 17 \tTraining Loss: 0.000128\n",
      "Epoch: 18 \tTraining Loss: 0.000128\n",
      "Epoch: 1 \tTraining Loss: 0.000165\n",
      "Epoch: 2 \tTraining Loss: 0.000165\n",
      "Epoch: 3 \tTraining Loss: 0.000165\n",
      "Epoch: 4 \tTraining Loss: 0.000165\n",
      "Epoch: 5 \tTraining Loss: 0.000165\n",
      "Epoch: 6 \tTraining Loss: 0.000165\n",
      "Epoch: 7 \tTraining Loss: 0.000165\n",
      "Epoch: 8 \tTraining Loss: 0.000165\n",
      "Epoch: 9 \tTraining Loss: 0.000165\n",
      "Epoch: 10 \tTraining Loss: 0.000165\n",
      "Epoch: 11 \tTraining Loss: 0.000165\n",
      "Epoch: 12 \tTraining Loss: 0.000165\n",
      "Epoch: 13 \tTraining Loss: 0.000165\n",
      "Epoch: 14 \tTraining Loss: 0.000165\n",
      "Epoch: 15 \tTraining Loss: 0.000165\n",
      "Epoch: 16 \tTraining Loss: 0.000165\n",
      "Epoch: 17 \tTraining Loss: 0.000165\n",
      "Epoch: 18 \tTraining Loss: 0.000165\n",
      "Epoch: 1 \tTraining Loss: 0.000277\n",
      "Epoch: 2 \tTraining Loss: 0.000277\n",
      "Epoch: 3 \tTraining Loss: 0.000277\n",
      "Epoch: 4 \tTraining Loss: 0.000277\n",
      "Epoch: 5 \tTraining Loss: 0.000277\n",
      "Epoch: 6 \tTraining Loss: 0.000277\n",
      "Epoch: 7 \tTraining Loss: 0.000277\n",
      "Epoch: 8 \tTraining Loss: 0.000277\n",
      "Epoch: 9 \tTraining Loss: 0.000277\n",
      "Epoch: 10 \tTraining Loss: 0.000277\n",
      "Epoch: 11 \tTraining Loss: 0.000277\n",
      "Epoch: 12 \tTraining Loss: 0.000277\n",
      "Epoch: 13 \tTraining Loss: 0.000277\n",
      "Epoch: 14 \tTraining Loss: 0.000277\n",
      "Epoch: 15 \tTraining Loss: 0.000277\n",
      "Epoch: 16 \tTraining Loss: 0.000277\n",
      "Epoch: 17 \tTraining Loss: 0.000277\n",
      "Epoch: 18 \tTraining Loss: 0.000277\n",
      "Epoch: 1 \tTraining Loss: 0.000187\n",
      "Epoch: 2 \tTraining Loss: 0.000187\n",
      "Epoch: 3 \tTraining Loss: 0.000187\n",
      "Epoch: 4 \tTraining Loss: 0.000187\n",
      "Epoch: 5 \tTraining Loss: 0.000187\n",
      "Epoch: 6 \tTraining Loss: 0.000187\n",
      "Epoch: 7 \tTraining Loss: 0.000187\n",
      "Epoch: 8 \tTraining Loss: 0.000187\n",
      "Epoch: 9 \tTraining Loss: 0.000187\n",
      "Epoch: 10 \tTraining Loss: 0.000187\n",
      "Epoch: 11 \tTraining Loss: 0.000187\n",
      "Epoch: 12 \tTraining Loss: 0.000187\n",
      "Epoch: 13 \tTraining Loss: 0.000187\n",
      "Epoch: 14 \tTraining Loss: 0.000187\n",
      "Epoch: 15 \tTraining Loss: 0.000187\n",
      "Epoch: 16 \tTraining Loss: 0.000187\n",
      "Epoch: 17 \tTraining Loss: 0.000187\n",
      "Epoch: 18 \tTraining Loss: 0.000187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000171\n",
      "Epoch: 2 \tTraining Loss: 0.000171\n",
      "Epoch: 3 \tTraining Loss: 0.000171\n",
      "Epoch: 4 \tTraining Loss: 0.000171\n",
      "Epoch: 5 \tTraining Loss: 0.000171\n",
      "Epoch: 6 \tTraining Loss: 0.000171\n",
      "Epoch: 7 \tTraining Loss: 0.000171\n",
      "Epoch: 8 \tTraining Loss: 0.000171\n",
      "Epoch: 9 \tTraining Loss: 0.000171\n",
      "Epoch: 10 \tTraining Loss: 0.000171\n",
      "Epoch: 11 \tTraining Loss: 0.000171\n",
      "Epoch: 12 \tTraining Loss: 0.000171\n",
      "Epoch: 13 \tTraining Loss: 0.000171\n",
      "Epoch: 14 \tTraining Loss: 0.000171\n",
      "Epoch: 15 \tTraining Loss: 0.000171\n",
      "Epoch: 16 \tTraining Loss: 0.000171\n",
      "Epoch: 17 \tTraining Loss: 0.000171\n",
      "Epoch: 18 \tTraining Loss: 0.000171\n",
      "Epoch: 1 \tTraining Loss: 0.000506\n",
      "Epoch: 2 \tTraining Loss: 0.000506\n",
      "Epoch: 3 \tTraining Loss: 0.000506\n",
      "Epoch: 4 \tTraining Loss: 0.000506\n",
      "Epoch: 5 \tTraining Loss: 0.000506\n",
      "Epoch: 6 \tTraining Loss: 0.000506\n",
      "Epoch: 7 \tTraining Loss: 0.000506\n",
      "Epoch: 8 \tTraining Loss: 0.000506\n",
      "Epoch: 9 \tTraining Loss: 0.000506\n",
      "Epoch: 10 \tTraining Loss: 0.000506\n",
      "Epoch: 11 \tTraining Loss: 0.000506\n",
      "Epoch: 12 \tTraining Loss: 0.000506\n",
      "Epoch: 13 \tTraining Loss: 0.000506\n",
      "Epoch: 14 \tTraining Loss: 0.000506\n",
      "Epoch: 15 \tTraining Loss: 0.000506\n",
      "Epoch: 16 \tTraining Loss: 0.000506\n",
      "Epoch: 17 \tTraining Loss: 0.000506\n",
      "Epoch: 18 \tTraining Loss: 0.000506\n",
      "Epoch: 1 \tTraining Loss: 0.000152\n",
      "Epoch: 2 \tTraining Loss: 0.000152\n",
      "Epoch: 3 \tTraining Loss: 0.000152\n",
      "Epoch: 4 \tTraining Loss: 0.000152\n",
      "Epoch: 5 \tTraining Loss: 0.000152\n",
      "Epoch: 6 \tTraining Loss: 0.000152\n",
      "Epoch: 7 \tTraining Loss: 0.000152\n",
      "Epoch: 8 \tTraining Loss: 0.000152\n",
      "Epoch: 9 \tTraining Loss: 0.000152\n",
      "Epoch: 10 \tTraining Loss: 0.000152\n",
      "Epoch: 11 \tTraining Loss: 0.000152\n",
      "Epoch: 12 \tTraining Loss: 0.000152\n",
      "Epoch: 13 \tTraining Loss: 0.000152\n",
      "Epoch: 14 \tTraining Loss: 0.000152\n",
      "Epoch: 15 \tTraining Loss: 0.000152\n",
      "Epoch: 16 \tTraining Loss: 0.000152\n",
      "Epoch: 17 \tTraining Loss: 0.000152\n",
      "Epoch: 18 \tTraining Loss: 0.000152\n",
      "Epoch: 1 \tTraining Loss: 0.000260\n",
      "Epoch: 2 \tTraining Loss: 0.000260\n",
      "Epoch: 3 \tTraining Loss: 0.000260\n",
      "Epoch: 4 \tTraining Loss: 0.000260\n",
      "Epoch: 5 \tTraining Loss: 0.000260\n",
      "Epoch: 6 \tTraining Loss: 0.000260\n",
      "Epoch: 7 \tTraining Loss: 0.000260\n",
      "Epoch: 8 \tTraining Loss: 0.000260\n",
      "Epoch: 9 \tTraining Loss: 0.000260\n",
      "Epoch: 10 \tTraining Loss: 0.000260\n",
      "Epoch: 11 \tTraining Loss: 0.000260\n",
      "Epoch: 12 \tTraining Loss: 0.000260\n",
      "Epoch: 13 \tTraining Loss: 0.000260\n",
      "Epoch: 14 \tTraining Loss: 0.000260\n",
      "Epoch: 15 \tTraining Loss: 0.000260\n",
      "Epoch: 16 \tTraining Loss: 0.000260\n",
      "Epoch: 17 \tTraining Loss: 0.000260\n",
      "Epoch: 18 \tTraining Loss: 0.000260\n",
      "Epoch: 1 \tTraining Loss: 0.000298\n",
      "Epoch: 2 \tTraining Loss: 0.000298\n",
      "Epoch: 3 \tTraining Loss: 0.000298\n",
      "Epoch: 4 \tTraining Loss: 0.000298\n",
      "Epoch: 5 \tTraining Loss: 0.000298\n",
      "Epoch: 6 \tTraining Loss: 0.000298\n",
      "Epoch: 7 \tTraining Loss: 0.000298\n",
      "Epoch: 8 \tTraining Loss: 0.000298\n",
      "Epoch: 9 \tTraining Loss: 0.000298\n",
      "Epoch: 10 \tTraining Loss: 0.000298\n",
      "Epoch: 11 \tTraining Loss: 0.000298\n",
      "Epoch: 12 \tTraining Loss: 0.000298\n",
      "Epoch: 13 \tTraining Loss: 0.000298\n",
      "Epoch: 14 \tTraining Loss: 0.000298\n",
      "Epoch: 15 \tTraining Loss: 0.000298\n",
      "Epoch: 16 \tTraining Loss: 0.000298\n",
      "Epoch: 17 \tTraining Loss: 0.000298\n",
      "Epoch: 18 \tTraining Loss: 0.000298\n",
      "Epoch: 1 \tTraining Loss: 0.000182\n",
      "Epoch: 2 \tTraining Loss: 0.000182\n",
      "Epoch: 3 \tTraining Loss: 0.000182\n",
      "Epoch: 4 \tTraining Loss: 0.000182\n",
      "Epoch: 5 \tTraining Loss: 0.000182\n",
      "Epoch: 6 \tTraining Loss: 0.000182\n",
      "Epoch: 7 \tTraining Loss: 0.000182\n",
      "Epoch: 8 \tTraining Loss: 0.000182\n",
      "Epoch: 9 \tTraining Loss: 0.000182\n",
      "Epoch: 10 \tTraining Loss: 0.000182\n",
      "Epoch: 11 \tTraining Loss: 0.000182\n",
      "Epoch: 12 \tTraining Loss: 0.000182\n",
      "Epoch: 13 \tTraining Loss: 0.000182\n",
      "Epoch: 14 \tTraining Loss: 0.000182\n",
      "Epoch: 15 \tTraining Loss: 0.000182\n",
      "Epoch: 16 \tTraining Loss: 0.000182\n",
      "Epoch: 17 \tTraining Loss: 0.000182\n",
      "Epoch: 18 \tTraining Loss: 0.000182\n",
      "Epoch: 1 \tTraining Loss: 0.000385\n",
      "Epoch: 2 \tTraining Loss: 0.000385\n",
      "Epoch: 3 \tTraining Loss: 0.000385\n",
      "Epoch: 4 \tTraining Loss: 0.000385\n",
      "Epoch: 5 \tTraining Loss: 0.000385\n",
      "Epoch: 6 \tTraining Loss: 0.000385\n",
      "Epoch: 7 \tTraining Loss: 0.000385\n",
      "Epoch: 8 \tTraining Loss: 0.000385\n",
      "Epoch: 9 \tTraining Loss: 0.000385\n",
      "Epoch: 10 \tTraining Loss: 0.000385\n",
      "Epoch: 11 \tTraining Loss: 0.000385\n",
      "Epoch: 12 \tTraining Loss: 0.000385\n",
      "Epoch: 13 \tTraining Loss: 0.000385\n",
      "Epoch: 14 \tTraining Loss: 0.000385\n",
      "Epoch: 15 \tTraining Loss: 0.000385\n",
      "Epoch: 16 \tTraining Loss: 0.000385\n",
      "Epoch: 17 \tTraining Loss: 0.000385\n",
      "Epoch: 18 \tTraining Loss: 0.000385\n",
      "Epoch: 1 \tTraining Loss: 0.000249\n",
      "Epoch: 2 \tTraining Loss: 0.000249\n",
      "Epoch: 3 \tTraining Loss: 0.000249\n",
      "Epoch: 4 \tTraining Loss: 0.000249\n",
      "Epoch: 5 \tTraining Loss: 0.000249\n",
      "Epoch: 6 \tTraining Loss: 0.000249\n",
      "Epoch: 7 \tTraining Loss: 0.000249\n",
      "Epoch: 8 \tTraining Loss: 0.000249\n",
      "Epoch: 9 \tTraining Loss: 0.000249\n",
      "Epoch: 10 \tTraining Loss: 0.000249\n",
      "Epoch: 11 \tTraining Loss: 0.000249\n",
      "Epoch: 12 \tTraining Loss: 0.000249\n",
      "Epoch: 13 \tTraining Loss: 0.000249\n",
      "Epoch: 14 \tTraining Loss: 0.000249\n",
      "Epoch: 15 \tTraining Loss: 0.000249\n",
      "Epoch: 16 \tTraining Loss: 0.000249\n",
      "Epoch: 17 \tTraining Loss: 0.000249\n",
      "Epoch: 18 \tTraining Loss: 0.000249\n",
      "Epoch: 1 \tTraining Loss: 0.000192\n",
      "Epoch: 2 \tTraining Loss: 0.000192\n",
      "Epoch: 3 \tTraining Loss: 0.000192\n",
      "Epoch: 4 \tTraining Loss: 0.000192\n",
      "Epoch: 5 \tTraining Loss: 0.000192\n",
      "Epoch: 6 \tTraining Loss: 0.000192\n",
      "Epoch: 7 \tTraining Loss: 0.000192\n",
      "Epoch: 8 \tTraining Loss: 0.000192\n",
      "Epoch: 9 \tTraining Loss: 0.000192\n",
      "Epoch: 10 \tTraining Loss: 0.000192\n",
      "Epoch: 11 \tTraining Loss: 0.000192\n",
      "Epoch: 12 \tTraining Loss: 0.000192\n",
      "Epoch: 13 \tTraining Loss: 0.000192\n",
      "Epoch: 14 \tTraining Loss: 0.000192\n",
      "Epoch: 15 \tTraining Loss: 0.000192\n",
      "Epoch: 16 \tTraining Loss: 0.000192\n",
      "Epoch: 17 \tTraining Loss: 0.000192\n",
      "Epoch: 18 \tTraining Loss: 0.000192\n",
      "Epoch: 1 \tTraining Loss: 0.000162\n",
      "Epoch: 2 \tTraining Loss: 0.000162\n",
      "Epoch: 3 \tTraining Loss: 0.000162\n",
      "Epoch: 4 \tTraining Loss: 0.000162\n",
      "Epoch: 5 \tTraining Loss: 0.000162\n",
      "Epoch: 6 \tTraining Loss: 0.000162\n",
      "Epoch: 7 \tTraining Loss: 0.000162\n",
      "Epoch: 8 \tTraining Loss: 0.000162\n",
      "Epoch: 9 \tTraining Loss: 0.000162\n",
      "Epoch: 10 \tTraining Loss: 0.000162\n",
      "Epoch: 11 \tTraining Loss: 0.000162\n",
      "Epoch: 12 \tTraining Loss: 0.000162\n",
      "Epoch: 13 \tTraining Loss: 0.000162\n",
      "Epoch: 14 \tTraining Loss: 0.000162\n",
      "Epoch: 15 \tTraining Loss: 0.000162\n",
      "Epoch: 16 \tTraining Loss: 0.000162\n",
      "Epoch: 17 \tTraining Loss: 0.000162\n",
      "Epoch: 18 \tTraining Loss: 0.000162\n",
      "Epoch: 1 \tTraining Loss: 0.000239\n",
      "Epoch: 2 \tTraining Loss: 0.000239\n",
      "Epoch: 3 \tTraining Loss: 0.000239\n",
      "Epoch: 4 \tTraining Loss: 0.000239\n",
      "Epoch: 5 \tTraining Loss: 0.000239\n",
      "Epoch: 6 \tTraining Loss: 0.000239\n",
      "Epoch: 7 \tTraining Loss: 0.000239\n",
      "Epoch: 8 \tTraining Loss: 0.000239\n",
      "Epoch: 9 \tTraining Loss: 0.000239\n",
      "Epoch: 10 \tTraining Loss: 0.000239\n",
      "Epoch: 11 \tTraining Loss: 0.000239\n",
      "Epoch: 12 \tTraining Loss: 0.000239\n",
      "Epoch: 13 \tTraining Loss: 0.000239\n",
      "Epoch: 14 \tTraining Loss: 0.000239\n",
      "Epoch: 15 \tTraining Loss: 0.000239\n",
      "Epoch: 16 \tTraining Loss: 0.000239\n",
      "Epoch: 17 \tTraining Loss: 0.000239\n",
      "Epoch: 18 \tTraining Loss: 0.000239\n",
      "Epoch: 1 \tTraining Loss: 0.000258\n",
      "Epoch: 2 \tTraining Loss: 0.000258\n",
      "Epoch: 3 \tTraining Loss: 0.000258\n",
      "Epoch: 4 \tTraining Loss: 0.000258\n",
      "Epoch: 5 \tTraining Loss: 0.000258\n",
      "Epoch: 6 \tTraining Loss: 0.000258\n",
      "Epoch: 7 \tTraining Loss: 0.000258\n",
      "Epoch: 8 \tTraining Loss: 0.000258\n",
      "Epoch: 9 \tTraining Loss: 0.000258\n",
      "Epoch: 10 \tTraining Loss: 0.000258\n",
      "Epoch: 11 \tTraining Loss: 0.000258\n",
      "Epoch: 12 \tTraining Loss: 0.000258\n",
      "Epoch: 13 \tTraining Loss: 0.000258\n",
      "Epoch: 14 \tTraining Loss: 0.000258\n",
      "Epoch: 15 \tTraining Loss: 0.000258\n",
      "Epoch: 16 \tTraining Loss: 0.000258\n",
      "Epoch: 17 \tTraining Loss: 0.000258\n",
      "Epoch: 18 \tTraining Loss: 0.000258\n",
      "Epoch: 1 \tTraining Loss: 0.000221\n",
      "Epoch: 2 \tTraining Loss: 0.000221\n",
      "Epoch: 3 \tTraining Loss: 0.000221\n",
      "Epoch: 4 \tTraining Loss: 0.000221\n",
      "Epoch: 5 \tTraining Loss: 0.000221\n",
      "Epoch: 6 \tTraining Loss: 0.000221\n",
      "Epoch: 7 \tTraining Loss: 0.000221\n",
      "Epoch: 8 \tTraining Loss: 0.000221\n",
      "Epoch: 9 \tTraining Loss: 0.000221\n",
      "Epoch: 10 \tTraining Loss: 0.000221\n",
      "Epoch: 11 \tTraining Loss: 0.000221\n",
      "Epoch: 12 \tTraining Loss: 0.000221\n",
      "Epoch: 13 \tTraining Loss: 0.000221\n",
      "Epoch: 14 \tTraining Loss: 0.000221\n",
      "Epoch: 15 \tTraining Loss: 0.000221\n",
      "Epoch: 16 \tTraining Loss: 0.000221\n",
      "Epoch: 17 \tTraining Loss: 0.000221\n",
      "Epoch: 18 \tTraining Loss: 0.000221\n",
      "Epoch: 1 \tTraining Loss: 0.000201\n",
      "Epoch: 2 \tTraining Loss: 0.000201\n",
      "Epoch: 3 \tTraining Loss: 0.000201\n",
      "Epoch: 4 \tTraining Loss: 0.000201\n",
      "Epoch: 5 \tTraining Loss: 0.000201\n",
      "Epoch: 6 \tTraining Loss: 0.000201\n",
      "Epoch: 7 \tTraining Loss: 0.000201\n",
      "Epoch: 8 \tTraining Loss: 0.000201\n",
      "Epoch: 9 \tTraining Loss: 0.000201\n",
      "Epoch: 10 \tTraining Loss: 0.000201\n",
      "Epoch: 11 \tTraining Loss: 0.000201\n",
      "Epoch: 12 \tTraining Loss: 0.000201\n",
      "Epoch: 13 \tTraining Loss: 0.000201\n",
      "Epoch: 14 \tTraining Loss: 0.000201\n",
      "Epoch: 15 \tTraining Loss: 0.000201\n",
      "Epoch: 16 \tTraining Loss: 0.000201\n",
      "Epoch: 17 \tTraining Loss: 0.000201\n",
      "Epoch: 18 \tTraining Loss: 0.000201\n",
      "Epoch: 1 \tTraining Loss: 0.000334\n",
      "Epoch: 2 \tTraining Loss: 0.000334\n",
      "Epoch: 3 \tTraining Loss: 0.000334\n",
      "Epoch: 4 \tTraining Loss: 0.000334\n",
      "Epoch: 5 \tTraining Loss: 0.000334\n",
      "Epoch: 6 \tTraining Loss: 0.000334\n",
      "Epoch: 7 \tTraining Loss: 0.000334\n",
      "Epoch: 8 \tTraining Loss: 0.000334\n",
      "Epoch: 9 \tTraining Loss: 0.000334\n",
      "Epoch: 10 \tTraining Loss: 0.000334\n",
      "Epoch: 11 \tTraining Loss: 0.000334\n",
      "Epoch: 12 \tTraining Loss: 0.000334\n",
      "Epoch: 13 \tTraining Loss: 0.000334\n",
      "Epoch: 14 \tTraining Loss: 0.000334\n",
      "Epoch: 15 \tTraining Loss: 0.000334\n",
      "Epoch: 16 \tTraining Loss: 0.000334\n",
      "Epoch: 17 \tTraining Loss: 0.000334\n",
      "Epoch: 18 \tTraining Loss: 0.000334\n",
      "Epoch: 1 \tTraining Loss: 0.000334\n",
      "Epoch: 2 \tTraining Loss: 0.000334\n",
      "Epoch: 3 \tTraining Loss: 0.000334\n",
      "Epoch: 4 \tTraining Loss: 0.000334\n",
      "Epoch: 5 \tTraining Loss: 0.000334\n",
      "Epoch: 6 \tTraining Loss: 0.000334\n",
      "Epoch: 7 \tTraining Loss: 0.000334\n",
      "Epoch: 8 \tTraining Loss: 0.000334\n",
      "Epoch: 9 \tTraining Loss: 0.000334\n",
      "Epoch: 10 \tTraining Loss: 0.000334\n",
      "Epoch: 11 \tTraining Loss: 0.000334\n",
      "Epoch: 12 \tTraining Loss: 0.000334\n",
      "Epoch: 13 \tTraining Loss: 0.000334\n",
      "Epoch: 14 \tTraining Loss: 0.000334\n",
      "Epoch: 15 \tTraining Loss: 0.000334\n",
      "Epoch: 16 \tTraining Loss: 0.000334\n",
      "Epoch: 17 \tTraining Loss: 0.000334\n",
      "Epoch: 18 \tTraining Loss: 0.000334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000265\n",
      "Epoch: 2 \tTraining Loss: 0.000265\n",
      "Epoch: 3 \tTraining Loss: 0.000265\n",
      "Epoch: 4 \tTraining Loss: 0.000265\n",
      "Epoch: 5 \tTraining Loss: 0.000265\n",
      "Epoch: 6 \tTraining Loss: 0.000265\n",
      "Epoch: 7 \tTraining Loss: 0.000265\n",
      "Epoch: 8 \tTraining Loss: 0.000265\n",
      "Epoch: 9 \tTraining Loss: 0.000265\n",
      "Epoch: 10 \tTraining Loss: 0.000265\n",
      "Epoch: 11 \tTraining Loss: 0.000265\n",
      "Epoch: 12 \tTraining Loss: 0.000265\n",
      "Epoch: 13 \tTraining Loss: 0.000265\n",
      "Epoch: 14 \tTraining Loss: 0.000265\n",
      "Epoch: 15 \tTraining Loss: 0.000265\n",
      "Epoch: 16 \tTraining Loss: 0.000265\n",
      "Epoch: 17 \tTraining Loss: 0.000265\n",
      "Epoch: 18 \tTraining Loss: 0.000265\n",
      "Epoch: 1 \tTraining Loss: 0.000106\n",
      "Epoch: 2 \tTraining Loss: 0.000106\n",
      "Epoch: 3 \tTraining Loss: 0.000106\n",
      "Epoch: 4 \tTraining Loss: 0.000106\n",
      "Epoch: 5 \tTraining Loss: 0.000106\n",
      "Epoch: 6 \tTraining Loss: 0.000106\n",
      "Epoch: 7 \tTraining Loss: 0.000106\n",
      "Epoch: 8 \tTraining Loss: 0.000106\n",
      "Epoch: 9 \tTraining Loss: 0.000106\n",
      "Epoch: 10 \tTraining Loss: 0.000106\n",
      "Epoch: 11 \tTraining Loss: 0.000106\n",
      "Epoch: 12 \tTraining Loss: 0.000106\n",
      "Epoch: 13 \tTraining Loss: 0.000106\n",
      "Epoch: 14 \tTraining Loss: 0.000106\n",
      "Epoch: 15 \tTraining Loss: 0.000106\n",
      "Epoch: 16 \tTraining Loss: 0.000106\n",
      "Epoch: 17 \tTraining Loss: 0.000106\n",
      "Epoch: 18 \tTraining Loss: 0.000106\n",
      "Epoch: 1 \tTraining Loss: 0.000316\n",
      "Epoch: 2 \tTraining Loss: 0.000316\n",
      "Epoch: 3 \tTraining Loss: 0.000316\n",
      "Epoch: 4 \tTraining Loss: 0.000316\n",
      "Epoch: 5 \tTraining Loss: 0.000316\n",
      "Epoch: 6 \tTraining Loss: 0.000316\n",
      "Epoch: 7 \tTraining Loss: 0.000316\n",
      "Epoch: 8 \tTraining Loss: 0.000316\n",
      "Epoch: 9 \tTraining Loss: 0.000316\n",
      "Epoch: 10 \tTraining Loss: 0.000316\n",
      "Epoch: 11 \tTraining Loss: 0.000316\n",
      "Epoch: 12 \tTraining Loss: 0.000316\n",
      "Epoch: 13 \tTraining Loss: 0.000316\n",
      "Epoch: 14 \tTraining Loss: 0.000316\n",
      "Epoch: 15 \tTraining Loss: 0.000316\n",
      "Epoch: 16 \tTraining Loss: 0.000316\n",
      "Epoch: 17 \tTraining Loss: 0.000316\n",
      "Epoch: 18 \tTraining Loss: 0.000316\n",
      "Epoch: 1 \tTraining Loss: 0.000203\n",
      "Epoch: 2 \tTraining Loss: 0.000203\n",
      "Epoch: 3 \tTraining Loss: 0.000203\n",
      "Epoch: 4 \tTraining Loss: 0.000203\n",
      "Epoch: 5 \tTraining Loss: 0.000203\n",
      "Epoch: 6 \tTraining Loss: 0.000203\n",
      "Epoch: 7 \tTraining Loss: 0.000203\n",
      "Epoch: 8 \tTraining Loss: 0.000203\n",
      "Epoch: 9 \tTraining Loss: 0.000203\n",
      "Epoch: 10 \tTraining Loss: 0.000203\n",
      "Epoch: 11 \tTraining Loss: 0.000203\n",
      "Epoch: 12 \tTraining Loss: 0.000203\n",
      "Epoch: 13 \tTraining Loss: 0.000203\n",
      "Epoch: 14 \tTraining Loss: 0.000203\n",
      "Epoch: 15 \tTraining Loss: 0.000203\n",
      "Epoch: 16 \tTraining Loss: 0.000203\n",
      "Epoch: 17 \tTraining Loss: 0.000203\n",
      "Epoch: 18 \tTraining Loss: 0.000203\n",
      "Epoch: 1 \tTraining Loss: 0.000173\n",
      "Epoch: 2 \tTraining Loss: 0.000173\n",
      "Epoch: 3 \tTraining Loss: 0.000173\n",
      "Epoch: 4 \tTraining Loss: 0.000173\n",
      "Epoch: 5 \tTraining Loss: 0.000173\n",
      "Epoch: 6 \tTraining Loss: 0.000173\n",
      "Epoch: 7 \tTraining Loss: 0.000173\n",
      "Epoch: 8 \tTraining Loss: 0.000173\n",
      "Epoch: 9 \tTraining Loss: 0.000173\n",
      "Epoch: 10 \tTraining Loss: 0.000173\n",
      "Epoch: 11 \tTraining Loss: 0.000173\n",
      "Epoch: 12 \tTraining Loss: 0.000173\n",
      "Epoch: 13 \tTraining Loss: 0.000173\n",
      "Epoch: 14 \tTraining Loss: 0.000173\n",
      "Epoch: 15 \tTraining Loss: 0.000173\n",
      "Epoch: 16 \tTraining Loss: 0.000173\n",
      "Epoch: 17 \tTraining Loss: 0.000173\n",
      "Epoch: 18 \tTraining Loss: 0.000173\n",
      "Epoch: 1 \tTraining Loss: 0.000373\n",
      "Epoch: 2 \tTraining Loss: 0.000373\n",
      "Epoch: 3 \tTraining Loss: 0.000373\n",
      "Epoch: 4 \tTraining Loss: 0.000373\n",
      "Epoch: 5 \tTraining Loss: 0.000373\n",
      "Epoch: 6 \tTraining Loss: 0.000373\n",
      "Epoch: 7 \tTraining Loss: 0.000373\n",
      "Epoch: 8 \tTraining Loss: 0.000373\n",
      "Epoch: 9 \tTraining Loss: 0.000373\n",
      "Epoch: 10 \tTraining Loss: 0.000373\n",
      "Epoch: 11 \tTraining Loss: 0.000373\n",
      "Epoch: 12 \tTraining Loss: 0.000373\n",
      "Epoch: 13 \tTraining Loss: 0.000373\n",
      "Epoch: 14 \tTraining Loss: 0.000373\n",
      "Epoch: 15 \tTraining Loss: 0.000373\n",
      "Epoch: 16 \tTraining Loss: 0.000373\n",
      "Epoch: 17 \tTraining Loss: 0.000373\n",
      "Epoch: 18 \tTraining Loss: 0.000373\n",
      "Epoch: 1 \tTraining Loss: 0.000270\n",
      "Epoch: 2 \tTraining Loss: 0.000270\n",
      "Epoch: 3 \tTraining Loss: 0.000270\n",
      "Epoch: 4 \tTraining Loss: 0.000270\n",
      "Epoch: 5 \tTraining Loss: 0.000270\n",
      "Epoch: 6 \tTraining Loss: 0.000270\n",
      "Epoch: 7 \tTraining Loss: 0.000270\n",
      "Epoch: 8 \tTraining Loss: 0.000270\n",
      "Epoch: 9 \tTraining Loss: 0.000270\n",
      "Epoch: 10 \tTraining Loss: 0.000270\n",
      "Epoch: 11 \tTraining Loss: 0.000270\n",
      "Epoch: 12 \tTraining Loss: 0.000270\n",
      "Epoch: 13 \tTraining Loss: 0.000270\n",
      "Epoch: 14 \tTraining Loss: 0.000270\n",
      "Epoch: 15 \tTraining Loss: 0.000270\n",
      "Epoch: 16 \tTraining Loss: 0.000270\n",
      "Epoch: 17 \tTraining Loss: 0.000270\n",
      "Epoch: 18 \tTraining Loss: 0.000270\n",
      "Epoch: 1 \tTraining Loss: 0.000181\n",
      "Epoch: 2 \tTraining Loss: 0.000181\n",
      "Epoch: 3 \tTraining Loss: 0.000181\n",
      "Epoch: 4 \tTraining Loss: 0.000181\n",
      "Epoch: 5 \tTraining Loss: 0.000181\n",
      "Epoch: 6 \tTraining Loss: 0.000181\n",
      "Epoch: 7 \tTraining Loss: 0.000181\n",
      "Epoch: 8 \tTraining Loss: 0.000181\n",
      "Epoch: 9 \tTraining Loss: 0.000181\n",
      "Epoch: 10 \tTraining Loss: 0.000181\n",
      "Epoch: 11 \tTraining Loss: 0.000181\n",
      "Epoch: 12 \tTraining Loss: 0.000181\n",
      "Epoch: 13 \tTraining Loss: 0.000181\n",
      "Epoch: 14 \tTraining Loss: 0.000181\n",
      "Epoch: 15 \tTraining Loss: 0.000181\n",
      "Epoch: 16 \tTraining Loss: 0.000181\n",
      "Epoch: 17 \tTraining Loss: 0.000181\n",
      "Epoch: 18 \tTraining Loss: 0.000181\n",
      "Epoch: 1 \tTraining Loss: 0.000229\n",
      "Epoch: 2 \tTraining Loss: 0.000229\n",
      "Epoch: 3 \tTraining Loss: 0.000229\n",
      "Epoch: 4 \tTraining Loss: 0.000229\n",
      "Epoch: 5 \tTraining Loss: 0.000229\n",
      "Epoch: 6 \tTraining Loss: 0.000229\n",
      "Epoch: 7 \tTraining Loss: 0.000229\n",
      "Epoch: 8 \tTraining Loss: 0.000229\n",
      "Epoch: 9 \tTraining Loss: 0.000229\n",
      "Epoch: 10 \tTraining Loss: 0.000229\n",
      "Epoch: 11 \tTraining Loss: 0.000229\n",
      "Epoch: 12 \tTraining Loss: 0.000229\n",
      "Epoch: 13 \tTraining Loss: 0.000229\n",
      "Epoch: 14 \tTraining Loss: 0.000229\n",
      "Epoch: 15 \tTraining Loss: 0.000229\n",
      "Epoch: 16 \tTraining Loss: 0.000229\n",
      "Epoch: 17 \tTraining Loss: 0.000229\n",
      "Epoch: 18 \tTraining Loss: 0.000229\n",
      "Epoch: 1 \tTraining Loss: 0.000256\n",
      "Epoch: 2 \tTraining Loss: 0.000256\n",
      "Epoch: 3 \tTraining Loss: 0.000256\n",
      "Epoch: 4 \tTraining Loss: 0.000256\n",
      "Epoch: 5 \tTraining Loss: 0.000256\n",
      "Epoch: 6 \tTraining Loss: 0.000256\n",
      "Epoch: 7 \tTraining Loss: 0.000256\n",
      "Epoch: 8 \tTraining Loss: 0.000256\n",
      "Epoch: 9 \tTraining Loss: 0.000256\n",
      "Epoch: 10 \tTraining Loss: 0.000256\n",
      "Epoch: 11 \tTraining Loss: 0.000256\n",
      "Epoch: 12 \tTraining Loss: 0.000256\n",
      "Epoch: 13 \tTraining Loss: 0.000256\n",
      "Epoch: 14 \tTraining Loss: 0.000256\n",
      "Epoch: 15 \tTraining Loss: 0.000256\n",
      "Epoch: 16 \tTraining Loss: 0.000256\n",
      "Epoch: 17 \tTraining Loss: 0.000256\n",
      "Epoch: 18 \tTraining Loss: 0.000256\n",
      "Epoch: 1 \tTraining Loss: 0.000167\n",
      "Epoch: 2 \tTraining Loss: 0.000167\n",
      "Epoch: 3 \tTraining Loss: 0.000167\n",
      "Epoch: 4 \tTraining Loss: 0.000167\n",
      "Epoch: 5 \tTraining Loss: 0.000167\n",
      "Epoch: 6 \tTraining Loss: 0.000167\n",
      "Epoch: 7 \tTraining Loss: 0.000167\n",
      "Epoch: 8 \tTraining Loss: 0.000167\n",
      "Epoch: 9 \tTraining Loss: 0.000167\n",
      "Epoch: 10 \tTraining Loss: 0.000167\n",
      "Epoch: 11 \tTraining Loss: 0.000167\n",
      "Epoch: 12 \tTraining Loss: 0.000167\n",
      "Epoch: 13 \tTraining Loss: 0.000167\n",
      "Epoch: 14 \tTraining Loss: 0.000167\n",
      "Epoch: 15 \tTraining Loss: 0.000167\n",
      "Epoch: 16 \tTraining Loss: 0.000167\n",
      "Epoch: 17 \tTraining Loss: 0.000167\n",
      "Epoch: 18 \tTraining Loss: 0.000167\n",
      "Epoch: 1 \tTraining Loss: 0.000230\n",
      "Epoch: 2 \tTraining Loss: 0.000230\n",
      "Epoch: 3 \tTraining Loss: 0.000230\n",
      "Epoch: 4 \tTraining Loss: 0.000230\n",
      "Epoch: 5 \tTraining Loss: 0.000230\n",
      "Epoch: 6 \tTraining Loss: 0.000230\n",
      "Epoch: 7 \tTraining Loss: 0.000230\n",
      "Epoch: 8 \tTraining Loss: 0.000230\n",
      "Epoch: 9 \tTraining Loss: 0.000230\n",
      "Epoch: 10 \tTraining Loss: 0.000230\n",
      "Epoch: 11 \tTraining Loss: 0.000230\n",
      "Epoch: 12 \tTraining Loss: 0.000230\n",
      "Epoch: 13 \tTraining Loss: 0.000230\n",
      "Epoch: 14 \tTraining Loss: 0.000230\n",
      "Epoch: 15 \tTraining Loss: 0.000230\n",
      "Epoch: 16 \tTraining Loss: 0.000230\n",
      "Epoch: 17 \tTraining Loss: 0.000230\n",
      "Epoch: 18 \tTraining Loss: 0.000230\n",
      "Epoch: 1 \tTraining Loss: 0.000267\n",
      "Epoch: 2 \tTraining Loss: 0.000267\n",
      "Epoch: 3 \tTraining Loss: 0.000267\n",
      "Epoch: 4 \tTraining Loss: 0.000267\n",
      "Epoch: 5 \tTraining Loss: 0.000267\n",
      "Epoch: 6 \tTraining Loss: 0.000267\n",
      "Epoch: 7 \tTraining Loss: 0.000267\n",
      "Epoch: 8 \tTraining Loss: 0.000267\n",
      "Epoch: 9 \tTraining Loss: 0.000267\n",
      "Epoch: 10 \tTraining Loss: 0.000267\n",
      "Epoch: 11 \tTraining Loss: 0.000267\n",
      "Epoch: 12 \tTraining Loss: 0.000267\n",
      "Epoch: 13 \tTraining Loss: 0.000267\n",
      "Epoch: 14 \tTraining Loss: 0.000267\n",
      "Epoch: 15 \tTraining Loss: 0.000267\n",
      "Epoch: 16 \tTraining Loss: 0.000267\n",
      "Epoch: 17 \tTraining Loss: 0.000267\n",
      "Epoch: 18 \tTraining Loss: 0.000267\n",
      "Epoch: 1 \tTraining Loss: 0.000247\n",
      "Epoch: 2 \tTraining Loss: 0.000247\n",
      "Epoch: 3 \tTraining Loss: 0.000247\n",
      "Epoch: 4 \tTraining Loss: 0.000247\n",
      "Epoch: 5 \tTraining Loss: 0.000247\n",
      "Epoch: 6 \tTraining Loss: 0.000247\n",
      "Epoch: 7 \tTraining Loss: 0.000247\n",
      "Epoch: 8 \tTraining Loss: 0.000247\n",
      "Epoch: 9 \tTraining Loss: 0.000247\n",
      "Epoch: 10 \tTraining Loss: 0.000247\n",
      "Epoch: 11 \tTraining Loss: 0.000247\n",
      "Epoch: 12 \tTraining Loss: 0.000247\n",
      "Epoch: 13 \tTraining Loss: 0.000247\n",
      "Epoch: 14 \tTraining Loss: 0.000247\n",
      "Epoch: 15 \tTraining Loss: 0.000247\n",
      "Epoch: 16 \tTraining Loss: 0.000247\n",
      "Epoch: 17 \tTraining Loss: 0.000247\n",
      "Epoch: 18 \tTraining Loss: 0.000247\n",
      "Epoch: 1 \tTraining Loss: 0.000504\n",
      "Epoch: 2 \tTraining Loss: 0.000504\n",
      "Epoch: 3 \tTraining Loss: 0.000504\n",
      "Epoch: 4 \tTraining Loss: 0.000504\n",
      "Epoch: 5 \tTraining Loss: 0.000504\n",
      "Epoch: 6 \tTraining Loss: 0.000504\n",
      "Epoch: 7 \tTraining Loss: 0.000504\n",
      "Epoch: 8 \tTraining Loss: 0.000504\n",
      "Epoch: 9 \tTraining Loss: 0.000504\n",
      "Epoch: 10 \tTraining Loss: 0.000504\n",
      "Epoch: 11 \tTraining Loss: 0.000504\n",
      "Epoch: 12 \tTraining Loss: 0.000504\n",
      "Epoch: 13 \tTraining Loss: 0.000504\n",
      "Epoch: 14 \tTraining Loss: 0.000504\n",
      "Epoch: 15 \tTraining Loss: 0.000504\n",
      "Epoch: 16 \tTraining Loss: 0.000504\n",
      "Epoch: 17 \tTraining Loss: 0.000504\n",
      "Epoch: 18 \tTraining Loss: 0.000504\n",
      "Epoch: 1 \tTraining Loss: 0.000216\n",
      "Epoch: 2 \tTraining Loss: 0.000216\n",
      "Epoch: 3 \tTraining Loss: 0.000216\n",
      "Epoch: 4 \tTraining Loss: 0.000216\n",
      "Epoch: 5 \tTraining Loss: 0.000216\n",
      "Epoch: 6 \tTraining Loss: 0.000216\n",
      "Epoch: 7 \tTraining Loss: 0.000216\n",
      "Epoch: 8 \tTraining Loss: 0.000216\n",
      "Epoch: 9 \tTraining Loss: 0.000216\n",
      "Epoch: 10 \tTraining Loss: 0.000216\n",
      "Epoch: 11 \tTraining Loss: 0.000216\n",
      "Epoch: 12 \tTraining Loss: 0.000216\n",
      "Epoch: 13 \tTraining Loss: 0.000216\n",
      "Epoch: 14 \tTraining Loss: 0.000216\n",
      "Epoch: 15 \tTraining Loss: 0.000216\n",
      "Epoch: 16 \tTraining Loss: 0.000216\n",
      "Epoch: 17 \tTraining Loss: 0.000216\n",
      "Epoch: 18 \tTraining Loss: 0.000216\n",
      "Epoch: 1 \tTraining Loss: 0.000138\n",
      "Epoch: 2 \tTraining Loss: 0.000138\n",
      "Epoch: 3 \tTraining Loss: 0.000138\n",
      "Epoch: 4 \tTraining Loss: 0.000138\n",
      "Epoch: 5 \tTraining Loss: 0.000138\n",
      "Epoch: 6 \tTraining Loss: 0.000138\n",
      "Epoch: 7 \tTraining Loss: 0.000138\n",
      "Epoch: 8 \tTraining Loss: 0.000138\n",
      "Epoch: 9 \tTraining Loss: 0.000138\n",
      "Epoch: 10 \tTraining Loss: 0.000138\n",
      "Epoch: 11 \tTraining Loss: 0.000138\n",
      "Epoch: 12 \tTraining Loss: 0.000138\n",
      "Epoch: 13 \tTraining Loss: 0.000138\n",
      "Epoch: 14 \tTraining Loss: 0.000138\n",
      "Epoch: 15 \tTraining Loss: 0.000138\n",
      "Epoch: 16 \tTraining Loss: 0.000138\n",
      "Epoch: 17 \tTraining Loss: 0.000138\n",
      "Epoch: 18 \tTraining Loss: 0.000138\n",
      "Epoch: 1 \tTraining Loss: 0.000151\n",
      "Epoch: 2 \tTraining Loss: 0.000151\n",
      "Epoch: 3 \tTraining Loss: 0.000151\n",
      "Epoch: 4 \tTraining Loss: 0.000151\n",
      "Epoch: 5 \tTraining Loss: 0.000151\n",
      "Epoch: 6 \tTraining Loss: 0.000151\n",
      "Epoch: 7 \tTraining Loss: 0.000151\n",
      "Epoch: 8 \tTraining Loss: 0.000151\n",
      "Epoch: 9 \tTraining Loss: 0.000151\n",
      "Epoch: 10 \tTraining Loss: 0.000151\n",
      "Epoch: 11 \tTraining Loss: 0.000151\n",
      "Epoch: 12 \tTraining Loss: 0.000151\n",
      "Epoch: 13 \tTraining Loss: 0.000151\n",
      "Epoch: 14 \tTraining Loss: 0.000151\n",
      "Epoch: 15 \tTraining Loss: 0.000151\n",
      "Epoch: 16 \tTraining Loss: 0.000151\n",
      "Epoch: 17 \tTraining Loss: 0.000151\n",
      "Epoch: 18 \tTraining Loss: 0.000151\n",
      "Epoch: 1 \tTraining Loss: 0.000216\n",
      "Epoch: 2 \tTraining Loss: 0.000216\n",
      "Epoch: 3 \tTraining Loss: 0.000216\n",
      "Epoch: 4 \tTraining Loss: 0.000216\n",
      "Epoch: 5 \tTraining Loss: 0.000216\n",
      "Epoch: 6 \tTraining Loss: 0.000216\n",
      "Epoch: 7 \tTraining Loss: 0.000216\n",
      "Epoch: 8 \tTraining Loss: 0.000216\n",
      "Epoch: 9 \tTraining Loss: 0.000216\n",
      "Epoch: 10 \tTraining Loss: 0.000216\n",
      "Epoch: 11 \tTraining Loss: 0.000216\n",
      "Epoch: 12 \tTraining Loss: 0.000216\n",
      "Epoch: 13 \tTraining Loss: 0.000216\n",
      "Epoch: 14 \tTraining Loss: 0.000216\n",
      "Epoch: 15 \tTraining Loss: 0.000216\n",
      "Epoch: 16 \tTraining Loss: 0.000216\n",
      "Epoch: 17 \tTraining Loss: 0.000216\n",
      "Epoch: 18 \tTraining Loss: 0.000216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000222\n",
      "Epoch: 2 \tTraining Loss: 0.000222\n",
      "Epoch: 3 \tTraining Loss: 0.000222\n",
      "Epoch: 4 \tTraining Loss: 0.000222\n",
      "Epoch: 5 \tTraining Loss: 0.000222\n",
      "Epoch: 6 \tTraining Loss: 0.000222\n",
      "Epoch: 7 \tTraining Loss: 0.000222\n",
      "Epoch: 8 \tTraining Loss: 0.000222\n",
      "Epoch: 9 \tTraining Loss: 0.000222\n",
      "Epoch: 10 \tTraining Loss: 0.000222\n",
      "Epoch: 11 \tTraining Loss: 0.000222\n",
      "Epoch: 12 \tTraining Loss: 0.000222\n",
      "Epoch: 13 \tTraining Loss: 0.000222\n",
      "Epoch: 14 \tTraining Loss: 0.000222\n",
      "Epoch: 15 \tTraining Loss: 0.000222\n",
      "Epoch: 16 \tTraining Loss: 0.000222\n",
      "Epoch: 17 \tTraining Loss: 0.000222\n",
      "Epoch: 18 \tTraining Loss: 0.000222\n",
      "Epoch: 1 \tTraining Loss: 0.000206\n",
      "Epoch: 2 \tTraining Loss: 0.000206\n",
      "Epoch: 3 \tTraining Loss: 0.000206\n",
      "Epoch: 4 \tTraining Loss: 0.000206\n",
      "Epoch: 5 \tTraining Loss: 0.000206\n",
      "Epoch: 6 \tTraining Loss: 0.000206\n",
      "Epoch: 7 \tTraining Loss: 0.000206\n",
      "Epoch: 8 \tTraining Loss: 0.000206\n",
      "Epoch: 9 \tTraining Loss: 0.000206\n",
      "Epoch: 10 \tTraining Loss: 0.000206\n",
      "Epoch: 11 \tTraining Loss: 0.000206\n",
      "Epoch: 12 \tTraining Loss: 0.000206\n",
      "Epoch: 13 \tTraining Loss: 0.000206\n",
      "Epoch: 14 \tTraining Loss: 0.000206\n",
      "Epoch: 15 \tTraining Loss: 0.000206\n",
      "Epoch: 16 \tTraining Loss: 0.000206\n",
      "Epoch: 17 \tTraining Loss: 0.000206\n",
      "Epoch: 18 \tTraining Loss: 0.000206\n",
      "Epoch: 1 \tTraining Loss: 0.000280\n",
      "Epoch: 2 \tTraining Loss: 0.000280\n",
      "Epoch: 3 \tTraining Loss: 0.000280\n",
      "Epoch: 4 \tTraining Loss: 0.000280\n",
      "Epoch: 5 \tTraining Loss: 0.000280\n",
      "Epoch: 6 \tTraining Loss: 0.000280\n",
      "Epoch: 7 \tTraining Loss: 0.000280\n",
      "Epoch: 8 \tTraining Loss: 0.000280\n",
      "Epoch: 9 \tTraining Loss: 0.000280\n",
      "Epoch: 10 \tTraining Loss: 0.000280\n",
      "Epoch: 11 \tTraining Loss: 0.000280\n",
      "Epoch: 12 \tTraining Loss: 0.000280\n",
      "Epoch: 13 \tTraining Loss: 0.000280\n",
      "Epoch: 14 \tTraining Loss: 0.000280\n",
      "Epoch: 15 \tTraining Loss: 0.000280\n",
      "Epoch: 16 \tTraining Loss: 0.000280\n",
      "Epoch: 17 \tTraining Loss: 0.000280\n",
      "Epoch: 18 \tTraining Loss: 0.000280\n",
      "Epoch: 1 \tTraining Loss: 0.000142\n",
      "Epoch: 2 \tTraining Loss: 0.000142\n",
      "Epoch: 3 \tTraining Loss: 0.000142\n",
      "Epoch: 4 \tTraining Loss: 0.000142\n",
      "Epoch: 5 \tTraining Loss: 0.000142\n",
      "Epoch: 6 \tTraining Loss: 0.000142\n",
      "Epoch: 7 \tTraining Loss: 0.000142\n",
      "Epoch: 8 \tTraining Loss: 0.000142\n",
      "Epoch: 9 \tTraining Loss: 0.000142\n",
      "Epoch: 10 \tTraining Loss: 0.000142\n",
      "Epoch: 11 \tTraining Loss: 0.000142\n",
      "Epoch: 12 \tTraining Loss: 0.000142\n",
      "Epoch: 13 \tTraining Loss: 0.000142\n",
      "Epoch: 14 \tTraining Loss: 0.000142\n",
      "Epoch: 15 \tTraining Loss: 0.000142\n",
      "Epoch: 16 \tTraining Loss: 0.000142\n",
      "Epoch: 17 \tTraining Loss: 0.000142\n",
      "Epoch: 18 \tTraining Loss: 0.000142\n",
      "Epoch: 1 \tTraining Loss: 0.000392\n",
      "Epoch: 2 \tTraining Loss: 0.000392\n",
      "Epoch: 3 \tTraining Loss: 0.000392\n",
      "Epoch: 4 \tTraining Loss: 0.000392\n",
      "Epoch: 5 \tTraining Loss: 0.000392\n",
      "Epoch: 6 \tTraining Loss: 0.000392\n",
      "Epoch: 7 \tTraining Loss: 0.000392\n",
      "Epoch: 8 \tTraining Loss: 0.000392\n",
      "Epoch: 9 \tTraining Loss: 0.000392\n",
      "Epoch: 10 \tTraining Loss: 0.000392\n",
      "Epoch: 11 \tTraining Loss: 0.000392\n",
      "Epoch: 12 \tTraining Loss: 0.000392\n",
      "Epoch: 13 \tTraining Loss: 0.000392\n",
      "Epoch: 14 \tTraining Loss: 0.000392\n",
      "Epoch: 15 \tTraining Loss: 0.000392\n",
      "Epoch: 16 \tTraining Loss: 0.000392\n",
      "Epoch: 17 \tTraining Loss: 0.000392\n",
      "Epoch: 18 \tTraining Loss: 0.000392\n",
      "Epoch: 1 \tTraining Loss: 0.000169\n",
      "Epoch: 2 \tTraining Loss: 0.000169\n",
      "Epoch: 3 \tTraining Loss: 0.000169\n",
      "Epoch: 4 \tTraining Loss: 0.000169\n",
      "Epoch: 5 \tTraining Loss: 0.000169\n",
      "Epoch: 6 \tTraining Loss: 0.000169\n",
      "Epoch: 7 \tTraining Loss: 0.000169\n",
      "Epoch: 8 \tTraining Loss: 0.000169\n",
      "Epoch: 9 \tTraining Loss: 0.000169\n",
      "Epoch: 10 \tTraining Loss: 0.000169\n",
      "Epoch: 11 \tTraining Loss: 0.000169\n",
      "Epoch: 12 \tTraining Loss: 0.000169\n",
      "Epoch: 13 \tTraining Loss: 0.000169\n",
      "Epoch: 14 \tTraining Loss: 0.000169\n",
      "Epoch: 15 \tTraining Loss: 0.000169\n",
      "Epoch: 16 \tTraining Loss: 0.000169\n",
      "Epoch: 17 \tTraining Loss: 0.000169\n",
      "Epoch: 18 \tTraining Loss: 0.000169\n",
      "Epoch: 1 \tTraining Loss: 0.000382\n",
      "Epoch: 2 \tTraining Loss: 0.000382\n",
      "Epoch: 3 \tTraining Loss: 0.000382\n",
      "Epoch: 4 \tTraining Loss: 0.000382\n",
      "Epoch: 5 \tTraining Loss: 0.000382\n",
      "Epoch: 6 \tTraining Loss: 0.000382\n",
      "Epoch: 7 \tTraining Loss: 0.000382\n",
      "Epoch: 8 \tTraining Loss: 0.000382\n",
      "Epoch: 9 \tTraining Loss: 0.000382\n",
      "Epoch: 10 \tTraining Loss: 0.000382\n",
      "Epoch: 11 \tTraining Loss: 0.000382\n",
      "Epoch: 12 \tTraining Loss: 0.000382\n",
      "Epoch: 13 \tTraining Loss: 0.000382\n",
      "Epoch: 14 \tTraining Loss: 0.000382\n",
      "Epoch: 15 \tTraining Loss: 0.000382\n",
      "Epoch: 16 \tTraining Loss: 0.000382\n",
      "Epoch: 17 \tTraining Loss: 0.000382\n",
      "Epoch: 18 \tTraining Loss: 0.000382\n",
      "Epoch: 1 \tTraining Loss: 0.000199\n",
      "Epoch: 2 \tTraining Loss: 0.000199\n",
      "Epoch: 3 \tTraining Loss: 0.000199\n",
      "Epoch: 4 \tTraining Loss: 0.000199\n",
      "Epoch: 5 \tTraining Loss: 0.000199\n",
      "Epoch: 6 \tTraining Loss: 0.000199\n",
      "Epoch: 7 \tTraining Loss: 0.000199\n",
      "Epoch: 8 \tTraining Loss: 0.000199\n",
      "Epoch: 9 \tTraining Loss: 0.000199\n",
      "Epoch: 10 \tTraining Loss: 0.000199\n",
      "Epoch: 11 \tTraining Loss: 0.000199\n",
      "Epoch: 12 \tTraining Loss: 0.000199\n",
      "Epoch: 13 \tTraining Loss: 0.000199\n",
      "Epoch: 14 \tTraining Loss: 0.000199\n",
      "Epoch: 15 \tTraining Loss: 0.000199\n",
      "Epoch: 16 \tTraining Loss: 0.000199\n",
      "Epoch: 17 \tTraining Loss: 0.000199\n",
      "Epoch: 18 \tTraining Loss: 0.000199\n",
      "Epoch: 1 \tTraining Loss: 0.000184\n",
      "Epoch: 2 \tTraining Loss: 0.000184\n",
      "Epoch: 3 \tTraining Loss: 0.000184\n",
      "Epoch: 4 \tTraining Loss: 0.000184\n",
      "Epoch: 5 \tTraining Loss: 0.000184\n",
      "Epoch: 6 \tTraining Loss: 0.000184\n",
      "Epoch: 7 \tTraining Loss: 0.000184\n",
      "Epoch: 8 \tTraining Loss: 0.000184\n",
      "Epoch: 9 \tTraining Loss: 0.000184\n",
      "Epoch: 10 \tTraining Loss: 0.000184\n",
      "Epoch: 11 \tTraining Loss: 0.000184\n",
      "Epoch: 12 \tTraining Loss: 0.000184\n",
      "Epoch: 13 \tTraining Loss: 0.000184\n",
      "Epoch: 14 \tTraining Loss: 0.000184\n",
      "Epoch: 15 \tTraining Loss: 0.000184\n",
      "Epoch: 16 \tTraining Loss: 0.000184\n",
      "Epoch: 17 \tTraining Loss: 0.000184\n",
      "Epoch: 18 \tTraining Loss: 0.000184\n",
      "Epoch: 1 \tTraining Loss: 0.000189\n",
      "Epoch: 2 \tTraining Loss: 0.000189\n",
      "Epoch: 3 \tTraining Loss: 0.000189\n",
      "Epoch: 4 \tTraining Loss: 0.000189\n",
      "Epoch: 5 \tTraining Loss: 0.000189\n",
      "Epoch: 6 \tTraining Loss: 0.000189\n",
      "Epoch: 7 \tTraining Loss: 0.000189\n",
      "Epoch: 8 \tTraining Loss: 0.000189\n",
      "Epoch: 9 \tTraining Loss: 0.000189\n",
      "Epoch: 10 \tTraining Loss: 0.000189\n",
      "Epoch: 11 \tTraining Loss: 0.000189\n",
      "Epoch: 12 \tTraining Loss: 0.000189\n",
      "Epoch: 13 \tTraining Loss: 0.000189\n",
      "Epoch: 14 \tTraining Loss: 0.000189\n",
      "Epoch: 15 \tTraining Loss: 0.000189\n",
      "Epoch: 16 \tTraining Loss: 0.000189\n",
      "Epoch: 17 \tTraining Loss: 0.000189\n",
      "Epoch: 18 \tTraining Loss: 0.000189\n",
      "Epoch: 1 \tTraining Loss: 0.000265\n",
      "Epoch: 2 \tTraining Loss: 0.000265\n",
      "Epoch: 3 \tTraining Loss: 0.000265\n",
      "Epoch: 4 \tTraining Loss: 0.000265\n",
      "Epoch: 5 \tTraining Loss: 0.000265\n",
      "Epoch: 6 \tTraining Loss: 0.000265\n",
      "Epoch: 7 \tTraining Loss: 0.000265\n",
      "Epoch: 8 \tTraining Loss: 0.000265\n",
      "Epoch: 9 \tTraining Loss: 0.000265\n",
      "Epoch: 10 \tTraining Loss: 0.000265\n",
      "Epoch: 11 \tTraining Loss: 0.000265\n",
      "Epoch: 12 \tTraining Loss: 0.000265\n",
      "Epoch: 13 \tTraining Loss: 0.000265\n",
      "Epoch: 14 \tTraining Loss: 0.000265\n",
      "Epoch: 15 \tTraining Loss: 0.000265\n",
      "Epoch: 16 \tTraining Loss: 0.000265\n",
      "Epoch: 17 \tTraining Loss: 0.000265\n",
      "Epoch: 18 \tTraining Loss: 0.000265\n",
      "Epoch: 1 \tTraining Loss: 0.000224\n",
      "Epoch: 2 \tTraining Loss: 0.000224\n",
      "Epoch: 3 \tTraining Loss: 0.000224\n",
      "Epoch: 4 \tTraining Loss: 0.000224\n",
      "Epoch: 5 \tTraining Loss: 0.000224\n",
      "Epoch: 6 \tTraining Loss: 0.000224\n",
      "Epoch: 7 \tTraining Loss: 0.000224\n",
      "Epoch: 8 \tTraining Loss: 0.000224\n",
      "Epoch: 9 \tTraining Loss: 0.000224\n",
      "Epoch: 10 \tTraining Loss: 0.000224\n",
      "Epoch: 11 \tTraining Loss: 0.000224\n",
      "Epoch: 12 \tTraining Loss: 0.000224\n",
      "Epoch: 13 \tTraining Loss: 0.000224\n",
      "Epoch: 14 \tTraining Loss: 0.000224\n",
      "Epoch: 15 \tTraining Loss: 0.000224\n",
      "Epoch: 16 \tTraining Loss: 0.000224\n",
      "Epoch: 17 \tTraining Loss: 0.000224\n",
      "Epoch: 18 \tTraining Loss: 0.000224\n",
      "Epoch: 1 \tTraining Loss: 0.000166\n",
      "Epoch: 2 \tTraining Loss: 0.000166\n",
      "Epoch: 3 \tTraining Loss: 0.000166\n",
      "Epoch: 4 \tTraining Loss: 0.000166\n",
      "Epoch: 5 \tTraining Loss: 0.000166\n",
      "Epoch: 6 \tTraining Loss: 0.000166\n",
      "Epoch: 7 \tTraining Loss: 0.000166\n",
      "Epoch: 8 \tTraining Loss: 0.000166\n",
      "Epoch: 9 \tTraining Loss: 0.000166\n",
      "Epoch: 10 \tTraining Loss: 0.000166\n",
      "Epoch: 11 \tTraining Loss: 0.000166\n",
      "Epoch: 12 \tTraining Loss: 0.000166\n",
      "Epoch: 13 \tTraining Loss: 0.000166\n",
      "Epoch: 14 \tTraining Loss: 0.000166\n",
      "Epoch: 15 \tTraining Loss: 0.000166\n",
      "Epoch: 16 \tTraining Loss: 0.000166\n",
      "Epoch: 17 \tTraining Loss: 0.000166\n",
      "Epoch: 18 \tTraining Loss: 0.000166\n",
      "Epoch: 1 \tTraining Loss: 0.000169\n",
      "Epoch: 2 \tTraining Loss: 0.000169\n",
      "Epoch: 3 \tTraining Loss: 0.000169\n",
      "Epoch: 4 \tTraining Loss: 0.000169\n",
      "Epoch: 5 \tTraining Loss: 0.000169\n",
      "Epoch: 6 \tTraining Loss: 0.000169\n",
      "Epoch: 7 \tTraining Loss: 0.000169\n",
      "Epoch: 8 \tTraining Loss: 0.000169\n",
      "Epoch: 9 \tTraining Loss: 0.000169\n",
      "Epoch: 10 \tTraining Loss: 0.000169\n",
      "Epoch: 11 \tTraining Loss: 0.000169\n",
      "Epoch: 12 \tTraining Loss: 0.000169\n",
      "Epoch: 13 \tTraining Loss: 0.000169\n",
      "Epoch: 14 \tTraining Loss: 0.000169\n",
      "Epoch: 15 \tTraining Loss: 0.000169\n",
      "Epoch: 16 \tTraining Loss: 0.000169\n",
      "Epoch: 17 \tTraining Loss: 0.000169\n",
      "Epoch: 18 \tTraining Loss: 0.000169\n",
      "Epoch: 1 \tTraining Loss: 0.000288\n",
      "Epoch: 2 \tTraining Loss: 0.000288\n",
      "Epoch: 3 \tTraining Loss: 0.000288\n",
      "Epoch: 4 \tTraining Loss: 0.000288\n",
      "Epoch: 5 \tTraining Loss: 0.000288\n",
      "Epoch: 6 \tTraining Loss: 0.000288\n",
      "Epoch: 7 \tTraining Loss: 0.000288\n",
      "Epoch: 8 \tTraining Loss: 0.000288\n",
      "Epoch: 9 \tTraining Loss: 0.000288\n",
      "Epoch: 10 \tTraining Loss: 0.000288\n",
      "Epoch: 11 \tTraining Loss: 0.000288\n",
      "Epoch: 12 \tTraining Loss: 0.000288\n",
      "Epoch: 13 \tTraining Loss: 0.000288\n",
      "Epoch: 14 \tTraining Loss: 0.000288\n",
      "Epoch: 15 \tTraining Loss: 0.000288\n",
      "Epoch: 16 \tTraining Loss: 0.000288\n",
      "Epoch: 17 \tTraining Loss: 0.000288\n",
      "Epoch: 18 \tTraining Loss: 0.000288\n",
      "Epoch: 1 \tTraining Loss: 0.000091\n",
      "Epoch: 2 \tTraining Loss: 0.000091\n",
      "Epoch: 3 \tTraining Loss: 0.000091\n",
      "Epoch: 4 \tTraining Loss: 0.000091\n",
      "Epoch: 5 \tTraining Loss: 0.000091\n",
      "Epoch: 6 \tTraining Loss: 0.000091\n",
      "Epoch: 7 \tTraining Loss: 0.000091\n",
      "Epoch: 8 \tTraining Loss: 0.000091\n",
      "Epoch: 9 \tTraining Loss: 0.000091\n",
      "Epoch: 10 \tTraining Loss: 0.000091\n",
      "Epoch: 11 \tTraining Loss: 0.000091\n",
      "Epoch: 12 \tTraining Loss: 0.000091\n",
      "Epoch: 13 \tTraining Loss: 0.000091\n",
      "Epoch: 14 \tTraining Loss: 0.000091\n",
      "Epoch: 15 \tTraining Loss: 0.000091\n",
      "Epoch: 16 \tTraining Loss: 0.000091\n",
      "Epoch: 17 \tTraining Loss: 0.000091\n",
      "Epoch: 18 \tTraining Loss: 0.000091\n",
      "Epoch: 1 \tTraining Loss: 0.000305\n",
      "Epoch: 2 \tTraining Loss: 0.000305\n",
      "Epoch: 3 \tTraining Loss: 0.000305\n",
      "Epoch: 4 \tTraining Loss: 0.000305\n",
      "Epoch: 5 \tTraining Loss: 0.000305\n",
      "Epoch: 6 \tTraining Loss: 0.000305\n",
      "Epoch: 7 \tTraining Loss: 0.000305\n",
      "Epoch: 8 \tTraining Loss: 0.000305\n",
      "Epoch: 9 \tTraining Loss: 0.000305\n",
      "Epoch: 10 \tTraining Loss: 0.000305\n",
      "Epoch: 11 \tTraining Loss: 0.000305\n",
      "Epoch: 12 \tTraining Loss: 0.000305\n",
      "Epoch: 13 \tTraining Loss: 0.000305\n",
      "Epoch: 14 \tTraining Loss: 0.000305\n",
      "Epoch: 15 \tTraining Loss: 0.000305\n",
      "Epoch: 16 \tTraining Loss: 0.000305\n",
      "Epoch: 17 \tTraining Loss: 0.000305\n",
      "Epoch: 18 \tTraining Loss: 0.000305\n",
      "Epoch: 1 \tTraining Loss: 0.000269\n",
      "Epoch: 2 \tTraining Loss: 0.000269\n",
      "Epoch: 3 \tTraining Loss: 0.000269\n",
      "Epoch: 4 \tTraining Loss: 0.000269\n",
      "Epoch: 5 \tTraining Loss: 0.000269\n",
      "Epoch: 6 \tTraining Loss: 0.000269\n",
      "Epoch: 7 \tTraining Loss: 0.000269\n",
      "Epoch: 8 \tTraining Loss: 0.000269\n",
      "Epoch: 9 \tTraining Loss: 0.000269\n",
      "Epoch: 10 \tTraining Loss: 0.000269\n",
      "Epoch: 11 \tTraining Loss: 0.000269\n",
      "Epoch: 12 \tTraining Loss: 0.000269\n",
      "Epoch: 13 \tTraining Loss: 0.000269\n",
      "Epoch: 14 \tTraining Loss: 0.000269\n",
      "Epoch: 15 \tTraining Loss: 0.000269\n",
      "Epoch: 16 \tTraining Loss: 0.000269\n",
      "Epoch: 17 \tTraining Loss: 0.000269\n",
      "Epoch: 18 \tTraining Loss: 0.000269\n",
      "Epoch: 1 \tTraining Loss: 0.000302\n",
      "Epoch: 2 \tTraining Loss: 0.000302\n",
      "Epoch: 3 \tTraining Loss: 0.000302\n",
      "Epoch: 4 \tTraining Loss: 0.000302\n",
      "Epoch: 5 \tTraining Loss: 0.000302\n",
      "Epoch: 6 \tTraining Loss: 0.000302\n",
      "Epoch: 7 \tTraining Loss: 0.000302\n",
      "Epoch: 8 \tTraining Loss: 0.000302\n",
      "Epoch: 9 \tTraining Loss: 0.000302\n",
      "Epoch: 10 \tTraining Loss: 0.000302\n",
      "Epoch: 11 \tTraining Loss: 0.000302\n",
      "Epoch: 12 \tTraining Loss: 0.000302\n",
      "Epoch: 13 \tTraining Loss: 0.000302\n",
      "Epoch: 14 \tTraining Loss: 0.000302\n",
      "Epoch: 15 \tTraining Loss: 0.000302\n",
      "Epoch: 16 \tTraining Loss: 0.000302\n",
      "Epoch: 17 \tTraining Loss: 0.000302\n",
      "Epoch: 18 \tTraining Loss: 0.000302\n",
      "Epoch: 1 \tTraining Loss: 0.000463\n",
      "Epoch: 2 \tTraining Loss: 0.000463\n",
      "Epoch: 3 \tTraining Loss: 0.000463\n",
      "Epoch: 4 \tTraining Loss: 0.000463\n",
      "Epoch: 5 \tTraining Loss: 0.000463\n",
      "Epoch: 6 \tTraining Loss: 0.000463\n",
      "Epoch: 7 \tTraining Loss: 0.000463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 \tTraining Loss: 0.000463\n",
      "Epoch: 9 \tTraining Loss: 0.000463\n",
      "Epoch: 10 \tTraining Loss: 0.000463\n",
      "Epoch: 11 \tTraining Loss: 0.000463\n",
      "Epoch: 12 \tTraining Loss: 0.000463\n",
      "Epoch: 13 \tTraining Loss: 0.000463\n",
      "Epoch: 14 \tTraining Loss: 0.000463\n",
      "Epoch: 15 \tTraining Loss: 0.000463\n",
      "Epoch: 16 \tTraining Loss: 0.000463\n",
      "Epoch: 17 \tTraining Loss: 0.000463\n",
      "Epoch: 18 \tTraining Loss: 0.000463\n",
      "Epoch: 1 \tTraining Loss: 0.000186\n",
      "Epoch: 2 \tTraining Loss: 0.000186\n",
      "Epoch: 3 \tTraining Loss: 0.000186\n",
      "Epoch: 4 \tTraining Loss: 0.000186\n",
      "Epoch: 5 \tTraining Loss: 0.000186\n",
      "Epoch: 6 \tTraining Loss: 0.000186\n",
      "Epoch: 7 \tTraining Loss: 0.000186\n",
      "Epoch: 8 \tTraining Loss: 0.000186\n",
      "Epoch: 9 \tTraining Loss: 0.000186\n",
      "Epoch: 10 \tTraining Loss: 0.000186\n",
      "Epoch: 11 \tTraining Loss: 0.000186\n",
      "Epoch: 12 \tTraining Loss: 0.000186\n",
      "Epoch: 13 \tTraining Loss: 0.000186\n",
      "Epoch: 14 \tTraining Loss: 0.000186\n",
      "Epoch: 15 \tTraining Loss: 0.000186\n",
      "Epoch: 16 \tTraining Loss: 0.000186\n",
      "Epoch: 17 \tTraining Loss: 0.000186\n",
      "Epoch: 18 \tTraining Loss: 0.000186\n",
      "Epoch: 1 \tTraining Loss: 0.000243\n",
      "Epoch: 2 \tTraining Loss: 0.000243\n",
      "Epoch: 3 \tTraining Loss: 0.000243\n",
      "Epoch: 4 \tTraining Loss: 0.000243\n",
      "Epoch: 5 \tTraining Loss: 0.000243\n",
      "Epoch: 6 \tTraining Loss: 0.000243\n",
      "Epoch: 7 \tTraining Loss: 0.000243\n",
      "Epoch: 8 \tTraining Loss: 0.000243\n",
      "Epoch: 9 \tTraining Loss: 0.000243\n",
      "Epoch: 10 \tTraining Loss: 0.000243\n",
      "Epoch: 11 \tTraining Loss: 0.000243\n",
      "Epoch: 12 \tTraining Loss: 0.000243\n",
      "Epoch: 13 \tTraining Loss: 0.000243\n",
      "Epoch: 14 \tTraining Loss: 0.000243\n",
      "Epoch: 15 \tTraining Loss: 0.000243\n",
      "Epoch: 16 \tTraining Loss: 0.000243\n",
      "Epoch: 17 \tTraining Loss: 0.000243\n",
      "Epoch: 18 \tTraining Loss: 0.000243\n",
      "Epoch: 1 \tTraining Loss: 0.000293\n",
      "Epoch: 2 \tTraining Loss: 0.000293\n",
      "Epoch: 3 \tTraining Loss: 0.000293\n",
      "Epoch: 4 \tTraining Loss: 0.000293\n",
      "Epoch: 5 \tTraining Loss: 0.000293\n",
      "Epoch: 6 \tTraining Loss: 0.000293\n",
      "Epoch: 7 \tTraining Loss: 0.000293\n",
      "Epoch: 8 \tTraining Loss: 0.000293\n",
      "Epoch: 9 \tTraining Loss: 0.000293\n",
      "Epoch: 10 \tTraining Loss: 0.000293\n",
      "Epoch: 11 \tTraining Loss: 0.000293\n",
      "Epoch: 12 \tTraining Loss: 0.000293\n",
      "Epoch: 13 \tTraining Loss: 0.000293\n",
      "Epoch: 14 \tTraining Loss: 0.000293\n",
      "Epoch: 15 \tTraining Loss: 0.000293\n",
      "Epoch: 16 \tTraining Loss: 0.000293\n",
      "Epoch: 17 \tTraining Loss: 0.000293\n",
      "Epoch: 18 \tTraining Loss: 0.000293\n",
      "Epoch: 1 \tTraining Loss: 0.000165\n",
      "Epoch: 2 \tTraining Loss: 0.000165\n",
      "Epoch: 3 \tTraining Loss: 0.000165\n",
      "Epoch: 4 \tTraining Loss: 0.000165\n",
      "Epoch: 5 \tTraining Loss: 0.000165\n",
      "Epoch: 6 \tTraining Loss: 0.000165\n",
      "Epoch: 7 \tTraining Loss: 0.000165\n",
      "Epoch: 8 \tTraining Loss: 0.000165\n",
      "Epoch: 9 \tTraining Loss: 0.000165\n",
      "Epoch: 10 \tTraining Loss: 0.000165\n",
      "Epoch: 11 \tTraining Loss: 0.000165\n",
      "Epoch: 12 \tTraining Loss: 0.000165\n",
      "Epoch: 13 \tTraining Loss: 0.000165\n",
      "Epoch: 14 \tTraining Loss: 0.000165\n",
      "Epoch: 15 \tTraining Loss: 0.000165\n",
      "Epoch: 16 \tTraining Loss: 0.000165\n",
      "Epoch: 17 \tTraining Loss: 0.000165\n",
      "Epoch: 18 \tTraining Loss: 0.000165\n",
      "Epoch: 1 \tTraining Loss: 0.000272\n",
      "Epoch: 2 \tTraining Loss: 0.000272\n",
      "Epoch: 3 \tTraining Loss: 0.000272\n",
      "Epoch: 4 \tTraining Loss: 0.000272\n",
      "Epoch: 5 \tTraining Loss: 0.000272\n",
      "Epoch: 6 \tTraining Loss: 0.000272\n",
      "Epoch: 7 \tTraining Loss: 0.000272\n",
      "Epoch: 8 \tTraining Loss: 0.000272\n",
      "Epoch: 9 \tTraining Loss: 0.000272\n",
      "Epoch: 10 \tTraining Loss: 0.000272\n",
      "Epoch: 11 \tTraining Loss: 0.000272\n",
      "Epoch: 12 \tTraining Loss: 0.000272\n",
      "Epoch: 13 \tTraining Loss: 0.000272\n",
      "Epoch: 14 \tTraining Loss: 0.000272\n",
      "Epoch: 15 \tTraining Loss: 0.000272\n",
      "Epoch: 16 \tTraining Loss: 0.000272\n",
      "Epoch: 17 \tTraining Loss: 0.000272\n",
      "Epoch: 18 \tTraining Loss: 0.000272\n",
      "Epoch: 1 \tTraining Loss: 0.000295\n",
      "Epoch: 2 \tTraining Loss: 0.000295\n",
      "Epoch: 3 \tTraining Loss: 0.000295\n",
      "Epoch: 4 \tTraining Loss: 0.000295\n",
      "Epoch: 5 \tTraining Loss: 0.000295\n",
      "Epoch: 6 \tTraining Loss: 0.000295\n",
      "Epoch: 7 \tTraining Loss: 0.000295\n",
      "Epoch: 8 \tTraining Loss: 0.000295\n",
      "Epoch: 9 \tTraining Loss: 0.000295\n",
      "Epoch: 10 \tTraining Loss: 0.000295\n",
      "Epoch: 11 \tTraining Loss: 0.000295\n",
      "Epoch: 12 \tTraining Loss: 0.000295\n",
      "Epoch: 13 \tTraining Loss: 0.000295\n",
      "Epoch: 14 \tTraining Loss: 0.000295\n",
      "Epoch: 15 \tTraining Loss: 0.000295\n",
      "Epoch: 16 \tTraining Loss: 0.000295\n",
      "Epoch: 17 \tTraining Loss: 0.000295\n",
      "Epoch: 18 \tTraining Loss: 0.000295\n",
      "Epoch: 1 \tTraining Loss: 0.000248\n",
      "Epoch: 2 \tTraining Loss: 0.000248\n",
      "Epoch: 3 \tTraining Loss: 0.000248\n",
      "Epoch: 4 \tTraining Loss: 0.000248\n",
      "Epoch: 5 \tTraining Loss: 0.000248\n",
      "Epoch: 6 \tTraining Loss: 0.000248\n",
      "Epoch: 7 \tTraining Loss: 0.000248\n",
      "Epoch: 8 \tTraining Loss: 0.000248\n",
      "Epoch: 9 \tTraining Loss: 0.000248\n",
      "Epoch: 10 \tTraining Loss: 0.000248\n",
      "Epoch: 11 \tTraining Loss: 0.000248\n",
      "Epoch: 12 \tTraining Loss: 0.000248\n",
      "Epoch: 13 \tTraining Loss: 0.000248\n",
      "Epoch: 14 \tTraining Loss: 0.000248\n",
      "Epoch: 15 \tTraining Loss: 0.000248\n",
      "Epoch: 16 \tTraining Loss: 0.000248\n",
      "Epoch: 17 \tTraining Loss: 0.000248\n",
      "Epoch: 18 \tTraining Loss: 0.000248\n",
      "Epoch: 1 \tTraining Loss: 0.000275\n",
      "Epoch: 2 \tTraining Loss: 0.000275\n",
      "Epoch: 3 \tTraining Loss: 0.000275\n",
      "Epoch: 4 \tTraining Loss: 0.000275\n",
      "Epoch: 5 \tTraining Loss: 0.000275\n",
      "Epoch: 6 \tTraining Loss: 0.000275\n",
      "Epoch: 7 \tTraining Loss: 0.000275\n",
      "Epoch: 8 \tTraining Loss: 0.000275\n",
      "Epoch: 9 \tTraining Loss: 0.000275\n",
      "Epoch: 10 \tTraining Loss: 0.000275\n",
      "Epoch: 11 \tTraining Loss: 0.000275\n",
      "Epoch: 12 \tTraining Loss: 0.000275\n",
      "Epoch: 13 \tTraining Loss: 0.000275\n",
      "Epoch: 14 \tTraining Loss: 0.000275\n",
      "Epoch: 15 \tTraining Loss: 0.000275\n",
      "Epoch: 16 \tTraining Loss: 0.000275\n",
      "Epoch: 17 \tTraining Loss: 0.000275\n",
      "Epoch: 18 \tTraining Loss: 0.000275\n",
      "Epoch: 1 \tTraining Loss: 0.000255\n",
      "Epoch: 2 \tTraining Loss: 0.000255\n",
      "Epoch: 3 \tTraining Loss: 0.000255\n",
      "Epoch: 4 \tTraining Loss: 0.000255\n",
      "Epoch: 5 \tTraining Loss: 0.000255\n",
      "Epoch: 6 \tTraining Loss: 0.000255\n",
      "Epoch: 7 \tTraining Loss: 0.000255\n",
      "Epoch: 8 \tTraining Loss: 0.000255\n",
      "Epoch: 9 \tTraining Loss: 0.000255\n",
      "Epoch: 10 \tTraining Loss: 0.000255\n",
      "Epoch: 11 \tTraining Loss: 0.000255\n",
      "Epoch: 12 \tTraining Loss: 0.000255\n",
      "Epoch: 13 \tTraining Loss: 0.000255\n",
      "Epoch: 14 \tTraining Loss: 0.000255\n",
      "Epoch: 15 \tTraining Loss: 0.000255\n",
      "Epoch: 16 \tTraining Loss: 0.000255\n",
      "Epoch: 17 \tTraining Loss: 0.000255\n",
      "Epoch: 18 \tTraining Loss: 0.000255\n",
      "Epoch: 1 \tTraining Loss: 0.000314\n",
      "Epoch: 2 \tTraining Loss: 0.000314\n",
      "Epoch: 3 \tTraining Loss: 0.000314\n",
      "Epoch: 4 \tTraining Loss: 0.000314\n",
      "Epoch: 5 \tTraining Loss: 0.000314\n",
      "Epoch: 6 \tTraining Loss: 0.000314\n",
      "Epoch: 7 \tTraining Loss: 0.000314\n",
      "Epoch: 8 \tTraining Loss: 0.000314\n",
      "Epoch: 9 \tTraining Loss: 0.000314\n",
      "Epoch: 10 \tTraining Loss: 0.000314\n",
      "Epoch: 11 \tTraining Loss: 0.000314\n",
      "Epoch: 12 \tTraining Loss: 0.000314\n",
      "Epoch: 13 \tTraining Loss: 0.000314\n",
      "Epoch: 14 \tTraining Loss: 0.000314\n",
      "Epoch: 15 \tTraining Loss: 0.000314\n",
      "Epoch: 16 \tTraining Loss: 0.000314\n",
      "Epoch: 17 \tTraining Loss: 0.000314\n",
      "Epoch: 18 \tTraining Loss: 0.000314\n",
      "Epoch: 1 \tTraining Loss: 0.000293\n",
      "Epoch: 2 \tTraining Loss: 0.000293\n",
      "Epoch: 3 \tTraining Loss: 0.000293\n",
      "Epoch: 4 \tTraining Loss: 0.000293\n",
      "Epoch: 5 \tTraining Loss: 0.000293\n",
      "Epoch: 6 \tTraining Loss: 0.000293\n",
      "Epoch: 7 \tTraining Loss: 0.000293\n",
      "Epoch: 8 \tTraining Loss: 0.000293\n",
      "Epoch: 9 \tTraining Loss: 0.000293\n",
      "Epoch: 10 \tTraining Loss: 0.000293\n",
      "Epoch: 11 \tTraining Loss: 0.000293\n",
      "Epoch: 12 \tTraining Loss: 0.000293\n",
      "Epoch: 13 \tTraining Loss: 0.000293\n",
      "Epoch: 14 \tTraining Loss: 0.000293\n",
      "Epoch: 15 \tTraining Loss: 0.000293\n",
      "Epoch: 16 \tTraining Loss: 0.000293\n",
      "Epoch: 17 \tTraining Loss: 0.000293\n",
      "Epoch: 18 \tTraining Loss: 0.000293\n",
      "Epoch: 1 \tTraining Loss: 0.000176\n",
      "Epoch: 2 \tTraining Loss: 0.000176\n",
      "Epoch: 3 \tTraining Loss: 0.000176\n",
      "Epoch: 4 \tTraining Loss: 0.000176\n",
      "Epoch: 5 \tTraining Loss: 0.000176\n",
      "Epoch: 6 \tTraining Loss: 0.000176\n",
      "Epoch: 7 \tTraining Loss: 0.000176\n",
      "Epoch: 8 \tTraining Loss: 0.000176\n",
      "Epoch: 9 \tTraining Loss: 0.000176\n",
      "Epoch: 10 \tTraining Loss: 0.000176\n",
      "Epoch: 11 \tTraining Loss: 0.000176\n",
      "Epoch: 12 \tTraining Loss: 0.000176\n",
      "Epoch: 13 \tTraining Loss: 0.000176\n",
      "Epoch: 14 \tTraining Loss: 0.000176\n",
      "Epoch: 15 \tTraining Loss: 0.000176\n",
      "Epoch: 16 \tTraining Loss: 0.000176\n",
      "Epoch: 17 \tTraining Loss: 0.000176\n",
      "Epoch: 18 \tTraining Loss: 0.000176\n",
      "Epoch: 1 \tTraining Loss: 0.000114\n",
      "Epoch: 2 \tTraining Loss: 0.000114\n",
      "Epoch: 3 \tTraining Loss: 0.000114\n",
      "Epoch: 4 \tTraining Loss: 0.000114\n",
      "Epoch: 5 \tTraining Loss: 0.000114\n",
      "Epoch: 6 \tTraining Loss: 0.000114\n",
      "Epoch: 7 \tTraining Loss: 0.000114\n",
      "Epoch: 8 \tTraining Loss: 0.000114\n",
      "Epoch: 9 \tTraining Loss: 0.000114\n",
      "Epoch: 10 \tTraining Loss: 0.000114\n",
      "Epoch: 11 \tTraining Loss: 0.000114\n",
      "Epoch: 12 \tTraining Loss: 0.000114\n",
      "Epoch: 13 \tTraining Loss: 0.000114\n",
      "Epoch: 14 \tTraining Loss: 0.000114\n",
      "Epoch: 15 \tTraining Loss: 0.000114\n",
      "Epoch: 16 \tTraining Loss: 0.000114\n",
      "Epoch: 17 \tTraining Loss: 0.000114\n",
      "Epoch: 18 \tTraining Loss: 0.000114\n",
      "Epoch: 1 \tTraining Loss: 0.000308\n",
      "Epoch: 2 \tTraining Loss: 0.000308\n",
      "Epoch: 3 \tTraining Loss: 0.000308\n",
      "Epoch: 4 \tTraining Loss: 0.000308\n",
      "Epoch: 5 \tTraining Loss: 0.000308\n",
      "Epoch: 6 \tTraining Loss: 0.000308\n",
      "Epoch: 7 \tTraining Loss: 0.000308\n",
      "Epoch: 8 \tTraining Loss: 0.000308\n",
      "Epoch: 9 \tTraining Loss: 0.000308\n",
      "Epoch: 10 \tTraining Loss: 0.000308\n",
      "Epoch: 11 \tTraining Loss: 0.000308\n",
      "Epoch: 12 \tTraining Loss: 0.000308\n",
      "Epoch: 13 \tTraining Loss: 0.000308\n",
      "Epoch: 14 \tTraining Loss: 0.000308\n",
      "Epoch: 15 \tTraining Loss: 0.000308\n",
      "Epoch: 16 \tTraining Loss: 0.000308\n",
      "Epoch: 17 \tTraining Loss: 0.000308\n",
      "Epoch: 18 \tTraining Loss: 0.000308\n",
      "Epoch: 1 \tTraining Loss: 0.000143\n",
      "Epoch: 2 \tTraining Loss: 0.000143\n",
      "Epoch: 3 \tTraining Loss: 0.000143\n",
      "Epoch: 4 \tTraining Loss: 0.000143\n",
      "Epoch: 5 \tTraining Loss: 0.000143\n",
      "Epoch: 6 \tTraining Loss: 0.000143\n",
      "Epoch: 7 \tTraining Loss: 0.000143\n",
      "Epoch: 8 \tTraining Loss: 0.000143\n",
      "Epoch: 9 \tTraining Loss: 0.000143\n",
      "Epoch: 10 \tTraining Loss: 0.000143\n",
      "Epoch: 11 \tTraining Loss: 0.000143\n",
      "Epoch: 12 \tTraining Loss: 0.000143\n",
      "Epoch: 13 \tTraining Loss: 0.000143\n",
      "Epoch: 14 \tTraining Loss: 0.000143\n",
      "Epoch: 15 \tTraining Loss: 0.000143\n",
      "Epoch: 16 \tTraining Loss: 0.000143\n",
      "Epoch: 17 \tTraining Loss: 0.000143\n",
      "Epoch: 18 \tTraining Loss: 0.000143\n",
      "Epoch: 1 \tTraining Loss: 0.000178\n",
      "Epoch: 2 \tTraining Loss: 0.000178\n",
      "Epoch: 3 \tTraining Loss: 0.000178\n",
      "Epoch: 4 \tTraining Loss: 0.000178\n",
      "Epoch: 5 \tTraining Loss: 0.000178\n",
      "Epoch: 6 \tTraining Loss: 0.000178\n",
      "Epoch: 7 \tTraining Loss: 0.000178\n",
      "Epoch: 8 \tTraining Loss: 0.000178\n",
      "Epoch: 9 \tTraining Loss: 0.000178\n",
      "Epoch: 10 \tTraining Loss: 0.000178\n",
      "Epoch: 11 \tTraining Loss: 0.000178\n",
      "Epoch: 12 \tTraining Loss: 0.000178\n",
      "Epoch: 13 \tTraining Loss: 0.000178\n",
      "Epoch: 14 \tTraining Loss: 0.000178\n",
      "Epoch: 15 \tTraining Loss: 0.000178\n",
      "Epoch: 16 \tTraining Loss: 0.000178\n",
      "Epoch: 17 \tTraining Loss: 0.000178\n",
      "Epoch: 18 \tTraining Loss: 0.000178\n",
      "Epoch: 1 \tTraining Loss: 0.000209\n",
      "Epoch: 2 \tTraining Loss: 0.000209\n",
      "Epoch: 3 \tTraining Loss: 0.000209\n",
      "Epoch: 4 \tTraining Loss: 0.000209\n",
      "Epoch: 5 \tTraining Loss: 0.000209\n",
      "Epoch: 6 \tTraining Loss: 0.000209\n",
      "Epoch: 7 \tTraining Loss: 0.000209\n",
      "Epoch: 8 \tTraining Loss: 0.000209\n",
      "Epoch: 9 \tTraining Loss: 0.000209\n",
      "Epoch: 10 \tTraining Loss: 0.000209\n",
      "Epoch: 11 \tTraining Loss: 0.000209\n",
      "Epoch: 12 \tTraining Loss: 0.000209\n",
      "Epoch: 13 \tTraining Loss: 0.000209\n",
      "Epoch: 14 \tTraining Loss: 0.000209\n",
      "Epoch: 15 \tTraining Loss: 0.000209\n",
      "Epoch: 16 \tTraining Loss: 0.000209\n",
      "Epoch: 17 \tTraining Loss: 0.000209\n",
      "Epoch: 18 \tTraining Loss: 0.000209\n",
      "Epoch: 1 \tTraining Loss: 0.000205\n",
      "Epoch: 2 \tTraining Loss: 0.000205\n",
      "Epoch: 3 \tTraining Loss: 0.000205\n",
      "Epoch: 4 \tTraining Loss: 0.000205\n",
      "Epoch: 5 \tTraining Loss: 0.000205\n",
      "Epoch: 6 \tTraining Loss: 0.000205\n",
      "Epoch: 7 \tTraining Loss: 0.000205\n",
      "Epoch: 8 \tTraining Loss: 0.000205\n",
      "Epoch: 9 \tTraining Loss: 0.000205\n",
      "Epoch: 10 \tTraining Loss: 0.000205\n",
      "Epoch: 11 \tTraining Loss: 0.000205\n",
      "Epoch: 12 \tTraining Loss: 0.000205\n",
      "Epoch: 13 \tTraining Loss: 0.000205\n",
      "Epoch: 14 \tTraining Loss: 0.000205\n",
      "Epoch: 15 \tTraining Loss: 0.000205\n",
      "Epoch: 16 \tTraining Loss: 0.000205\n",
      "Epoch: 17 \tTraining Loss: 0.000205\n",
      "Epoch: 18 \tTraining Loss: 0.000205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000282\n",
      "Epoch: 2 \tTraining Loss: 0.000282\n",
      "Epoch: 3 \tTraining Loss: 0.000282\n",
      "Epoch: 4 \tTraining Loss: 0.000282\n",
      "Epoch: 5 \tTraining Loss: 0.000282\n",
      "Epoch: 6 \tTraining Loss: 0.000282\n",
      "Epoch: 7 \tTraining Loss: 0.000282\n",
      "Epoch: 8 \tTraining Loss: 0.000282\n",
      "Epoch: 9 \tTraining Loss: 0.000282\n",
      "Epoch: 10 \tTraining Loss: 0.000282\n",
      "Epoch: 11 \tTraining Loss: 0.000282\n",
      "Epoch: 12 \tTraining Loss: 0.000282\n",
      "Epoch: 13 \tTraining Loss: 0.000282\n",
      "Epoch: 14 \tTraining Loss: 0.000282\n",
      "Epoch: 15 \tTraining Loss: 0.000282\n",
      "Epoch: 16 \tTraining Loss: 0.000282\n",
      "Epoch: 17 \tTraining Loss: 0.000282\n",
      "Epoch: 18 \tTraining Loss: 0.000282\n",
      "Epoch: 1 \tTraining Loss: 0.000363\n",
      "Epoch: 2 \tTraining Loss: 0.000363\n",
      "Epoch: 3 \tTraining Loss: 0.000363\n",
      "Epoch: 4 \tTraining Loss: 0.000363\n",
      "Epoch: 5 \tTraining Loss: 0.000363\n",
      "Epoch: 6 \tTraining Loss: 0.000363\n",
      "Epoch: 7 \tTraining Loss: 0.000363\n",
      "Epoch: 8 \tTraining Loss: 0.000363\n",
      "Epoch: 9 \tTraining Loss: 0.000363\n",
      "Epoch: 10 \tTraining Loss: 0.000363\n",
      "Epoch: 11 \tTraining Loss: 0.000363\n",
      "Epoch: 12 \tTraining Loss: 0.000363\n",
      "Epoch: 13 \tTraining Loss: 0.000363\n",
      "Epoch: 14 \tTraining Loss: 0.000363\n",
      "Epoch: 15 \tTraining Loss: 0.000363\n",
      "Epoch: 16 \tTraining Loss: 0.000363\n",
      "Epoch: 17 \tTraining Loss: 0.000363\n",
      "Epoch: 18 \tTraining Loss: 0.000363\n",
      "Epoch: 1 \tTraining Loss: 0.000233\n",
      "Epoch: 2 \tTraining Loss: 0.000233\n",
      "Epoch: 3 \tTraining Loss: 0.000233\n",
      "Epoch: 4 \tTraining Loss: 0.000233\n",
      "Epoch: 5 \tTraining Loss: 0.000233\n",
      "Epoch: 6 \tTraining Loss: 0.000233\n",
      "Epoch: 7 \tTraining Loss: 0.000233\n",
      "Epoch: 8 \tTraining Loss: 0.000233\n",
      "Epoch: 9 \tTraining Loss: 0.000233\n",
      "Epoch: 10 \tTraining Loss: 0.000233\n",
      "Epoch: 11 \tTraining Loss: 0.000233\n",
      "Epoch: 12 \tTraining Loss: 0.000233\n",
      "Epoch: 13 \tTraining Loss: 0.000233\n",
      "Epoch: 14 \tTraining Loss: 0.000233\n",
      "Epoch: 15 \tTraining Loss: 0.000233\n",
      "Epoch: 16 \tTraining Loss: 0.000233\n",
      "Epoch: 17 \tTraining Loss: 0.000233\n",
      "Epoch: 18 \tTraining Loss: 0.000233\n",
      "Epoch: 1 \tTraining Loss: 0.000322\n",
      "Epoch: 2 \tTraining Loss: 0.000322\n",
      "Epoch: 3 \tTraining Loss: 0.000322\n",
      "Epoch: 4 \tTraining Loss: 0.000322\n",
      "Epoch: 5 \tTraining Loss: 0.000322\n",
      "Epoch: 6 \tTraining Loss: 0.000322\n",
      "Epoch: 7 \tTraining Loss: 0.000322\n",
      "Epoch: 8 \tTraining Loss: 0.000322\n",
      "Epoch: 9 \tTraining Loss: 0.000322\n",
      "Epoch: 10 \tTraining Loss: 0.000322\n",
      "Epoch: 11 \tTraining Loss: 0.000322\n",
      "Epoch: 12 \tTraining Loss: 0.000322\n",
      "Epoch: 13 \tTraining Loss: 0.000322\n",
      "Epoch: 14 \tTraining Loss: 0.000322\n",
      "Epoch: 15 \tTraining Loss: 0.000322\n",
      "Epoch: 16 \tTraining Loss: 0.000322\n",
      "Epoch: 17 \tTraining Loss: 0.000322\n",
      "Epoch: 18 \tTraining Loss: 0.000322\n",
      "Epoch: 1 \tTraining Loss: 0.000276\n",
      "Epoch: 2 \tTraining Loss: 0.000276\n",
      "Epoch: 3 \tTraining Loss: 0.000276\n",
      "Epoch: 4 \tTraining Loss: 0.000276\n",
      "Epoch: 5 \tTraining Loss: 0.000276\n",
      "Epoch: 6 \tTraining Loss: 0.000276\n",
      "Epoch: 7 \tTraining Loss: 0.000276\n",
      "Epoch: 8 \tTraining Loss: 0.000276\n",
      "Epoch: 9 \tTraining Loss: 0.000276\n",
      "Epoch: 10 \tTraining Loss: 0.000276\n",
      "Epoch: 11 \tTraining Loss: 0.000276\n",
      "Epoch: 12 \tTraining Loss: 0.000276\n",
      "Epoch: 13 \tTraining Loss: 0.000276\n",
      "Epoch: 14 \tTraining Loss: 0.000276\n",
      "Epoch: 15 \tTraining Loss: 0.000276\n",
      "Epoch: 16 \tTraining Loss: 0.000276\n",
      "Epoch: 17 \tTraining Loss: 0.000276\n",
      "Epoch: 18 \tTraining Loss: 0.000276\n",
      "Epoch: 1 \tTraining Loss: 0.000239\n",
      "Epoch: 2 \tTraining Loss: 0.000239\n",
      "Epoch: 3 \tTraining Loss: 0.000239\n",
      "Epoch: 4 \tTraining Loss: 0.000239\n",
      "Epoch: 5 \tTraining Loss: 0.000239\n",
      "Epoch: 6 \tTraining Loss: 0.000239\n",
      "Epoch: 7 \tTraining Loss: 0.000239\n",
      "Epoch: 8 \tTraining Loss: 0.000239\n",
      "Epoch: 9 \tTraining Loss: 0.000239\n",
      "Epoch: 10 \tTraining Loss: 0.000239\n",
      "Epoch: 11 \tTraining Loss: 0.000239\n",
      "Epoch: 12 \tTraining Loss: 0.000239\n",
      "Epoch: 13 \tTraining Loss: 0.000239\n",
      "Epoch: 14 \tTraining Loss: 0.000239\n",
      "Epoch: 15 \tTraining Loss: 0.000239\n",
      "Epoch: 16 \tTraining Loss: 0.000239\n",
      "Epoch: 17 \tTraining Loss: 0.000239\n",
      "Epoch: 18 \tTraining Loss: 0.000239\n",
      "Epoch: 1 \tTraining Loss: 0.000245\n",
      "Epoch: 2 \tTraining Loss: 0.000245\n",
      "Epoch: 3 \tTraining Loss: 0.000245\n",
      "Epoch: 4 \tTraining Loss: 0.000245\n",
      "Epoch: 5 \tTraining Loss: 0.000245\n",
      "Epoch: 6 \tTraining Loss: 0.000245\n",
      "Epoch: 7 \tTraining Loss: 0.000245\n",
      "Epoch: 8 \tTraining Loss: 0.000245\n",
      "Epoch: 9 \tTraining Loss: 0.000245\n",
      "Epoch: 10 \tTraining Loss: 0.000245\n",
      "Epoch: 11 \tTraining Loss: 0.000245\n",
      "Epoch: 12 \tTraining Loss: 0.000245\n",
      "Epoch: 13 \tTraining Loss: 0.000245\n",
      "Epoch: 14 \tTraining Loss: 0.000245\n",
      "Epoch: 15 \tTraining Loss: 0.000245\n",
      "Epoch: 16 \tTraining Loss: 0.000245\n",
      "Epoch: 17 \tTraining Loss: 0.000245\n",
      "Epoch: 18 \tTraining Loss: 0.000245\n",
      "Epoch: 1 \tTraining Loss: 0.000297\n",
      "Epoch: 2 \tTraining Loss: 0.000297\n",
      "Epoch: 3 \tTraining Loss: 0.000297\n",
      "Epoch: 4 \tTraining Loss: 0.000297\n",
      "Epoch: 5 \tTraining Loss: 0.000297\n",
      "Epoch: 6 \tTraining Loss: 0.000297\n",
      "Epoch: 7 \tTraining Loss: 0.000297\n",
      "Epoch: 8 \tTraining Loss: 0.000297\n",
      "Epoch: 9 \tTraining Loss: 0.000297\n",
      "Epoch: 10 \tTraining Loss: 0.000297\n",
      "Epoch: 11 \tTraining Loss: 0.000297\n",
      "Epoch: 12 \tTraining Loss: 0.000297\n",
      "Epoch: 13 \tTraining Loss: 0.000297\n",
      "Epoch: 14 \tTraining Loss: 0.000297\n",
      "Epoch: 15 \tTraining Loss: 0.000297\n",
      "Epoch: 16 \tTraining Loss: 0.000297\n",
      "Epoch: 17 \tTraining Loss: 0.000297\n",
      "Epoch: 18 \tTraining Loss: 0.000297\n",
      "Epoch: 1 \tTraining Loss: 0.000128\n",
      "Epoch: 2 \tTraining Loss: 0.000128\n",
      "Epoch: 3 \tTraining Loss: 0.000128\n",
      "Epoch: 4 \tTraining Loss: 0.000128\n",
      "Epoch: 5 \tTraining Loss: 0.000128\n",
      "Epoch: 6 \tTraining Loss: 0.000128\n",
      "Epoch: 7 \tTraining Loss: 0.000128\n",
      "Epoch: 8 \tTraining Loss: 0.000128\n",
      "Epoch: 9 \tTraining Loss: 0.000128\n",
      "Epoch: 10 \tTraining Loss: 0.000128\n",
      "Epoch: 11 \tTraining Loss: 0.000128\n",
      "Epoch: 12 \tTraining Loss: 0.000128\n",
      "Epoch: 13 \tTraining Loss: 0.000128\n",
      "Epoch: 14 \tTraining Loss: 0.000128\n",
      "Epoch: 15 \tTraining Loss: 0.000128\n",
      "Epoch: 16 \tTraining Loss: 0.000128\n",
      "Epoch: 17 \tTraining Loss: 0.000128\n",
      "Epoch: 18 \tTraining Loss: 0.000128\n",
      "Epoch: 1 \tTraining Loss: 0.000199\n",
      "Epoch: 2 \tTraining Loss: 0.000199\n",
      "Epoch: 3 \tTraining Loss: 0.000199\n",
      "Epoch: 4 \tTraining Loss: 0.000199\n",
      "Epoch: 5 \tTraining Loss: 0.000199\n",
      "Epoch: 6 \tTraining Loss: 0.000199\n",
      "Epoch: 7 \tTraining Loss: 0.000199\n",
      "Epoch: 8 \tTraining Loss: 0.000199\n",
      "Epoch: 9 \tTraining Loss: 0.000199\n",
      "Epoch: 10 \tTraining Loss: 0.000199\n",
      "Epoch: 11 \tTraining Loss: 0.000199\n",
      "Epoch: 12 \tTraining Loss: 0.000199\n",
      "Epoch: 13 \tTraining Loss: 0.000199\n",
      "Epoch: 14 \tTraining Loss: 0.000199\n",
      "Epoch: 15 \tTraining Loss: 0.000199\n",
      "Epoch: 16 \tTraining Loss: 0.000199\n",
      "Epoch: 17 \tTraining Loss: 0.000199\n",
      "Epoch: 18 \tTraining Loss: 0.000199\n",
      "Epoch: 1 \tTraining Loss: 0.000159\n",
      "Epoch: 2 \tTraining Loss: 0.000159\n",
      "Epoch: 3 \tTraining Loss: 0.000159\n",
      "Epoch: 4 \tTraining Loss: 0.000159\n",
      "Epoch: 5 \tTraining Loss: 0.000159\n",
      "Epoch: 6 \tTraining Loss: 0.000159\n",
      "Epoch: 7 \tTraining Loss: 0.000159\n",
      "Epoch: 8 \tTraining Loss: 0.000159\n",
      "Epoch: 9 \tTraining Loss: 0.000159\n",
      "Epoch: 10 \tTraining Loss: 0.000159\n",
      "Epoch: 11 \tTraining Loss: 0.000159\n",
      "Epoch: 12 \tTraining Loss: 0.000159\n",
      "Epoch: 13 \tTraining Loss: 0.000159\n",
      "Epoch: 14 \tTraining Loss: 0.000159\n",
      "Epoch: 15 \tTraining Loss: 0.000159\n",
      "Epoch: 16 \tTraining Loss: 0.000159\n",
      "Epoch: 17 \tTraining Loss: 0.000159\n",
      "Epoch: 18 \tTraining Loss: 0.000159\n",
      "Epoch: 1 \tTraining Loss: 0.000244\n",
      "Epoch: 2 \tTraining Loss: 0.000244\n",
      "Epoch: 3 \tTraining Loss: 0.000244\n",
      "Epoch: 4 \tTraining Loss: 0.000244\n",
      "Epoch: 5 \tTraining Loss: 0.000244\n",
      "Epoch: 6 \tTraining Loss: 0.000244\n",
      "Epoch: 7 \tTraining Loss: 0.000244\n",
      "Epoch: 8 \tTraining Loss: 0.000244\n",
      "Epoch: 9 \tTraining Loss: 0.000244\n",
      "Epoch: 10 \tTraining Loss: 0.000244\n",
      "Epoch: 11 \tTraining Loss: 0.000244\n",
      "Epoch: 12 \tTraining Loss: 0.000244\n",
      "Epoch: 13 \tTraining Loss: 0.000244\n",
      "Epoch: 14 \tTraining Loss: 0.000244\n",
      "Epoch: 15 \tTraining Loss: 0.000244\n",
      "Epoch: 16 \tTraining Loss: 0.000244\n",
      "Epoch: 17 \tTraining Loss: 0.000244\n",
      "Epoch: 18 \tTraining Loss: 0.000244\n",
      "Epoch: 1 \tTraining Loss: 0.000211\n",
      "Epoch: 2 \tTraining Loss: 0.000211\n",
      "Epoch: 3 \tTraining Loss: 0.000211\n",
      "Epoch: 4 \tTraining Loss: 0.000211\n",
      "Epoch: 5 \tTraining Loss: 0.000211\n",
      "Epoch: 6 \tTraining Loss: 0.000211\n",
      "Epoch: 7 \tTraining Loss: 0.000211\n",
      "Epoch: 8 \tTraining Loss: 0.000211\n",
      "Epoch: 9 \tTraining Loss: 0.000211\n",
      "Epoch: 10 \tTraining Loss: 0.000211\n",
      "Epoch: 11 \tTraining Loss: 0.000211\n",
      "Epoch: 12 \tTraining Loss: 0.000211\n",
      "Epoch: 13 \tTraining Loss: 0.000211\n",
      "Epoch: 14 \tTraining Loss: 0.000211\n",
      "Epoch: 15 \tTraining Loss: 0.000211\n",
      "Epoch: 16 \tTraining Loss: 0.000211\n",
      "Epoch: 17 \tTraining Loss: 0.000211\n",
      "Epoch: 18 \tTraining Loss: 0.000211\n",
      "Epoch: 1 \tTraining Loss: 0.000239\n",
      "Epoch: 2 \tTraining Loss: 0.000239\n",
      "Epoch: 3 \tTraining Loss: 0.000239\n",
      "Epoch: 4 \tTraining Loss: 0.000239\n",
      "Epoch: 5 \tTraining Loss: 0.000239\n",
      "Epoch: 6 \tTraining Loss: 0.000239\n",
      "Epoch: 7 \tTraining Loss: 0.000239\n",
      "Epoch: 8 \tTraining Loss: 0.000239\n",
      "Epoch: 9 \tTraining Loss: 0.000239\n",
      "Epoch: 10 \tTraining Loss: 0.000239\n",
      "Epoch: 11 \tTraining Loss: 0.000239\n",
      "Epoch: 12 \tTraining Loss: 0.000239\n",
      "Epoch: 13 \tTraining Loss: 0.000239\n",
      "Epoch: 14 \tTraining Loss: 0.000239\n",
      "Epoch: 15 \tTraining Loss: 0.000239\n",
      "Epoch: 16 \tTraining Loss: 0.000239\n",
      "Epoch: 17 \tTraining Loss: 0.000239\n",
      "Epoch: 18 \tTraining Loss: 0.000239\n",
      "Epoch: 1 \tTraining Loss: 0.000228\n",
      "Epoch: 2 \tTraining Loss: 0.000228\n",
      "Epoch: 3 \tTraining Loss: 0.000228\n",
      "Epoch: 4 \tTraining Loss: 0.000228\n",
      "Epoch: 5 \tTraining Loss: 0.000228\n",
      "Epoch: 6 \tTraining Loss: 0.000228\n",
      "Epoch: 7 \tTraining Loss: 0.000228\n",
      "Epoch: 8 \tTraining Loss: 0.000228\n",
      "Epoch: 9 \tTraining Loss: 0.000228\n",
      "Epoch: 10 \tTraining Loss: 0.000228\n",
      "Epoch: 11 \tTraining Loss: 0.000228\n",
      "Epoch: 12 \tTraining Loss: 0.000228\n",
      "Epoch: 13 \tTraining Loss: 0.000228\n",
      "Epoch: 14 \tTraining Loss: 0.000228\n",
      "Epoch: 15 \tTraining Loss: 0.000228\n",
      "Epoch: 16 \tTraining Loss: 0.000228\n",
      "Epoch: 17 \tTraining Loss: 0.000228\n",
      "Epoch: 18 \tTraining Loss: 0.000228\n",
      "Epoch: 1 \tTraining Loss: 0.000163\n",
      "Epoch: 2 \tTraining Loss: 0.000163\n",
      "Epoch: 3 \tTraining Loss: 0.000163\n",
      "Epoch: 4 \tTraining Loss: 0.000163\n",
      "Epoch: 5 \tTraining Loss: 0.000163\n",
      "Epoch: 6 \tTraining Loss: 0.000163\n",
      "Epoch: 7 \tTraining Loss: 0.000163\n",
      "Epoch: 8 \tTraining Loss: 0.000163\n",
      "Epoch: 9 \tTraining Loss: 0.000163\n",
      "Epoch: 10 \tTraining Loss: 0.000163\n",
      "Epoch: 11 \tTraining Loss: 0.000163\n",
      "Epoch: 12 \tTraining Loss: 0.000163\n",
      "Epoch: 13 \tTraining Loss: 0.000163\n",
      "Epoch: 14 \tTraining Loss: 0.000163\n",
      "Epoch: 15 \tTraining Loss: 0.000163\n",
      "Epoch: 16 \tTraining Loss: 0.000163\n",
      "Epoch: 17 \tTraining Loss: 0.000163\n",
      "Epoch: 18 \tTraining Loss: 0.000163\n",
      "Epoch: 1 \tTraining Loss: 0.000110\n",
      "Epoch: 2 \tTraining Loss: 0.000110\n",
      "Epoch: 3 \tTraining Loss: 0.000110\n",
      "Epoch: 4 \tTraining Loss: 0.000110\n",
      "Epoch: 5 \tTraining Loss: 0.000110\n",
      "Epoch: 6 \tTraining Loss: 0.000110\n",
      "Epoch: 7 \tTraining Loss: 0.000110\n",
      "Epoch: 8 \tTraining Loss: 0.000110\n",
      "Epoch: 9 \tTraining Loss: 0.000110\n",
      "Epoch: 10 \tTraining Loss: 0.000110\n",
      "Epoch: 11 \tTraining Loss: 0.000110\n",
      "Epoch: 12 \tTraining Loss: 0.000110\n",
      "Epoch: 13 \tTraining Loss: 0.000110\n",
      "Epoch: 14 \tTraining Loss: 0.000110\n",
      "Epoch: 15 \tTraining Loss: 0.000110\n",
      "Epoch: 16 \tTraining Loss: 0.000110\n",
      "Epoch: 17 \tTraining Loss: 0.000110\n",
      "Epoch: 18 \tTraining Loss: 0.000110\n",
      "Epoch: 1 \tTraining Loss: 0.000228\n",
      "Epoch: 2 \tTraining Loss: 0.000228\n",
      "Epoch: 3 \tTraining Loss: 0.000228\n",
      "Epoch: 4 \tTraining Loss: 0.000228\n",
      "Epoch: 5 \tTraining Loss: 0.000228\n",
      "Epoch: 6 \tTraining Loss: 0.000228\n",
      "Epoch: 7 \tTraining Loss: 0.000228\n",
      "Epoch: 8 \tTraining Loss: 0.000228\n",
      "Epoch: 9 \tTraining Loss: 0.000228\n",
      "Epoch: 10 \tTraining Loss: 0.000228\n",
      "Epoch: 11 \tTraining Loss: 0.000228\n",
      "Epoch: 12 \tTraining Loss: 0.000228\n",
      "Epoch: 13 \tTraining Loss: 0.000228\n",
      "Epoch: 14 \tTraining Loss: 0.000228\n",
      "Epoch: 15 \tTraining Loss: 0.000228\n",
      "Epoch: 16 \tTraining Loss: 0.000228\n",
      "Epoch: 17 \tTraining Loss: 0.000228\n",
      "Epoch: 18 \tTraining Loss: 0.000228\n",
      "Epoch: 1 \tTraining Loss: 0.000151\n",
      "Epoch: 2 \tTraining Loss: 0.000151\n",
      "Epoch: 3 \tTraining Loss: 0.000151\n",
      "Epoch: 4 \tTraining Loss: 0.000151\n",
      "Epoch: 5 \tTraining Loss: 0.000151\n",
      "Epoch: 6 \tTraining Loss: 0.000151\n",
      "Epoch: 7 \tTraining Loss: 0.000151\n",
      "Epoch: 8 \tTraining Loss: 0.000151\n",
      "Epoch: 9 \tTraining Loss: 0.000151\n",
      "Epoch: 10 \tTraining Loss: 0.000151\n",
      "Epoch: 11 \tTraining Loss: 0.000151\n",
      "Epoch: 12 \tTraining Loss: 0.000151\n",
      "Epoch: 13 \tTraining Loss: 0.000151\n",
      "Epoch: 14 \tTraining Loss: 0.000151\n",
      "Epoch: 15 \tTraining Loss: 0.000151\n",
      "Epoch: 16 \tTraining Loss: 0.000151\n",
      "Epoch: 17 \tTraining Loss: 0.000151\n",
      "Epoch: 18 \tTraining Loss: 0.000151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000128\n",
      "Epoch: 2 \tTraining Loss: 0.000128\n",
      "Epoch: 3 \tTraining Loss: 0.000128\n",
      "Epoch: 4 \tTraining Loss: 0.000128\n",
      "Epoch: 5 \tTraining Loss: 0.000128\n",
      "Epoch: 6 \tTraining Loss: 0.000128\n",
      "Epoch: 7 \tTraining Loss: 0.000128\n",
      "Epoch: 8 \tTraining Loss: 0.000128\n",
      "Epoch: 9 \tTraining Loss: 0.000128\n",
      "Epoch: 10 \tTraining Loss: 0.000128\n",
      "Epoch: 11 \tTraining Loss: 0.000128\n",
      "Epoch: 12 \tTraining Loss: 0.000128\n",
      "Epoch: 13 \tTraining Loss: 0.000128\n",
      "Epoch: 14 \tTraining Loss: 0.000128\n",
      "Epoch: 15 \tTraining Loss: 0.000128\n",
      "Epoch: 16 \tTraining Loss: 0.000128\n",
      "Epoch: 17 \tTraining Loss: 0.000128\n",
      "Epoch: 18 \tTraining Loss: 0.000128\n",
      "Epoch: 1 \tTraining Loss: 0.000412\n",
      "Epoch: 2 \tTraining Loss: 0.000412\n",
      "Epoch: 3 \tTraining Loss: 0.000412\n",
      "Epoch: 4 \tTraining Loss: 0.000412\n",
      "Epoch: 5 \tTraining Loss: 0.000412\n",
      "Epoch: 6 \tTraining Loss: 0.000412\n",
      "Epoch: 7 \tTraining Loss: 0.000412\n",
      "Epoch: 8 \tTraining Loss: 0.000412\n",
      "Epoch: 9 \tTraining Loss: 0.000412\n",
      "Epoch: 10 \tTraining Loss: 0.000412\n",
      "Epoch: 11 \tTraining Loss: 0.000412\n",
      "Epoch: 12 \tTraining Loss: 0.000412\n",
      "Epoch: 13 \tTraining Loss: 0.000412\n",
      "Epoch: 14 \tTraining Loss: 0.000412\n",
      "Epoch: 15 \tTraining Loss: 0.000412\n",
      "Epoch: 16 \tTraining Loss: 0.000412\n",
      "Epoch: 17 \tTraining Loss: 0.000412\n",
      "Epoch: 18 \tTraining Loss: 0.000412\n",
      "Epoch: 1 \tTraining Loss: 0.000319\n",
      "Epoch: 2 \tTraining Loss: 0.000319\n",
      "Epoch: 3 \tTraining Loss: 0.000319\n",
      "Epoch: 4 \tTraining Loss: 0.000319\n",
      "Epoch: 5 \tTraining Loss: 0.000319\n",
      "Epoch: 6 \tTraining Loss: 0.000319\n",
      "Epoch: 7 \tTraining Loss: 0.000319\n",
      "Epoch: 8 \tTraining Loss: 0.000319\n",
      "Epoch: 9 \tTraining Loss: 0.000319\n",
      "Epoch: 10 \tTraining Loss: 0.000319\n",
      "Epoch: 11 \tTraining Loss: 0.000319\n",
      "Epoch: 12 \tTraining Loss: 0.000319\n",
      "Epoch: 13 \tTraining Loss: 0.000319\n",
      "Epoch: 14 \tTraining Loss: 0.000319\n",
      "Epoch: 15 \tTraining Loss: 0.000319\n",
      "Epoch: 16 \tTraining Loss: 0.000319\n",
      "Epoch: 17 \tTraining Loss: 0.000319\n",
      "Epoch: 18 \tTraining Loss: 0.000319\n",
      "Epoch: 1 \tTraining Loss: 0.000383\n",
      "Epoch: 2 \tTraining Loss: 0.000383\n",
      "Epoch: 3 \tTraining Loss: 0.000383\n",
      "Epoch: 4 \tTraining Loss: 0.000383\n",
      "Epoch: 5 \tTraining Loss: 0.000383\n",
      "Epoch: 6 \tTraining Loss: 0.000383\n",
      "Epoch: 7 \tTraining Loss: 0.000383\n",
      "Epoch: 8 \tTraining Loss: 0.000383\n",
      "Epoch: 9 \tTraining Loss: 0.000383\n",
      "Epoch: 10 \tTraining Loss: 0.000383\n",
      "Epoch: 11 \tTraining Loss: 0.000383\n",
      "Epoch: 12 \tTraining Loss: 0.000383\n",
      "Epoch: 13 \tTraining Loss: 0.000383\n",
      "Epoch: 14 \tTraining Loss: 0.000383\n",
      "Epoch: 15 \tTraining Loss: 0.000383\n",
      "Epoch: 16 \tTraining Loss: 0.000383\n",
      "Epoch: 17 \tTraining Loss: 0.000383\n",
      "Epoch: 18 \tTraining Loss: 0.000383\n",
      "Epoch: 1 \tTraining Loss: 0.000234\n",
      "Epoch: 2 \tTraining Loss: 0.000234\n",
      "Epoch: 3 \tTraining Loss: 0.000234\n",
      "Epoch: 4 \tTraining Loss: 0.000234\n",
      "Epoch: 5 \tTraining Loss: 0.000234\n",
      "Epoch: 6 \tTraining Loss: 0.000234\n",
      "Epoch: 7 \tTraining Loss: 0.000234\n",
      "Epoch: 8 \tTraining Loss: 0.000234\n",
      "Epoch: 9 \tTraining Loss: 0.000234\n",
      "Epoch: 10 \tTraining Loss: 0.000234\n",
      "Epoch: 11 \tTraining Loss: 0.000234\n",
      "Epoch: 12 \tTraining Loss: 0.000234\n",
      "Epoch: 13 \tTraining Loss: 0.000234\n",
      "Epoch: 14 \tTraining Loss: 0.000234\n",
      "Epoch: 15 \tTraining Loss: 0.000234\n",
      "Epoch: 16 \tTraining Loss: 0.000234\n",
      "Epoch: 17 \tTraining Loss: 0.000234\n",
      "Epoch: 18 \tTraining Loss: 0.000234\n",
      "Epoch: 1 \tTraining Loss: 0.000269\n",
      "Epoch: 2 \tTraining Loss: 0.000269\n",
      "Epoch: 3 \tTraining Loss: 0.000269\n",
      "Epoch: 4 \tTraining Loss: 0.000269\n",
      "Epoch: 5 \tTraining Loss: 0.000269\n",
      "Epoch: 6 \tTraining Loss: 0.000269\n",
      "Epoch: 7 \tTraining Loss: 0.000269\n",
      "Epoch: 8 \tTraining Loss: 0.000269\n",
      "Epoch: 9 \tTraining Loss: 0.000269\n",
      "Epoch: 10 \tTraining Loss: 0.000269\n",
      "Epoch: 11 \tTraining Loss: 0.000269\n",
      "Epoch: 12 \tTraining Loss: 0.000269\n",
      "Epoch: 13 \tTraining Loss: 0.000269\n",
      "Epoch: 14 \tTraining Loss: 0.000269\n",
      "Epoch: 15 \tTraining Loss: 0.000269\n",
      "Epoch: 16 \tTraining Loss: 0.000269\n",
      "Epoch: 17 \tTraining Loss: 0.000269\n",
      "Epoch: 18 \tTraining Loss: 0.000269\n",
      "Epoch: 1 \tTraining Loss: 0.000191\n",
      "Epoch: 2 \tTraining Loss: 0.000191\n",
      "Epoch: 3 \tTraining Loss: 0.000191\n",
      "Epoch: 4 \tTraining Loss: 0.000191\n",
      "Epoch: 5 \tTraining Loss: 0.000191\n",
      "Epoch: 6 \tTraining Loss: 0.000191\n",
      "Epoch: 7 \tTraining Loss: 0.000191\n",
      "Epoch: 8 \tTraining Loss: 0.000191\n",
      "Epoch: 9 \tTraining Loss: 0.000191\n",
      "Epoch: 10 \tTraining Loss: 0.000191\n",
      "Epoch: 11 \tTraining Loss: 0.000191\n",
      "Epoch: 12 \tTraining Loss: 0.000191\n",
      "Epoch: 13 \tTraining Loss: 0.000191\n",
      "Epoch: 14 \tTraining Loss: 0.000191\n",
      "Epoch: 15 \tTraining Loss: 0.000191\n",
      "Epoch: 16 \tTraining Loss: 0.000191\n",
      "Epoch: 17 \tTraining Loss: 0.000191\n",
      "Epoch: 18 \tTraining Loss: 0.000191\n",
      "Epoch: 1 \tTraining Loss: 0.000118\n",
      "Epoch: 2 \tTraining Loss: 0.000118\n",
      "Epoch: 3 \tTraining Loss: 0.000118\n",
      "Epoch: 4 \tTraining Loss: 0.000118\n",
      "Epoch: 5 \tTraining Loss: 0.000118\n",
      "Epoch: 6 \tTraining Loss: 0.000118\n",
      "Epoch: 7 \tTraining Loss: 0.000118\n",
      "Epoch: 8 \tTraining Loss: 0.000118\n",
      "Epoch: 9 \tTraining Loss: 0.000118\n",
      "Epoch: 10 \tTraining Loss: 0.000118\n",
      "Epoch: 11 \tTraining Loss: 0.000118\n",
      "Epoch: 12 \tTraining Loss: 0.000118\n",
      "Epoch: 13 \tTraining Loss: 0.000118\n",
      "Epoch: 14 \tTraining Loss: 0.000118\n",
      "Epoch: 15 \tTraining Loss: 0.000118\n",
      "Epoch: 16 \tTraining Loss: 0.000118\n",
      "Epoch: 17 \tTraining Loss: 0.000118\n",
      "Epoch: 18 \tTraining Loss: 0.000118\n",
      "Epoch: 1 \tTraining Loss: 0.000246\n",
      "Epoch: 2 \tTraining Loss: 0.000246\n",
      "Epoch: 3 \tTraining Loss: 0.000246\n",
      "Epoch: 4 \tTraining Loss: 0.000246\n",
      "Epoch: 5 \tTraining Loss: 0.000246\n",
      "Epoch: 6 \tTraining Loss: 0.000246\n",
      "Epoch: 7 \tTraining Loss: 0.000246\n",
      "Epoch: 8 \tTraining Loss: 0.000246\n",
      "Epoch: 9 \tTraining Loss: 0.000246\n",
      "Epoch: 10 \tTraining Loss: 0.000246\n",
      "Epoch: 11 \tTraining Loss: 0.000246\n",
      "Epoch: 12 \tTraining Loss: 0.000246\n",
      "Epoch: 13 \tTraining Loss: 0.000246\n",
      "Epoch: 14 \tTraining Loss: 0.000246\n",
      "Epoch: 15 \tTraining Loss: 0.000246\n",
      "Epoch: 16 \tTraining Loss: 0.000246\n",
      "Epoch: 17 \tTraining Loss: 0.000246\n",
      "Epoch: 18 \tTraining Loss: 0.000246\n",
      "Epoch: 1 \tTraining Loss: 0.000214\n",
      "Epoch: 2 \tTraining Loss: 0.000214\n",
      "Epoch: 3 \tTraining Loss: 0.000214\n",
      "Epoch: 4 \tTraining Loss: 0.000214\n",
      "Epoch: 5 \tTraining Loss: 0.000214\n",
      "Epoch: 6 \tTraining Loss: 0.000214\n",
      "Epoch: 7 \tTraining Loss: 0.000214\n",
      "Epoch: 8 \tTraining Loss: 0.000214\n",
      "Epoch: 9 \tTraining Loss: 0.000214\n",
      "Epoch: 10 \tTraining Loss: 0.000214\n",
      "Epoch: 11 \tTraining Loss: 0.000214\n",
      "Epoch: 12 \tTraining Loss: 0.000214\n",
      "Epoch: 13 \tTraining Loss: 0.000214\n",
      "Epoch: 14 \tTraining Loss: 0.000214\n",
      "Epoch: 15 \tTraining Loss: 0.000214\n",
      "Epoch: 16 \tTraining Loss: 0.000214\n",
      "Epoch: 17 \tTraining Loss: 0.000214\n",
      "Epoch: 18 \tTraining Loss: 0.000214\n",
      "Epoch: 1 \tTraining Loss: 0.000189\n",
      "Epoch: 2 \tTraining Loss: 0.000189\n",
      "Epoch: 3 \tTraining Loss: 0.000189\n",
      "Epoch: 4 \tTraining Loss: 0.000189\n",
      "Epoch: 5 \tTraining Loss: 0.000189\n",
      "Epoch: 6 \tTraining Loss: 0.000189\n",
      "Epoch: 7 \tTraining Loss: 0.000189\n",
      "Epoch: 8 \tTraining Loss: 0.000189\n",
      "Epoch: 9 \tTraining Loss: 0.000189\n",
      "Epoch: 10 \tTraining Loss: 0.000189\n",
      "Epoch: 11 \tTraining Loss: 0.000189\n",
      "Epoch: 12 \tTraining Loss: 0.000189\n",
      "Epoch: 13 \tTraining Loss: 0.000189\n",
      "Epoch: 14 \tTraining Loss: 0.000189\n",
      "Epoch: 15 \tTraining Loss: 0.000189\n",
      "Epoch: 16 \tTraining Loss: 0.000189\n",
      "Epoch: 17 \tTraining Loss: 0.000189\n",
      "Epoch: 18 \tTraining Loss: 0.000189\n",
      "Epoch: 1 \tTraining Loss: 0.000378\n",
      "Epoch: 2 \tTraining Loss: 0.000378\n",
      "Epoch: 3 \tTraining Loss: 0.000378\n",
      "Epoch: 4 \tTraining Loss: 0.000378\n",
      "Epoch: 5 \tTraining Loss: 0.000378\n",
      "Epoch: 6 \tTraining Loss: 0.000378\n",
      "Epoch: 7 \tTraining Loss: 0.000378\n",
      "Epoch: 8 \tTraining Loss: 0.000378\n",
      "Epoch: 9 \tTraining Loss: 0.000378\n",
      "Epoch: 10 \tTraining Loss: 0.000378\n",
      "Epoch: 11 \tTraining Loss: 0.000378\n",
      "Epoch: 12 \tTraining Loss: 0.000378\n",
      "Epoch: 13 \tTraining Loss: 0.000378\n",
      "Epoch: 14 \tTraining Loss: 0.000378\n",
      "Epoch: 15 \tTraining Loss: 0.000378\n",
      "Epoch: 16 \tTraining Loss: 0.000378\n",
      "Epoch: 17 \tTraining Loss: 0.000378\n",
      "Epoch: 18 \tTraining Loss: 0.000378\n",
      "Epoch: 1 \tTraining Loss: 0.000116\n",
      "Epoch: 2 \tTraining Loss: 0.000116\n",
      "Epoch: 3 \tTraining Loss: 0.000116\n",
      "Epoch: 4 \tTraining Loss: 0.000116\n",
      "Epoch: 5 \tTraining Loss: 0.000116\n",
      "Epoch: 6 \tTraining Loss: 0.000116\n",
      "Epoch: 7 \tTraining Loss: 0.000116\n",
      "Epoch: 8 \tTraining Loss: 0.000116\n",
      "Epoch: 9 \tTraining Loss: 0.000116\n",
      "Epoch: 10 \tTraining Loss: 0.000116\n",
      "Epoch: 11 \tTraining Loss: 0.000116\n",
      "Epoch: 12 \tTraining Loss: 0.000116\n",
      "Epoch: 13 \tTraining Loss: 0.000116\n",
      "Epoch: 14 \tTraining Loss: 0.000116\n",
      "Epoch: 15 \tTraining Loss: 0.000116\n",
      "Epoch: 16 \tTraining Loss: 0.000116\n",
      "Epoch: 17 \tTraining Loss: 0.000116\n",
      "Epoch: 18 \tTraining Loss: 0.000116\n",
      "Epoch: 1 \tTraining Loss: 0.000131\n",
      "Epoch: 2 \tTraining Loss: 0.000131\n",
      "Epoch: 3 \tTraining Loss: 0.000131\n",
      "Epoch: 4 \tTraining Loss: 0.000131\n",
      "Epoch: 5 \tTraining Loss: 0.000131\n",
      "Epoch: 6 \tTraining Loss: 0.000131\n",
      "Epoch: 7 \tTraining Loss: 0.000131\n",
      "Epoch: 8 \tTraining Loss: 0.000131\n",
      "Epoch: 9 \tTraining Loss: 0.000131\n",
      "Epoch: 10 \tTraining Loss: 0.000131\n",
      "Epoch: 11 \tTraining Loss: 0.000131\n",
      "Epoch: 12 \tTraining Loss: 0.000131\n",
      "Epoch: 13 \tTraining Loss: 0.000131\n",
      "Epoch: 14 \tTraining Loss: 0.000131\n",
      "Epoch: 15 \tTraining Loss: 0.000131\n",
      "Epoch: 16 \tTraining Loss: 0.000131\n",
      "Epoch: 17 \tTraining Loss: 0.000131\n",
      "Epoch: 18 \tTraining Loss: 0.000131\n",
      "Epoch: 1 \tTraining Loss: 0.000146\n",
      "Epoch: 2 \tTraining Loss: 0.000146\n",
      "Epoch: 3 \tTraining Loss: 0.000146\n",
      "Epoch: 4 \tTraining Loss: 0.000146\n",
      "Epoch: 5 \tTraining Loss: 0.000146\n",
      "Epoch: 6 \tTraining Loss: 0.000146\n",
      "Epoch: 7 \tTraining Loss: 0.000146\n",
      "Epoch: 8 \tTraining Loss: 0.000146\n",
      "Epoch: 9 \tTraining Loss: 0.000146\n",
      "Epoch: 10 \tTraining Loss: 0.000146\n",
      "Epoch: 11 \tTraining Loss: 0.000146\n",
      "Epoch: 12 \tTraining Loss: 0.000146\n",
      "Epoch: 13 \tTraining Loss: 0.000146\n",
      "Epoch: 14 \tTraining Loss: 0.000146\n",
      "Epoch: 15 \tTraining Loss: 0.000146\n",
      "Epoch: 16 \tTraining Loss: 0.000146\n",
      "Epoch: 17 \tTraining Loss: 0.000146\n",
      "Epoch: 18 \tTraining Loss: 0.000146\n",
      "Epoch: 1 \tTraining Loss: 0.000253\n",
      "Epoch: 2 \tTraining Loss: 0.000253\n",
      "Epoch: 3 \tTraining Loss: 0.000253\n",
      "Epoch: 4 \tTraining Loss: 0.000253\n",
      "Epoch: 5 \tTraining Loss: 0.000253\n",
      "Epoch: 6 \tTraining Loss: 0.000253\n",
      "Epoch: 7 \tTraining Loss: 0.000253\n",
      "Epoch: 8 \tTraining Loss: 0.000253\n",
      "Epoch: 9 \tTraining Loss: 0.000253\n",
      "Epoch: 10 \tTraining Loss: 0.000253\n",
      "Epoch: 11 \tTraining Loss: 0.000253\n",
      "Epoch: 12 \tTraining Loss: 0.000253\n",
      "Epoch: 13 \tTraining Loss: 0.000253\n",
      "Epoch: 14 \tTraining Loss: 0.000253\n",
      "Epoch: 15 \tTraining Loss: 0.000253\n",
      "Epoch: 16 \tTraining Loss: 0.000253\n",
      "Epoch: 17 \tTraining Loss: 0.000253\n",
      "Epoch: 18 \tTraining Loss: 0.000253\n",
      "Epoch: 1 \tTraining Loss: 0.000075\n",
      "Epoch: 2 \tTraining Loss: 0.000075\n",
      "Epoch: 3 \tTraining Loss: 0.000075\n",
      "Epoch: 4 \tTraining Loss: 0.000075\n",
      "Epoch: 5 \tTraining Loss: 0.000075\n",
      "Epoch: 6 \tTraining Loss: 0.000075\n",
      "Epoch: 7 \tTraining Loss: 0.000075\n",
      "Epoch: 8 \tTraining Loss: 0.000075\n",
      "Epoch: 9 \tTraining Loss: 0.000075\n",
      "Epoch: 10 \tTraining Loss: 0.000075\n",
      "Epoch: 11 \tTraining Loss: 0.000075\n",
      "Epoch: 12 \tTraining Loss: 0.000075\n",
      "Epoch: 13 \tTraining Loss: 0.000075\n",
      "Epoch: 14 \tTraining Loss: 0.000075\n",
      "Epoch: 15 \tTraining Loss: 0.000075\n",
      "Epoch: 16 \tTraining Loss: 0.000075\n",
      "Epoch: 17 \tTraining Loss: 0.000075\n",
      "Epoch: 18 \tTraining Loss: 0.000075\n",
      "Epoch: 1 \tTraining Loss: 0.000126\n",
      "Epoch: 2 \tTraining Loss: 0.000126\n",
      "Epoch: 3 \tTraining Loss: 0.000126\n",
      "Epoch: 4 \tTraining Loss: 0.000126\n",
      "Epoch: 5 \tTraining Loss: 0.000126\n",
      "Epoch: 6 \tTraining Loss: 0.000126\n",
      "Epoch: 7 \tTraining Loss: 0.000126\n",
      "Epoch: 8 \tTraining Loss: 0.000126\n",
      "Epoch: 9 \tTraining Loss: 0.000126\n",
      "Epoch: 10 \tTraining Loss: 0.000126\n",
      "Epoch: 11 \tTraining Loss: 0.000126\n",
      "Epoch: 12 \tTraining Loss: 0.000126\n",
      "Epoch: 13 \tTraining Loss: 0.000126\n",
      "Epoch: 14 \tTraining Loss: 0.000126\n",
      "Epoch: 15 \tTraining Loss: 0.000126\n",
      "Epoch: 16 \tTraining Loss: 0.000126\n",
      "Epoch: 17 \tTraining Loss: 0.000126\n",
      "Epoch: 18 \tTraining Loss: 0.000126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000134\n",
      "Epoch: 2 \tTraining Loss: 0.000134\n",
      "Epoch: 3 \tTraining Loss: 0.000134\n",
      "Epoch: 4 \tTraining Loss: 0.000134\n",
      "Epoch: 5 \tTraining Loss: 0.000134\n",
      "Epoch: 6 \tTraining Loss: 0.000134\n",
      "Epoch: 7 \tTraining Loss: 0.000134\n",
      "Epoch: 8 \tTraining Loss: 0.000134\n",
      "Epoch: 9 \tTraining Loss: 0.000134\n",
      "Epoch: 10 \tTraining Loss: 0.000134\n",
      "Epoch: 11 \tTraining Loss: 0.000134\n",
      "Epoch: 12 \tTraining Loss: 0.000134\n",
      "Epoch: 13 \tTraining Loss: 0.000134\n",
      "Epoch: 14 \tTraining Loss: 0.000134\n",
      "Epoch: 15 \tTraining Loss: 0.000134\n",
      "Epoch: 16 \tTraining Loss: 0.000134\n",
      "Epoch: 17 \tTraining Loss: 0.000134\n",
      "Epoch: 18 \tTraining Loss: 0.000134\n",
      "Epoch: 1 \tTraining Loss: 0.000258\n",
      "Epoch: 2 \tTraining Loss: 0.000258\n",
      "Epoch: 3 \tTraining Loss: 0.000258\n",
      "Epoch: 4 \tTraining Loss: 0.000258\n",
      "Epoch: 5 \tTraining Loss: 0.000258\n",
      "Epoch: 6 \tTraining Loss: 0.000258\n",
      "Epoch: 7 \tTraining Loss: 0.000258\n",
      "Epoch: 8 \tTraining Loss: 0.000258\n",
      "Epoch: 9 \tTraining Loss: 0.000258\n",
      "Epoch: 10 \tTraining Loss: 0.000258\n",
      "Epoch: 11 \tTraining Loss: 0.000258\n",
      "Epoch: 12 \tTraining Loss: 0.000258\n",
      "Epoch: 13 \tTraining Loss: 0.000258\n",
      "Epoch: 14 \tTraining Loss: 0.000258\n",
      "Epoch: 15 \tTraining Loss: 0.000258\n",
      "Epoch: 16 \tTraining Loss: 0.000258\n",
      "Epoch: 17 \tTraining Loss: 0.000258\n",
      "Epoch: 18 \tTraining Loss: 0.000258\n",
      "Epoch: 1 \tTraining Loss: 0.000371\n",
      "Epoch: 2 \tTraining Loss: 0.000371\n",
      "Epoch: 3 \tTraining Loss: 0.000371\n",
      "Epoch: 4 \tTraining Loss: 0.000371\n",
      "Epoch: 5 \tTraining Loss: 0.000371\n",
      "Epoch: 6 \tTraining Loss: 0.000371\n",
      "Epoch: 7 \tTraining Loss: 0.000371\n",
      "Epoch: 8 \tTraining Loss: 0.000371\n",
      "Epoch: 9 \tTraining Loss: 0.000371\n",
      "Epoch: 10 \tTraining Loss: 0.000371\n",
      "Epoch: 11 \tTraining Loss: 0.000371\n",
      "Epoch: 12 \tTraining Loss: 0.000371\n",
      "Epoch: 13 \tTraining Loss: 0.000371\n",
      "Epoch: 14 \tTraining Loss: 0.000371\n",
      "Epoch: 15 \tTraining Loss: 0.000371\n",
      "Epoch: 16 \tTraining Loss: 0.000371\n",
      "Epoch: 17 \tTraining Loss: 0.000371\n",
      "Epoch: 18 \tTraining Loss: 0.000371\n",
      "Epoch: 1 \tTraining Loss: 0.000083\n",
      "Epoch: 2 \tTraining Loss: 0.000083\n",
      "Epoch: 3 \tTraining Loss: 0.000083\n",
      "Epoch: 4 \tTraining Loss: 0.000083\n",
      "Epoch: 5 \tTraining Loss: 0.000083\n",
      "Epoch: 6 \tTraining Loss: 0.000083\n",
      "Epoch: 7 \tTraining Loss: 0.000083\n",
      "Epoch: 8 \tTraining Loss: 0.000083\n",
      "Epoch: 9 \tTraining Loss: 0.000083\n",
      "Epoch: 10 \tTraining Loss: 0.000083\n",
      "Epoch: 11 \tTraining Loss: 0.000083\n",
      "Epoch: 12 \tTraining Loss: 0.000083\n",
      "Epoch: 13 \tTraining Loss: 0.000083\n",
      "Epoch: 14 \tTraining Loss: 0.000083\n",
      "Epoch: 15 \tTraining Loss: 0.000083\n",
      "Epoch: 16 \tTraining Loss: 0.000083\n",
      "Epoch: 17 \tTraining Loss: 0.000083\n",
      "Epoch: 18 \tTraining Loss: 0.000083\n",
      "Epoch: 1 \tTraining Loss: 0.000216\n",
      "Epoch: 2 \tTraining Loss: 0.000216\n",
      "Epoch: 3 \tTraining Loss: 0.000216\n",
      "Epoch: 4 \tTraining Loss: 0.000216\n",
      "Epoch: 5 \tTraining Loss: 0.000216\n",
      "Epoch: 6 \tTraining Loss: 0.000216\n",
      "Epoch: 7 \tTraining Loss: 0.000216\n",
      "Epoch: 8 \tTraining Loss: 0.000216\n",
      "Epoch: 9 \tTraining Loss: 0.000216\n",
      "Epoch: 10 \tTraining Loss: 0.000216\n",
      "Epoch: 11 \tTraining Loss: 0.000216\n",
      "Epoch: 12 \tTraining Loss: 0.000216\n",
      "Epoch: 13 \tTraining Loss: 0.000216\n",
      "Epoch: 14 \tTraining Loss: 0.000216\n",
      "Epoch: 15 \tTraining Loss: 0.000216\n",
      "Epoch: 16 \tTraining Loss: 0.000216\n",
      "Epoch: 17 \tTraining Loss: 0.000216\n",
      "Epoch: 18 \tTraining Loss: 0.000216\n",
      "Epoch: 1 \tTraining Loss: 0.000166\n",
      "Epoch: 2 \tTraining Loss: 0.000166\n",
      "Epoch: 3 \tTraining Loss: 0.000166\n",
      "Epoch: 4 \tTraining Loss: 0.000166\n",
      "Epoch: 5 \tTraining Loss: 0.000166\n",
      "Epoch: 6 \tTraining Loss: 0.000166\n",
      "Epoch: 7 \tTraining Loss: 0.000166\n",
      "Epoch: 8 \tTraining Loss: 0.000166\n",
      "Epoch: 9 \tTraining Loss: 0.000166\n",
      "Epoch: 10 \tTraining Loss: 0.000166\n",
      "Epoch: 11 \tTraining Loss: 0.000166\n",
      "Epoch: 12 \tTraining Loss: 0.000166\n",
      "Epoch: 13 \tTraining Loss: 0.000166\n",
      "Epoch: 14 \tTraining Loss: 0.000166\n",
      "Epoch: 15 \tTraining Loss: 0.000166\n",
      "Epoch: 16 \tTraining Loss: 0.000166\n",
      "Epoch: 17 \tTraining Loss: 0.000166\n",
      "Epoch: 18 \tTraining Loss: 0.000166\n",
      "Epoch: 1 \tTraining Loss: 0.000286\n",
      "Epoch: 2 \tTraining Loss: 0.000286\n",
      "Epoch: 3 \tTraining Loss: 0.000286\n",
      "Epoch: 4 \tTraining Loss: 0.000286\n",
      "Epoch: 5 \tTraining Loss: 0.000286\n",
      "Epoch: 6 \tTraining Loss: 0.000286\n",
      "Epoch: 7 \tTraining Loss: 0.000286\n",
      "Epoch: 8 \tTraining Loss: 0.000286\n",
      "Epoch: 9 \tTraining Loss: 0.000286\n",
      "Epoch: 10 \tTraining Loss: 0.000286\n",
      "Epoch: 11 \tTraining Loss: 0.000286\n",
      "Epoch: 12 \tTraining Loss: 0.000286\n",
      "Epoch: 13 \tTraining Loss: 0.000286\n",
      "Epoch: 14 \tTraining Loss: 0.000286\n",
      "Epoch: 15 \tTraining Loss: 0.000286\n",
      "Epoch: 16 \tTraining Loss: 0.000286\n",
      "Epoch: 17 \tTraining Loss: 0.000286\n",
      "Epoch: 18 \tTraining Loss: 0.000286\n",
      "Epoch: 1 \tTraining Loss: 0.000217\n",
      "Epoch: 2 \tTraining Loss: 0.000217\n",
      "Epoch: 3 \tTraining Loss: 0.000217\n",
      "Epoch: 4 \tTraining Loss: 0.000217\n",
      "Epoch: 5 \tTraining Loss: 0.000217\n",
      "Epoch: 6 \tTraining Loss: 0.000217\n",
      "Epoch: 7 \tTraining Loss: 0.000217\n",
      "Epoch: 8 \tTraining Loss: 0.000217\n",
      "Epoch: 9 \tTraining Loss: 0.000217\n",
      "Epoch: 10 \tTraining Loss: 0.000217\n",
      "Epoch: 11 \tTraining Loss: 0.000217\n",
      "Epoch: 12 \tTraining Loss: 0.000217\n",
      "Epoch: 13 \tTraining Loss: 0.000217\n",
      "Epoch: 14 \tTraining Loss: 0.000217\n",
      "Epoch: 15 \tTraining Loss: 0.000217\n",
      "Epoch: 16 \tTraining Loss: 0.000217\n",
      "Epoch: 17 \tTraining Loss: 0.000217\n",
      "Epoch: 18 \tTraining Loss: 0.000217\n",
      "Epoch: 1 \tTraining Loss: 0.000254\n",
      "Epoch: 2 \tTraining Loss: 0.000254\n",
      "Epoch: 3 \tTraining Loss: 0.000254\n",
      "Epoch: 4 \tTraining Loss: 0.000254\n",
      "Epoch: 5 \tTraining Loss: 0.000254\n",
      "Epoch: 6 \tTraining Loss: 0.000254\n",
      "Epoch: 7 \tTraining Loss: 0.000254\n",
      "Epoch: 8 \tTraining Loss: 0.000254\n",
      "Epoch: 9 \tTraining Loss: 0.000254\n",
      "Epoch: 10 \tTraining Loss: 0.000254\n",
      "Epoch: 11 \tTraining Loss: 0.000254\n",
      "Epoch: 12 \tTraining Loss: 0.000254\n",
      "Epoch: 13 \tTraining Loss: 0.000254\n",
      "Epoch: 14 \tTraining Loss: 0.000254\n",
      "Epoch: 15 \tTraining Loss: 0.000254\n",
      "Epoch: 16 \tTraining Loss: 0.000254\n",
      "Epoch: 17 \tTraining Loss: 0.000254\n",
      "Epoch: 18 \tTraining Loss: 0.000254\n",
      "Epoch: 1 \tTraining Loss: 0.000243\n",
      "Epoch: 2 \tTraining Loss: 0.000243\n",
      "Epoch: 3 \tTraining Loss: 0.000243\n",
      "Epoch: 4 \tTraining Loss: 0.000243\n",
      "Epoch: 5 \tTraining Loss: 0.000243\n",
      "Epoch: 6 \tTraining Loss: 0.000243\n",
      "Epoch: 7 \tTraining Loss: 0.000243\n",
      "Epoch: 8 \tTraining Loss: 0.000243\n",
      "Epoch: 9 \tTraining Loss: 0.000243\n",
      "Epoch: 10 \tTraining Loss: 0.000243\n",
      "Epoch: 11 \tTraining Loss: 0.000243\n",
      "Epoch: 12 \tTraining Loss: 0.000243\n",
      "Epoch: 13 \tTraining Loss: 0.000243\n",
      "Epoch: 14 \tTraining Loss: 0.000243\n",
      "Epoch: 15 \tTraining Loss: 0.000243\n",
      "Epoch: 16 \tTraining Loss: 0.000243\n",
      "Epoch: 17 \tTraining Loss: 0.000243\n",
      "Epoch: 18 \tTraining Loss: 0.000243\n",
      "Epoch: 1 \tTraining Loss: 0.000164\n",
      "Epoch: 2 \tTraining Loss: 0.000164\n",
      "Epoch: 3 \tTraining Loss: 0.000164\n",
      "Epoch: 4 \tTraining Loss: 0.000164\n",
      "Epoch: 5 \tTraining Loss: 0.000164\n",
      "Epoch: 6 \tTraining Loss: 0.000164\n",
      "Epoch: 7 \tTraining Loss: 0.000164\n",
      "Epoch: 8 \tTraining Loss: 0.000164\n",
      "Epoch: 9 \tTraining Loss: 0.000164\n",
      "Epoch: 10 \tTraining Loss: 0.000164\n",
      "Epoch: 11 \tTraining Loss: 0.000164\n",
      "Epoch: 12 \tTraining Loss: 0.000164\n",
      "Epoch: 13 \tTraining Loss: 0.000164\n",
      "Epoch: 14 \tTraining Loss: 0.000164\n",
      "Epoch: 15 \tTraining Loss: 0.000164\n",
      "Epoch: 16 \tTraining Loss: 0.000164\n",
      "Epoch: 17 \tTraining Loss: 0.000164\n",
      "Epoch: 18 \tTraining Loss: 0.000164\n",
      "Epoch: 1 \tTraining Loss: 0.000311\n",
      "Epoch: 2 \tTraining Loss: 0.000311\n",
      "Epoch: 3 \tTraining Loss: 0.000311\n",
      "Epoch: 4 \tTraining Loss: 0.000311\n",
      "Epoch: 5 \tTraining Loss: 0.000311\n",
      "Epoch: 6 \tTraining Loss: 0.000311\n",
      "Epoch: 7 \tTraining Loss: 0.000311\n",
      "Epoch: 8 \tTraining Loss: 0.000311\n",
      "Epoch: 9 \tTraining Loss: 0.000311\n",
      "Epoch: 10 \tTraining Loss: 0.000311\n",
      "Epoch: 11 \tTraining Loss: 0.000311\n",
      "Epoch: 12 \tTraining Loss: 0.000311\n",
      "Epoch: 13 \tTraining Loss: 0.000311\n",
      "Epoch: 14 \tTraining Loss: 0.000311\n",
      "Epoch: 15 \tTraining Loss: 0.000311\n",
      "Epoch: 16 \tTraining Loss: 0.000311\n",
      "Epoch: 17 \tTraining Loss: 0.000311\n",
      "Epoch: 18 \tTraining Loss: 0.000311\n",
      "Epoch: 1 \tTraining Loss: 0.000219\n",
      "Epoch: 2 \tTraining Loss: 0.000219\n",
      "Epoch: 3 \tTraining Loss: 0.000219\n",
      "Epoch: 4 \tTraining Loss: 0.000219\n",
      "Epoch: 5 \tTraining Loss: 0.000219\n",
      "Epoch: 6 \tTraining Loss: 0.000219\n",
      "Epoch: 7 \tTraining Loss: 0.000219\n",
      "Epoch: 8 \tTraining Loss: 0.000219\n",
      "Epoch: 9 \tTraining Loss: 0.000219\n",
      "Epoch: 10 \tTraining Loss: 0.000219\n",
      "Epoch: 11 \tTraining Loss: 0.000219\n",
      "Epoch: 12 \tTraining Loss: 0.000219\n",
      "Epoch: 13 \tTraining Loss: 0.000219\n",
      "Epoch: 14 \tTraining Loss: 0.000219\n",
      "Epoch: 15 \tTraining Loss: 0.000219\n",
      "Epoch: 16 \tTraining Loss: 0.000219\n",
      "Epoch: 17 \tTraining Loss: 0.000219\n",
      "Epoch: 18 \tTraining Loss: 0.000219\n",
      "Epoch: 1 \tTraining Loss: 0.000142\n",
      "Epoch: 2 \tTraining Loss: 0.000142\n",
      "Epoch: 3 \tTraining Loss: 0.000142\n",
      "Epoch: 4 \tTraining Loss: 0.000142\n",
      "Epoch: 5 \tTraining Loss: 0.000142\n",
      "Epoch: 6 \tTraining Loss: 0.000142\n",
      "Epoch: 7 \tTraining Loss: 0.000142\n",
      "Epoch: 8 \tTraining Loss: 0.000142\n",
      "Epoch: 9 \tTraining Loss: 0.000142\n",
      "Epoch: 10 \tTraining Loss: 0.000142\n",
      "Epoch: 11 \tTraining Loss: 0.000142\n",
      "Epoch: 12 \tTraining Loss: 0.000142\n",
      "Epoch: 13 \tTraining Loss: 0.000142\n",
      "Epoch: 14 \tTraining Loss: 0.000142\n",
      "Epoch: 15 \tTraining Loss: 0.000142\n",
      "Epoch: 16 \tTraining Loss: 0.000142\n",
      "Epoch: 17 \tTraining Loss: 0.000142\n",
      "Epoch: 18 \tTraining Loss: 0.000142\n",
      "Epoch: 1 \tTraining Loss: 0.000148\n",
      "Epoch: 2 \tTraining Loss: 0.000148\n",
      "Epoch: 3 \tTraining Loss: 0.000148\n",
      "Epoch: 4 \tTraining Loss: 0.000148\n",
      "Epoch: 5 \tTraining Loss: 0.000148\n",
      "Epoch: 6 \tTraining Loss: 0.000148\n",
      "Epoch: 7 \tTraining Loss: 0.000148\n",
      "Epoch: 8 \tTraining Loss: 0.000148\n",
      "Epoch: 9 \tTraining Loss: 0.000148\n",
      "Epoch: 10 \tTraining Loss: 0.000148\n",
      "Epoch: 11 \tTraining Loss: 0.000148\n",
      "Epoch: 12 \tTraining Loss: 0.000148\n",
      "Epoch: 13 \tTraining Loss: 0.000148\n",
      "Epoch: 14 \tTraining Loss: 0.000148\n",
      "Epoch: 15 \tTraining Loss: 0.000148\n",
      "Epoch: 16 \tTraining Loss: 0.000148\n",
      "Epoch: 17 \tTraining Loss: 0.000148\n",
      "Epoch: 18 \tTraining Loss: 0.000148\n",
      "Epoch: 1 \tTraining Loss: 0.000203\n",
      "Epoch: 2 \tTraining Loss: 0.000203\n",
      "Epoch: 3 \tTraining Loss: 0.000203\n",
      "Epoch: 4 \tTraining Loss: 0.000203\n",
      "Epoch: 5 \tTraining Loss: 0.000203\n",
      "Epoch: 6 \tTraining Loss: 0.000203\n",
      "Epoch: 7 \tTraining Loss: 0.000203\n",
      "Epoch: 8 \tTraining Loss: 0.000203\n",
      "Epoch: 9 \tTraining Loss: 0.000203\n",
      "Epoch: 10 \tTraining Loss: 0.000203\n",
      "Epoch: 11 \tTraining Loss: 0.000203\n",
      "Epoch: 12 \tTraining Loss: 0.000203\n",
      "Epoch: 13 \tTraining Loss: 0.000203\n",
      "Epoch: 14 \tTraining Loss: 0.000203\n",
      "Epoch: 15 \tTraining Loss: 0.000203\n",
      "Epoch: 16 \tTraining Loss: 0.000203\n",
      "Epoch: 17 \tTraining Loss: 0.000203\n",
      "Epoch: 18 \tTraining Loss: 0.000203\n",
      "Epoch: 1 \tTraining Loss: 0.000151\n",
      "Epoch: 2 \tTraining Loss: 0.000151\n",
      "Epoch: 3 \tTraining Loss: 0.000151\n",
      "Epoch: 4 \tTraining Loss: 0.000151\n",
      "Epoch: 5 \tTraining Loss: 0.000151\n",
      "Epoch: 6 \tTraining Loss: 0.000151\n",
      "Epoch: 7 \tTraining Loss: 0.000151\n",
      "Epoch: 8 \tTraining Loss: 0.000151\n",
      "Epoch: 9 \tTraining Loss: 0.000151\n",
      "Epoch: 10 \tTraining Loss: 0.000151\n",
      "Epoch: 11 \tTraining Loss: 0.000151\n",
      "Epoch: 12 \tTraining Loss: 0.000151\n",
      "Epoch: 13 \tTraining Loss: 0.000151\n",
      "Epoch: 14 \tTraining Loss: 0.000151\n",
      "Epoch: 15 \tTraining Loss: 0.000151\n",
      "Epoch: 16 \tTraining Loss: 0.000151\n",
      "Epoch: 17 \tTraining Loss: 0.000151\n",
      "Epoch: 18 \tTraining Loss: 0.000151\n",
      "Epoch: 1 \tTraining Loss: 0.000235\n",
      "Epoch: 2 \tTraining Loss: 0.000235\n",
      "Epoch: 3 \tTraining Loss: 0.000235\n",
      "Epoch: 4 \tTraining Loss: 0.000235\n",
      "Epoch: 5 \tTraining Loss: 0.000235\n",
      "Epoch: 6 \tTraining Loss: 0.000235\n",
      "Epoch: 7 \tTraining Loss: 0.000235\n",
      "Epoch: 8 \tTraining Loss: 0.000235\n",
      "Epoch: 9 \tTraining Loss: 0.000235\n",
      "Epoch: 10 \tTraining Loss: 0.000235\n",
      "Epoch: 11 \tTraining Loss: 0.000235\n",
      "Epoch: 12 \tTraining Loss: 0.000235\n",
      "Epoch: 13 \tTraining Loss: 0.000235\n",
      "Epoch: 14 \tTraining Loss: 0.000235\n",
      "Epoch: 15 \tTraining Loss: 0.000235\n",
      "Epoch: 16 \tTraining Loss: 0.000235\n",
      "Epoch: 17 \tTraining Loss: 0.000235\n",
      "Epoch: 18 \tTraining Loss: 0.000235\n",
      "Epoch: 1 \tTraining Loss: 0.000246\n",
      "Epoch: 2 \tTraining Loss: 0.000246\n",
      "Epoch: 3 \tTraining Loss: 0.000246\n",
      "Epoch: 4 \tTraining Loss: 0.000246\n",
      "Epoch: 5 \tTraining Loss: 0.000246\n",
      "Epoch: 6 \tTraining Loss: 0.000246\n",
      "Epoch: 7 \tTraining Loss: 0.000246\n",
      "Epoch: 8 \tTraining Loss: 0.000246\n",
      "Epoch: 9 \tTraining Loss: 0.000246\n",
      "Epoch: 10 \tTraining Loss: 0.000246\n",
      "Epoch: 11 \tTraining Loss: 0.000246\n",
      "Epoch: 12 \tTraining Loss: 0.000246\n",
      "Epoch: 13 \tTraining Loss: 0.000246\n",
      "Epoch: 14 \tTraining Loss: 0.000246\n",
      "Epoch: 15 \tTraining Loss: 0.000246\n",
      "Epoch: 16 \tTraining Loss: 0.000246\n",
      "Epoch: 17 \tTraining Loss: 0.000246\n",
      "Epoch: 18 \tTraining Loss: 0.000246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000272\n",
      "Epoch: 2 \tTraining Loss: 0.000272\n",
      "Epoch: 3 \tTraining Loss: 0.000272\n",
      "Epoch: 4 \tTraining Loss: 0.000272\n",
      "Epoch: 5 \tTraining Loss: 0.000272\n",
      "Epoch: 6 \tTraining Loss: 0.000272\n",
      "Epoch: 7 \tTraining Loss: 0.000272\n",
      "Epoch: 8 \tTraining Loss: 0.000272\n",
      "Epoch: 9 \tTraining Loss: 0.000272\n",
      "Epoch: 10 \tTraining Loss: 0.000272\n",
      "Epoch: 11 \tTraining Loss: 0.000272\n",
      "Epoch: 12 \tTraining Loss: 0.000272\n",
      "Epoch: 13 \tTraining Loss: 0.000272\n",
      "Epoch: 14 \tTraining Loss: 0.000272\n",
      "Epoch: 15 \tTraining Loss: 0.000272\n",
      "Epoch: 16 \tTraining Loss: 0.000272\n",
      "Epoch: 17 \tTraining Loss: 0.000272\n",
      "Epoch: 18 \tTraining Loss: 0.000272\n",
      "Epoch: 1 \tTraining Loss: 0.000128\n",
      "Epoch: 2 \tTraining Loss: 0.000128\n",
      "Epoch: 3 \tTraining Loss: 0.000128\n",
      "Epoch: 4 \tTraining Loss: 0.000128\n",
      "Epoch: 5 \tTraining Loss: 0.000128\n",
      "Epoch: 6 \tTraining Loss: 0.000128\n",
      "Epoch: 7 \tTraining Loss: 0.000128\n",
      "Epoch: 8 \tTraining Loss: 0.000128\n",
      "Epoch: 9 \tTraining Loss: 0.000128\n",
      "Epoch: 10 \tTraining Loss: 0.000128\n",
      "Epoch: 11 \tTraining Loss: 0.000128\n",
      "Epoch: 12 \tTraining Loss: 0.000128\n",
      "Epoch: 13 \tTraining Loss: 0.000128\n",
      "Epoch: 14 \tTraining Loss: 0.000128\n",
      "Epoch: 15 \tTraining Loss: 0.000128\n",
      "Epoch: 16 \tTraining Loss: 0.000128\n",
      "Epoch: 17 \tTraining Loss: 0.000128\n",
      "Epoch: 18 \tTraining Loss: 0.000128\n",
      "Epoch: 1 \tTraining Loss: 0.000116\n",
      "Epoch: 2 \tTraining Loss: 0.000116\n",
      "Epoch: 3 \tTraining Loss: 0.000116\n",
      "Epoch: 4 \tTraining Loss: 0.000116\n",
      "Epoch: 5 \tTraining Loss: 0.000116\n",
      "Epoch: 6 \tTraining Loss: 0.000116\n",
      "Epoch: 7 \tTraining Loss: 0.000116\n",
      "Epoch: 8 \tTraining Loss: 0.000116\n",
      "Epoch: 9 \tTraining Loss: 0.000116\n",
      "Epoch: 10 \tTraining Loss: 0.000116\n",
      "Epoch: 11 \tTraining Loss: 0.000116\n",
      "Epoch: 12 \tTraining Loss: 0.000116\n",
      "Epoch: 13 \tTraining Loss: 0.000116\n",
      "Epoch: 14 \tTraining Loss: 0.000116\n",
      "Epoch: 15 \tTraining Loss: 0.000116\n",
      "Epoch: 16 \tTraining Loss: 0.000116\n",
      "Epoch: 17 \tTraining Loss: 0.000116\n",
      "Epoch: 18 \tTraining Loss: 0.000116\n",
      "Epoch: 1 \tTraining Loss: 0.000215\n",
      "Epoch: 2 \tTraining Loss: 0.000215\n",
      "Epoch: 3 \tTraining Loss: 0.000215\n",
      "Epoch: 4 \tTraining Loss: 0.000215\n",
      "Epoch: 5 \tTraining Loss: 0.000215\n",
      "Epoch: 6 \tTraining Loss: 0.000215\n",
      "Epoch: 7 \tTraining Loss: 0.000215\n",
      "Epoch: 8 \tTraining Loss: 0.000215\n",
      "Epoch: 9 \tTraining Loss: 0.000215\n",
      "Epoch: 10 \tTraining Loss: 0.000215\n",
      "Epoch: 11 \tTraining Loss: 0.000215\n",
      "Epoch: 12 \tTraining Loss: 0.000215\n",
      "Epoch: 13 \tTraining Loss: 0.000215\n",
      "Epoch: 14 \tTraining Loss: 0.000215\n",
      "Epoch: 15 \tTraining Loss: 0.000215\n",
      "Epoch: 16 \tTraining Loss: 0.000215\n",
      "Epoch: 17 \tTraining Loss: 0.000215\n",
      "Epoch: 18 \tTraining Loss: 0.000215\n",
      "Epoch: 1 \tTraining Loss: 0.000291\n",
      "Epoch: 2 \tTraining Loss: 0.000291\n",
      "Epoch: 3 \tTraining Loss: 0.000291\n",
      "Epoch: 4 \tTraining Loss: 0.000291\n",
      "Epoch: 5 \tTraining Loss: 0.000291\n",
      "Epoch: 6 \tTraining Loss: 0.000291\n",
      "Epoch: 7 \tTraining Loss: 0.000291\n",
      "Epoch: 8 \tTraining Loss: 0.000291\n",
      "Epoch: 9 \tTraining Loss: 0.000291\n",
      "Epoch: 10 \tTraining Loss: 0.000291\n",
      "Epoch: 11 \tTraining Loss: 0.000291\n",
      "Epoch: 12 \tTraining Loss: 0.000291\n",
      "Epoch: 13 \tTraining Loss: 0.000291\n",
      "Epoch: 14 \tTraining Loss: 0.000291\n",
      "Epoch: 15 \tTraining Loss: 0.000291\n",
      "Epoch: 16 \tTraining Loss: 0.000291\n",
      "Epoch: 17 \tTraining Loss: 0.000291\n",
      "Epoch: 18 \tTraining Loss: 0.000291\n",
      "Epoch: 1 \tTraining Loss: 0.000085\n",
      "Epoch: 2 \tTraining Loss: 0.000085\n",
      "Epoch: 3 \tTraining Loss: 0.000085\n",
      "Epoch: 4 \tTraining Loss: 0.000085\n",
      "Epoch: 5 \tTraining Loss: 0.000085\n",
      "Epoch: 6 \tTraining Loss: 0.000085\n",
      "Epoch: 7 \tTraining Loss: 0.000085\n",
      "Epoch: 8 \tTraining Loss: 0.000085\n",
      "Epoch: 9 \tTraining Loss: 0.000085\n",
      "Epoch: 10 \tTraining Loss: 0.000085\n",
      "Epoch: 11 \tTraining Loss: 0.000085\n",
      "Epoch: 12 \tTraining Loss: 0.000085\n",
      "Epoch: 13 \tTraining Loss: 0.000085\n",
      "Epoch: 14 \tTraining Loss: 0.000085\n",
      "Epoch: 15 \tTraining Loss: 0.000085\n",
      "Epoch: 16 \tTraining Loss: 0.000085\n",
      "Epoch: 17 \tTraining Loss: 0.000085\n",
      "Epoch: 18 \tTraining Loss: 0.000085\n",
      "Epoch: 1 \tTraining Loss: 0.000249\n",
      "Epoch: 2 \tTraining Loss: 0.000249\n",
      "Epoch: 3 \tTraining Loss: 0.000249\n",
      "Epoch: 4 \tTraining Loss: 0.000249\n",
      "Epoch: 5 \tTraining Loss: 0.000249\n",
      "Epoch: 6 \tTraining Loss: 0.000249\n",
      "Epoch: 7 \tTraining Loss: 0.000249\n",
      "Epoch: 8 \tTraining Loss: 0.000249\n",
      "Epoch: 9 \tTraining Loss: 0.000249\n",
      "Epoch: 10 \tTraining Loss: 0.000249\n",
      "Epoch: 11 \tTraining Loss: 0.000249\n",
      "Epoch: 12 \tTraining Loss: 0.000249\n",
      "Epoch: 13 \tTraining Loss: 0.000249\n",
      "Epoch: 14 \tTraining Loss: 0.000249\n",
      "Epoch: 15 \tTraining Loss: 0.000249\n",
      "Epoch: 16 \tTraining Loss: 0.000249\n",
      "Epoch: 17 \tTraining Loss: 0.000249\n",
      "Epoch: 18 \tTraining Loss: 0.000249\n",
      "Epoch: 1 \tTraining Loss: 0.000141\n",
      "Epoch: 2 \tTraining Loss: 0.000141\n",
      "Epoch: 3 \tTraining Loss: 0.000141\n",
      "Epoch: 4 \tTraining Loss: 0.000141\n",
      "Epoch: 5 \tTraining Loss: 0.000141\n",
      "Epoch: 6 \tTraining Loss: 0.000141\n",
      "Epoch: 7 \tTraining Loss: 0.000141\n",
      "Epoch: 8 \tTraining Loss: 0.000141\n",
      "Epoch: 9 \tTraining Loss: 0.000141\n",
      "Epoch: 10 \tTraining Loss: 0.000141\n",
      "Epoch: 11 \tTraining Loss: 0.000141\n",
      "Epoch: 12 \tTraining Loss: 0.000141\n",
      "Epoch: 13 \tTraining Loss: 0.000141\n",
      "Epoch: 14 \tTraining Loss: 0.000141\n",
      "Epoch: 15 \tTraining Loss: 0.000141\n",
      "Epoch: 16 \tTraining Loss: 0.000141\n",
      "Epoch: 17 \tTraining Loss: 0.000141\n",
      "Epoch: 18 \tTraining Loss: 0.000141\n",
      "Epoch: 1 \tTraining Loss: 0.000139\n",
      "Epoch: 2 \tTraining Loss: 0.000139\n",
      "Epoch: 3 \tTraining Loss: 0.000139\n",
      "Epoch: 4 \tTraining Loss: 0.000139\n",
      "Epoch: 5 \tTraining Loss: 0.000139\n",
      "Epoch: 6 \tTraining Loss: 0.000139\n",
      "Epoch: 7 \tTraining Loss: 0.000139\n",
      "Epoch: 8 \tTraining Loss: 0.000139\n",
      "Epoch: 9 \tTraining Loss: 0.000139\n",
      "Epoch: 10 \tTraining Loss: 0.000139\n",
      "Epoch: 11 \tTraining Loss: 0.000139\n",
      "Epoch: 12 \tTraining Loss: 0.000139\n",
      "Epoch: 13 \tTraining Loss: 0.000139\n",
      "Epoch: 14 \tTraining Loss: 0.000139\n",
      "Epoch: 15 \tTraining Loss: 0.000139\n",
      "Epoch: 16 \tTraining Loss: 0.000139\n",
      "Epoch: 17 \tTraining Loss: 0.000139\n",
      "Epoch: 18 \tTraining Loss: 0.000139\n",
      "Epoch: 1 \tTraining Loss: 0.000131\n",
      "Epoch: 2 \tTraining Loss: 0.000131\n",
      "Epoch: 3 \tTraining Loss: 0.000131\n",
      "Epoch: 4 \tTraining Loss: 0.000131\n",
      "Epoch: 5 \tTraining Loss: 0.000131\n",
      "Epoch: 6 \tTraining Loss: 0.000131\n",
      "Epoch: 7 \tTraining Loss: 0.000131\n",
      "Epoch: 8 \tTraining Loss: 0.000131\n",
      "Epoch: 9 \tTraining Loss: 0.000131\n",
      "Epoch: 10 \tTraining Loss: 0.000131\n",
      "Epoch: 11 \tTraining Loss: 0.000131\n",
      "Epoch: 12 \tTraining Loss: 0.000131\n",
      "Epoch: 13 \tTraining Loss: 0.000131\n",
      "Epoch: 14 \tTraining Loss: 0.000131\n",
      "Epoch: 15 \tTraining Loss: 0.000131\n",
      "Epoch: 16 \tTraining Loss: 0.000131\n",
      "Epoch: 17 \tTraining Loss: 0.000131\n",
      "Epoch: 18 \tTraining Loss: 0.000131\n",
      "Epoch: 1 \tTraining Loss: 0.000111\n",
      "Epoch: 2 \tTraining Loss: 0.000111\n",
      "Epoch: 3 \tTraining Loss: 0.000111\n",
      "Epoch: 4 \tTraining Loss: 0.000111\n",
      "Epoch: 5 \tTraining Loss: 0.000111\n",
      "Epoch: 6 \tTraining Loss: 0.000111\n",
      "Epoch: 7 \tTraining Loss: 0.000111\n",
      "Epoch: 8 \tTraining Loss: 0.000111\n",
      "Epoch: 9 \tTraining Loss: 0.000111\n",
      "Epoch: 10 \tTraining Loss: 0.000111\n",
      "Epoch: 11 \tTraining Loss: 0.000111\n",
      "Epoch: 12 \tTraining Loss: 0.000111\n",
      "Epoch: 13 \tTraining Loss: 0.000111\n",
      "Epoch: 14 \tTraining Loss: 0.000111\n",
      "Epoch: 15 \tTraining Loss: 0.000111\n",
      "Epoch: 16 \tTraining Loss: 0.000111\n",
      "Epoch: 17 \tTraining Loss: 0.000111\n",
      "Epoch: 18 \tTraining Loss: 0.000111\n",
      "Epoch: 1 \tTraining Loss: 0.000262\n",
      "Epoch: 2 \tTraining Loss: 0.000262\n",
      "Epoch: 3 \tTraining Loss: 0.000262\n",
      "Epoch: 4 \tTraining Loss: 0.000262\n",
      "Epoch: 5 \tTraining Loss: 0.000262\n",
      "Epoch: 6 \tTraining Loss: 0.000262\n",
      "Epoch: 7 \tTraining Loss: 0.000262\n",
      "Epoch: 8 \tTraining Loss: 0.000262\n",
      "Epoch: 9 \tTraining Loss: 0.000262\n",
      "Epoch: 10 \tTraining Loss: 0.000262\n",
      "Epoch: 11 \tTraining Loss: 0.000262\n",
      "Epoch: 12 \tTraining Loss: 0.000262\n",
      "Epoch: 13 \tTraining Loss: 0.000262\n",
      "Epoch: 14 \tTraining Loss: 0.000262\n",
      "Epoch: 15 \tTraining Loss: 0.000262\n",
      "Epoch: 16 \tTraining Loss: 0.000262\n",
      "Epoch: 17 \tTraining Loss: 0.000262\n",
      "Epoch: 18 \tTraining Loss: 0.000262\n",
      "Epoch: 1 \tTraining Loss: 0.000207\n",
      "Epoch: 2 \tTraining Loss: 0.000207\n",
      "Epoch: 3 \tTraining Loss: 0.000207\n",
      "Epoch: 4 \tTraining Loss: 0.000207\n",
      "Epoch: 5 \tTraining Loss: 0.000207\n",
      "Epoch: 6 \tTraining Loss: 0.000207\n",
      "Epoch: 7 \tTraining Loss: 0.000207\n",
      "Epoch: 8 \tTraining Loss: 0.000207\n",
      "Epoch: 9 \tTraining Loss: 0.000207\n",
      "Epoch: 10 \tTraining Loss: 0.000207\n",
      "Epoch: 11 \tTraining Loss: 0.000207\n",
      "Epoch: 12 \tTraining Loss: 0.000207\n",
      "Epoch: 13 \tTraining Loss: 0.000207\n",
      "Epoch: 14 \tTraining Loss: 0.000207\n",
      "Epoch: 15 \tTraining Loss: 0.000207\n",
      "Epoch: 16 \tTraining Loss: 0.000207\n",
      "Epoch: 17 \tTraining Loss: 0.000207\n",
      "Epoch: 18 \tTraining Loss: 0.000207\n",
      "Epoch: 1 \tTraining Loss: 0.000231\n",
      "Epoch: 2 \tTraining Loss: 0.000231\n",
      "Epoch: 3 \tTraining Loss: 0.000231\n",
      "Epoch: 4 \tTraining Loss: 0.000231\n",
      "Epoch: 5 \tTraining Loss: 0.000231\n",
      "Epoch: 6 \tTraining Loss: 0.000231\n",
      "Epoch: 7 \tTraining Loss: 0.000231\n",
      "Epoch: 8 \tTraining Loss: 0.000231\n",
      "Epoch: 9 \tTraining Loss: 0.000231\n",
      "Epoch: 10 \tTraining Loss: 0.000231\n",
      "Epoch: 11 \tTraining Loss: 0.000231\n",
      "Epoch: 12 \tTraining Loss: 0.000231\n",
      "Epoch: 13 \tTraining Loss: 0.000231\n",
      "Epoch: 14 \tTraining Loss: 0.000231\n",
      "Epoch: 15 \tTraining Loss: 0.000231\n",
      "Epoch: 16 \tTraining Loss: 0.000231\n",
      "Epoch: 17 \tTraining Loss: 0.000231\n",
      "Epoch: 18 \tTraining Loss: 0.000231\n",
      "Epoch: 1 \tTraining Loss: 0.000233\n",
      "Epoch: 2 \tTraining Loss: 0.000233\n",
      "Epoch: 3 \tTraining Loss: 0.000233\n",
      "Epoch: 4 \tTraining Loss: 0.000233\n",
      "Epoch: 5 \tTraining Loss: 0.000233\n",
      "Epoch: 6 \tTraining Loss: 0.000233\n",
      "Epoch: 7 \tTraining Loss: 0.000233\n",
      "Epoch: 8 \tTraining Loss: 0.000233\n",
      "Epoch: 9 \tTraining Loss: 0.000233\n",
      "Epoch: 10 \tTraining Loss: 0.000233\n",
      "Epoch: 11 \tTraining Loss: 0.000233\n",
      "Epoch: 12 \tTraining Loss: 0.000233\n",
      "Epoch: 13 \tTraining Loss: 0.000233\n",
      "Epoch: 14 \tTraining Loss: 0.000233\n",
      "Epoch: 15 \tTraining Loss: 0.000233\n",
      "Epoch: 16 \tTraining Loss: 0.000233\n",
      "Epoch: 17 \tTraining Loss: 0.000233\n",
      "Epoch: 18 \tTraining Loss: 0.000233\n",
      "Epoch: 1 \tTraining Loss: 0.000378\n",
      "Epoch: 2 \tTraining Loss: 0.000378\n",
      "Epoch: 3 \tTraining Loss: 0.000378\n",
      "Epoch: 4 \tTraining Loss: 0.000378\n",
      "Epoch: 5 \tTraining Loss: 0.000378\n",
      "Epoch: 6 \tTraining Loss: 0.000378\n",
      "Epoch: 7 \tTraining Loss: 0.000378\n",
      "Epoch: 8 \tTraining Loss: 0.000378\n",
      "Epoch: 9 \tTraining Loss: 0.000378\n",
      "Epoch: 10 \tTraining Loss: 0.000378\n",
      "Epoch: 11 \tTraining Loss: 0.000378\n",
      "Epoch: 12 \tTraining Loss: 0.000378\n",
      "Epoch: 13 \tTraining Loss: 0.000378\n",
      "Epoch: 14 \tTraining Loss: 0.000378\n",
      "Epoch: 15 \tTraining Loss: 0.000378\n",
      "Epoch: 16 \tTraining Loss: 0.000378\n",
      "Epoch: 17 \tTraining Loss: 0.000378\n",
      "Epoch: 18 \tTraining Loss: 0.000378\n",
      "Epoch: 1 \tTraining Loss: 0.000226\n",
      "Epoch: 2 \tTraining Loss: 0.000226\n",
      "Epoch: 3 \tTraining Loss: 0.000226\n",
      "Epoch: 4 \tTraining Loss: 0.000226\n",
      "Epoch: 5 \tTraining Loss: 0.000226\n",
      "Epoch: 6 \tTraining Loss: 0.000226\n",
      "Epoch: 7 \tTraining Loss: 0.000226\n",
      "Epoch: 8 \tTraining Loss: 0.000226\n",
      "Epoch: 9 \tTraining Loss: 0.000226\n",
      "Epoch: 10 \tTraining Loss: 0.000226\n",
      "Epoch: 11 \tTraining Loss: 0.000226\n",
      "Epoch: 12 \tTraining Loss: 0.000226\n",
      "Epoch: 13 \tTraining Loss: 0.000226\n",
      "Epoch: 14 \tTraining Loss: 0.000226\n",
      "Epoch: 15 \tTraining Loss: 0.000226\n",
      "Epoch: 16 \tTraining Loss: 0.000226\n",
      "Epoch: 17 \tTraining Loss: 0.000226\n",
      "Epoch: 18 \tTraining Loss: 0.000226\n",
      "Epoch: 1 \tTraining Loss: 0.000190\n",
      "Epoch: 2 \tTraining Loss: 0.000190\n",
      "Epoch: 3 \tTraining Loss: 0.000190\n",
      "Epoch: 4 \tTraining Loss: 0.000190\n",
      "Epoch: 5 \tTraining Loss: 0.000190\n",
      "Epoch: 6 \tTraining Loss: 0.000190\n",
      "Epoch: 7 \tTraining Loss: 0.000190\n",
      "Epoch: 8 \tTraining Loss: 0.000190\n",
      "Epoch: 9 \tTraining Loss: 0.000190\n",
      "Epoch: 10 \tTraining Loss: 0.000190\n",
      "Epoch: 11 \tTraining Loss: 0.000190\n",
      "Epoch: 12 \tTraining Loss: 0.000190\n",
      "Epoch: 13 \tTraining Loss: 0.000190\n",
      "Epoch: 14 \tTraining Loss: 0.000190\n",
      "Epoch: 15 \tTraining Loss: 0.000190\n",
      "Epoch: 16 \tTraining Loss: 0.000190\n",
      "Epoch: 17 \tTraining Loss: 0.000190\n",
      "Epoch: 18 \tTraining Loss: 0.000190\n",
      "Epoch: 1 \tTraining Loss: 0.000234\n",
      "Epoch: 2 \tTraining Loss: 0.000234\n",
      "Epoch: 3 \tTraining Loss: 0.000234\n",
      "Epoch: 4 \tTraining Loss: 0.000234\n",
      "Epoch: 5 \tTraining Loss: 0.000234\n",
      "Epoch: 6 \tTraining Loss: 0.000234\n",
      "Epoch: 7 \tTraining Loss: 0.000234\n",
      "Epoch: 8 \tTraining Loss: 0.000234\n",
      "Epoch: 9 \tTraining Loss: 0.000234\n",
      "Epoch: 10 \tTraining Loss: 0.000234\n",
      "Epoch: 11 \tTraining Loss: 0.000234\n",
      "Epoch: 12 \tTraining Loss: 0.000234\n",
      "Epoch: 13 \tTraining Loss: 0.000234\n",
      "Epoch: 14 \tTraining Loss: 0.000234\n",
      "Epoch: 15 \tTraining Loss: 0.000234\n",
      "Epoch: 16 \tTraining Loss: 0.000234\n",
      "Epoch: 17 \tTraining Loss: 0.000234\n",
      "Epoch: 18 \tTraining Loss: 0.000234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000155\n",
      "Epoch: 2 \tTraining Loss: 0.000155\n",
      "Epoch: 3 \tTraining Loss: 0.000155\n",
      "Epoch: 4 \tTraining Loss: 0.000155\n",
      "Epoch: 5 \tTraining Loss: 0.000155\n",
      "Epoch: 6 \tTraining Loss: 0.000155\n",
      "Epoch: 7 \tTraining Loss: 0.000155\n",
      "Epoch: 8 \tTraining Loss: 0.000155\n",
      "Epoch: 9 \tTraining Loss: 0.000155\n",
      "Epoch: 10 \tTraining Loss: 0.000155\n",
      "Epoch: 11 \tTraining Loss: 0.000155\n",
      "Epoch: 12 \tTraining Loss: 0.000155\n",
      "Epoch: 13 \tTraining Loss: 0.000155\n",
      "Epoch: 14 \tTraining Loss: 0.000155\n",
      "Epoch: 15 \tTraining Loss: 0.000155\n",
      "Epoch: 16 \tTraining Loss: 0.000155\n",
      "Epoch: 17 \tTraining Loss: 0.000155\n",
      "Epoch: 18 \tTraining Loss: 0.000155\n",
      "Epoch: 1 \tTraining Loss: 0.000214\n",
      "Epoch: 2 \tTraining Loss: 0.000214\n",
      "Epoch: 3 \tTraining Loss: 0.000214\n",
      "Epoch: 4 \tTraining Loss: 0.000214\n",
      "Epoch: 5 \tTraining Loss: 0.000214\n",
      "Epoch: 6 \tTraining Loss: 0.000214\n",
      "Epoch: 7 \tTraining Loss: 0.000214\n",
      "Epoch: 8 \tTraining Loss: 0.000214\n",
      "Epoch: 9 \tTraining Loss: 0.000214\n",
      "Epoch: 10 \tTraining Loss: 0.000214\n",
      "Epoch: 11 \tTraining Loss: 0.000214\n",
      "Epoch: 12 \tTraining Loss: 0.000214\n",
      "Epoch: 13 \tTraining Loss: 0.000214\n",
      "Epoch: 14 \tTraining Loss: 0.000214\n",
      "Epoch: 15 \tTraining Loss: 0.000214\n",
      "Epoch: 16 \tTraining Loss: 0.000214\n",
      "Epoch: 17 \tTraining Loss: 0.000214\n",
      "Epoch: 18 \tTraining Loss: 0.000214\n",
      "Epoch: 1 \tTraining Loss: 0.000130\n",
      "Epoch: 2 \tTraining Loss: 0.000130\n",
      "Epoch: 3 \tTraining Loss: 0.000130\n",
      "Epoch: 4 \tTraining Loss: 0.000130\n",
      "Epoch: 5 \tTraining Loss: 0.000130\n",
      "Epoch: 6 \tTraining Loss: 0.000130\n",
      "Epoch: 7 \tTraining Loss: 0.000130\n",
      "Epoch: 8 \tTraining Loss: 0.000130\n",
      "Epoch: 9 \tTraining Loss: 0.000130\n",
      "Epoch: 10 \tTraining Loss: 0.000130\n",
      "Epoch: 11 \tTraining Loss: 0.000130\n",
      "Epoch: 12 \tTraining Loss: 0.000130\n",
      "Epoch: 13 \tTraining Loss: 0.000130\n",
      "Epoch: 14 \tTraining Loss: 0.000130\n",
      "Epoch: 15 \tTraining Loss: 0.000130\n",
      "Epoch: 16 \tTraining Loss: 0.000130\n",
      "Epoch: 17 \tTraining Loss: 0.000130\n",
      "Epoch: 18 \tTraining Loss: 0.000130\n",
      "Epoch: 1 \tTraining Loss: 0.000197\n",
      "Epoch: 2 \tTraining Loss: 0.000197\n",
      "Epoch: 3 \tTraining Loss: 0.000197\n",
      "Epoch: 4 \tTraining Loss: 0.000197\n",
      "Epoch: 5 \tTraining Loss: 0.000197\n",
      "Epoch: 6 \tTraining Loss: 0.000197\n",
      "Epoch: 7 \tTraining Loss: 0.000197\n",
      "Epoch: 8 \tTraining Loss: 0.000197\n",
      "Epoch: 9 \tTraining Loss: 0.000197\n",
      "Epoch: 10 \tTraining Loss: 0.000197\n",
      "Epoch: 11 \tTraining Loss: 0.000197\n",
      "Epoch: 12 \tTraining Loss: 0.000197\n",
      "Epoch: 13 \tTraining Loss: 0.000197\n",
      "Epoch: 14 \tTraining Loss: 0.000197\n",
      "Epoch: 15 \tTraining Loss: 0.000197\n",
      "Epoch: 16 \tTraining Loss: 0.000197\n",
      "Epoch: 17 \tTraining Loss: 0.000197\n",
      "Epoch: 18 \tTraining Loss: 0.000197\n",
      "Epoch: 1 \tTraining Loss: 0.000231\n",
      "Epoch: 2 \tTraining Loss: 0.000231\n",
      "Epoch: 3 \tTraining Loss: 0.000231\n",
      "Epoch: 4 \tTraining Loss: 0.000231\n",
      "Epoch: 5 \tTraining Loss: 0.000231\n",
      "Epoch: 6 \tTraining Loss: 0.000231\n",
      "Epoch: 7 \tTraining Loss: 0.000231\n",
      "Epoch: 8 \tTraining Loss: 0.000231\n",
      "Epoch: 9 \tTraining Loss: 0.000231\n",
      "Epoch: 10 \tTraining Loss: 0.000231\n",
      "Epoch: 11 \tTraining Loss: 0.000231\n",
      "Epoch: 12 \tTraining Loss: 0.000231\n",
      "Epoch: 13 \tTraining Loss: 0.000231\n",
      "Epoch: 14 \tTraining Loss: 0.000231\n",
      "Epoch: 15 \tTraining Loss: 0.000231\n",
      "Epoch: 16 \tTraining Loss: 0.000231\n",
      "Epoch: 17 \tTraining Loss: 0.000231\n",
      "Epoch: 18 \tTraining Loss: 0.000231\n",
      "Epoch: 1 \tTraining Loss: 0.000129\n",
      "Epoch: 2 \tTraining Loss: 0.000129\n",
      "Epoch: 3 \tTraining Loss: 0.000129\n",
      "Epoch: 4 \tTraining Loss: 0.000129\n",
      "Epoch: 5 \tTraining Loss: 0.000129\n",
      "Epoch: 6 \tTraining Loss: 0.000129\n",
      "Epoch: 7 \tTraining Loss: 0.000129\n",
      "Epoch: 8 \tTraining Loss: 0.000129\n",
      "Epoch: 9 \tTraining Loss: 0.000129\n",
      "Epoch: 10 \tTraining Loss: 0.000129\n",
      "Epoch: 11 \tTraining Loss: 0.000129\n",
      "Epoch: 12 \tTraining Loss: 0.000129\n",
      "Epoch: 13 \tTraining Loss: 0.000129\n",
      "Epoch: 14 \tTraining Loss: 0.000129\n",
      "Epoch: 15 \tTraining Loss: 0.000129\n",
      "Epoch: 16 \tTraining Loss: 0.000129\n",
      "Epoch: 17 \tTraining Loss: 0.000129\n",
      "Epoch: 18 \tTraining Loss: 0.000129\n",
      "Epoch: 1 \tTraining Loss: 0.000203\n",
      "Epoch: 2 \tTraining Loss: 0.000203\n",
      "Epoch: 3 \tTraining Loss: 0.000203\n",
      "Epoch: 4 \tTraining Loss: 0.000203\n",
      "Epoch: 5 \tTraining Loss: 0.000203\n",
      "Epoch: 6 \tTraining Loss: 0.000203\n",
      "Epoch: 7 \tTraining Loss: 0.000203\n",
      "Epoch: 8 \tTraining Loss: 0.000203\n",
      "Epoch: 9 \tTraining Loss: 0.000203\n",
      "Epoch: 10 \tTraining Loss: 0.000203\n",
      "Epoch: 11 \tTraining Loss: 0.000203\n",
      "Epoch: 12 \tTraining Loss: 0.000203\n",
      "Epoch: 13 \tTraining Loss: 0.000203\n",
      "Epoch: 14 \tTraining Loss: 0.000203\n",
      "Epoch: 15 \tTraining Loss: 0.000203\n",
      "Epoch: 16 \tTraining Loss: 0.000203\n",
      "Epoch: 17 \tTraining Loss: 0.000203\n",
      "Epoch: 18 \tTraining Loss: 0.000203\n",
      "Epoch: 1 \tTraining Loss: 0.000186\n",
      "Epoch: 2 \tTraining Loss: 0.000186\n",
      "Epoch: 3 \tTraining Loss: 0.000186\n",
      "Epoch: 4 \tTraining Loss: 0.000186\n",
      "Epoch: 5 \tTraining Loss: 0.000186\n",
      "Epoch: 6 \tTraining Loss: 0.000186\n",
      "Epoch: 7 \tTraining Loss: 0.000186\n",
      "Epoch: 8 \tTraining Loss: 0.000186\n",
      "Epoch: 9 \tTraining Loss: 0.000186\n",
      "Epoch: 10 \tTraining Loss: 0.000186\n",
      "Epoch: 11 \tTraining Loss: 0.000186\n",
      "Epoch: 12 \tTraining Loss: 0.000186\n",
      "Epoch: 13 \tTraining Loss: 0.000186\n",
      "Epoch: 14 \tTraining Loss: 0.000186\n",
      "Epoch: 15 \tTraining Loss: 0.000186\n",
      "Epoch: 16 \tTraining Loss: 0.000186\n",
      "Epoch: 17 \tTraining Loss: 0.000186\n",
      "Epoch: 18 \tTraining Loss: 0.000186\n",
      "Epoch: 1 \tTraining Loss: 0.000289\n",
      "Epoch: 2 \tTraining Loss: 0.000289\n",
      "Epoch: 3 \tTraining Loss: 0.000289\n",
      "Epoch: 4 \tTraining Loss: 0.000289\n",
      "Epoch: 5 \tTraining Loss: 0.000289\n",
      "Epoch: 6 \tTraining Loss: 0.000289\n",
      "Epoch: 7 \tTraining Loss: 0.000289\n",
      "Epoch: 8 \tTraining Loss: 0.000289\n",
      "Epoch: 9 \tTraining Loss: 0.000289\n",
      "Epoch: 10 \tTraining Loss: 0.000289\n",
      "Epoch: 11 \tTraining Loss: 0.000289\n",
      "Epoch: 12 \tTraining Loss: 0.000289\n",
      "Epoch: 13 \tTraining Loss: 0.000289\n",
      "Epoch: 14 \tTraining Loss: 0.000289\n",
      "Epoch: 15 \tTraining Loss: 0.000289\n",
      "Epoch: 16 \tTraining Loss: 0.000289\n",
      "Epoch: 17 \tTraining Loss: 0.000289\n",
      "Epoch: 18 \tTraining Loss: 0.000289\n",
      "Epoch: 1 \tTraining Loss: 0.000276\n",
      "Epoch: 2 \tTraining Loss: 0.000276\n",
      "Epoch: 3 \tTraining Loss: 0.000276\n",
      "Epoch: 4 \tTraining Loss: 0.000276\n",
      "Epoch: 5 \tTraining Loss: 0.000276\n",
      "Epoch: 6 \tTraining Loss: 0.000276\n",
      "Epoch: 7 \tTraining Loss: 0.000276\n",
      "Epoch: 8 \tTraining Loss: 0.000276\n",
      "Epoch: 9 \tTraining Loss: 0.000276\n",
      "Epoch: 10 \tTraining Loss: 0.000276\n",
      "Epoch: 11 \tTraining Loss: 0.000276\n",
      "Epoch: 12 \tTraining Loss: 0.000276\n",
      "Epoch: 13 \tTraining Loss: 0.000276\n",
      "Epoch: 14 \tTraining Loss: 0.000276\n",
      "Epoch: 15 \tTraining Loss: 0.000276\n",
      "Epoch: 16 \tTraining Loss: 0.000276\n",
      "Epoch: 17 \tTraining Loss: 0.000276\n",
      "Epoch: 18 \tTraining Loss: 0.000276\n",
      "Epoch: 1 \tTraining Loss: 0.000263\n",
      "Epoch: 2 \tTraining Loss: 0.000263\n",
      "Epoch: 3 \tTraining Loss: 0.000263\n",
      "Epoch: 4 \tTraining Loss: 0.000263\n",
      "Epoch: 5 \tTraining Loss: 0.000263\n",
      "Epoch: 6 \tTraining Loss: 0.000263\n",
      "Epoch: 7 \tTraining Loss: 0.000263\n",
      "Epoch: 8 \tTraining Loss: 0.000263\n",
      "Epoch: 9 \tTraining Loss: 0.000263\n",
      "Epoch: 10 \tTraining Loss: 0.000263\n",
      "Epoch: 11 \tTraining Loss: 0.000263\n",
      "Epoch: 12 \tTraining Loss: 0.000263\n",
      "Epoch: 13 \tTraining Loss: 0.000263\n",
      "Epoch: 14 \tTraining Loss: 0.000263\n",
      "Epoch: 15 \tTraining Loss: 0.000263\n",
      "Epoch: 16 \tTraining Loss: 0.000263\n",
      "Epoch: 17 \tTraining Loss: 0.000263\n",
      "Epoch: 18 \tTraining Loss: 0.000263\n",
      "Epoch: 1 \tTraining Loss: 0.000178\n",
      "Epoch: 2 \tTraining Loss: 0.000178\n",
      "Epoch: 3 \tTraining Loss: 0.000178\n",
      "Epoch: 4 \tTraining Loss: 0.000178\n",
      "Epoch: 5 \tTraining Loss: 0.000178\n",
      "Epoch: 6 \tTraining Loss: 0.000178\n",
      "Epoch: 7 \tTraining Loss: 0.000178\n",
      "Epoch: 8 \tTraining Loss: 0.000178\n",
      "Epoch: 9 \tTraining Loss: 0.000178\n",
      "Epoch: 10 \tTraining Loss: 0.000178\n",
      "Epoch: 11 \tTraining Loss: 0.000178\n",
      "Epoch: 12 \tTraining Loss: 0.000178\n",
      "Epoch: 13 \tTraining Loss: 0.000178\n",
      "Epoch: 14 \tTraining Loss: 0.000178\n",
      "Epoch: 15 \tTraining Loss: 0.000178\n",
      "Epoch: 16 \tTraining Loss: 0.000178\n",
      "Epoch: 17 \tTraining Loss: 0.000178\n",
      "Epoch: 18 \tTraining Loss: 0.000178\n",
      "Epoch: 1 \tTraining Loss: 0.000122\n",
      "Epoch: 2 \tTraining Loss: 0.000122\n",
      "Epoch: 3 \tTraining Loss: 0.000122\n",
      "Epoch: 4 \tTraining Loss: 0.000122\n",
      "Epoch: 5 \tTraining Loss: 0.000122\n",
      "Epoch: 6 \tTraining Loss: 0.000122\n",
      "Epoch: 7 \tTraining Loss: 0.000122\n",
      "Epoch: 8 \tTraining Loss: 0.000122\n",
      "Epoch: 9 \tTraining Loss: 0.000122\n",
      "Epoch: 10 \tTraining Loss: 0.000122\n",
      "Epoch: 11 \tTraining Loss: 0.000122\n",
      "Epoch: 12 \tTraining Loss: 0.000122\n",
      "Epoch: 13 \tTraining Loss: 0.000122\n",
      "Epoch: 14 \tTraining Loss: 0.000122\n",
      "Epoch: 15 \tTraining Loss: 0.000122\n",
      "Epoch: 16 \tTraining Loss: 0.000122\n",
      "Epoch: 17 \tTraining Loss: 0.000122\n",
      "Epoch: 18 \tTraining Loss: 0.000122\n",
      "Epoch: 1 \tTraining Loss: 0.000239\n",
      "Epoch: 2 \tTraining Loss: 0.000239\n",
      "Epoch: 3 \tTraining Loss: 0.000239\n",
      "Epoch: 4 \tTraining Loss: 0.000239\n",
      "Epoch: 5 \tTraining Loss: 0.000239\n",
      "Epoch: 6 \tTraining Loss: 0.000239\n",
      "Epoch: 7 \tTraining Loss: 0.000239\n",
      "Epoch: 8 \tTraining Loss: 0.000239\n",
      "Epoch: 9 \tTraining Loss: 0.000239\n",
      "Epoch: 10 \tTraining Loss: 0.000239\n",
      "Epoch: 11 \tTraining Loss: 0.000239\n",
      "Epoch: 12 \tTraining Loss: 0.000239\n",
      "Epoch: 13 \tTraining Loss: 0.000239\n",
      "Epoch: 14 \tTraining Loss: 0.000239\n",
      "Epoch: 15 \tTraining Loss: 0.000239\n",
      "Epoch: 16 \tTraining Loss: 0.000239\n",
      "Epoch: 17 \tTraining Loss: 0.000239\n",
      "Epoch: 18 \tTraining Loss: 0.000239\n",
      "Epoch: 1 \tTraining Loss: 0.000155\n",
      "Epoch: 2 \tTraining Loss: 0.000155\n",
      "Epoch: 3 \tTraining Loss: 0.000155\n",
      "Epoch: 4 \tTraining Loss: 0.000155\n",
      "Epoch: 5 \tTraining Loss: 0.000155\n",
      "Epoch: 6 \tTraining Loss: 0.000155\n",
      "Epoch: 7 \tTraining Loss: 0.000155\n",
      "Epoch: 8 \tTraining Loss: 0.000155\n",
      "Epoch: 9 \tTraining Loss: 0.000155\n",
      "Epoch: 10 \tTraining Loss: 0.000155\n",
      "Epoch: 11 \tTraining Loss: 0.000155\n",
      "Epoch: 12 \tTraining Loss: 0.000155\n",
      "Epoch: 13 \tTraining Loss: 0.000155\n",
      "Epoch: 14 \tTraining Loss: 0.000155\n",
      "Epoch: 15 \tTraining Loss: 0.000155\n",
      "Epoch: 16 \tTraining Loss: 0.000155\n",
      "Epoch: 17 \tTraining Loss: 0.000155\n",
      "Epoch: 18 \tTraining Loss: 0.000155\n",
      "Epoch: 1 \tTraining Loss: 0.000181\n",
      "Epoch: 2 \tTraining Loss: 0.000181\n",
      "Epoch: 3 \tTraining Loss: 0.000181\n",
      "Epoch: 4 \tTraining Loss: 0.000181\n",
      "Epoch: 5 \tTraining Loss: 0.000181\n",
      "Epoch: 6 \tTraining Loss: 0.000181\n",
      "Epoch: 7 \tTraining Loss: 0.000181\n",
      "Epoch: 8 \tTraining Loss: 0.000181\n",
      "Epoch: 9 \tTraining Loss: 0.000181\n",
      "Epoch: 10 \tTraining Loss: 0.000181\n",
      "Epoch: 11 \tTraining Loss: 0.000181\n",
      "Epoch: 12 \tTraining Loss: 0.000181\n",
      "Epoch: 13 \tTraining Loss: 0.000181\n",
      "Epoch: 14 \tTraining Loss: 0.000181\n",
      "Epoch: 15 \tTraining Loss: 0.000181\n",
      "Epoch: 16 \tTraining Loss: 0.000181\n",
      "Epoch: 17 \tTraining Loss: 0.000181\n",
      "Epoch: 18 \tTraining Loss: 0.000181\n",
      "Epoch: 1 \tTraining Loss: 0.000181\n",
      "Epoch: 2 \tTraining Loss: 0.000181\n",
      "Epoch: 3 \tTraining Loss: 0.000181\n",
      "Epoch: 4 \tTraining Loss: 0.000181\n",
      "Epoch: 5 \tTraining Loss: 0.000181\n",
      "Epoch: 6 \tTraining Loss: 0.000181\n",
      "Epoch: 7 \tTraining Loss: 0.000181\n",
      "Epoch: 8 \tTraining Loss: 0.000181\n",
      "Epoch: 9 \tTraining Loss: 0.000181\n",
      "Epoch: 10 \tTraining Loss: 0.000181\n",
      "Epoch: 11 \tTraining Loss: 0.000181\n",
      "Epoch: 12 \tTraining Loss: 0.000181\n",
      "Epoch: 13 \tTraining Loss: 0.000181\n",
      "Epoch: 14 \tTraining Loss: 0.000181\n",
      "Epoch: 15 \tTraining Loss: 0.000181\n",
      "Epoch: 16 \tTraining Loss: 0.000181\n",
      "Epoch: 17 \tTraining Loss: 0.000181\n",
      "Epoch: 18 \tTraining Loss: 0.000181\n",
      "Epoch: 1 \tTraining Loss: 0.000159\n",
      "Epoch: 2 \tTraining Loss: 0.000159\n",
      "Epoch: 3 \tTraining Loss: 0.000159\n",
      "Epoch: 4 \tTraining Loss: 0.000159\n",
      "Epoch: 5 \tTraining Loss: 0.000159\n",
      "Epoch: 6 \tTraining Loss: 0.000159\n",
      "Epoch: 7 \tTraining Loss: 0.000159\n",
      "Epoch: 8 \tTraining Loss: 0.000159\n",
      "Epoch: 9 \tTraining Loss: 0.000159\n",
      "Epoch: 10 \tTraining Loss: 0.000159\n",
      "Epoch: 11 \tTraining Loss: 0.000159\n",
      "Epoch: 12 \tTraining Loss: 0.000159\n",
      "Epoch: 13 \tTraining Loss: 0.000159\n",
      "Epoch: 14 \tTraining Loss: 0.000159\n",
      "Epoch: 15 \tTraining Loss: 0.000159\n",
      "Epoch: 16 \tTraining Loss: 0.000159\n",
      "Epoch: 17 \tTraining Loss: 0.000159\n",
      "Epoch: 18 \tTraining Loss: 0.000159\n",
      "Epoch: 1 \tTraining Loss: 0.000380\n",
      "Epoch: 2 \tTraining Loss: 0.000380\n",
      "Epoch: 3 \tTraining Loss: 0.000380\n",
      "Epoch: 4 \tTraining Loss: 0.000380\n",
      "Epoch: 5 \tTraining Loss: 0.000380\n",
      "Epoch: 6 \tTraining Loss: 0.000380\n",
      "Epoch: 7 \tTraining Loss: 0.000380\n",
      "Epoch: 8 \tTraining Loss: 0.000380\n",
      "Epoch: 9 \tTraining Loss: 0.000380\n",
      "Epoch: 10 \tTraining Loss: 0.000380\n",
      "Epoch: 11 \tTraining Loss: 0.000380\n",
      "Epoch: 12 \tTraining Loss: 0.000380\n",
      "Epoch: 13 \tTraining Loss: 0.000380\n",
      "Epoch: 14 \tTraining Loss: 0.000380\n",
      "Epoch: 15 \tTraining Loss: 0.000380\n",
      "Epoch: 16 \tTraining Loss: 0.000380\n",
      "Epoch: 17 \tTraining Loss: 0.000380\n",
      "Epoch: 18 \tTraining Loss: 0.000380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000202\n",
      "Epoch: 2 \tTraining Loss: 0.000202\n",
      "Epoch: 3 \tTraining Loss: 0.000202\n",
      "Epoch: 4 \tTraining Loss: 0.000202\n",
      "Epoch: 5 \tTraining Loss: 0.000202\n",
      "Epoch: 6 \tTraining Loss: 0.000202\n",
      "Epoch: 7 \tTraining Loss: 0.000202\n",
      "Epoch: 8 \tTraining Loss: 0.000202\n",
      "Epoch: 9 \tTraining Loss: 0.000202\n",
      "Epoch: 10 \tTraining Loss: 0.000202\n",
      "Epoch: 11 \tTraining Loss: 0.000202\n",
      "Epoch: 12 \tTraining Loss: 0.000202\n",
      "Epoch: 13 \tTraining Loss: 0.000202\n",
      "Epoch: 14 \tTraining Loss: 0.000202\n",
      "Epoch: 15 \tTraining Loss: 0.000202\n",
      "Epoch: 16 \tTraining Loss: 0.000202\n",
      "Epoch: 17 \tTraining Loss: 0.000202\n",
      "Epoch: 18 \tTraining Loss: 0.000202\n",
      "Epoch: 1 \tTraining Loss: 0.000181\n",
      "Epoch: 2 \tTraining Loss: 0.000181\n",
      "Epoch: 3 \tTraining Loss: 0.000181\n",
      "Epoch: 4 \tTraining Loss: 0.000181\n",
      "Epoch: 5 \tTraining Loss: 0.000181\n",
      "Epoch: 6 \tTraining Loss: 0.000181\n",
      "Epoch: 7 \tTraining Loss: 0.000181\n",
      "Epoch: 8 \tTraining Loss: 0.000181\n",
      "Epoch: 9 \tTraining Loss: 0.000181\n",
      "Epoch: 10 \tTraining Loss: 0.000181\n",
      "Epoch: 11 \tTraining Loss: 0.000181\n",
      "Epoch: 12 \tTraining Loss: 0.000181\n",
      "Epoch: 13 \tTraining Loss: 0.000181\n",
      "Epoch: 14 \tTraining Loss: 0.000181\n",
      "Epoch: 15 \tTraining Loss: 0.000181\n",
      "Epoch: 16 \tTraining Loss: 0.000181\n",
      "Epoch: 17 \tTraining Loss: 0.000181\n",
      "Epoch: 18 \tTraining Loss: 0.000181\n",
      "Epoch: 1 \tTraining Loss: 0.000163\n",
      "Epoch: 2 \tTraining Loss: 0.000163\n",
      "Epoch: 3 \tTraining Loss: 0.000163\n",
      "Epoch: 4 \tTraining Loss: 0.000163\n",
      "Epoch: 5 \tTraining Loss: 0.000163\n",
      "Epoch: 6 \tTraining Loss: 0.000163\n",
      "Epoch: 7 \tTraining Loss: 0.000163\n",
      "Epoch: 8 \tTraining Loss: 0.000163\n",
      "Epoch: 9 \tTraining Loss: 0.000163\n",
      "Epoch: 10 \tTraining Loss: 0.000163\n",
      "Epoch: 11 \tTraining Loss: 0.000163\n",
      "Epoch: 12 \tTraining Loss: 0.000163\n",
      "Epoch: 13 \tTraining Loss: 0.000163\n",
      "Epoch: 14 \tTraining Loss: 0.000163\n",
      "Epoch: 15 \tTraining Loss: 0.000163\n",
      "Epoch: 16 \tTraining Loss: 0.000163\n",
      "Epoch: 17 \tTraining Loss: 0.000163\n",
      "Epoch: 18 \tTraining Loss: 0.000163\n",
      "Epoch: 1 \tTraining Loss: 0.000132\n",
      "Epoch: 2 \tTraining Loss: 0.000132\n",
      "Epoch: 3 \tTraining Loss: 0.000132\n",
      "Epoch: 4 \tTraining Loss: 0.000132\n",
      "Epoch: 5 \tTraining Loss: 0.000132\n",
      "Epoch: 6 \tTraining Loss: 0.000132\n",
      "Epoch: 7 \tTraining Loss: 0.000132\n",
      "Epoch: 8 \tTraining Loss: 0.000132\n",
      "Epoch: 9 \tTraining Loss: 0.000132\n",
      "Epoch: 10 \tTraining Loss: 0.000132\n",
      "Epoch: 11 \tTraining Loss: 0.000132\n",
      "Epoch: 12 \tTraining Loss: 0.000132\n",
      "Epoch: 13 \tTraining Loss: 0.000132\n",
      "Epoch: 14 \tTraining Loss: 0.000132\n",
      "Epoch: 15 \tTraining Loss: 0.000132\n",
      "Epoch: 16 \tTraining Loss: 0.000132\n",
      "Epoch: 17 \tTraining Loss: 0.000132\n",
      "Epoch: 18 \tTraining Loss: 0.000132\n",
      "Epoch: 1 \tTraining Loss: 0.000352\n",
      "Epoch: 2 \tTraining Loss: 0.000352\n",
      "Epoch: 3 \tTraining Loss: 0.000352\n",
      "Epoch: 4 \tTraining Loss: 0.000352\n",
      "Epoch: 5 \tTraining Loss: 0.000352\n",
      "Epoch: 6 \tTraining Loss: 0.000352\n",
      "Epoch: 7 \tTraining Loss: 0.000352\n",
      "Epoch: 8 \tTraining Loss: 0.000352\n",
      "Epoch: 9 \tTraining Loss: 0.000352\n",
      "Epoch: 10 \tTraining Loss: 0.000352\n",
      "Epoch: 11 \tTraining Loss: 0.000352\n",
      "Epoch: 12 \tTraining Loss: 0.000352\n",
      "Epoch: 13 \tTraining Loss: 0.000352\n",
      "Epoch: 14 \tTraining Loss: 0.000352\n",
      "Epoch: 15 \tTraining Loss: 0.000352\n",
      "Epoch: 16 \tTraining Loss: 0.000352\n",
      "Epoch: 17 \tTraining Loss: 0.000352\n",
      "Epoch: 18 \tTraining Loss: 0.000352\n",
      "Epoch: 1 \tTraining Loss: 0.000272\n",
      "Epoch: 2 \tTraining Loss: 0.000272\n",
      "Epoch: 3 \tTraining Loss: 0.000272\n",
      "Epoch: 4 \tTraining Loss: 0.000272\n",
      "Epoch: 5 \tTraining Loss: 0.000272\n",
      "Epoch: 6 \tTraining Loss: 0.000272\n",
      "Epoch: 7 \tTraining Loss: 0.000272\n",
      "Epoch: 8 \tTraining Loss: 0.000272\n",
      "Epoch: 9 \tTraining Loss: 0.000272\n",
      "Epoch: 10 \tTraining Loss: 0.000272\n",
      "Epoch: 11 \tTraining Loss: 0.000272\n",
      "Epoch: 12 \tTraining Loss: 0.000272\n",
      "Epoch: 13 \tTraining Loss: 0.000272\n",
      "Epoch: 14 \tTraining Loss: 0.000272\n",
      "Epoch: 15 \tTraining Loss: 0.000272\n",
      "Epoch: 16 \tTraining Loss: 0.000272\n",
      "Epoch: 17 \tTraining Loss: 0.000272\n",
      "Epoch: 18 \tTraining Loss: 0.000272\n",
      "Epoch: 1 \tTraining Loss: 0.000262\n",
      "Epoch: 2 \tTraining Loss: 0.000262\n",
      "Epoch: 3 \tTraining Loss: 0.000262\n",
      "Epoch: 4 \tTraining Loss: 0.000262\n",
      "Epoch: 5 \tTraining Loss: 0.000262\n",
      "Epoch: 6 \tTraining Loss: 0.000262\n",
      "Epoch: 7 \tTraining Loss: 0.000262\n",
      "Epoch: 8 \tTraining Loss: 0.000262\n",
      "Epoch: 9 \tTraining Loss: 0.000262\n",
      "Epoch: 10 \tTraining Loss: 0.000262\n",
      "Epoch: 11 \tTraining Loss: 0.000262\n",
      "Epoch: 12 \tTraining Loss: 0.000262\n",
      "Epoch: 13 \tTraining Loss: 0.000262\n",
      "Epoch: 14 \tTraining Loss: 0.000262\n",
      "Epoch: 15 \tTraining Loss: 0.000262\n",
      "Epoch: 16 \tTraining Loss: 0.000262\n",
      "Epoch: 17 \tTraining Loss: 0.000262\n",
      "Epoch: 18 \tTraining Loss: 0.000262\n",
      "Epoch: 1 \tTraining Loss: 0.000203\n",
      "Epoch: 2 \tTraining Loss: 0.000203\n",
      "Epoch: 3 \tTraining Loss: 0.000203\n",
      "Epoch: 4 \tTraining Loss: 0.000203\n",
      "Epoch: 5 \tTraining Loss: 0.000203\n",
      "Epoch: 6 \tTraining Loss: 0.000203\n",
      "Epoch: 7 \tTraining Loss: 0.000203\n",
      "Epoch: 8 \tTraining Loss: 0.000203\n",
      "Epoch: 9 \tTraining Loss: 0.000203\n",
      "Epoch: 10 \tTraining Loss: 0.000203\n",
      "Epoch: 11 \tTraining Loss: 0.000203\n",
      "Epoch: 12 \tTraining Loss: 0.000203\n",
      "Epoch: 13 \tTraining Loss: 0.000203\n",
      "Epoch: 14 \tTraining Loss: 0.000203\n",
      "Epoch: 15 \tTraining Loss: 0.000203\n",
      "Epoch: 16 \tTraining Loss: 0.000203\n",
      "Epoch: 17 \tTraining Loss: 0.000203\n",
      "Epoch: 18 \tTraining Loss: 0.000203\n",
      "Epoch: 1 \tTraining Loss: 0.000170\n",
      "Epoch: 2 \tTraining Loss: 0.000170\n",
      "Epoch: 3 \tTraining Loss: 0.000170\n",
      "Epoch: 4 \tTraining Loss: 0.000170\n",
      "Epoch: 5 \tTraining Loss: 0.000170\n",
      "Epoch: 6 \tTraining Loss: 0.000170\n",
      "Epoch: 7 \tTraining Loss: 0.000170\n",
      "Epoch: 8 \tTraining Loss: 0.000170\n",
      "Epoch: 9 \tTraining Loss: 0.000170\n",
      "Epoch: 10 \tTraining Loss: 0.000170\n",
      "Epoch: 11 \tTraining Loss: 0.000170\n",
      "Epoch: 12 \tTraining Loss: 0.000170\n",
      "Epoch: 13 \tTraining Loss: 0.000170\n",
      "Epoch: 14 \tTraining Loss: 0.000170\n",
      "Epoch: 15 \tTraining Loss: 0.000170\n",
      "Epoch: 16 \tTraining Loss: 0.000170\n",
      "Epoch: 17 \tTraining Loss: 0.000170\n",
      "Epoch: 18 \tTraining Loss: 0.000170\n",
      "Epoch: 1 \tTraining Loss: 0.000435\n",
      "Epoch: 2 \tTraining Loss: 0.000435\n",
      "Epoch: 3 \tTraining Loss: 0.000435\n",
      "Epoch: 4 \tTraining Loss: 0.000435\n",
      "Epoch: 5 \tTraining Loss: 0.000435\n",
      "Epoch: 6 \tTraining Loss: 0.000435\n",
      "Epoch: 7 \tTraining Loss: 0.000435\n",
      "Epoch: 8 \tTraining Loss: 0.000435\n",
      "Epoch: 9 \tTraining Loss: 0.000435\n",
      "Epoch: 10 \tTraining Loss: 0.000435\n",
      "Epoch: 11 \tTraining Loss: 0.000435\n",
      "Epoch: 12 \tTraining Loss: 0.000435\n",
      "Epoch: 13 \tTraining Loss: 0.000435\n",
      "Epoch: 14 \tTraining Loss: 0.000435\n",
      "Epoch: 15 \tTraining Loss: 0.000435\n",
      "Epoch: 16 \tTraining Loss: 0.000435\n",
      "Epoch: 17 \tTraining Loss: 0.000435\n",
      "Epoch: 18 \tTraining Loss: 0.000435\n",
      "Epoch: 1 \tTraining Loss: 0.000121\n",
      "Epoch: 2 \tTraining Loss: 0.000121\n",
      "Epoch: 3 \tTraining Loss: 0.000121\n",
      "Epoch: 4 \tTraining Loss: 0.000121\n",
      "Epoch: 5 \tTraining Loss: 0.000121\n",
      "Epoch: 6 \tTraining Loss: 0.000121\n",
      "Epoch: 7 \tTraining Loss: 0.000121\n",
      "Epoch: 8 \tTraining Loss: 0.000121\n",
      "Epoch: 9 \tTraining Loss: 0.000121\n",
      "Epoch: 10 \tTraining Loss: 0.000121\n",
      "Epoch: 11 \tTraining Loss: 0.000121\n",
      "Epoch: 12 \tTraining Loss: 0.000121\n",
      "Epoch: 13 \tTraining Loss: 0.000121\n",
      "Epoch: 14 \tTraining Loss: 0.000121\n",
      "Epoch: 15 \tTraining Loss: 0.000121\n",
      "Epoch: 16 \tTraining Loss: 0.000121\n",
      "Epoch: 17 \tTraining Loss: 0.000121\n",
      "Epoch: 18 \tTraining Loss: 0.000121\n",
      "Epoch: 1 \tTraining Loss: 0.000130\n",
      "Epoch: 2 \tTraining Loss: 0.000130\n",
      "Epoch: 3 \tTraining Loss: 0.000130\n",
      "Epoch: 4 \tTraining Loss: 0.000130\n",
      "Epoch: 5 \tTraining Loss: 0.000130\n",
      "Epoch: 6 \tTraining Loss: 0.000130\n",
      "Epoch: 7 \tTraining Loss: 0.000130\n",
      "Epoch: 8 \tTraining Loss: 0.000130\n",
      "Epoch: 9 \tTraining Loss: 0.000130\n",
      "Epoch: 10 \tTraining Loss: 0.000130\n",
      "Epoch: 11 \tTraining Loss: 0.000130\n",
      "Epoch: 12 \tTraining Loss: 0.000130\n",
      "Epoch: 13 \tTraining Loss: 0.000130\n",
      "Epoch: 14 \tTraining Loss: 0.000130\n",
      "Epoch: 15 \tTraining Loss: 0.000130\n",
      "Epoch: 16 \tTraining Loss: 0.000130\n",
      "Epoch: 17 \tTraining Loss: 0.000130\n",
      "Epoch: 18 \tTraining Loss: 0.000130\n",
      "Epoch: 1 \tTraining Loss: 0.000161\n",
      "Epoch: 2 \tTraining Loss: 0.000161\n",
      "Epoch: 3 \tTraining Loss: 0.000161\n",
      "Epoch: 4 \tTraining Loss: 0.000161\n",
      "Epoch: 5 \tTraining Loss: 0.000161\n",
      "Epoch: 6 \tTraining Loss: 0.000161\n",
      "Epoch: 7 \tTraining Loss: 0.000161\n",
      "Epoch: 8 \tTraining Loss: 0.000161\n",
      "Epoch: 9 \tTraining Loss: 0.000161\n",
      "Epoch: 10 \tTraining Loss: 0.000161\n",
      "Epoch: 11 \tTraining Loss: 0.000161\n",
      "Epoch: 12 \tTraining Loss: 0.000161\n",
      "Epoch: 13 \tTraining Loss: 0.000161\n",
      "Epoch: 14 \tTraining Loss: 0.000161\n",
      "Epoch: 15 \tTraining Loss: 0.000161\n",
      "Epoch: 16 \tTraining Loss: 0.000161\n",
      "Epoch: 17 \tTraining Loss: 0.000161\n",
      "Epoch: 18 \tTraining Loss: 0.000161\n",
      "Epoch: 1 \tTraining Loss: 0.000354\n",
      "Epoch: 2 \tTraining Loss: 0.000354\n",
      "Epoch: 3 \tTraining Loss: 0.000354\n",
      "Epoch: 4 \tTraining Loss: 0.000354\n",
      "Epoch: 5 \tTraining Loss: 0.000354\n",
      "Epoch: 6 \tTraining Loss: 0.000354\n",
      "Epoch: 7 \tTraining Loss: 0.000354\n",
      "Epoch: 8 \tTraining Loss: 0.000354\n",
      "Epoch: 9 \tTraining Loss: 0.000354\n",
      "Epoch: 10 \tTraining Loss: 0.000354\n",
      "Epoch: 11 \tTraining Loss: 0.000354\n",
      "Epoch: 12 \tTraining Loss: 0.000354\n",
      "Epoch: 13 \tTraining Loss: 0.000354\n",
      "Epoch: 14 \tTraining Loss: 0.000354\n",
      "Epoch: 15 \tTraining Loss: 0.000354\n",
      "Epoch: 16 \tTraining Loss: 0.000354\n",
      "Epoch: 17 \tTraining Loss: 0.000354\n",
      "Epoch: 18 \tTraining Loss: 0.000354\n",
      "Epoch: 1 \tTraining Loss: 0.000157\n",
      "Epoch: 2 \tTraining Loss: 0.000157\n",
      "Epoch: 3 \tTraining Loss: 0.000157\n",
      "Epoch: 4 \tTraining Loss: 0.000157\n",
      "Epoch: 5 \tTraining Loss: 0.000157\n",
      "Epoch: 6 \tTraining Loss: 0.000157\n",
      "Epoch: 7 \tTraining Loss: 0.000157\n",
      "Epoch: 8 \tTraining Loss: 0.000157\n",
      "Epoch: 9 \tTraining Loss: 0.000157\n",
      "Epoch: 10 \tTraining Loss: 0.000157\n",
      "Epoch: 11 \tTraining Loss: 0.000157\n",
      "Epoch: 12 \tTraining Loss: 0.000157\n",
      "Epoch: 13 \tTraining Loss: 0.000157\n",
      "Epoch: 14 \tTraining Loss: 0.000157\n",
      "Epoch: 15 \tTraining Loss: 0.000157\n",
      "Epoch: 16 \tTraining Loss: 0.000157\n",
      "Epoch: 17 \tTraining Loss: 0.000157\n",
      "Epoch: 18 \tTraining Loss: 0.000157\n",
      "Epoch: 1 \tTraining Loss: 0.000395\n",
      "Epoch: 2 \tTraining Loss: 0.000395\n",
      "Epoch: 3 \tTraining Loss: 0.000395\n",
      "Epoch: 4 \tTraining Loss: 0.000395\n",
      "Epoch: 5 \tTraining Loss: 0.000395\n",
      "Epoch: 6 \tTraining Loss: 0.000395\n",
      "Epoch: 7 \tTraining Loss: 0.000395\n",
      "Epoch: 8 \tTraining Loss: 0.000395\n",
      "Epoch: 9 \tTraining Loss: 0.000395\n",
      "Epoch: 10 \tTraining Loss: 0.000395\n",
      "Epoch: 11 \tTraining Loss: 0.000395\n",
      "Epoch: 12 \tTraining Loss: 0.000395\n",
      "Epoch: 13 \tTraining Loss: 0.000395\n",
      "Epoch: 14 \tTraining Loss: 0.000395\n",
      "Epoch: 15 \tTraining Loss: 0.000395\n",
      "Epoch: 16 \tTraining Loss: 0.000395\n",
      "Epoch: 17 \tTraining Loss: 0.000395\n",
      "Epoch: 18 \tTraining Loss: 0.000395\n",
      "Epoch: 1 \tTraining Loss: 0.000272\n",
      "Epoch: 2 \tTraining Loss: 0.000272\n",
      "Epoch: 3 \tTraining Loss: 0.000272\n",
      "Epoch: 4 \tTraining Loss: 0.000272\n",
      "Epoch: 5 \tTraining Loss: 0.000272\n",
      "Epoch: 6 \tTraining Loss: 0.000272\n",
      "Epoch: 7 \tTraining Loss: 0.000272\n",
      "Epoch: 8 \tTraining Loss: 0.000272\n",
      "Epoch: 9 \tTraining Loss: 0.000272\n",
      "Epoch: 10 \tTraining Loss: 0.000272\n",
      "Epoch: 11 \tTraining Loss: 0.000272\n",
      "Epoch: 12 \tTraining Loss: 0.000272\n",
      "Epoch: 13 \tTraining Loss: 0.000272\n",
      "Epoch: 14 \tTraining Loss: 0.000272\n",
      "Epoch: 15 \tTraining Loss: 0.000272\n",
      "Epoch: 16 \tTraining Loss: 0.000272\n",
      "Epoch: 17 \tTraining Loss: 0.000272\n",
      "Epoch: 18 \tTraining Loss: 0.000272\n",
      "Epoch: 1 \tTraining Loss: 0.000273\n",
      "Epoch: 2 \tTraining Loss: 0.000273\n",
      "Epoch: 3 \tTraining Loss: 0.000273\n",
      "Epoch: 4 \tTraining Loss: 0.000273\n",
      "Epoch: 5 \tTraining Loss: 0.000273\n",
      "Epoch: 6 \tTraining Loss: 0.000273\n",
      "Epoch: 7 \tTraining Loss: 0.000273\n",
      "Epoch: 8 \tTraining Loss: 0.000273\n",
      "Epoch: 9 \tTraining Loss: 0.000273\n",
      "Epoch: 10 \tTraining Loss: 0.000273\n",
      "Epoch: 11 \tTraining Loss: 0.000273\n",
      "Epoch: 12 \tTraining Loss: 0.000273\n",
      "Epoch: 13 \tTraining Loss: 0.000273\n",
      "Epoch: 14 \tTraining Loss: 0.000273\n",
      "Epoch: 15 \tTraining Loss: 0.000273\n",
      "Epoch: 16 \tTraining Loss: 0.000273\n",
      "Epoch: 17 \tTraining Loss: 0.000273\n",
      "Epoch: 18 \tTraining Loss: 0.000273\n",
      "Epoch: 1 \tTraining Loss: 0.000208\n",
      "Epoch: 2 \tTraining Loss: 0.000208\n",
      "Epoch: 3 \tTraining Loss: 0.000208\n",
      "Epoch: 4 \tTraining Loss: 0.000208\n",
      "Epoch: 5 \tTraining Loss: 0.000208\n",
      "Epoch: 6 \tTraining Loss: 0.000208\n",
      "Epoch: 7 \tTraining Loss: 0.000208\n",
      "Epoch: 8 \tTraining Loss: 0.000208\n",
      "Epoch: 9 \tTraining Loss: 0.000208\n",
      "Epoch: 10 \tTraining Loss: 0.000208\n",
      "Epoch: 11 \tTraining Loss: 0.000208\n",
      "Epoch: 12 \tTraining Loss: 0.000208\n",
      "Epoch: 13 \tTraining Loss: 0.000208\n",
      "Epoch: 14 \tTraining Loss: 0.000208\n",
      "Epoch: 15 \tTraining Loss: 0.000208\n",
      "Epoch: 16 \tTraining Loss: 0.000208\n",
      "Epoch: 17 \tTraining Loss: 0.000208\n",
      "Epoch: 18 \tTraining Loss: 0.000208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000146\n",
      "Epoch: 2 \tTraining Loss: 0.000146\n",
      "Epoch: 3 \tTraining Loss: 0.000146\n",
      "Epoch: 4 \tTraining Loss: 0.000146\n",
      "Epoch: 5 \tTraining Loss: 0.000146\n",
      "Epoch: 6 \tTraining Loss: 0.000146\n",
      "Epoch: 7 \tTraining Loss: 0.000146\n",
      "Epoch: 8 \tTraining Loss: 0.000146\n",
      "Epoch: 9 \tTraining Loss: 0.000146\n",
      "Epoch: 10 \tTraining Loss: 0.000146\n",
      "Epoch: 11 \tTraining Loss: 0.000146\n",
      "Epoch: 12 \tTraining Loss: 0.000146\n",
      "Epoch: 13 \tTraining Loss: 0.000146\n",
      "Epoch: 14 \tTraining Loss: 0.000146\n",
      "Epoch: 15 \tTraining Loss: 0.000146\n",
      "Epoch: 16 \tTraining Loss: 0.000146\n",
      "Epoch: 17 \tTraining Loss: 0.000146\n",
      "Epoch: 18 \tTraining Loss: 0.000146\n",
      "Epoch: 1 \tTraining Loss: 0.000261\n",
      "Epoch: 2 \tTraining Loss: 0.000261\n",
      "Epoch: 3 \tTraining Loss: 0.000261\n",
      "Epoch: 4 \tTraining Loss: 0.000261\n",
      "Epoch: 5 \tTraining Loss: 0.000261\n",
      "Epoch: 6 \tTraining Loss: 0.000261\n",
      "Epoch: 7 \tTraining Loss: 0.000261\n",
      "Epoch: 8 \tTraining Loss: 0.000261\n",
      "Epoch: 9 \tTraining Loss: 0.000261\n",
      "Epoch: 10 \tTraining Loss: 0.000261\n",
      "Epoch: 11 \tTraining Loss: 0.000261\n",
      "Epoch: 12 \tTraining Loss: 0.000261\n",
      "Epoch: 13 \tTraining Loss: 0.000261\n",
      "Epoch: 14 \tTraining Loss: 0.000261\n",
      "Epoch: 15 \tTraining Loss: 0.000261\n",
      "Epoch: 16 \tTraining Loss: 0.000261\n",
      "Epoch: 17 \tTraining Loss: 0.000261\n",
      "Epoch: 18 \tTraining Loss: 0.000261\n",
      "Epoch: 1 \tTraining Loss: 0.000219\n",
      "Epoch: 2 \tTraining Loss: 0.000219\n",
      "Epoch: 3 \tTraining Loss: 0.000219\n",
      "Epoch: 4 \tTraining Loss: 0.000219\n",
      "Epoch: 5 \tTraining Loss: 0.000219\n",
      "Epoch: 6 \tTraining Loss: 0.000219\n",
      "Epoch: 7 \tTraining Loss: 0.000219\n",
      "Epoch: 8 \tTraining Loss: 0.000219\n",
      "Epoch: 9 \tTraining Loss: 0.000219\n",
      "Epoch: 10 \tTraining Loss: 0.000219\n",
      "Epoch: 11 \tTraining Loss: 0.000219\n",
      "Epoch: 12 \tTraining Loss: 0.000219\n",
      "Epoch: 13 \tTraining Loss: 0.000219\n",
      "Epoch: 14 \tTraining Loss: 0.000219\n",
      "Epoch: 15 \tTraining Loss: 0.000219\n",
      "Epoch: 16 \tTraining Loss: 0.000219\n",
      "Epoch: 17 \tTraining Loss: 0.000219\n",
      "Epoch: 18 \tTraining Loss: 0.000219\n",
      "Epoch: 1 \tTraining Loss: 0.000398\n",
      "Epoch: 2 \tTraining Loss: 0.000398\n",
      "Epoch: 3 \tTraining Loss: 0.000398\n",
      "Epoch: 4 \tTraining Loss: 0.000398\n",
      "Epoch: 5 \tTraining Loss: 0.000398\n",
      "Epoch: 6 \tTraining Loss: 0.000398\n",
      "Epoch: 7 \tTraining Loss: 0.000398\n",
      "Epoch: 8 \tTraining Loss: 0.000398\n",
      "Epoch: 9 \tTraining Loss: 0.000398\n",
      "Epoch: 10 \tTraining Loss: 0.000398\n",
      "Epoch: 11 \tTraining Loss: 0.000398\n",
      "Epoch: 12 \tTraining Loss: 0.000398\n",
      "Epoch: 13 \tTraining Loss: 0.000398\n",
      "Epoch: 14 \tTraining Loss: 0.000398\n",
      "Epoch: 15 \tTraining Loss: 0.000398\n",
      "Epoch: 16 \tTraining Loss: 0.000398\n",
      "Epoch: 17 \tTraining Loss: 0.000398\n",
      "Epoch: 18 \tTraining Loss: 0.000398\n",
      "Epoch: 1 \tTraining Loss: 0.000284\n",
      "Epoch: 2 \tTraining Loss: 0.000284\n",
      "Epoch: 3 \tTraining Loss: 0.000284\n",
      "Epoch: 4 \tTraining Loss: 0.000284\n",
      "Epoch: 5 \tTraining Loss: 0.000284\n",
      "Epoch: 6 \tTraining Loss: 0.000284\n",
      "Epoch: 7 \tTraining Loss: 0.000284\n",
      "Epoch: 8 \tTraining Loss: 0.000284\n",
      "Epoch: 9 \tTraining Loss: 0.000284\n",
      "Epoch: 10 \tTraining Loss: 0.000284\n",
      "Epoch: 11 \tTraining Loss: 0.000284\n",
      "Epoch: 12 \tTraining Loss: 0.000284\n",
      "Epoch: 13 \tTraining Loss: 0.000284\n",
      "Epoch: 14 \tTraining Loss: 0.000284\n",
      "Epoch: 15 \tTraining Loss: 0.000284\n",
      "Epoch: 16 \tTraining Loss: 0.000284\n",
      "Epoch: 17 \tTraining Loss: 0.000284\n",
      "Epoch: 18 \tTraining Loss: 0.000284\n",
      "Epoch: 1 \tTraining Loss: 0.000196\n",
      "Epoch: 2 \tTraining Loss: 0.000196\n",
      "Epoch: 3 \tTraining Loss: 0.000196\n",
      "Epoch: 4 \tTraining Loss: 0.000196\n",
      "Epoch: 5 \tTraining Loss: 0.000196\n",
      "Epoch: 6 \tTraining Loss: 0.000196\n",
      "Epoch: 7 \tTraining Loss: 0.000196\n",
      "Epoch: 8 \tTraining Loss: 0.000196\n",
      "Epoch: 9 \tTraining Loss: 0.000196\n",
      "Epoch: 10 \tTraining Loss: 0.000196\n",
      "Epoch: 11 \tTraining Loss: 0.000196\n",
      "Epoch: 12 \tTraining Loss: 0.000196\n",
      "Epoch: 13 \tTraining Loss: 0.000196\n",
      "Epoch: 14 \tTraining Loss: 0.000196\n",
      "Epoch: 15 \tTraining Loss: 0.000196\n",
      "Epoch: 16 \tTraining Loss: 0.000196\n",
      "Epoch: 17 \tTraining Loss: 0.000196\n",
      "Epoch: 18 \tTraining Loss: 0.000196\n",
      "Epoch: 1 \tTraining Loss: 0.000408\n",
      "Epoch: 2 \tTraining Loss: 0.000408\n",
      "Epoch: 3 \tTraining Loss: 0.000408\n",
      "Epoch: 4 \tTraining Loss: 0.000408\n",
      "Epoch: 5 \tTraining Loss: 0.000408\n",
      "Epoch: 6 \tTraining Loss: 0.000408\n",
      "Epoch: 7 \tTraining Loss: 0.000408\n",
      "Epoch: 8 \tTraining Loss: 0.000408\n",
      "Epoch: 9 \tTraining Loss: 0.000408\n",
      "Epoch: 10 \tTraining Loss: 0.000408\n",
      "Epoch: 11 \tTraining Loss: 0.000408\n",
      "Epoch: 12 \tTraining Loss: 0.000408\n",
      "Epoch: 13 \tTraining Loss: 0.000408\n",
      "Epoch: 14 \tTraining Loss: 0.000408\n",
      "Epoch: 15 \tTraining Loss: 0.000408\n",
      "Epoch: 16 \tTraining Loss: 0.000408\n",
      "Epoch: 17 \tTraining Loss: 0.000408\n",
      "Epoch: 18 \tTraining Loss: 0.000408\n",
      "Epoch: 1 \tTraining Loss: 0.000247\n",
      "Epoch: 2 \tTraining Loss: 0.000247\n",
      "Epoch: 3 \tTraining Loss: 0.000247\n",
      "Epoch: 4 \tTraining Loss: 0.000247\n",
      "Epoch: 5 \tTraining Loss: 0.000247\n",
      "Epoch: 6 \tTraining Loss: 0.000247\n",
      "Epoch: 7 \tTraining Loss: 0.000247\n",
      "Epoch: 8 \tTraining Loss: 0.000247\n",
      "Epoch: 9 \tTraining Loss: 0.000247\n",
      "Epoch: 10 \tTraining Loss: 0.000247\n",
      "Epoch: 11 \tTraining Loss: 0.000247\n",
      "Epoch: 12 \tTraining Loss: 0.000247\n",
      "Epoch: 13 \tTraining Loss: 0.000247\n",
      "Epoch: 14 \tTraining Loss: 0.000247\n",
      "Epoch: 15 \tTraining Loss: 0.000247\n",
      "Epoch: 16 \tTraining Loss: 0.000247\n",
      "Epoch: 17 \tTraining Loss: 0.000247\n",
      "Epoch: 18 \tTraining Loss: 0.000247\n",
      "Epoch: 1 \tTraining Loss: 0.000239\n",
      "Epoch: 2 \tTraining Loss: 0.000239\n",
      "Epoch: 3 \tTraining Loss: 0.000239\n",
      "Epoch: 4 \tTraining Loss: 0.000239\n",
      "Epoch: 5 \tTraining Loss: 0.000239\n",
      "Epoch: 6 \tTraining Loss: 0.000239\n",
      "Epoch: 7 \tTraining Loss: 0.000239\n",
      "Epoch: 8 \tTraining Loss: 0.000239\n",
      "Epoch: 9 \tTraining Loss: 0.000239\n",
      "Epoch: 10 \tTraining Loss: 0.000239\n",
      "Epoch: 11 \tTraining Loss: 0.000239\n",
      "Epoch: 12 \tTraining Loss: 0.000239\n",
      "Epoch: 13 \tTraining Loss: 0.000239\n",
      "Epoch: 14 \tTraining Loss: 0.000239\n",
      "Epoch: 15 \tTraining Loss: 0.000239\n",
      "Epoch: 16 \tTraining Loss: 0.000239\n",
      "Epoch: 17 \tTraining Loss: 0.000239\n",
      "Epoch: 18 \tTraining Loss: 0.000239\n",
      "Epoch: 1 \tTraining Loss: 0.000291\n",
      "Epoch: 2 \tTraining Loss: 0.000291\n",
      "Epoch: 3 \tTraining Loss: 0.000291\n",
      "Epoch: 4 \tTraining Loss: 0.000291\n",
      "Epoch: 5 \tTraining Loss: 0.000291\n",
      "Epoch: 6 \tTraining Loss: 0.000291\n",
      "Epoch: 7 \tTraining Loss: 0.000291\n",
      "Epoch: 8 \tTraining Loss: 0.000291\n",
      "Epoch: 9 \tTraining Loss: 0.000291\n",
      "Epoch: 10 \tTraining Loss: 0.000291\n",
      "Epoch: 11 \tTraining Loss: 0.000291\n",
      "Epoch: 12 \tTraining Loss: 0.000291\n",
      "Epoch: 13 \tTraining Loss: 0.000291\n",
      "Epoch: 14 \tTraining Loss: 0.000291\n",
      "Epoch: 15 \tTraining Loss: 0.000291\n",
      "Epoch: 16 \tTraining Loss: 0.000291\n",
      "Epoch: 17 \tTraining Loss: 0.000291\n",
      "Epoch: 18 \tTraining Loss: 0.000291\n",
      "Epoch: 1 \tTraining Loss: 0.000202\n",
      "Epoch: 2 \tTraining Loss: 0.000202\n",
      "Epoch: 3 \tTraining Loss: 0.000202\n",
      "Epoch: 4 \tTraining Loss: 0.000202\n",
      "Epoch: 5 \tTraining Loss: 0.000202\n",
      "Epoch: 6 \tTraining Loss: 0.000202\n",
      "Epoch: 7 \tTraining Loss: 0.000202\n",
      "Epoch: 8 \tTraining Loss: 0.000202\n",
      "Epoch: 9 \tTraining Loss: 0.000202\n",
      "Epoch: 10 \tTraining Loss: 0.000202\n",
      "Epoch: 11 \tTraining Loss: 0.000202\n",
      "Epoch: 12 \tTraining Loss: 0.000202\n",
      "Epoch: 13 \tTraining Loss: 0.000202\n",
      "Epoch: 14 \tTraining Loss: 0.000202\n",
      "Epoch: 15 \tTraining Loss: 0.000202\n",
      "Epoch: 16 \tTraining Loss: 0.000202\n",
      "Epoch: 17 \tTraining Loss: 0.000202\n",
      "Epoch: 18 \tTraining Loss: 0.000202\n",
      "Epoch: 1 \tTraining Loss: 0.000172\n",
      "Epoch: 2 \tTraining Loss: 0.000172\n",
      "Epoch: 3 \tTraining Loss: 0.000172\n",
      "Epoch: 4 \tTraining Loss: 0.000172\n",
      "Epoch: 5 \tTraining Loss: 0.000172\n",
      "Epoch: 6 \tTraining Loss: 0.000172\n",
      "Epoch: 7 \tTraining Loss: 0.000172\n",
      "Epoch: 8 \tTraining Loss: 0.000172\n",
      "Epoch: 9 \tTraining Loss: 0.000172\n",
      "Epoch: 10 \tTraining Loss: 0.000172\n",
      "Epoch: 11 \tTraining Loss: 0.000172\n",
      "Epoch: 12 \tTraining Loss: 0.000172\n",
      "Epoch: 13 \tTraining Loss: 0.000172\n",
      "Epoch: 14 \tTraining Loss: 0.000172\n",
      "Epoch: 15 \tTraining Loss: 0.000172\n",
      "Epoch: 16 \tTraining Loss: 0.000172\n",
      "Epoch: 17 \tTraining Loss: 0.000172\n",
      "Epoch: 18 \tTraining Loss: 0.000172\n",
      "Epoch: 1 \tTraining Loss: 0.000264\n",
      "Epoch: 2 \tTraining Loss: 0.000264\n",
      "Epoch: 3 \tTraining Loss: 0.000264\n",
      "Epoch: 4 \tTraining Loss: 0.000264\n",
      "Epoch: 5 \tTraining Loss: 0.000264\n",
      "Epoch: 6 \tTraining Loss: 0.000264\n",
      "Epoch: 7 \tTraining Loss: 0.000264\n",
      "Epoch: 8 \tTraining Loss: 0.000264\n",
      "Epoch: 9 \tTraining Loss: 0.000264\n",
      "Epoch: 10 \tTraining Loss: 0.000264\n",
      "Epoch: 11 \tTraining Loss: 0.000264\n",
      "Epoch: 12 \tTraining Loss: 0.000264\n",
      "Epoch: 13 \tTraining Loss: 0.000264\n",
      "Epoch: 14 \tTraining Loss: 0.000264\n",
      "Epoch: 15 \tTraining Loss: 0.000264\n",
      "Epoch: 16 \tTraining Loss: 0.000264\n",
      "Epoch: 17 \tTraining Loss: 0.000264\n",
      "Epoch: 18 \tTraining Loss: 0.000264\n",
      "Epoch: 1 \tTraining Loss: 0.000255\n",
      "Epoch: 2 \tTraining Loss: 0.000255\n",
      "Epoch: 3 \tTraining Loss: 0.000255\n",
      "Epoch: 4 \tTraining Loss: 0.000255\n",
      "Epoch: 5 \tTraining Loss: 0.000255\n",
      "Epoch: 6 \tTraining Loss: 0.000255\n",
      "Epoch: 7 \tTraining Loss: 0.000255\n",
      "Epoch: 8 \tTraining Loss: 0.000255\n",
      "Epoch: 9 \tTraining Loss: 0.000255\n",
      "Epoch: 10 \tTraining Loss: 0.000255\n",
      "Epoch: 11 \tTraining Loss: 0.000255\n",
      "Epoch: 12 \tTraining Loss: 0.000255\n",
      "Epoch: 13 \tTraining Loss: 0.000255\n",
      "Epoch: 14 \tTraining Loss: 0.000255\n",
      "Epoch: 15 \tTraining Loss: 0.000255\n",
      "Epoch: 16 \tTraining Loss: 0.000255\n",
      "Epoch: 17 \tTraining Loss: 0.000255\n",
      "Epoch: 18 \tTraining Loss: 0.000255\n",
      "Epoch: 1 \tTraining Loss: 0.000149\n",
      "Epoch: 2 \tTraining Loss: 0.000149\n",
      "Epoch: 3 \tTraining Loss: 0.000149\n",
      "Epoch: 4 \tTraining Loss: 0.000149\n",
      "Epoch: 5 \tTraining Loss: 0.000149\n",
      "Epoch: 6 \tTraining Loss: 0.000149\n",
      "Epoch: 7 \tTraining Loss: 0.000149\n",
      "Epoch: 8 \tTraining Loss: 0.000149\n",
      "Epoch: 9 \tTraining Loss: 0.000149\n",
      "Epoch: 10 \tTraining Loss: 0.000149\n",
      "Epoch: 11 \tTraining Loss: 0.000149\n",
      "Epoch: 12 \tTraining Loss: 0.000149\n",
      "Epoch: 13 \tTraining Loss: 0.000149\n",
      "Epoch: 14 \tTraining Loss: 0.000149\n",
      "Epoch: 15 \tTraining Loss: 0.000149\n",
      "Epoch: 16 \tTraining Loss: 0.000149\n",
      "Epoch: 17 \tTraining Loss: 0.000149\n",
      "Epoch: 18 \tTraining Loss: 0.000149\n",
      "Epoch: 1 \tTraining Loss: 0.000160\n",
      "Epoch: 2 \tTraining Loss: 0.000160\n",
      "Epoch: 3 \tTraining Loss: 0.000160\n",
      "Epoch: 4 \tTraining Loss: 0.000160\n",
      "Epoch: 5 \tTraining Loss: 0.000160\n",
      "Epoch: 6 \tTraining Loss: 0.000160\n",
      "Epoch: 7 \tTraining Loss: 0.000160\n",
      "Epoch: 8 \tTraining Loss: 0.000160\n",
      "Epoch: 9 \tTraining Loss: 0.000160\n",
      "Epoch: 10 \tTraining Loss: 0.000160\n",
      "Epoch: 11 \tTraining Loss: 0.000160\n",
      "Epoch: 12 \tTraining Loss: 0.000160\n",
      "Epoch: 13 \tTraining Loss: 0.000160\n",
      "Epoch: 14 \tTraining Loss: 0.000160\n",
      "Epoch: 15 \tTraining Loss: 0.000160\n",
      "Epoch: 16 \tTraining Loss: 0.000160\n",
      "Epoch: 17 \tTraining Loss: 0.000160\n",
      "Epoch: 18 \tTraining Loss: 0.000160\n",
      "Epoch: 1 \tTraining Loss: 0.000246\n",
      "Epoch: 2 \tTraining Loss: 0.000246\n",
      "Epoch: 3 \tTraining Loss: 0.000246\n",
      "Epoch: 4 \tTraining Loss: 0.000246\n",
      "Epoch: 5 \tTraining Loss: 0.000246\n",
      "Epoch: 6 \tTraining Loss: 0.000246\n",
      "Epoch: 7 \tTraining Loss: 0.000246\n",
      "Epoch: 8 \tTraining Loss: 0.000246\n",
      "Epoch: 9 \tTraining Loss: 0.000246\n",
      "Epoch: 10 \tTraining Loss: 0.000246\n",
      "Epoch: 11 \tTraining Loss: 0.000246\n",
      "Epoch: 12 \tTraining Loss: 0.000246\n",
      "Epoch: 13 \tTraining Loss: 0.000246\n",
      "Epoch: 14 \tTraining Loss: 0.000246\n",
      "Epoch: 15 \tTraining Loss: 0.000246\n",
      "Epoch: 16 \tTraining Loss: 0.000246\n",
      "Epoch: 17 \tTraining Loss: 0.000246\n",
      "Epoch: 18 \tTraining Loss: 0.000246\n",
      "Epoch: 1 \tTraining Loss: 0.000217\n",
      "Epoch: 2 \tTraining Loss: 0.000217\n",
      "Epoch: 3 \tTraining Loss: 0.000217\n",
      "Epoch: 4 \tTraining Loss: 0.000217\n",
      "Epoch: 5 \tTraining Loss: 0.000217\n",
      "Epoch: 6 \tTraining Loss: 0.000217\n",
      "Epoch: 7 \tTraining Loss: 0.000217\n",
      "Epoch: 8 \tTraining Loss: 0.000217\n",
      "Epoch: 9 \tTraining Loss: 0.000217\n",
      "Epoch: 10 \tTraining Loss: 0.000217\n",
      "Epoch: 11 \tTraining Loss: 0.000217\n",
      "Epoch: 12 \tTraining Loss: 0.000217\n",
      "Epoch: 13 \tTraining Loss: 0.000217\n",
      "Epoch: 14 \tTraining Loss: 0.000217\n",
      "Epoch: 15 \tTraining Loss: 0.000217\n",
      "Epoch: 16 \tTraining Loss: 0.000217\n",
      "Epoch: 17 \tTraining Loss: 0.000217\n",
      "Epoch: 18 \tTraining Loss: 0.000217\n",
      "Epoch: 1 \tTraining Loss: 0.000107\n",
      "Epoch: 2 \tTraining Loss: 0.000107\n",
      "Epoch: 3 \tTraining Loss: 0.000107\n",
      "Epoch: 4 \tTraining Loss: 0.000107\n",
      "Epoch: 5 \tTraining Loss: 0.000107\n",
      "Epoch: 6 \tTraining Loss: 0.000107\n",
      "Epoch: 7 \tTraining Loss: 0.000107\n",
      "Epoch: 8 \tTraining Loss: 0.000107\n",
      "Epoch: 9 \tTraining Loss: 0.000107\n",
      "Epoch: 10 \tTraining Loss: 0.000107\n",
      "Epoch: 11 \tTraining Loss: 0.000107\n",
      "Epoch: 12 \tTraining Loss: 0.000107\n",
      "Epoch: 13 \tTraining Loss: 0.000107\n",
      "Epoch: 14 \tTraining Loss: 0.000107\n",
      "Epoch: 15 \tTraining Loss: 0.000107\n",
      "Epoch: 16 \tTraining Loss: 0.000107\n",
      "Epoch: 17 \tTraining Loss: 0.000107\n",
      "Epoch: 18 \tTraining Loss: 0.000107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000196\n",
      "Epoch: 2 \tTraining Loss: 0.000196\n",
      "Epoch: 3 \tTraining Loss: 0.000196\n",
      "Epoch: 4 \tTraining Loss: 0.000196\n",
      "Epoch: 5 \tTraining Loss: 0.000196\n",
      "Epoch: 6 \tTraining Loss: 0.000196\n",
      "Epoch: 7 \tTraining Loss: 0.000196\n",
      "Epoch: 8 \tTraining Loss: 0.000196\n",
      "Epoch: 9 \tTraining Loss: 0.000196\n",
      "Epoch: 10 \tTraining Loss: 0.000196\n",
      "Epoch: 11 \tTraining Loss: 0.000196\n",
      "Epoch: 12 \tTraining Loss: 0.000196\n",
      "Epoch: 13 \tTraining Loss: 0.000196\n",
      "Epoch: 14 \tTraining Loss: 0.000196\n",
      "Epoch: 15 \tTraining Loss: 0.000196\n",
      "Epoch: 16 \tTraining Loss: 0.000196\n",
      "Epoch: 17 \tTraining Loss: 0.000196\n",
      "Epoch: 18 \tTraining Loss: 0.000196\n",
      "Epoch: 1 \tTraining Loss: 0.000245\n",
      "Epoch: 2 \tTraining Loss: 0.000245\n",
      "Epoch: 3 \tTraining Loss: 0.000245\n",
      "Epoch: 4 \tTraining Loss: 0.000245\n",
      "Epoch: 5 \tTraining Loss: 0.000245\n",
      "Epoch: 6 \tTraining Loss: 0.000245\n",
      "Epoch: 7 \tTraining Loss: 0.000245\n",
      "Epoch: 8 \tTraining Loss: 0.000245\n",
      "Epoch: 9 \tTraining Loss: 0.000245\n",
      "Epoch: 10 \tTraining Loss: 0.000245\n",
      "Epoch: 11 \tTraining Loss: 0.000245\n",
      "Epoch: 12 \tTraining Loss: 0.000245\n",
      "Epoch: 13 \tTraining Loss: 0.000245\n",
      "Epoch: 14 \tTraining Loss: 0.000245\n",
      "Epoch: 15 \tTraining Loss: 0.000245\n",
      "Epoch: 16 \tTraining Loss: 0.000245\n",
      "Epoch: 17 \tTraining Loss: 0.000245\n",
      "Epoch: 18 \tTraining Loss: 0.000245\n",
      "Epoch: 1 \tTraining Loss: 0.000195\n",
      "Epoch: 2 \tTraining Loss: 0.000195\n",
      "Epoch: 3 \tTraining Loss: 0.000195\n",
      "Epoch: 4 \tTraining Loss: 0.000195\n",
      "Epoch: 5 \tTraining Loss: 0.000195\n",
      "Epoch: 6 \tTraining Loss: 0.000195\n",
      "Epoch: 7 \tTraining Loss: 0.000195\n",
      "Epoch: 8 \tTraining Loss: 0.000195\n",
      "Epoch: 9 \tTraining Loss: 0.000195\n",
      "Epoch: 10 \tTraining Loss: 0.000195\n",
      "Epoch: 11 \tTraining Loss: 0.000195\n",
      "Epoch: 12 \tTraining Loss: 0.000195\n",
      "Epoch: 13 \tTraining Loss: 0.000195\n",
      "Epoch: 14 \tTraining Loss: 0.000195\n",
      "Epoch: 15 \tTraining Loss: 0.000195\n",
      "Epoch: 16 \tTraining Loss: 0.000195\n",
      "Epoch: 17 \tTraining Loss: 0.000195\n",
      "Epoch: 18 \tTraining Loss: 0.000195\n",
      "Epoch: 1 \tTraining Loss: 0.000198\n",
      "Epoch: 2 \tTraining Loss: 0.000198\n",
      "Epoch: 3 \tTraining Loss: 0.000198\n",
      "Epoch: 4 \tTraining Loss: 0.000198\n",
      "Epoch: 5 \tTraining Loss: 0.000198\n",
      "Epoch: 6 \tTraining Loss: 0.000198\n",
      "Epoch: 7 \tTraining Loss: 0.000198\n",
      "Epoch: 8 \tTraining Loss: 0.000198\n",
      "Epoch: 9 \tTraining Loss: 0.000198\n",
      "Epoch: 10 \tTraining Loss: 0.000198\n",
      "Epoch: 11 \tTraining Loss: 0.000198\n",
      "Epoch: 12 \tTraining Loss: 0.000198\n",
      "Epoch: 13 \tTraining Loss: 0.000198\n",
      "Epoch: 14 \tTraining Loss: 0.000198\n",
      "Epoch: 15 \tTraining Loss: 0.000198\n",
      "Epoch: 16 \tTraining Loss: 0.000198\n",
      "Epoch: 17 \tTraining Loss: 0.000198\n",
      "Epoch: 18 \tTraining Loss: 0.000198\n",
      "Epoch: 1 \tTraining Loss: 0.000115\n",
      "Epoch: 2 \tTraining Loss: 0.000115\n",
      "Epoch: 3 \tTraining Loss: 0.000115\n",
      "Epoch: 4 \tTraining Loss: 0.000115\n",
      "Epoch: 5 \tTraining Loss: 0.000115\n",
      "Epoch: 6 \tTraining Loss: 0.000115\n",
      "Epoch: 7 \tTraining Loss: 0.000115\n",
      "Epoch: 8 \tTraining Loss: 0.000115\n",
      "Epoch: 9 \tTraining Loss: 0.000115\n",
      "Epoch: 10 \tTraining Loss: 0.000115\n",
      "Epoch: 11 \tTraining Loss: 0.000115\n",
      "Epoch: 12 \tTraining Loss: 0.000115\n",
      "Epoch: 13 \tTraining Loss: 0.000115\n",
      "Epoch: 14 \tTraining Loss: 0.000115\n",
      "Epoch: 15 \tTraining Loss: 0.000115\n",
      "Epoch: 16 \tTraining Loss: 0.000115\n",
      "Epoch: 17 \tTraining Loss: 0.000115\n",
      "Epoch: 18 \tTraining Loss: 0.000115\n",
      "Epoch: 1 \tTraining Loss: 0.000182\n",
      "Epoch: 2 \tTraining Loss: 0.000182\n",
      "Epoch: 3 \tTraining Loss: 0.000182\n",
      "Epoch: 4 \tTraining Loss: 0.000182\n",
      "Epoch: 5 \tTraining Loss: 0.000182\n",
      "Epoch: 6 \tTraining Loss: 0.000182\n",
      "Epoch: 7 \tTraining Loss: 0.000182\n",
      "Epoch: 8 \tTraining Loss: 0.000182\n",
      "Epoch: 9 \tTraining Loss: 0.000182\n",
      "Epoch: 10 \tTraining Loss: 0.000182\n",
      "Epoch: 11 \tTraining Loss: 0.000182\n",
      "Epoch: 12 \tTraining Loss: 0.000182\n",
      "Epoch: 13 \tTraining Loss: 0.000182\n",
      "Epoch: 14 \tTraining Loss: 0.000182\n",
      "Epoch: 15 \tTraining Loss: 0.000182\n",
      "Epoch: 16 \tTraining Loss: 0.000182\n",
      "Epoch: 17 \tTraining Loss: 0.000182\n",
      "Epoch: 18 \tTraining Loss: 0.000182\n",
      "Epoch: 1 \tTraining Loss: 0.000203\n",
      "Epoch: 2 \tTraining Loss: 0.000203\n",
      "Epoch: 3 \tTraining Loss: 0.000203\n",
      "Epoch: 4 \tTraining Loss: 0.000203\n",
      "Epoch: 5 \tTraining Loss: 0.000203\n",
      "Epoch: 6 \tTraining Loss: 0.000203\n",
      "Epoch: 7 \tTraining Loss: 0.000203\n",
      "Epoch: 8 \tTraining Loss: 0.000203\n",
      "Epoch: 9 \tTraining Loss: 0.000203\n",
      "Epoch: 10 \tTraining Loss: 0.000203\n",
      "Epoch: 11 \tTraining Loss: 0.000203\n",
      "Epoch: 12 \tTraining Loss: 0.000203\n",
      "Epoch: 13 \tTraining Loss: 0.000203\n",
      "Epoch: 14 \tTraining Loss: 0.000203\n",
      "Epoch: 15 \tTraining Loss: 0.000203\n",
      "Epoch: 16 \tTraining Loss: 0.000203\n",
      "Epoch: 17 \tTraining Loss: 0.000203\n",
      "Epoch: 18 \tTraining Loss: 0.000203\n",
      "Epoch: 1 \tTraining Loss: 0.000261\n",
      "Epoch: 2 \tTraining Loss: 0.000261\n",
      "Epoch: 3 \tTraining Loss: 0.000261\n",
      "Epoch: 4 \tTraining Loss: 0.000261\n",
      "Epoch: 5 \tTraining Loss: 0.000261\n",
      "Epoch: 6 \tTraining Loss: 0.000261\n",
      "Epoch: 7 \tTraining Loss: 0.000261\n",
      "Epoch: 8 \tTraining Loss: 0.000261\n",
      "Epoch: 9 \tTraining Loss: 0.000261\n",
      "Epoch: 10 \tTraining Loss: 0.000261\n",
      "Epoch: 11 \tTraining Loss: 0.000261\n",
      "Epoch: 12 \tTraining Loss: 0.000261\n",
      "Epoch: 13 \tTraining Loss: 0.000261\n",
      "Epoch: 14 \tTraining Loss: 0.000261\n",
      "Epoch: 15 \tTraining Loss: 0.000261\n",
      "Epoch: 16 \tTraining Loss: 0.000261\n",
      "Epoch: 17 \tTraining Loss: 0.000261\n",
      "Epoch: 18 \tTraining Loss: 0.000261\n",
      "Epoch: 1 \tTraining Loss: 0.000192\n",
      "Epoch: 2 \tTraining Loss: 0.000192\n",
      "Epoch: 3 \tTraining Loss: 0.000192\n",
      "Epoch: 4 \tTraining Loss: 0.000192\n",
      "Epoch: 5 \tTraining Loss: 0.000192\n",
      "Epoch: 6 \tTraining Loss: 0.000192\n",
      "Epoch: 7 \tTraining Loss: 0.000192\n",
      "Epoch: 8 \tTraining Loss: 0.000192\n",
      "Epoch: 9 \tTraining Loss: 0.000192\n",
      "Epoch: 10 \tTraining Loss: 0.000192\n",
      "Epoch: 11 \tTraining Loss: 0.000192\n",
      "Epoch: 12 \tTraining Loss: 0.000192\n",
      "Epoch: 13 \tTraining Loss: 0.000192\n",
      "Epoch: 14 \tTraining Loss: 0.000192\n",
      "Epoch: 15 \tTraining Loss: 0.000192\n",
      "Epoch: 16 \tTraining Loss: 0.000192\n",
      "Epoch: 17 \tTraining Loss: 0.000192\n",
      "Epoch: 18 \tTraining Loss: 0.000192\n",
      "Epoch: 1 \tTraining Loss: 0.000140\n",
      "Epoch: 2 \tTraining Loss: 0.000140\n",
      "Epoch: 3 \tTraining Loss: 0.000140\n",
      "Epoch: 4 \tTraining Loss: 0.000140\n",
      "Epoch: 5 \tTraining Loss: 0.000140\n",
      "Epoch: 6 \tTraining Loss: 0.000140\n",
      "Epoch: 7 \tTraining Loss: 0.000140\n",
      "Epoch: 8 \tTraining Loss: 0.000140\n",
      "Epoch: 9 \tTraining Loss: 0.000140\n",
      "Epoch: 10 \tTraining Loss: 0.000140\n",
      "Epoch: 11 \tTraining Loss: 0.000140\n",
      "Epoch: 12 \tTraining Loss: 0.000140\n",
      "Epoch: 13 \tTraining Loss: 0.000140\n",
      "Epoch: 14 \tTraining Loss: 0.000140\n",
      "Epoch: 15 \tTraining Loss: 0.000140\n",
      "Epoch: 16 \tTraining Loss: 0.000140\n",
      "Epoch: 17 \tTraining Loss: 0.000140\n",
      "Epoch: 18 \tTraining Loss: 0.000140\n",
      "Epoch: 1 \tTraining Loss: 0.000158\n",
      "Epoch: 2 \tTraining Loss: 0.000158\n",
      "Epoch: 3 \tTraining Loss: 0.000158\n",
      "Epoch: 4 \tTraining Loss: 0.000158\n",
      "Epoch: 5 \tTraining Loss: 0.000158\n",
      "Epoch: 6 \tTraining Loss: 0.000158\n",
      "Epoch: 7 \tTraining Loss: 0.000158\n",
      "Epoch: 8 \tTraining Loss: 0.000158\n",
      "Epoch: 9 \tTraining Loss: 0.000158\n",
      "Epoch: 10 \tTraining Loss: 0.000158\n",
      "Epoch: 11 \tTraining Loss: 0.000158\n",
      "Epoch: 12 \tTraining Loss: 0.000158\n",
      "Epoch: 13 \tTraining Loss: 0.000158\n",
      "Epoch: 14 \tTraining Loss: 0.000158\n",
      "Epoch: 15 \tTraining Loss: 0.000158\n",
      "Epoch: 16 \tTraining Loss: 0.000158\n",
      "Epoch: 17 \tTraining Loss: 0.000158\n",
      "Epoch: 18 \tTraining Loss: 0.000158\n",
      "Epoch: 1 \tTraining Loss: 0.000244\n",
      "Epoch: 2 \tTraining Loss: 0.000244\n",
      "Epoch: 3 \tTraining Loss: 0.000244\n",
      "Epoch: 4 \tTraining Loss: 0.000244\n",
      "Epoch: 5 \tTraining Loss: 0.000244\n",
      "Epoch: 6 \tTraining Loss: 0.000244\n",
      "Epoch: 7 \tTraining Loss: 0.000244\n",
      "Epoch: 8 \tTraining Loss: 0.000244\n",
      "Epoch: 9 \tTraining Loss: 0.000244\n",
      "Epoch: 10 \tTraining Loss: 0.000244\n",
      "Epoch: 11 \tTraining Loss: 0.000244\n",
      "Epoch: 12 \tTraining Loss: 0.000244\n",
      "Epoch: 13 \tTraining Loss: 0.000244\n",
      "Epoch: 14 \tTraining Loss: 0.000244\n",
      "Epoch: 15 \tTraining Loss: 0.000244\n",
      "Epoch: 16 \tTraining Loss: 0.000244\n",
      "Epoch: 17 \tTraining Loss: 0.000244\n",
      "Epoch: 18 \tTraining Loss: 0.000244\n",
      "Epoch: 1 \tTraining Loss: 0.000232\n",
      "Epoch: 2 \tTraining Loss: 0.000232\n",
      "Epoch: 3 \tTraining Loss: 0.000232\n",
      "Epoch: 4 \tTraining Loss: 0.000232\n",
      "Epoch: 5 \tTraining Loss: 0.000232\n",
      "Epoch: 6 \tTraining Loss: 0.000232\n",
      "Epoch: 7 \tTraining Loss: 0.000232\n",
      "Epoch: 8 \tTraining Loss: 0.000232\n",
      "Epoch: 9 \tTraining Loss: 0.000232\n",
      "Epoch: 10 \tTraining Loss: 0.000232\n",
      "Epoch: 11 \tTraining Loss: 0.000232\n",
      "Epoch: 12 \tTraining Loss: 0.000232\n",
      "Epoch: 13 \tTraining Loss: 0.000232\n",
      "Epoch: 14 \tTraining Loss: 0.000232\n",
      "Epoch: 15 \tTraining Loss: 0.000232\n",
      "Epoch: 16 \tTraining Loss: 0.000232\n",
      "Epoch: 17 \tTraining Loss: 0.000232\n",
      "Epoch: 18 \tTraining Loss: 0.000232\n",
      "Epoch: 1 \tTraining Loss: 0.000173\n",
      "Epoch: 2 \tTraining Loss: 0.000173\n",
      "Epoch: 3 \tTraining Loss: 0.000173\n",
      "Epoch: 4 \tTraining Loss: 0.000173\n",
      "Epoch: 5 \tTraining Loss: 0.000173\n",
      "Epoch: 6 \tTraining Loss: 0.000173\n",
      "Epoch: 7 \tTraining Loss: 0.000173\n",
      "Epoch: 8 \tTraining Loss: 0.000173\n",
      "Epoch: 9 \tTraining Loss: 0.000173\n",
      "Epoch: 10 \tTraining Loss: 0.000173\n",
      "Epoch: 11 \tTraining Loss: 0.000173\n",
      "Epoch: 12 \tTraining Loss: 0.000173\n",
      "Epoch: 13 \tTraining Loss: 0.000173\n",
      "Epoch: 14 \tTraining Loss: 0.000173\n",
      "Epoch: 15 \tTraining Loss: 0.000173\n",
      "Epoch: 16 \tTraining Loss: 0.000173\n",
      "Epoch: 17 \tTraining Loss: 0.000173\n",
      "Epoch: 18 \tTraining Loss: 0.000173\n",
      "Epoch: 1 \tTraining Loss: 0.000247\n",
      "Epoch: 2 \tTraining Loss: 0.000247\n",
      "Epoch: 3 \tTraining Loss: 0.000247\n",
      "Epoch: 4 \tTraining Loss: 0.000247\n",
      "Epoch: 5 \tTraining Loss: 0.000247\n",
      "Epoch: 6 \tTraining Loss: 0.000247\n",
      "Epoch: 7 \tTraining Loss: 0.000247\n",
      "Epoch: 8 \tTraining Loss: 0.000247\n",
      "Epoch: 9 \tTraining Loss: 0.000247\n",
      "Epoch: 10 \tTraining Loss: 0.000247\n",
      "Epoch: 11 \tTraining Loss: 0.000247\n",
      "Epoch: 12 \tTraining Loss: 0.000247\n",
      "Epoch: 13 \tTraining Loss: 0.000247\n",
      "Epoch: 14 \tTraining Loss: 0.000247\n",
      "Epoch: 15 \tTraining Loss: 0.000247\n",
      "Epoch: 16 \tTraining Loss: 0.000247\n",
      "Epoch: 17 \tTraining Loss: 0.000247\n",
      "Epoch: 18 \tTraining Loss: 0.000247\n",
      "Epoch: 1 \tTraining Loss: 0.000182\n",
      "Epoch: 2 \tTraining Loss: 0.000182\n",
      "Epoch: 3 \tTraining Loss: 0.000182\n",
      "Epoch: 4 \tTraining Loss: 0.000182\n",
      "Epoch: 5 \tTraining Loss: 0.000182\n",
      "Epoch: 6 \tTraining Loss: 0.000182\n",
      "Epoch: 7 \tTraining Loss: 0.000182\n",
      "Epoch: 8 \tTraining Loss: 0.000182\n",
      "Epoch: 9 \tTraining Loss: 0.000182\n",
      "Epoch: 10 \tTraining Loss: 0.000182\n",
      "Epoch: 11 \tTraining Loss: 0.000182\n",
      "Epoch: 12 \tTraining Loss: 0.000182\n",
      "Epoch: 13 \tTraining Loss: 0.000182\n",
      "Epoch: 14 \tTraining Loss: 0.000182\n",
      "Epoch: 15 \tTraining Loss: 0.000182\n",
      "Epoch: 16 \tTraining Loss: 0.000182\n",
      "Epoch: 17 \tTraining Loss: 0.000182\n",
      "Epoch: 18 \tTraining Loss: 0.000182\n",
      "Epoch: 1 \tTraining Loss: 0.000298\n",
      "Epoch: 2 \tTraining Loss: 0.000298\n",
      "Epoch: 3 \tTraining Loss: 0.000298\n",
      "Epoch: 4 \tTraining Loss: 0.000298\n",
      "Epoch: 5 \tTraining Loss: 0.000298\n",
      "Epoch: 6 \tTraining Loss: 0.000298\n",
      "Epoch: 7 \tTraining Loss: 0.000298\n",
      "Epoch: 8 \tTraining Loss: 0.000298\n",
      "Epoch: 9 \tTraining Loss: 0.000298\n",
      "Epoch: 10 \tTraining Loss: 0.000298\n",
      "Epoch: 11 \tTraining Loss: 0.000298\n",
      "Epoch: 12 \tTraining Loss: 0.000298\n",
      "Epoch: 13 \tTraining Loss: 0.000298\n",
      "Epoch: 14 \tTraining Loss: 0.000298\n",
      "Epoch: 15 \tTraining Loss: 0.000298\n",
      "Epoch: 16 \tTraining Loss: 0.000298\n",
      "Epoch: 17 \tTraining Loss: 0.000298\n",
      "Epoch: 18 \tTraining Loss: 0.000298\n",
      "Epoch: 1 \tTraining Loss: 0.000150\n",
      "Epoch: 2 \tTraining Loss: 0.000150\n",
      "Epoch: 3 \tTraining Loss: 0.000150\n",
      "Epoch: 4 \tTraining Loss: 0.000150\n",
      "Epoch: 5 \tTraining Loss: 0.000150\n",
      "Epoch: 6 \tTraining Loss: 0.000150\n",
      "Epoch: 7 \tTraining Loss: 0.000150\n",
      "Epoch: 8 \tTraining Loss: 0.000150\n",
      "Epoch: 9 \tTraining Loss: 0.000150\n",
      "Epoch: 10 \tTraining Loss: 0.000150\n",
      "Epoch: 11 \tTraining Loss: 0.000150\n",
      "Epoch: 12 \tTraining Loss: 0.000150\n",
      "Epoch: 13 \tTraining Loss: 0.000150\n",
      "Epoch: 14 \tTraining Loss: 0.000150\n",
      "Epoch: 15 \tTraining Loss: 0.000150\n",
      "Epoch: 16 \tTraining Loss: 0.000150\n",
      "Epoch: 17 \tTraining Loss: 0.000150\n",
      "Epoch: 18 \tTraining Loss: 0.000150\n",
      "Epoch: 1 \tTraining Loss: 0.000215\n",
      "Epoch: 2 \tTraining Loss: 0.000215\n",
      "Epoch: 3 \tTraining Loss: 0.000215\n",
      "Epoch: 4 \tTraining Loss: 0.000215\n",
      "Epoch: 5 \tTraining Loss: 0.000215\n",
      "Epoch: 6 \tTraining Loss: 0.000215\n",
      "Epoch: 7 \tTraining Loss: 0.000215\n",
      "Epoch: 8 \tTraining Loss: 0.000215\n",
      "Epoch: 9 \tTraining Loss: 0.000215\n",
      "Epoch: 10 \tTraining Loss: 0.000215\n",
      "Epoch: 11 \tTraining Loss: 0.000215\n",
      "Epoch: 12 \tTraining Loss: 0.000215\n",
      "Epoch: 13 \tTraining Loss: 0.000215\n",
      "Epoch: 14 \tTraining Loss: 0.000215\n",
      "Epoch: 15 \tTraining Loss: 0.000215\n",
      "Epoch: 16 \tTraining Loss: 0.000215\n",
      "Epoch: 17 \tTraining Loss: 0.000215\n",
      "Epoch: 18 \tTraining Loss: 0.000215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000170\n",
      "Epoch: 2 \tTraining Loss: 0.000170\n",
      "Epoch: 3 \tTraining Loss: 0.000170\n",
      "Epoch: 4 \tTraining Loss: 0.000170\n",
      "Epoch: 5 \tTraining Loss: 0.000170\n",
      "Epoch: 6 \tTraining Loss: 0.000170\n",
      "Epoch: 7 \tTraining Loss: 0.000170\n",
      "Epoch: 8 \tTraining Loss: 0.000170\n",
      "Epoch: 9 \tTraining Loss: 0.000170\n",
      "Epoch: 10 \tTraining Loss: 0.000170\n",
      "Epoch: 11 \tTraining Loss: 0.000170\n",
      "Epoch: 12 \tTraining Loss: 0.000170\n",
      "Epoch: 13 \tTraining Loss: 0.000170\n",
      "Epoch: 14 \tTraining Loss: 0.000170\n",
      "Epoch: 15 \tTraining Loss: 0.000170\n",
      "Epoch: 16 \tTraining Loss: 0.000170\n",
      "Epoch: 17 \tTraining Loss: 0.000170\n",
      "Epoch: 18 \tTraining Loss: 0.000170\n",
      "Epoch: 1 \tTraining Loss: 0.000184\n",
      "Epoch: 2 \tTraining Loss: 0.000184\n",
      "Epoch: 3 \tTraining Loss: 0.000184\n",
      "Epoch: 4 \tTraining Loss: 0.000184\n",
      "Epoch: 5 \tTraining Loss: 0.000184\n",
      "Epoch: 6 \tTraining Loss: 0.000184\n",
      "Epoch: 7 \tTraining Loss: 0.000184\n",
      "Epoch: 8 \tTraining Loss: 0.000184\n",
      "Epoch: 9 \tTraining Loss: 0.000184\n",
      "Epoch: 10 \tTraining Loss: 0.000184\n",
      "Epoch: 11 \tTraining Loss: 0.000184\n",
      "Epoch: 12 \tTraining Loss: 0.000184\n",
      "Epoch: 13 \tTraining Loss: 0.000184\n",
      "Epoch: 14 \tTraining Loss: 0.000184\n",
      "Epoch: 15 \tTraining Loss: 0.000184\n",
      "Epoch: 16 \tTraining Loss: 0.000184\n",
      "Epoch: 17 \tTraining Loss: 0.000184\n",
      "Epoch: 18 \tTraining Loss: 0.000184\n",
      "Epoch: 1 \tTraining Loss: 0.000211\n",
      "Epoch: 2 \tTraining Loss: 0.000211\n",
      "Epoch: 3 \tTraining Loss: 0.000211\n",
      "Epoch: 4 \tTraining Loss: 0.000211\n",
      "Epoch: 5 \tTraining Loss: 0.000211\n",
      "Epoch: 6 \tTraining Loss: 0.000211\n",
      "Epoch: 7 \tTraining Loss: 0.000211\n",
      "Epoch: 8 \tTraining Loss: 0.000211\n",
      "Epoch: 9 \tTraining Loss: 0.000211\n",
      "Epoch: 10 \tTraining Loss: 0.000211\n",
      "Epoch: 11 \tTraining Loss: 0.000211\n",
      "Epoch: 12 \tTraining Loss: 0.000211\n",
      "Epoch: 13 \tTraining Loss: 0.000211\n",
      "Epoch: 14 \tTraining Loss: 0.000211\n",
      "Epoch: 15 \tTraining Loss: 0.000211\n",
      "Epoch: 16 \tTraining Loss: 0.000211\n",
      "Epoch: 17 \tTraining Loss: 0.000211\n",
      "Epoch: 18 \tTraining Loss: 0.000211\n",
      "Epoch: 1 \tTraining Loss: 0.000197\n",
      "Epoch: 2 \tTraining Loss: 0.000197\n",
      "Epoch: 3 \tTraining Loss: 0.000197\n",
      "Epoch: 4 \tTraining Loss: 0.000197\n",
      "Epoch: 5 \tTraining Loss: 0.000197\n",
      "Epoch: 6 \tTraining Loss: 0.000197\n",
      "Epoch: 7 \tTraining Loss: 0.000197\n",
      "Epoch: 8 \tTraining Loss: 0.000197\n",
      "Epoch: 9 \tTraining Loss: 0.000197\n",
      "Epoch: 10 \tTraining Loss: 0.000197\n",
      "Epoch: 11 \tTraining Loss: 0.000197\n",
      "Epoch: 12 \tTraining Loss: 0.000197\n",
      "Epoch: 13 \tTraining Loss: 0.000197\n",
      "Epoch: 14 \tTraining Loss: 0.000197\n",
      "Epoch: 15 \tTraining Loss: 0.000197\n",
      "Epoch: 16 \tTraining Loss: 0.000197\n",
      "Epoch: 17 \tTraining Loss: 0.000197\n",
      "Epoch: 18 \tTraining Loss: 0.000197\n",
      "Epoch: 1 \tTraining Loss: 0.000132\n",
      "Epoch: 2 \tTraining Loss: 0.000132\n",
      "Epoch: 3 \tTraining Loss: 0.000132\n",
      "Epoch: 4 \tTraining Loss: 0.000132\n",
      "Epoch: 5 \tTraining Loss: 0.000132\n",
      "Epoch: 6 \tTraining Loss: 0.000132\n",
      "Epoch: 7 \tTraining Loss: 0.000132\n",
      "Epoch: 8 \tTraining Loss: 0.000132\n",
      "Epoch: 9 \tTraining Loss: 0.000132\n",
      "Epoch: 10 \tTraining Loss: 0.000132\n",
      "Epoch: 11 \tTraining Loss: 0.000132\n",
      "Epoch: 12 \tTraining Loss: 0.000132\n",
      "Epoch: 13 \tTraining Loss: 0.000132\n",
      "Epoch: 14 \tTraining Loss: 0.000132\n",
      "Epoch: 15 \tTraining Loss: 0.000132\n",
      "Epoch: 16 \tTraining Loss: 0.000132\n",
      "Epoch: 17 \tTraining Loss: 0.000132\n",
      "Epoch: 18 \tTraining Loss: 0.000132\n",
      "Epoch: 1 \tTraining Loss: 0.000241\n",
      "Epoch: 2 \tTraining Loss: 0.000241\n",
      "Epoch: 3 \tTraining Loss: 0.000241\n",
      "Epoch: 4 \tTraining Loss: 0.000241\n",
      "Epoch: 5 \tTraining Loss: 0.000241\n",
      "Epoch: 6 \tTraining Loss: 0.000241\n",
      "Epoch: 7 \tTraining Loss: 0.000241\n",
      "Epoch: 8 \tTraining Loss: 0.000241\n",
      "Epoch: 9 \tTraining Loss: 0.000241\n",
      "Epoch: 10 \tTraining Loss: 0.000241\n",
      "Epoch: 11 \tTraining Loss: 0.000241\n",
      "Epoch: 12 \tTraining Loss: 0.000241\n",
      "Epoch: 13 \tTraining Loss: 0.000241\n",
      "Epoch: 14 \tTraining Loss: 0.000241\n",
      "Epoch: 15 \tTraining Loss: 0.000241\n",
      "Epoch: 16 \tTraining Loss: 0.000241\n",
      "Epoch: 17 \tTraining Loss: 0.000241\n",
      "Epoch: 18 \tTraining Loss: 0.000241\n",
      "Epoch: 1 \tTraining Loss: 0.000220\n",
      "Epoch: 2 \tTraining Loss: 0.000220\n",
      "Epoch: 3 \tTraining Loss: 0.000220\n",
      "Epoch: 4 \tTraining Loss: 0.000220\n",
      "Epoch: 5 \tTraining Loss: 0.000220\n",
      "Epoch: 6 \tTraining Loss: 0.000220\n",
      "Epoch: 7 \tTraining Loss: 0.000220\n",
      "Epoch: 8 \tTraining Loss: 0.000220\n",
      "Epoch: 9 \tTraining Loss: 0.000220\n",
      "Epoch: 10 \tTraining Loss: 0.000220\n",
      "Epoch: 11 \tTraining Loss: 0.000220\n",
      "Epoch: 12 \tTraining Loss: 0.000220\n",
      "Epoch: 13 \tTraining Loss: 0.000220\n",
      "Epoch: 14 \tTraining Loss: 0.000220\n",
      "Epoch: 15 \tTraining Loss: 0.000220\n",
      "Epoch: 16 \tTraining Loss: 0.000220\n",
      "Epoch: 17 \tTraining Loss: 0.000220\n",
      "Epoch: 18 \tTraining Loss: 0.000220\n",
      "Epoch: 1 \tTraining Loss: 0.000198\n",
      "Epoch: 2 \tTraining Loss: 0.000198\n",
      "Epoch: 3 \tTraining Loss: 0.000198\n",
      "Epoch: 4 \tTraining Loss: 0.000198\n",
      "Epoch: 5 \tTraining Loss: 0.000198\n",
      "Epoch: 6 \tTraining Loss: 0.000198\n",
      "Epoch: 7 \tTraining Loss: 0.000198\n",
      "Epoch: 8 \tTraining Loss: 0.000198\n",
      "Epoch: 9 \tTraining Loss: 0.000198\n",
      "Epoch: 10 \tTraining Loss: 0.000198\n",
      "Epoch: 11 \tTraining Loss: 0.000198\n",
      "Epoch: 12 \tTraining Loss: 0.000198\n",
      "Epoch: 13 \tTraining Loss: 0.000198\n",
      "Epoch: 14 \tTraining Loss: 0.000198\n",
      "Epoch: 15 \tTraining Loss: 0.000198\n",
      "Epoch: 16 \tTraining Loss: 0.000198\n",
      "Epoch: 17 \tTraining Loss: 0.000198\n",
      "Epoch: 18 \tTraining Loss: 0.000198\n",
      "Epoch: 1 \tTraining Loss: 0.000276\n",
      "Epoch: 2 \tTraining Loss: 0.000276\n",
      "Epoch: 3 \tTraining Loss: 0.000276\n",
      "Epoch: 4 \tTraining Loss: 0.000276\n",
      "Epoch: 5 \tTraining Loss: 0.000276\n",
      "Epoch: 6 \tTraining Loss: 0.000276\n",
      "Epoch: 7 \tTraining Loss: 0.000276\n",
      "Epoch: 8 \tTraining Loss: 0.000276\n",
      "Epoch: 9 \tTraining Loss: 0.000276\n",
      "Epoch: 10 \tTraining Loss: 0.000276\n",
      "Epoch: 11 \tTraining Loss: 0.000276\n",
      "Epoch: 12 \tTraining Loss: 0.000276\n",
      "Epoch: 13 \tTraining Loss: 0.000276\n",
      "Epoch: 14 \tTraining Loss: 0.000276\n",
      "Epoch: 15 \tTraining Loss: 0.000276\n",
      "Epoch: 16 \tTraining Loss: 0.000276\n",
      "Epoch: 17 \tTraining Loss: 0.000276\n",
      "Epoch: 18 \tTraining Loss: 0.000276\n",
      "Epoch: 1 \tTraining Loss: 0.000313\n",
      "Epoch: 2 \tTraining Loss: 0.000313\n",
      "Epoch: 3 \tTraining Loss: 0.000313\n",
      "Epoch: 4 \tTraining Loss: 0.000313\n",
      "Epoch: 5 \tTraining Loss: 0.000313\n",
      "Epoch: 6 \tTraining Loss: 0.000313\n",
      "Epoch: 7 \tTraining Loss: 0.000313\n",
      "Epoch: 8 \tTraining Loss: 0.000313\n",
      "Epoch: 9 \tTraining Loss: 0.000313\n",
      "Epoch: 10 \tTraining Loss: 0.000313\n",
      "Epoch: 11 \tTraining Loss: 0.000313\n",
      "Epoch: 12 \tTraining Loss: 0.000313\n",
      "Epoch: 13 \tTraining Loss: 0.000313\n",
      "Epoch: 14 \tTraining Loss: 0.000313\n",
      "Epoch: 15 \tTraining Loss: 0.000313\n",
      "Epoch: 16 \tTraining Loss: 0.000313\n",
      "Epoch: 17 \tTraining Loss: 0.000313\n",
      "Epoch: 18 \tTraining Loss: 0.000313\n",
      "Epoch: 1 \tTraining Loss: 0.000284\n",
      "Epoch: 2 \tTraining Loss: 0.000284\n",
      "Epoch: 3 \tTraining Loss: 0.000284\n",
      "Epoch: 4 \tTraining Loss: 0.000284\n",
      "Epoch: 5 \tTraining Loss: 0.000284\n",
      "Epoch: 6 \tTraining Loss: 0.000284\n",
      "Epoch: 7 \tTraining Loss: 0.000284\n",
      "Epoch: 8 \tTraining Loss: 0.000284\n",
      "Epoch: 9 \tTraining Loss: 0.000284\n",
      "Epoch: 10 \tTraining Loss: 0.000284\n",
      "Epoch: 11 \tTraining Loss: 0.000284\n",
      "Epoch: 12 \tTraining Loss: 0.000284\n",
      "Epoch: 13 \tTraining Loss: 0.000284\n",
      "Epoch: 14 \tTraining Loss: 0.000284\n",
      "Epoch: 15 \tTraining Loss: 0.000284\n",
      "Epoch: 16 \tTraining Loss: 0.000284\n",
      "Epoch: 17 \tTraining Loss: 0.000284\n",
      "Epoch: 18 \tTraining Loss: 0.000284\n",
      "Epoch: 1 \tTraining Loss: 0.000150\n",
      "Epoch: 2 \tTraining Loss: 0.000150\n",
      "Epoch: 3 \tTraining Loss: 0.000150\n",
      "Epoch: 4 \tTraining Loss: 0.000150\n",
      "Epoch: 5 \tTraining Loss: 0.000150\n",
      "Epoch: 6 \tTraining Loss: 0.000150\n",
      "Epoch: 7 \tTraining Loss: 0.000150\n",
      "Epoch: 8 \tTraining Loss: 0.000150\n",
      "Epoch: 9 \tTraining Loss: 0.000150\n",
      "Epoch: 10 \tTraining Loss: 0.000150\n",
      "Epoch: 11 \tTraining Loss: 0.000150\n",
      "Epoch: 12 \tTraining Loss: 0.000150\n",
      "Epoch: 13 \tTraining Loss: 0.000150\n",
      "Epoch: 14 \tTraining Loss: 0.000150\n",
      "Epoch: 15 \tTraining Loss: 0.000150\n",
      "Epoch: 16 \tTraining Loss: 0.000150\n",
      "Epoch: 17 \tTraining Loss: 0.000150\n",
      "Epoch: 18 \tTraining Loss: 0.000150\n",
      "Epoch: 1 \tTraining Loss: 0.000318\n",
      "Epoch: 2 \tTraining Loss: 0.000318\n",
      "Epoch: 3 \tTraining Loss: 0.000318\n",
      "Epoch: 4 \tTraining Loss: 0.000318\n",
      "Epoch: 5 \tTraining Loss: 0.000318\n",
      "Epoch: 6 \tTraining Loss: 0.000318\n",
      "Epoch: 7 \tTraining Loss: 0.000318\n",
      "Epoch: 8 \tTraining Loss: 0.000318\n",
      "Epoch: 9 \tTraining Loss: 0.000318\n",
      "Epoch: 10 \tTraining Loss: 0.000318\n",
      "Epoch: 11 \tTraining Loss: 0.000318\n",
      "Epoch: 12 \tTraining Loss: 0.000318\n",
      "Epoch: 13 \tTraining Loss: 0.000318\n",
      "Epoch: 14 \tTraining Loss: 0.000318\n",
      "Epoch: 15 \tTraining Loss: 0.000318\n",
      "Epoch: 16 \tTraining Loss: 0.000318\n",
      "Epoch: 17 \tTraining Loss: 0.000318\n",
      "Epoch: 18 \tTraining Loss: 0.000318\n",
      "Epoch: 1 \tTraining Loss: 0.000112\n",
      "Epoch: 2 \tTraining Loss: 0.000112\n",
      "Epoch: 3 \tTraining Loss: 0.000112\n",
      "Epoch: 4 \tTraining Loss: 0.000112\n",
      "Epoch: 5 \tTraining Loss: 0.000112\n",
      "Epoch: 6 \tTraining Loss: 0.000112\n",
      "Epoch: 7 \tTraining Loss: 0.000112\n",
      "Epoch: 8 \tTraining Loss: 0.000112\n",
      "Epoch: 9 \tTraining Loss: 0.000112\n",
      "Epoch: 10 \tTraining Loss: 0.000112\n",
      "Epoch: 11 \tTraining Loss: 0.000112\n",
      "Epoch: 12 \tTraining Loss: 0.000112\n",
      "Epoch: 13 \tTraining Loss: 0.000112\n",
      "Epoch: 14 \tTraining Loss: 0.000112\n",
      "Epoch: 15 \tTraining Loss: 0.000112\n",
      "Epoch: 16 \tTraining Loss: 0.000112\n",
      "Epoch: 17 \tTraining Loss: 0.000112\n",
      "Epoch: 18 \tTraining Loss: 0.000112\n",
      "Epoch: 1 \tTraining Loss: 0.000289\n",
      "Epoch: 2 \tTraining Loss: 0.000289\n",
      "Epoch: 3 \tTraining Loss: 0.000289\n",
      "Epoch: 4 \tTraining Loss: 0.000289\n",
      "Epoch: 5 \tTraining Loss: 0.000289\n",
      "Epoch: 6 \tTraining Loss: 0.000289\n",
      "Epoch: 7 \tTraining Loss: 0.000289\n",
      "Epoch: 8 \tTraining Loss: 0.000289\n",
      "Epoch: 9 \tTraining Loss: 0.000289\n",
      "Epoch: 10 \tTraining Loss: 0.000289\n",
      "Epoch: 11 \tTraining Loss: 0.000289\n",
      "Epoch: 12 \tTraining Loss: 0.000289\n",
      "Epoch: 13 \tTraining Loss: 0.000289\n",
      "Epoch: 14 \tTraining Loss: 0.000289\n",
      "Epoch: 15 \tTraining Loss: 0.000289\n",
      "Epoch: 16 \tTraining Loss: 0.000289\n",
      "Epoch: 17 \tTraining Loss: 0.000289\n",
      "Epoch: 18 \tTraining Loss: 0.000289\n",
      "Epoch: 1 \tTraining Loss: 0.000414\n",
      "Epoch: 2 \tTraining Loss: 0.000414\n",
      "Epoch: 3 \tTraining Loss: 0.000414\n",
      "Epoch: 4 \tTraining Loss: 0.000414\n",
      "Epoch: 5 \tTraining Loss: 0.000414\n",
      "Epoch: 6 \tTraining Loss: 0.000414\n",
      "Epoch: 7 \tTraining Loss: 0.000414\n",
      "Epoch: 8 \tTraining Loss: 0.000414\n",
      "Epoch: 9 \tTraining Loss: 0.000414\n",
      "Epoch: 10 \tTraining Loss: 0.000414\n",
      "Epoch: 11 \tTraining Loss: 0.000414\n",
      "Epoch: 12 \tTraining Loss: 0.000414\n",
      "Epoch: 13 \tTraining Loss: 0.000414\n",
      "Epoch: 14 \tTraining Loss: 0.000414\n",
      "Epoch: 15 \tTraining Loss: 0.000414\n",
      "Epoch: 16 \tTraining Loss: 0.000414\n",
      "Epoch: 17 \tTraining Loss: 0.000414\n",
      "Epoch: 18 \tTraining Loss: 0.000414\n",
      "Epoch: 1 \tTraining Loss: 0.000228\n",
      "Epoch: 2 \tTraining Loss: 0.000228\n",
      "Epoch: 3 \tTraining Loss: 0.000228\n",
      "Epoch: 4 \tTraining Loss: 0.000228\n",
      "Epoch: 5 \tTraining Loss: 0.000228\n",
      "Epoch: 6 \tTraining Loss: 0.000228\n",
      "Epoch: 7 \tTraining Loss: 0.000228\n",
      "Epoch: 8 \tTraining Loss: 0.000228\n",
      "Epoch: 9 \tTraining Loss: 0.000228\n",
      "Epoch: 10 \tTraining Loss: 0.000228\n",
      "Epoch: 11 \tTraining Loss: 0.000228\n",
      "Epoch: 12 \tTraining Loss: 0.000228\n",
      "Epoch: 13 \tTraining Loss: 0.000228\n",
      "Epoch: 14 \tTraining Loss: 0.000228\n",
      "Epoch: 15 \tTraining Loss: 0.000228\n",
      "Epoch: 16 \tTraining Loss: 0.000228\n",
      "Epoch: 17 \tTraining Loss: 0.000228\n",
      "Epoch: 18 \tTraining Loss: 0.000228\n",
      "Epoch: 1 \tTraining Loss: 0.000259\n",
      "Epoch: 2 \tTraining Loss: 0.000259\n",
      "Epoch: 3 \tTraining Loss: 0.000259\n",
      "Epoch: 4 \tTraining Loss: 0.000259\n",
      "Epoch: 5 \tTraining Loss: 0.000259\n",
      "Epoch: 6 \tTraining Loss: 0.000259\n",
      "Epoch: 7 \tTraining Loss: 0.000259\n",
      "Epoch: 8 \tTraining Loss: 0.000259\n",
      "Epoch: 9 \tTraining Loss: 0.000259\n",
      "Epoch: 10 \tTraining Loss: 0.000259\n",
      "Epoch: 11 \tTraining Loss: 0.000259\n",
      "Epoch: 12 \tTraining Loss: 0.000259\n",
      "Epoch: 13 \tTraining Loss: 0.000259\n",
      "Epoch: 14 \tTraining Loss: 0.000259\n",
      "Epoch: 15 \tTraining Loss: 0.000259\n",
      "Epoch: 16 \tTraining Loss: 0.000259\n",
      "Epoch: 17 \tTraining Loss: 0.000259\n",
      "Epoch: 18 \tTraining Loss: 0.000259\n",
      "Epoch: 1 \tTraining Loss: 0.000118\n",
      "Epoch: 2 \tTraining Loss: 0.000118\n",
      "Epoch: 3 \tTraining Loss: 0.000118\n",
      "Epoch: 4 \tTraining Loss: 0.000118\n",
      "Epoch: 5 \tTraining Loss: 0.000118\n",
      "Epoch: 6 \tTraining Loss: 0.000118\n",
      "Epoch: 7 \tTraining Loss: 0.000118\n",
      "Epoch: 8 \tTraining Loss: 0.000118\n",
      "Epoch: 9 \tTraining Loss: 0.000118\n",
      "Epoch: 10 \tTraining Loss: 0.000118\n",
      "Epoch: 11 \tTraining Loss: 0.000118\n",
      "Epoch: 12 \tTraining Loss: 0.000118\n",
      "Epoch: 13 \tTraining Loss: 0.000118\n",
      "Epoch: 14 \tTraining Loss: 0.000118\n",
      "Epoch: 15 \tTraining Loss: 0.000118\n",
      "Epoch: 16 \tTraining Loss: 0.000118\n",
      "Epoch: 17 \tTraining Loss: 0.000118\n",
      "Epoch: 18 \tTraining Loss: 0.000118\n",
      "Epoch: 1 \tTraining Loss: 0.000244\n",
      "Epoch: 2 \tTraining Loss: 0.000244\n",
      "Epoch: 3 \tTraining Loss: 0.000244\n",
      "Epoch: 4 \tTraining Loss: 0.000244\n",
      "Epoch: 5 \tTraining Loss: 0.000244\n",
      "Epoch: 6 \tTraining Loss: 0.000244\n",
      "Epoch: 7 \tTraining Loss: 0.000244\n",
      "Epoch: 8 \tTraining Loss: 0.000244\n",
      "Epoch: 9 \tTraining Loss: 0.000244\n",
      "Epoch: 10 \tTraining Loss: 0.000244\n",
      "Epoch: 11 \tTraining Loss: 0.000244\n",
      "Epoch: 12 \tTraining Loss: 0.000244\n",
      "Epoch: 13 \tTraining Loss: 0.000244\n",
      "Epoch: 14 \tTraining Loss: 0.000244\n",
      "Epoch: 15 \tTraining Loss: 0.000244\n",
      "Epoch: 16 \tTraining Loss: 0.000244\n",
      "Epoch: 17 \tTraining Loss: 0.000244\n",
      "Epoch: 18 \tTraining Loss: 0.000244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000186\n",
      "Epoch: 2 \tTraining Loss: 0.000186\n",
      "Epoch: 3 \tTraining Loss: 0.000186\n",
      "Epoch: 4 \tTraining Loss: 0.000186\n",
      "Epoch: 5 \tTraining Loss: 0.000186\n",
      "Epoch: 6 \tTraining Loss: 0.000186\n",
      "Epoch: 7 \tTraining Loss: 0.000186\n",
      "Epoch: 8 \tTraining Loss: 0.000186\n",
      "Epoch: 9 \tTraining Loss: 0.000186\n",
      "Epoch: 10 \tTraining Loss: 0.000186\n",
      "Epoch: 11 \tTraining Loss: 0.000186\n",
      "Epoch: 12 \tTraining Loss: 0.000186\n",
      "Epoch: 13 \tTraining Loss: 0.000186\n",
      "Epoch: 14 \tTraining Loss: 0.000186\n",
      "Epoch: 15 \tTraining Loss: 0.000186\n",
      "Epoch: 16 \tTraining Loss: 0.000186\n",
      "Epoch: 17 \tTraining Loss: 0.000186\n",
      "Epoch: 18 \tTraining Loss: 0.000186\n",
      "Epoch: 1 \tTraining Loss: 0.000492\n",
      "Epoch: 2 \tTraining Loss: 0.000492\n",
      "Epoch: 3 \tTraining Loss: 0.000492\n",
      "Epoch: 4 \tTraining Loss: 0.000492\n",
      "Epoch: 5 \tTraining Loss: 0.000492\n",
      "Epoch: 6 \tTraining Loss: 0.000492\n",
      "Epoch: 7 \tTraining Loss: 0.000492\n",
      "Epoch: 8 \tTraining Loss: 0.000492\n",
      "Epoch: 9 \tTraining Loss: 0.000492\n",
      "Epoch: 10 \tTraining Loss: 0.000492\n",
      "Epoch: 11 \tTraining Loss: 0.000492\n",
      "Epoch: 12 \tTraining Loss: 0.000492\n",
      "Epoch: 13 \tTraining Loss: 0.000492\n",
      "Epoch: 14 \tTraining Loss: 0.000492\n",
      "Epoch: 15 \tTraining Loss: 0.000492\n",
      "Epoch: 16 \tTraining Loss: 0.000492\n",
      "Epoch: 17 \tTraining Loss: 0.000492\n",
      "Epoch: 18 \tTraining Loss: 0.000492\n",
      "Epoch: 1 \tTraining Loss: 0.000175\n",
      "Epoch: 2 \tTraining Loss: 0.000175\n",
      "Epoch: 3 \tTraining Loss: 0.000175\n",
      "Epoch: 4 \tTraining Loss: 0.000175\n",
      "Epoch: 5 \tTraining Loss: 0.000175\n",
      "Epoch: 6 \tTraining Loss: 0.000175\n",
      "Epoch: 7 \tTraining Loss: 0.000175\n",
      "Epoch: 8 \tTraining Loss: 0.000175\n",
      "Epoch: 9 \tTraining Loss: 0.000175\n",
      "Epoch: 10 \tTraining Loss: 0.000175\n",
      "Epoch: 11 \tTraining Loss: 0.000175\n",
      "Epoch: 12 \tTraining Loss: 0.000175\n",
      "Epoch: 13 \tTraining Loss: 0.000175\n",
      "Epoch: 14 \tTraining Loss: 0.000175\n",
      "Epoch: 15 \tTraining Loss: 0.000175\n",
      "Epoch: 16 \tTraining Loss: 0.000175\n",
      "Epoch: 17 \tTraining Loss: 0.000175\n",
      "Epoch: 18 \tTraining Loss: 0.000175\n",
      "Epoch: 1 \tTraining Loss: 0.000123\n",
      "Epoch: 2 \tTraining Loss: 0.000123\n",
      "Epoch: 3 \tTraining Loss: 0.000123\n",
      "Epoch: 4 \tTraining Loss: 0.000123\n",
      "Epoch: 5 \tTraining Loss: 0.000123\n",
      "Epoch: 6 \tTraining Loss: 0.000123\n",
      "Epoch: 7 \tTraining Loss: 0.000123\n",
      "Epoch: 8 \tTraining Loss: 0.000123\n",
      "Epoch: 9 \tTraining Loss: 0.000123\n",
      "Epoch: 10 \tTraining Loss: 0.000123\n",
      "Epoch: 11 \tTraining Loss: 0.000123\n",
      "Epoch: 12 \tTraining Loss: 0.000123\n",
      "Epoch: 13 \tTraining Loss: 0.000123\n",
      "Epoch: 14 \tTraining Loss: 0.000123\n",
      "Epoch: 15 \tTraining Loss: 0.000123\n",
      "Epoch: 16 \tTraining Loss: 0.000123\n",
      "Epoch: 17 \tTraining Loss: 0.000123\n",
      "Epoch: 18 \tTraining Loss: 0.000123\n",
      "Epoch: 1 \tTraining Loss: 0.000298\n",
      "Epoch: 2 \tTraining Loss: 0.000298\n",
      "Epoch: 3 \tTraining Loss: 0.000298\n",
      "Epoch: 4 \tTraining Loss: 0.000298\n",
      "Epoch: 5 \tTraining Loss: 0.000298\n",
      "Epoch: 6 \tTraining Loss: 0.000298\n",
      "Epoch: 7 \tTraining Loss: 0.000298\n",
      "Epoch: 8 \tTraining Loss: 0.000298\n",
      "Epoch: 9 \tTraining Loss: 0.000298\n",
      "Epoch: 10 \tTraining Loss: 0.000298\n",
      "Epoch: 11 \tTraining Loss: 0.000298\n",
      "Epoch: 12 \tTraining Loss: 0.000298\n",
      "Epoch: 13 \tTraining Loss: 0.000298\n",
      "Epoch: 14 \tTraining Loss: 0.000298\n",
      "Epoch: 15 \tTraining Loss: 0.000298\n",
      "Epoch: 16 \tTraining Loss: 0.000298\n",
      "Epoch: 17 \tTraining Loss: 0.000298\n",
      "Epoch: 18 \tTraining Loss: 0.000298\n",
      "Epoch: 1 \tTraining Loss: 0.000213\n",
      "Epoch: 2 \tTraining Loss: 0.000213\n",
      "Epoch: 3 \tTraining Loss: 0.000213\n",
      "Epoch: 4 \tTraining Loss: 0.000213\n",
      "Epoch: 5 \tTraining Loss: 0.000213\n",
      "Epoch: 6 \tTraining Loss: 0.000213\n",
      "Epoch: 7 \tTraining Loss: 0.000213\n",
      "Epoch: 8 \tTraining Loss: 0.000213\n",
      "Epoch: 9 \tTraining Loss: 0.000213\n",
      "Epoch: 10 \tTraining Loss: 0.000213\n",
      "Epoch: 11 \tTraining Loss: 0.000213\n",
      "Epoch: 12 \tTraining Loss: 0.000213\n",
      "Epoch: 13 \tTraining Loss: 0.000213\n",
      "Epoch: 14 \tTraining Loss: 0.000213\n",
      "Epoch: 15 \tTraining Loss: 0.000213\n",
      "Epoch: 16 \tTraining Loss: 0.000213\n",
      "Epoch: 17 \tTraining Loss: 0.000213\n",
      "Epoch: 18 \tTraining Loss: 0.000213\n",
      "Epoch: 1 \tTraining Loss: 0.000112\n",
      "Epoch: 2 \tTraining Loss: 0.000112\n",
      "Epoch: 3 \tTraining Loss: 0.000112\n",
      "Epoch: 4 \tTraining Loss: 0.000112\n",
      "Epoch: 5 \tTraining Loss: 0.000112\n",
      "Epoch: 6 \tTraining Loss: 0.000112\n",
      "Epoch: 7 \tTraining Loss: 0.000112\n",
      "Epoch: 8 \tTraining Loss: 0.000112\n",
      "Epoch: 9 \tTraining Loss: 0.000112\n",
      "Epoch: 10 \tTraining Loss: 0.000112\n",
      "Epoch: 11 \tTraining Loss: 0.000112\n",
      "Epoch: 12 \tTraining Loss: 0.000112\n",
      "Epoch: 13 \tTraining Loss: 0.000112\n",
      "Epoch: 14 \tTraining Loss: 0.000112\n",
      "Epoch: 15 \tTraining Loss: 0.000112\n",
      "Epoch: 16 \tTraining Loss: 0.000112\n",
      "Epoch: 17 \tTraining Loss: 0.000112\n",
      "Epoch: 18 \tTraining Loss: 0.000112\n",
      "Epoch: 1 \tTraining Loss: 0.000211\n",
      "Epoch: 2 \tTraining Loss: 0.000211\n",
      "Epoch: 3 \tTraining Loss: 0.000211\n",
      "Epoch: 4 \tTraining Loss: 0.000211\n",
      "Epoch: 5 \tTraining Loss: 0.000211\n",
      "Epoch: 6 \tTraining Loss: 0.000211\n",
      "Epoch: 7 \tTraining Loss: 0.000211\n",
      "Epoch: 8 \tTraining Loss: 0.000211\n",
      "Epoch: 9 \tTraining Loss: 0.000211\n",
      "Epoch: 10 \tTraining Loss: 0.000211\n",
      "Epoch: 11 \tTraining Loss: 0.000211\n",
      "Epoch: 12 \tTraining Loss: 0.000211\n",
      "Epoch: 13 \tTraining Loss: 0.000211\n",
      "Epoch: 14 \tTraining Loss: 0.000211\n",
      "Epoch: 15 \tTraining Loss: 0.000211\n",
      "Epoch: 16 \tTraining Loss: 0.000211\n",
      "Epoch: 17 \tTraining Loss: 0.000211\n",
      "Epoch: 18 \tTraining Loss: 0.000211\n",
      "Epoch: 1 \tTraining Loss: 0.000175\n",
      "Epoch: 2 \tTraining Loss: 0.000175\n",
      "Epoch: 3 \tTraining Loss: 0.000175\n",
      "Epoch: 4 \tTraining Loss: 0.000175\n",
      "Epoch: 5 \tTraining Loss: 0.000175\n",
      "Epoch: 6 \tTraining Loss: 0.000175\n",
      "Epoch: 7 \tTraining Loss: 0.000175\n",
      "Epoch: 8 \tTraining Loss: 0.000175\n",
      "Epoch: 9 \tTraining Loss: 0.000175\n",
      "Epoch: 10 \tTraining Loss: 0.000175\n",
      "Epoch: 11 \tTraining Loss: 0.000175\n",
      "Epoch: 12 \tTraining Loss: 0.000175\n",
      "Epoch: 13 \tTraining Loss: 0.000175\n",
      "Epoch: 14 \tTraining Loss: 0.000175\n",
      "Epoch: 15 \tTraining Loss: 0.000175\n",
      "Epoch: 16 \tTraining Loss: 0.000175\n",
      "Epoch: 17 \tTraining Loss: 0.000175\n",
      "Epoch: 18 \tTraining Loss: 0.000175\n",
      "Epoch: 1 \tTraining Loss: 0.000351\n",
      "Epoch: 2 \tTraining Loss: 0.000351\n",
      "Epoch: 3 \tTraining Loss: 0.000351\n",
      "Epoch: 4 \tTraining Loss: 0.000351\n",
      "Epoch: 5 \tTraining Loss: 0.000351\n",
      "Epoch: 6 \tTraining Loss: 0.000351\n",
      "Epoch: 7 \tTraining Loss: 0.000351\n",
      "Epoch: 8 \tTraining Loss: 0.000351\n",
      "Epoch: 9 \tTraining Loss: 0.000351\n",
      "Epoch: 10 \tTraining Loss: 0.000351\n",
      "Epoch: 11 \tTraining Loss: 0.000351\n",
      "Epoch: 12 \tTraining Loss: 0.000351\n",
      "Epoch: 13 \tTraining Loss: 0.000351\n",
      "Epoch: 14 \tTraining Loss: 0.000351\n",
      "Epoch: 15 \tTraining Loss: 0.000351\n",
      "Epoch: 16 \tTraining Loss: 0.000351\n",
      "Epoch: 17 \tTraining Loss: 0.000351\n",
      "Epoch: 18 \tTraining Loss: 0.000351\n",
      "Epoch: 1 \tTraining Loss: 0.000108\n",
      "Epoch: 2 \tTraining Loss: 0.000108\n",
      "Epoch: 3 \tTraining Loss: 0.000108\n",
      "Epoch: 4 \tTraining Loss: 0.000108\n",
      "Epoch: 5 \tTraining Loss: 0.000108\n",
      "Epoch: 6 \tTraining Loss: 0.000108\n",
      "Epoch: 7 \tTraining Loss: 0.000108\n",
      "Epoch: 8 \tTraining Loss: 0.000108\n",
      "Epoch: 9 \tTraining Loss: 0.000108\n",
      "Epoch: 10 \tTraining Loss: 0.000108\n",
      "Epoch: 11 \tTraining Loss: 0.000108\n",
      "Epoch: 12 \tTraining Loss: 0.000108\n",
      "Epoch: 13 \tTraining Loss: 0.000108\n",
      "Epoch: 14 \tTraining Loss: 0.000108\n",
      "Epoch: 15 \tTraining Loss: 0.000108\n",
      "Epoch: 16 \tTraining Loss: 0.000108\n",
      "Epoch: 17 \tTraining Loss: 0.000108\n",
      "Epoch: 18 \tTraining Loss: 0.000108\n",
      "Epoch: 1 \tTraining Loss: 0.000273\n",
      "Epoch: 2 \tTraining Loss: 0.000273\n",
      "Epoch: 3 \tTraining Loss: 0.000273\n",
      "Epoch: 4 \tTraining Loss: 0.000273\n",
      "Epoch: 5 \tTraining Loss: 0.000273\n",
      "Epoch: 6 \tTraining Loss: 0.000273\n",
      "Epoch: 7 \tTraining Loss: 0.000273\n",
      "Epoch: 8 \tTraining Loss: 0.000273\n",
      "Epoch: 9 \tTraining Loss: 0.000273\n",
      "Epoch: 10 \tTraining Loss: 0.000273\n",
      "Epoch: 11 \tTraining Loss: 0.000273\n",
      "Epoch: 12 \tTraining Loss: 0.000273\n",
      "Epoch: 13 \tTraining Loss: 0.000273\n",
      "Epoch: 14 \tTraining Loss: 0.000273\n",
      "Epoch: 15 \tTraining Loss: 0.000273\n",
      "Epoch: 16 \tTraining Loss: 0.000273\n",
      "Epoch: 17 \tTraining Loss: 0.000273\n",
      "Epoch: 18 \tTraining Loss: 0.000273\n",
      "Epoch: 1 \tTraining Loss: 0.000226\n",
      "Epoch: 2 \tTraining Loss: 0.000226\n",
      "Epoch: 3 \tTraining Loss: 0.000226\n",
      "Epoch: 4 \tTraining Loss: 0.000226\n",
      "Epoch: 5 \tTraining Loss: 0.000226\n",
      "Epoch: 6 \tTraining Loss: 0.000226\n",
      "Epoch: 7 \tTraining Loss: 0.000226\n",
      "Epoch: 8 \tTraining Loss: 0.000226\n",
      "Epoch: 9 \tTraining Loss: 0.000226\n",
      "Epoch: 10 \tTraining Loss: 0.000226\n",
      "Epoch: 11 \tTraining Loss: 0.000226\n",
      "Epoch: 12 \tTraining Loss: 0.000226\n",
      "Epoch: 13 \tTraining Loss: 0.000226\n",
      "Epoch: 14 \tTraining Loss: 0.000226\n",
      "Epoch: 15 \tTraining Loss: 0.000226\n",
      "Epoch: 16 \tTraining Loss: 0.000226\n",
      "Epoch: 17 \tTraining Loss: 0.000226\n",
      "Epoch: 18 \tTraining Loss: 0.000226\n",
      "Epoch: 1 \tTraining Loss: 0.000291\n",
      "Epoch: 2 \tTraining Loss: 0.000291\n",
      "Epoch: 3 \tTraining Loss: 0.000291\n",
      "Epoch: 4 \tTraining Loss: 0.000291\n",
      "Epoch: 5 \tTraining Loss: 0.000291\n",
      "Epoch: 6 \tTraining Loss: 0.000291\n",
      "Epoch: 7 \tTraining Loss: 0.000291\n",
      "Epoch: 8 \tTraining Loss: 0.000291\n",
      "Epoch: 9 \tTraining Loss: 0.000291\n",
      "Epoch: 10 \tTraining Loss: 0.000291\n",
      "Epoch: 11 \tTraining Loss: 0.000291\n",
      "Epoch: 12 \tTraining Loss: 0.000291\n",
      "Epoch: 13 \tTraining Loss: 0.000291\n",
      "Epoch: 14 \tTraining Loss: 0.000291\n",
      "Epoch: 15 \tTraining Loss: 0.000291\n",
      "Epoch: 16 \tTraining Loss: 0.000291\n",
      "Epoch: 17 \tTraining Loss: 0.000291\n",
      "Epoch: 18 \tTraining Loss: 0.000291\n",
      "Epoch: 1 \tTraining Loss: 0.000202\n",
      "Epoch: 2 \tTraining Loss: 0.000202\n",
      "Epoch: 3 \tTraining Loss: 0.000202\n",
      "Epoch: 4 \tTraining Loss: 0.000202\n",
      "Epoch: 5 \tTraining Loss: 0.000202\n",
      "Epoch: 6 \tTraining Loss: 0.000202\n",
      "Epoch: 7 \tTraining Loss: 0.000202\n",
      "Epoch: 8 \tTraining Loss: 0.000202\n",
      "Epoch: 9 \tTraining Loss: 0.000202\n",
      "Epoch: 10 \tTraining Loss: 0.000202\n",
      "Epoch: 11 \tTraining Loss: 0.000202\n",
      "Epoch: 12 \tTraining Loss: 0.000202\n",
      "Epoch: 13 \tTraining Loss: 0.000202\n",
      "Epoch: 14 \tTraining Loss: 0.000202\n",
      "Epoch: 15 \tTraining Loss: 0.000202\n",
      "Epoch: 16 \tTraining Loss: 0.000202\n",
      "Epoch: 17 \tTraining Loss: 0.000202\n",
      "Epoch: 18 \tTraining Loss: 0.000202\n",
      "Epoch: 1 \tTraining Loss: 0.000237\n",
      "Epoch: 2 \tTraining Loss: 0.000237\n",
      "Epoch: 3 \tTraining Loss: 0.000237\n",
      "Epoch: 4 \tTraining Loss: 0.000237\n",
      "Epoch: 5 \tTraining Loss: 0.000237\n",
      "Epoch: 6 \tTraining Loss: 0.000237\n",
      "Epoch: 7 \tTraining Loss: 0.000237\n",
      "Epoch: 8 \tTraining Loss: 0.000237\n",
      "Epoch: 9 \tTraining Loss: 0.000237\n",
      "Epoch: 10 \tTraining Loss: 0.000237\n",
      "Epoch: 11 \tTraining Loss: 0.000237\n",
      "Epoch: 12 \tTraining Loss: 0.000237\n",
      "Epoch: 13 \tTraining Loss: 0.000237\n",
      "Epoch: 14 \tTraining Loss: 0.000237\n",
      "Epoch: 15 \tTraining Loss: 0.000237\n",
      "Epoch: 16 \tTraining Loss: 0.000237\n",
      "Epoch: 17 \tTraining Loss: 0.000237\n",
      "Epoch: 18 \tTraining Loss: 0.000237\n",
      "Epoch: 1 \tTraining Loss: 0.000304\n",
      "Epoch: 2 \tTraining Loss: 0.000304\n",
      "Epoch: 3 \tTraining Loss: 0.000304\n",
      "Epoch: 4 \tTraining Loss: 0.000304\n",
      "Epoch: 5 \tTraining Loss: 0.000304\n",
      "Epoch: 6 \tTraining Loss: 0.000304\n",
      "Epoch: 7 \tTraining Loss: 0.000304\n",
      "Epoch: 8 \tTraining Loss: 0.000304\n",
      "Epoch: 9 \tTraining Loss: 0.000304\n",
      "Epoch: 10 \tTraining Loss: 0.000304\n",
      "Epoch: 11 \tTraining Loss: 0.000304\n",
      "Epoch: 12 \tTraining Loss: 0.000304\n",
      "Epoch: 13 \tTraining Loss: 0.000304\n",
      "Epoch: 14 \tTraining Loss: 0.000304\n",
      "Epoch: 15 \tTraining Loss: 0.000304\n",
      "Epoch: 16 \tTraining Loss: 0.000304\n",
      "Epoch: 17 \tTraining Loss: 0.000304\n",
      "Epoch: 18 \tTraining Loss: 0.000304\n",
      "Epoch: 1 \tTraining Loss: 0.000109\n",
      "Epoch: 2 \tTraining Loss: 0.000109\n",
      "Epoch: 3 \tTraining Loss: 0.000109\n",
      "Epoch: 4 \tTraining Loss: 0.000109\n",
      "Epoch: 5 \tTraining Loss: 0.000109\n",
      "Epoch: 6 \tTraining Loss: 0.000109\n",
      "Epoch: 7 \tTraining Loss: 0.000109\n",
      "Epoch: 8 \tTraining Loss: 0.000109\n",
      "Epoch: 9 \tTraining Loss: 0.000109\n",
      "Epoch: 10 \tTraining Loss: 0.000109\n",
      "Epoch: 11 \tTraining Loss: 0.000109\n",
      "Epoch: 12 \tTraining Loss: 0.000109\n",
      "Epoch: 13 \tTraining Loss: 0.000109\n",
      "Epoch: 14 \tTraining Loss: 0.000109\n",
      "Epoch: 15 \tTraining Loss: 0.000109\n",
      "Epoch: 16 \tTraining Loss: 0.000109\n",
      "Epoch: 17 \tTraining Loss: 0.000109\n",
      "Epoch: 18 \tTraining Loss: 0.000109\n",
      "Epoch: 1 \tTraining Loss: 0.000352\n",
      "Epoch: 2 \tTraining Loss: 0.000352\n",
      "Epoch: 3 \tTraining Loss: 0.000352\n",
      "Epoch: 4 \tTraining Loss: 0.000352\n",
      "Epoch: 5 \tTraining Loss: 0.000352\n",
      "Epoch: 6 \tTraining Loss: 0.000352\n",
      "Epoch: 7 \tTraining Loss: 0.000352\n",
      "Epoch: 8 \tTraining Loss: 0.000352\n",
      "Epoch: 9 \tTraining Loss: 0.000352\n",
      "Epoch: 10 \tTraining Loss: 0.000352\n",
      "Epoch: 11 \tTraining Loss: 0.000352\n",
      "Epoch: 12 \tTraining Loss: 0.000352\n",
      "Epoch: 13 \tTraining Loss: 0.000352\n",
      "Epoch: 14 \tTraining Loss: 0.000352\n",
      "Epoch: 15 \tTraining Loss: 0.000352\n",
      "Epoch: 16 \tTraining Loss: 0.000352\n",
      "Epoch: 17 \tTraining Loss: 0.000352\n",
      "Epoch: 18 \tTraining Loss: 0.000352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000308\n",
      "Epoch: 2 \tTraining Loss: 0.000308\n",
      "Epoch: 3 \tTraining Loss: 0.000308\n",
      "Epoch: 4 \tTraining Loss: 0.000308\n",
      "Epoch: 5 \tTraining Loss: 0.000308\n",
      "Epoch: 6 \tTraining Loss: 0.000308\n",
      "Epoch: 7 \tTraining Loss: 0.000308\n",
      "Epoch: 8 \tTraining Loss: 0.000308\n",
      "Epoch: 9 \tTraining Loss: 0.000308\n",
      "Epoch: 10 \tTraining Loss: 0.000308\n",
      "Epoch: 11 \tTraining Loss: 0.000308\n",
      "Epoch: 12 \tTraining Loss: 0.000308\n",
      "Epoch: 13 \tTraining Loss: 0.000308\n",
      "Epoch: 14 \tTraining Loss: 0.000308\n",
      "Epoch: 15 \tTraining Loss: 0.000308\n",
      "Epoch: 16 \tTraining Loss: 0.000308\n",
      "Epoch: 17 \tTraining Loss: 0.000308\n",
      "Epoch: 18 \tTraining Loss: 0.000308\n",
      "Epoch: 1 \tTraining Loss: 0.000315\n",
      "Epoch: 2 \tTraining Loss: 0.000315\n",
      "Epoch: 3 \tTraining Loss: 0.000315\n",
      "Epoch: 4 \tTraining Loss: 0.000315\n",
      "Epoch: 5 \tTraining Loss: 0.000315\n",
      "Epoch: 6 \tTraining Loss: 0.000315\n",
      "Epoch: 7 \tTraining Loss: 0.000315\n",
      "Epoch: 8 \tTraining Loss: 0.000315\n",
      "Epoch: 9 \tTraining Loss: 0.000315\n",
      "Epoch: 10 \tTraining Loss: 0.000315\n",
      "Epoch: 11 \tTraining Loss: 0.000315\n",
      "Epoch: 12 \tTraining Loss: 0.000315\n",
      "Epoch: 13 \tTraining Loss: 0.000315\n",
      "Epoch: 14 \tTraining Loss: 0.000315\n",
      "Epoch: 15 \tTraining Loss: 0.000315\n",
      "Epoch: 16 \tTraining Loss: 0.000315\n",
      "Epoch: 17 \tTraining Loss: 0.000315\n",
      "Epoch: 18 \tTraining Loss: 0.000315\n",
      "Epoch: 1 \tTraining Loss: 0.000208\n",
      "Epoch: 2 \tTraining Loss: 0.000208\n",
      "Epoch: 3 \tTraining Loss: 0.000208\n",
      "Epoch: 4 \tTraining Loss: 0.000208\n",
      "Epoch: 5 \tTraining Loss: 0.000208\n",
      "Epoch: 6 \tTraining Loss: 0.000208\n",
      "Epoch: 7 \tTraining Loss: 0.000208\n",
      "Epoch: 8 \tTraining Loss: 0.000208\n",
      "Epoch: 9 \tTraining Loss: 0.000208\n",
      "Epoch: 10 \tTraining Loss: 0.000208\n",
      "Epoch: 11 \tTraining Loss: 0.000208\n",
      "Epoch: 12 \tTraining Loss: 0.000208\n",
      "Epoch: 13 \tTraining Loss: 0.000208\n",
      "Epoch: 14 \tTraining Loss: 0.000208\n",
      "Epoch: 15 \tTraining Loss: 0.000208\n",
      "Epoch: 16 \tTraining Loss: 0.000208\n",
      "Epoch: 17 \tTraining Loss: 0.000208\n",
      "Epoch: 18 \tTraining Loss: 0.000208\n",
      "Epoch: 1 \tTraining Loss: 0.000250\n",
      "Epoch: 2 \tTraining Loss: 0.000250\n",
      "Epoch: 3 \tTraining Loss: 0.000250\n",
      "Epoch: 4 \tTraining Loss: 0.000250\n",
      "Epoch: 5 \tTraining Loss: 0.000250\n",
      "Epoch: 6 \tTraining Loss: 0.000250\n",
      "Epoch: 7 \tTraining Loss: 0.000250\n",
      "Epoch: 8 \tTraining Loss: 0.000250\n",
      "Epoch: 9 \tTraining Loss: 0.000250\n",
      "Epoch: 10 \tTraining Loss: 0.000250\n",
      "Epoch: 11 \tTraining Loss: 0.000250\n",
      "Epoch: 12 \tTraining Loss: 0.000250\n",
      "Epoch: 13 \tTraining Loss: 0.000250\n",
      "Epoch: 14 \tTraining Loss: 0.000250\n",
      "Epoch: 15 \tTraining Loss: 0.000250\n",
      "Epoch: 16 \tTraining Loss: 0.000250\n",
      "Epoch: 17 \tTraining Loss: 0.000250\n",
      "Epoch: 18 \tTraining Loss: 0.000250\n",
      "Epoch: 1 \tTraining Loss: 0.000326\n",
      "Epoch: 2 \tTraining Loss: 0.000326\n",
      "Epoch: 3 \tTraining Loss: 0.000326\n",
      "Epoch: 4 \tTraining Loss: 0.000326\n",
      "Epoch: 5 \tTraining Loss: 0.000326\n",
      "Epoch: 6 \tTraining Loss: 0.000326\n",
      "Epoch: 7 \tTraining Loss: 0.000326\n",
      "Epoch: 8 \tTraining Loss: 0.000326\n",
      "Epoch: 9 \tTraining Loss: 0.000326\n",
      "Epoch: 10 \tTraining Loss: 0.000326\n",
      "Epoch: 11 \tTraining Loss: 0.000326\n",
      "Epoch: 12 \tTraining Loss: 0.000326\n",
      "Epoch: 13 \tTraining Loss: 0.000326\n",
      "Epoch: 14 \tTraining Loss: 0.000326\n",
      "Epoch: 15 \tTraining Loss: 0.000326\n",
      "Epoch: 16 \tTraining Loss: 0.000326\n",
      "Epoch: 17 \tTraining Loss: 0.000326\n",
      "Epoch: 18 \tTraining Loss: 0.000326\n",
      "Epoch: 1 \tTraining Loss: 0.000106\n",
      "Epoch: 2 \tTraining Loss: 0.000106\n",
      "Epoch: 3 \tTraining Loss: 0.000106\n",
      "Epoch: 4 \tTraining Loss: 0.000106\n",
      "Epoch: 5 \tTraining Loss: 0.000106\n",
      "Epoch: 6 \tTraining Loss: 0.000106\n",
      "Epoch: 7 \tTraining Loss: 0.000106\n",
      "Epoch: 8 \tTraining Loss: 0.000106\n",
      "Epoch: 9 \tTraining Loss: 0.000106\n",
      "Epoch: 10 \tTraining Loss: 0.000106\n",
      "Epoch: 11 \tTraining Loss: 0.000106\n",
      "Epoch: 12 \tTraining Loss: 0.000106\n",
      "Epoch: 13 \tTraining Loss: 0.000106\n",
      "Epoch: 14 \tTraining Loss: 0.000106\n",
      "Epoch: 15 \tTraining Loss: 0.000106\n",
      "Epoch: 16 \tTraining Loss: 0.000106\n",
      "Epoch: 17 \tTraining Loss: 0.000106\n",
      "Epoch: 18 \tTraining Loss: 0.000106\n",
      "Epoch: 1 \tTraining Loss: 0.000240\n",
      "Epoch: 2 \tTraining Loss: 0.000240\n",
      "Epoch: 3 \tTraining Loss: 0.000240\n",
      "Epoch: 4 \tTraining Loss: 0.000240\n",
      "Epoch: 5 \tTraining Loss: 0.000240\n",
      "Epoch: 6 \tTraining Loss: 0.000240\n",
      "Epoch: 7 \tTraining Loss: 0.000240\n",
      "Epoch: 8 \tTraining Loss: 0.000240\n",
      "Epoch: 9 \tTraining Loss: 0.000240\n",
      "Epoch: 10 \tTraining Loss: 0.000240\n",
      "Epoch: 11 \tTraining Loss: 0.000240\n",
      "Epoch: 12 \tTraining Loss: 0.000240\n",
      "Epoch: 13 \tTraining Loss: 0.000240\n",
      "Epoch: 14 \tTraining Loss: 0.000240\n",
      "Epoch: 15 \tTraining Loss: 0.000240\n",
      "Epoch: 16 \tTraining Loss: 0.000240\n",
      "Epoch: 17 \tTraining Loss: 0.000240\n",
      "Epoch: 18 \tTraining Loss: 0.000240\n",
      "Epoch: 1 \tTraining Loss: 0.000161\n",
      "Epoch: 2 \tTraining Loss: 0.000161\n",
      "Epoch: 3 \tTraining Loss: 0.000161\n",
      "Epoch: 4 \tTraining Loss: 0.000161\n",
      "Epoch: 5 \tTraining Loss: 0.000161\n",
      "Epoch: 6 \tTraining Loss: 0.000161\n",
      "Epoch: 7 \tTraining Loss: 0.000161\n",
      "Epoch: 8 \tTraining Loss: 0.000161\n",
      "Epoch: 9 \tTraining Loss: 0.000161\n",
      "Epoch: 10 \tTraining Loss: 0.000161\n",
      "Epoch: 11 \tTraining Loss: 0.000161\n",
      "Epoch: 12 \tTraining Loss: 0.000161\n",
      "Epoch: 13 \tTraining Loss: 0.000161\n",
      "Epoch: 14 \tTraining Loss: 0.000161\n",
      "Epoch: 15 \tTraining Loss: 0.000161\n",
      "Epoch: 16 \tTraining Loss: 0.000161\n",
      "Epoch: 17 \tTraining Loss: 0.000161\n",
      "Epoch: 18 \tTraining Loss: 0.000161\n",
      "Epoch: 1 \tTraining Loss: 0.000163\n",
      "Epoch: 2 \tTraining Loss: 0.000163\n",
      "Epoch: 3 \tTraining Loss: 0.000163\n",
      "Epoch: 4 \tTraining Loss: 0.000163\n",
      "Epoch: 5 \tTraining Loss: 0.000163\n",
      "Epoch: 6 \tTraining Loss: 0.000163\n",
      "Epoch: 7 \tTraining Loss: 0.000163\n",
      "Epoch: 8 \tTraining Loss: 0.000163\n",
      "Epoch: 9 \tTraining Loss: 0.000163\n",
      "Epoch: 10 \tTraining Loss: 0.000163\n",
      "Epoch: 11 \tTraining Loss: 0.000163\n",
      "Epoch: 12 \tTraining Loss: 0.000163\n",
      "Epoch: 13 \tTraining Loss: 0.000163\n",
      "Epoch: 14 \tTraining Loss: 0.000163\n",
      "Epoch: 15 \tTraining Loss: 0.000163\n",
      "Epoch: 16 \tTraining Loss: 0.000163\n",
      "Epoch: 17 \tTraining Loss: 0.000163\n",
      "Epoch: 18 \tTraining Loss: 0.000163\n",
      "Epoch: 1 \tTraining Loss: 0.000207\n",
      "Epoch: 2 \tTraining Loss: 0.000207\n",
      "Epoch: 3 \tTraining Loss: 0.000207\n",
      "Epoch: 4 \tTraining Loss: 0.000207\n",
      "Epoch: 5 \tTraining Loss: 0.000207\n",
      "Epoch: 6 \tTraining Loss: 0.000207\n",
      "Epoch: 7 \tTraining Loss: 0.000207\n",
      "Epoch: 8 \tTraining Loss: 0.000207\n",
      "Epoch: 9 \tTraining Loss: 0.000207\n",
      "Epoch: 10 \tTraining Loss: 0.000207\n",
      "Epoch: 11 \tTraining Loss: 0.000207\n",
      "Epoch: 12 \tTraining Loss: 0.000207\n",
      "Epoch: 13 \tTraining Loss: 0.000207\n",
      "Epoch: 14 \tTraining Loss: 0.000207\n",
      "Epoch: 15 \tTraining Loss: 0.000207\n",
      "Epoch: 16 \tTraining Loss: 0.000207\n",
      "Epoch: 17 \tTraining Loss: 0.000207\n",
      "Epoch: 18 \tTraining Loss: 0.000207\n",
      "Epoch: 1 \tTraining Loss: 0.000142\n",
      "Epoch: 2 \tTraining Loss: 0.000142\n",
      "Epoch: 3 \tTraining Loss: 0.000142\n",
      "Epoch: 4 \tTraining Loss: 0.000142\n",
      "Epoch: 5 \tTraining Loss: 0.000142\n",
      "Epoch: 6 \tTraining Loss: 0.000142\n",
      "Epoch: 7 \tTraining Loss: 0.000142\n",
      "Epoch: 8 \tTraining Loss: 0.000142\n",
      "Epoch: 9 \tTraining Loss: 0.000142\n",
      "Epoch: 10 \tTraining Loss: 0.000142\n",
      "Epoch: 11 \tTraining Loss: 0.000142\n",
      "Epoch: 12 \tTraining Loss: 0.000142\n",
      "Epoch: 13 \tTraining Loss: 0.000142\n",
      "Epoch: 14 \tTraining Loss: 0.000142\n",
      "Epoch: 15 \tTraining Loss: 0.000142\n",
      "Epoch: 16 \tTraining Loss: 0.000142\n",
      "Epoch: 17 \tTraining Loss: 0.000142\n",
      "Epoch: 18 \tTraining Loss: 0.000142\n",
      "Epoch: 1 \tTraining Loss: 0.000112\n",
      "Epoch: 2 \tTraining Loss: 0.000112\n",
      "Epoch: 3 \tTraining Loss: 0.000112\n",
      "Epoch: 4 \tTraining Loss: 0.000112\n",
      "Epoch: 5 \tTraining Loss: 0.000112\n",
      "Epoch: 6 \tTraining Loss: 0.000112\n",
      "Epoch: 7 \tTraining Loss: 0.000112\n",
      "Epoch: 8 \tTraining Loss: 0.000112\n",
      "Epoch: 9 \tTraining Loss: 0.000112\n",
      "Epoch: 10 \tTraining Loss: 0.000112\n",
      "Epoch: 11 \tTraining Loss: 0.000112\n",
      "Epoch: 12 \tTraining Loss: 0.000112\n",
      "Epoch: 13 \tTraining Loss: 0.000112\n",
      "Epoch: 14 \tTraining Loss: 0.000112\n",
      "Epoch: 15 \tTraining Loss: 0.000112\n",
      "Epoch: 16 \tTraining Loss: 0.000112\n",
      "Epoch: 17 \tTraining Loss: 0.000112\n",
      "Epoch: 18 \tTraining Loss: 0.000112\n",
      "Epoch: 1 \tTraining Loss: 0.000191\n",
      "Epoch: 2 \tTraining Loss: 0.000191\n",
      "Epoch: 3 \tTraining Loss: 0.000191\n",
      "Epoch: 4 \tTraining Loss: 0.000191\n",
      "Epoch: 5 \tTraining Loss: 0.000191\n",
      "Epoch: 6 \tTraining Loss: 0.000191\n",
      "Epoch: 7 \tTraining Loss: 0.000191\n",
      "Epoch: 8 \tTraining Loss: 0.000191\n",
      "Epoch: 9 \tTraining Loss: 0.000191\n",
      "Epoch: 10 \tTraining Loss: 0.000191\n",
      "Epoch: 11 \tTraining Loss: 0.000191\n",
      "Epoch: 12 \tTraining Loss: 0.000191\n",
      "Epoch: 13 \tTraining Loss: 0.000191\n",
      "Epoch: 14 \tTraining Loss: 0.000191\n",
      "Epoch: 15 \tTraining Loss: 0.000191\n",
      "Epoch: 16 \tTraining Loss: 0.000191\n",
      "Epoch: 17 \tTraining Loss: 0.000191\n",
      "Epoch: 18 \tTraining Loss: 0.000191\n",
      "Epoch: 1 \tTraining Loss: 0.000159\n",
      "Epoch: 2 \tTraining Loss: 0.000159\n",
      "Epoch: 3 \tTraining Loss: 0.000159\n",
      "Epoch: 4 \tTraining Loss: 0.000159\n",
      "Epoch: 5 \tTraining Loss: 0.000159\n",
      "Epoch: 6 \tTraining Loss: 0.000159\n",
      "Epoch: 7 \tTraining Loss: 0.000159\n",
      "Epoch: 8 \tTraining Loss: 0.000159\n",
      "Epoch: 9 \tTraining Loss: 0.000159\n",
      "Epoch: 10 \tTraining Loss: 0.000159\n",
      "Epoch: 11 \tTraining Loss: 0.000159\n",
      "Epoch: 12 \tTraining Loss: 0.000159\n",
      "Epoch: 13 \tTraining Loss: 0.000159\n",
      "Epoch: 14 \tTraining Loss: 0.000159\n",
      "Epoch: 15 \tTraining Loss: 0.000159\n",
      "Epoch: 16 \tTraining Loss: 0.000159\n",
      "Epoch: 17 \tTraining Loss: 0.000159\n",
      "Epoch: 18 \tTraining Loss: 0.000159\n",
      "Epoch: 1 \tTraining Loss: 0.000185\n",
      "Epoch: 2 \tTraining Loss: 0.000185\n",
      "Epoch: 3 \tTraining Loss: 0.000185\n",
      "Epoch: 4 \tTraining Loss: 0.000185\n",
      "Epoch: 5 \tTraining Loss: 0.000185\n",
      "Epoch: 6 \tTraining Loss: 0.000185\n",
      "Epoch: 7 \tTraining Loss: 0.000185\n",
      "Epoch: 8 \tTraining Loss: 0.000185\n",
      "Epoch: 9 \tTraining Loss: 0.000185\n",
      "Epoch: 10 \tTraining Loss: 0.000185\n",
      "Epoch: 11 \tTraining Loss: 0.000185\n",
      "Epoch: 12 \tTraining Loss: 0.000185\n",
      "Epoch: 13 \tTraining Loss: 0.000185\n",
      "Epoch: 14 \tTraining Loss: 0.000185\n",
      "Epoch: 15 \tTraining Loss: 0.000185\n",
      "Epoch: 16 \tTraining Loss: 0.000185\n",
      "Epoch: 17 \tTraining Loss: 0.000185\n",
      "Epoch: 18 \tTraining Loss: 0.000185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000231\n",
      "Epoch: 2 \tTraining Loss: 0.000231\n",
      "Epoch: 3 \tTraining Loss: 0.000231\n",
      "Epoch: 4 \tTraining Loss: 0.000231\n",
      "Epoch: 5 \tTraining Loss: 0.000231\n",
      "Epoch: 6 \tTraining Loss: 0.000231\n",
      "Epoch: 7 \tTraining Loss: 0.000231\n",
      "Epoch: 8 \tTraining Loss: 0.000231\n",
      "Epoch: 9 \tTraining Loss: 0.000231\n",
      "Epoch: 10 \tTraining Loss: 0.000231\n",
      "Epoch: 11 \tTraining Loss: 0.000231\n",
      "Epoch: 12 \tTraining Loss: 0.000231\n",
      "Epoch: 13 \tTraining Loss: 0.000231\n",
      "Epoch: 14 \tTraining Loss: 0.000231\n",
      "Epoch: 15 \tTraining Loss: 0.000231\n",
      "Epoch: 16 \tTraining Loss: 0.000231\n",
      "Epoch: 17 \tTraining Loss: 0.000231\n",
      "Epoch: 18 \tTraining Loss: 0.000231\n",
      "Epoch: 1 \tTraining Loss: 0.000253\n",
      "Epoch: 2 \tTraining Loss: 0.000253\n",
      "Epoch: 3 \tTraining Loss: 0.000253\n",
      "Epoch: 4 \tTraining Loss: 0.000253\n",
      "Epoch: 5 \tTraining Loss: 0.000253\n",
      "Epoch: 6 \tTraining Loss: 0.000253\n",
      "Epoch: 7 \tTraining Loss: 0.000253\n",
      "Epoch: 8 \tTraining Loss: 0.000253\n",
      "Epoch: 9 \tTraining Loss: 0.000253\n",
      "Epoch: 10 \tTraining Loss: 0.000253\n",
      "Epoch: 11 \tTraining Loss: 0.000253\n",
      "Epoch: 12 \tTraining Loss: 0.000253\n",
      "Epoch: 13 \tTraining Loss: 0.000253\n",
      "Epoch: 14 \tTraining Loss: 0.000253\n",
      "Epoch: 15 \tTraining Loss: 0.000253\n",
      "Epoch: 16 \tTraining Loss: 0.000253\n",
      "Epoch: 17 \tTraining Loss: 0.000253\n",
      "Epoch: 18 \tTraining Loss: 0.000253\n",
      "Epoch: 1 \tTraining Loss: 0.000243\n",
      "Epoch: 2 \tTraining Loss: 0.000243\n",
      "Epoch: 3 \tTraining Loss: 0.000243\n",
      "Epoch: 4 \tTraining Loss: 0.000243\n",
      "Epoch: 5 \tTraining Loss: 0.000243\n",
      "Epoch: 6 \tTraining Loss: 0.000243\n",
      "Epoch: 7 \tTraining Loss: 0.000243\n",
      "Epoch: 8 \tTraining Loss: 0.000243\n",
      "Epoch: 9 \tTraining Loss: 0.000243\n",
      "Epoch: 10 \tTraining Loss: 0.000243\n",
      "Epoch: 11 \tTraining Loss: 0.000243\n",
      "Epoch: 12 \tTraining Loss: 0.000243\n",
      "Epoch: 13 \tTraining Loss: 0.000243\n",
      "Epoch: 14 \tTraining Loss: 0.000243\n",
      "Epoch: 15 \tTraining Loss: 0.000243\n",
      "Epoch: 16 \tTraining Loss: 0.000243\n",
      "Epoch: 17 \tTraining Loss: 0.000243\n",
      "Epoch: 18 \tTraining Loss: 0.000243\n",
      "Epoch: 1 \tTraining Loss: 0.000132\n",
      "Epoch: 2 \tTraining Loss: 0.000132\n",
      "Epoch: 3 \tTraining Loss: 0.000132\n",
      "Epoch: 4 \tTraining Loss: 0.000132\n",
      "Epoch: 5 \tTraining Loss: 0.000132\n",
      "Epoch: 6 \tTraining Loss: 0.000132\n",
      "Epoch: 7 \tTraining Loss: 0.000132\n",
      "Epoch: 8 \tTraining Loss: 0.000132\n",
      "Epoch: 9 \tTraining Loss: 0.000132\n",
      "Epoch: 10 \tTraining Loss: 0.000132\n",
      "Epoch: 11 \tTraining Loss: 0.000132\n",
      "Epoch: 12 \tTraining Loss: 0.000132\n",
      "Epoch: 13 \tTraining Loss: 0.000132\n",
      "Epoch: 14 \tTraining Loss: 0.000132\n",
      "Epoch: 15 \tTraining Loss: 0.000132\n",
      "Epoch: 16 \tTraining Loss: 0.000132\n",
      "Epoch: 17 \tTraining Loss: 0.000132\n",
      "Epoch: 18 \tTraining Loss: 0.000132\n",
      "Epoch: 1 \tTraining Loss: 0.000206\n",
      "Epoch: 2 \tTraining Loss: 0.000206\n",
      "Epoch: 3 \tTraining Loss: 0.000206\n",
      "Epoch: 4 \tTraining Loss: 0.000206\n",
      "Epoch: 5 \tTraining Loss: 0.000206\n",
      "Epoch: 6 \tTraining Loss: 0.000206\n",
      "Epoch: 7 \tTraining Loss: 0.000206\n",
      "Epoch: 8 \tTraining Loss: 0.000206\n",
      "Epoch: 9 \tTraining Loss: 0.000206\n",
      "Epoch: 10 \tTraining Loss: 0.000206\n",
      "Epoch: 11 \tTraining Loss: 0.000206\n",
      "Epoch: 12 \tTraining Loss: 0.000206\n",
      "Epoch: 13 \tTraining Loss: 0.000206\n",
      "Epoch: 14 \tTraining Loss: 0.000206\n",
      "Epoch: 15 \tTraining Loss: 0.000206\n",
      "Epoch: 16 \tTraining Loss: 0.000206\n",
      "Epoch: 17 \tTraining Loss: 0.000206\n",
      "Epoch: 18 \tTraining Loss: 0.000206\n",
      "Epoch: 1 \tTraining Loss: 0.000228\n",
      "Epoch: 2 \tTraining Loss: 0.000228\n",
      "Epoch: 3 \tTraining Loss: 0.000228\n",
      "Epoch: 4 \tTraining Loss: 0.000228\n",
      "Epoch: 5 \tTraining Loss: 0.000228\n",
      "Epoch: 6 \tTraining Loss: 0.000228\n",
      "Epoch: 7 \tTraining Loss: 0.000228\n",
      "Epoch: 8 \tTraining Loss: 0.000228\n",
      "Epoch: 9 \tTraining Loss: 0.000228\n",
      "Epoch: 10 \tTraining Loss: 0.000228\n",
      "Epoch: 11 \tTraining Loss: 0.000228\n",
      "Epoch: 12 \tTraining Loss: 0.000228\n",
      "Epoch: 13 \tTraining Loss: 0.000228\n",
      "Epoch: 14 \tTraining Loss: 0.000228\n",
      "Epoch: 15 \tTraining Loss: 0.000228\n",
      "Epoch: 16 \tTraining Loss: 0.000228\n",
      "Epoch: 17 \tTraining Loss: 0.000228\n",
      "Epoch: 18 \tTraining Loss: 0.000228\n",
      "Epoch: 1 \tTraining Loss: 0.000142\n",
      "Epoch: 2 \tTraining Loss: 0.000142\n",
      "Epoch: 3 \tTraining Loss: 0.000142\n",
      "Epoch: 4 \tTraining Loss: 0.000142\n",
      "Epoch: 5 \tTraining Loss: 0.000142\n",
      "Epoch: 6 \tTraining Loss: 0.000142\n",
      "Epoch: 7 \tTraining Loss: 0.000142\n",
      "Epoch: 8 \tTraining Loss: 0.000142\n",
      "Epoch: 9 \tTraining Loss: 0.000142\n",
      "Epoch: 10 \tTraining Loss: 0.000142\n",
      "Epoch: 11 \tTraining Loss: 0.000142\n",
      "Epoch: 12 \tTraining Loss: 0.000142\n",
      "Epoch: 13 \tTraining Loss: 0.000142\n",
      "Epoch: 14 \tTraining Loss: 0.000142\n",
      "Epoch: 15 \tTraining Loss: 0.000142\n",
      "Epoch: 16 \tTraining Loss: 0.000142\n",
      "Epoch: 17 \tTraining Loss: 0.000142\n",
      "Epoch: 18 \tTraining Loss: 0.000142\n",
      "Epoch: 1 \tTraining Loss: 0.000127\n",
      "Epoch: 2 \tTraining Loss: 0.000127\n",
      "Epoch: 3 \tTraining Loss: 0.000127\n",
      "Epoch: 4 \tTraining Loss: 0.000127\n",
      "Epoch: 5 \tTraining Loss: 0.000127\n",
      "Epoch: 6 \tTraining Loss: 0.000127\n",
      "Epoch: 7 \tTraining Loss: 0.000127\n",
      "Epoch: 8 \tTraining Loss: 0.000127\n",
      "Epoch: 9 \tTraining Loss: 0.000127\n",
      "Epoch: 10 \tTraining Loss: 0.000127\n",
      "Epoch: 11 \tTraining Loss: 0.000127\n",
      "Epoch: 12 \tTraining Loss: 0.000127\n",
      "Epoch: 13 \tTraining Loss: 0.000127\n",
      "Epoch: 14 \tTraining Loss: 0.000127\n",
      "Epoch: 15 \tTraining Loss: 0.000127\n",
      "Epoch: 16 \tTraining Loss: 0.000127\n",
      "Epoch: 17 \tTraining Loss: 0.000127\n",
      "Epoch: 18 \tTraining Loss: 0.000127\n",
      "Epoch: 1 \tTraining Loss: 0.000201\n",
      "Epoch: 2 \tTraining Loss: 0.000201\n",
      "Epoch: 3 \tTraining Loss: 0.000201\n",
      "Epoch: 4 \tTraining Loss: 0.000201\n",
      "Epoch: 5 \tTraining Loss: 0.000201\n",
      "Epoch: 6 \tTraining Loss: 0.000201\n",
      "Epoch: 7 \tTraining Loss: 0.000201\n",
      "Epoch: 8 \tTraining Loss: 0.000201\n",
      "Epoch: 9 \tTraining Loss: 0.000201\n",
      "Epoch: 10 \tTraining Loss: 0.000201\n",
      "Epoch: 11 \tTraining Loss: 0.000201\n",
      "Epoch: 12 \tTraining Loss: 0.000201\n",
      "Epoch: 13 \tTraining Loss: 0.000201\n",
      "Epoch: 14 \tTraining Loss: 0.000201\n",
      "Epoch: 15 \tTraining Loss: 0.000201\n",
      "Epoch: 16 \tTraining Loss: 0.000201\n",
      "Epoch: 17 \tTraining Loss: 0.000201\n",
      "Epoch: 18 \tTraining Loss: 0.000201\n",
      "Epoch: 1 \tTraining Loss: 0.000354\n",
      "Epoch: 2 \tTraining Loss: 0.000354\n",
      "Epoch: 3 \tTraining Loss: 0.000354\n",
      "Epoch: 4 \tTraining Loss: 0.000354\n",
      "Epoch: 5 \tTraining Loss: 0.000354\n",
      "Epoch: 6 \tTraining Loss: 0.000354\n",
      "Epoch: 7 \tTraining Loss: 0.000354\n",
      "Epoch: 8 \tTraining Loss: 0.000354\n",
      "Epoch: 9 \tTraining Loss: 0.000354\n",
      "Epoch: 10 \tTraining Loss: 0.000354\n",
      "Epoch: 11 \tTraining Loss: 0.000354\n",
      "Epoch: 12 \tTraining Loss: 0.000354\n",
      "Epoch: 13 \tTraining Loss: 0.000354\n",
      "Epoch: 14 \tTraining Loss: 0.000354\n",
      "Epoch: 15 \tTraining Loss: 0.000354\n",
      "Epoch: 16 \tTraining Loss: 0.000354\n",
      "Epoch: 17 \tTraining Loss: 0.000354\n",
      "Epoch: 18 \tTraining Loss: 0.000354\n",
      "Epoch: 1 \tTraining Loss: 0.000237\n",
      "Epoch: 2 \tTraining Loss: 0.000237\n",
      "Epoch: 3 \tTraining Loss: 0.000237\n",
      "Epoch: 4 \tTraining Loss: 0.000237\n",
      "Epoch: 5 \tTraining Loss: 0.000237\n",
      "Epoch: 6 \tTraining Loss: 0.000237\n",
      "Epoch: 7 \tTraining Loss: 0.000237\n",
      "Epoch: 8 \tTraining Loss: 0.000237\n",
      "Epoch: 9 \tTraining Loss: 0.000237\n",
      "Epoch: 10 \tTraining Loss: 0.000237\n",
      "Epoch: 11 \tTraining Loss: 0.000237\n",
      "Epoch: 12 \tTraining Loss: 0.000237\n",
      "Epoch: 13 \tTraining Loss: 0.000237\n",
      "Epoch: 14 \tTraining Loss: 0.000237\n",
      "Epoch: 15 \tTraining Loss: 0.000237\n",
      "Epoch: 16 \tTraining Loss: 0.000237\n",
      "Epoch: 17 \tTraining Loss: 0.000237\n",
      "Epoch: 18 \tTraining Loss: 0.000237\n",
      "Epoch: 1 \tTraining Loss: 0.000171\n",
      "Epoch: 2 \tTraining Loss: 0.000171\n",
      "Epoch: 3 \tTraining Loss: 0.000171\n",
      "Epoch: 4 \tTraining Loss: 0.000171\n",
      "Epoch: 5 \tTraining Loss: 0.000171\n",
      "Epoch: 6 \tTraining Loss: 0.000171\n",
      "Epoch: 7 \tTraining Loss: 0.000171\n",
      "Epoch: 8 \tTraining Loss: 0.000171\n",
      "Epoch: 9 \tTraining Loss: 0.000171\n",
      "Epoch: 10 \tTraining Loss: 0.000171\n",
      "Epoch: 11 \tTraining Loss: 0.000171\n",
      "Epoch: 12 \tTraining Loss: 0.000171\n",
      "Epoch: 13 \tTraining Loss: 0.000171\n",
      "Epoch: 14 \tTraining Loss: 0.000171\n",
      "Epoch: 15 \tTraining Loss: 0.000171\n",
      "Epoch: 16 \tTraining Loss: 0.000171\n",
      "Epoch: 17 \tTraining Loss: 0.000171\n",
      "Epoch: 18 \tTraining Loss: 0.000171\n",
      "Epoch: 1 \tTraining Loss: 0.000158\n",
      "Epoch: 2 \tTraining Loss: 0.000158\n",
      "Epoch: 3 \tTraining Loss: 0.000158\n",
      "Epoch: 4 \tTraining Loss: 0.000158\n",
      "Epoch: 5 \tTraining Loss: 0.000158\n",
      "Epoch: 6 \tTraining Loss: 0.000158\n",
      "Epoch: 7 \tTraining Loss: 0.000158\n",
      "Epoch: 8 \tTraining Loss: 0.000158\n",
      "Epoch: 9 \tTraining Loss: 0.000158\n",
      "Epoch: 10 \tTraining Loss: 0.000158\n",
      "Epoch: 11 \tTraining Loss: 0.000158\n",
      "Epoch: 12 \tTraining Loss: 0.000158\n",
      "Epoch: 13 \tTraining Loss: 0.000158\n",
      "Epoch: 14 \tTraining Loss: 0.000158\n",
      "Epoch: 15 \tTraining Loss: 0.000158\n",
      "Epoch: 16 \tTraining Loss: 0.000158\n",
      "Epoch: 17 \tTraining Loss: 0.000158\n",
      "Epoch: 18 \tTraining Loss: 0.000158\n",
      "Epoch: 1 \tTraining Loss: 0.000211\n",
      "Epoch: 2 \tTraining Loss: 0.000211\n",
      "Epoch: 3 \tTraining Loss: 0.000211\n",
      "Epoch: 4 \tTraining Loss: 0.000211\n",
      "Epoch: 5 \tTraining Loss: 0.000211\n",
      "Epoch: 6 \tTraining Loss: 0.000211\n",
      "Epoch: 7 \tTraining Loss: 0.000211\n",
      "Epoch: 8 \tTraining Loss: 0.000211\n",
      "Epoch: 9 \tTraining Loss: 0.000211\n",
      "Epoch: 10 \tTraining Loss: 0.000211\n",
      "Epoch: 11 \tTraining Loss: 0.000211\n",
      "Epoch: 12 \tTraining Loss: 0.000211\n",
      "Epoch: 13 \tTraining Loss: 0.000211\n",
      "Epoch: 14 \tTraining Loss: 0.000211\n",
      "Epoch: 15 \tTraining Loss: 0.000211\n",
      "Epoch: 16 \tTraining Loss: 0.000211\n",
      "Epoch: 17 \tTraining Loss: 0.000211\n",
      "Epoch: 18 \tTraining Loss: 0.000211\n",
      "Epoch: 1 \tTraining Loss: 0.000220\n",
      "Epoch: 2 \tTraining Loss: 0.000220\n",
      "Epoch: 3 \tTraining Loss: 0.000220\n",
      "Epoch: 4 \tTraining Loss: 0.000220\n",
      "Epoch: 5 \tTraining Loss: 0.000220\n",
      "Epoch: 6 \tTraining Loss: 0.000220\n",
      "Epoch: 7 \tTraining Loss: 0.000220\n",
      "Epoch: 8 \tTraining Loss: 0.000220\n",
      "Epoch: 9 \tTraining Loss: 0.000220\n",
      "Epoch: 10 \tTraining Loss: 0.000220\n",
      "Epoch: 11 \tTraining Loss: 0.000220\n",
      "Epoch: 12 \tTraining Loss: 0.000220\n",
      "Epoch: 13 \tTraining Loss: 0.000220\n",
      "Epoch: 14 \tTraining Loss: 0.000220\n",
      "Epoch: 15 \tTraining Loss: 0.000220\n",
      "Epoch: 16 \tTraining Loss: 0.000220\n",
      "Epoch: 17 \tTraining Loss: 0.000220\n",
      "Epoch: 18 \tTraining Loss: 0.000220\n",
      "Epoch: 1 \tTraining Loss: 0.000208\n",
      "Epoch: 2 \tTraining Loss: 0.000208\n",
      "Epoch: 3 \tTraining Loss: 0.000208\n",
      "Epoch: 4 \tTraining Loss: 0.000208\n",
      "Epoch: 5 \tTraining Loss: 0.000208\n",
      "Epoch: 6 \tTraining Loss: 0.000208\n",
      "Epoch: 7 \tTraining Loss: 0.000208\n",
      "Epoch: 8 \tTraining Loss: 0.000208\n",
      "Epoch: 9 \tTraining Loss: 0.000208\n",
      "Epoch: 10 \tTraining Loss: 0.000208\n",
      "Epoch: 11 \tTraining Loss: 0.000208\n",
      "Epoch: 12 \tTraining Loss: 0.000208\n",
      "Epoch: 13 \tTraining Loss: 0.000208\n",
      "Epoch: 14 \tTraining Loss: 0.000208\n",
      "Epoch: 15 \tTraining Loss: 0.000208\n",
      "Epoch: 16 \tTraining Loss: 0.000208\n",
      "Epoch: 17 \tTraining Loss: 0.000208\n",
      "Epoch: 18 \tTraining Loss: 0.000208\n",
      "Epoch: 1 \tTraining Loss: 0.000130\n",
      "Epoch: 2 \tTraining Loss: 0.000130\n",
      "Epoch: 3 \tTraining Loss: 0.000130\n",
      "Epoch: 4 \tTraining Loss: 0.000130\n",
      "Epoch: 5 \tTraining Loss: 0.000130\n",
      "Epoch: 6 \tTraining Loss: 0.000130\n",
      "Epoch: 7 \tTraining Loss: 0.000130\n",
      "Epoch: 8 \tTraining Loss: 0.000130\n",
      "Epoch: 9 \tTraining Loss: 0.000130\n",
      "Epoch: 10 \tTraining Loss: 0.000130\n",
      "Epoch: 11 \tTraining Loss: 0.000130\n",
      "Epoch: 12 \tTraining Loss: 0.000130\n",
      "Epoch: 13 \tTraining Loss: 0.000130\n",
      "Epoch: 14 \tTraining Loss: 0.000130\n",
      "Epoch: 15 \tTraining Loss: 0.000130\n",
      "Epoch: 16 \tTraining Loss: 0.000130\n",
      "Epoch: 17 \tTraining Loss: 0.000130\n",
      "Epoch: 18 \tTraining Loss: 0.000130\n",
      "Epoch: 1 \tTraining Loss: 0.000251\n",
      "Epoch: 2 \tTraining Loss: 0.000251\n",
      "Epoch: 3 \tTraining Loss: 0.000251\n",
      "Epoch: 4 \tTraining Loss: 0.000251\n",
      "Epoch: 5 \tTraining Loss: 0.000251\n",
      "Epoch: 6 \tTraining Loss: 0.000251\n",
      "Epoch: 7 \tTraining Loss: 0.000251\n",
      "Epoch: 8 \tTraining Loss: 0.000251\n",
      "Epoch: 9 \tTraining Loss: 0.000251\n",
      "Epoch: 10 \tTraining Loss: 0.000251\n",
      "Epoch: 11 \tTraining Loss: 0.000251\n",
      "Epoch: 12 \tTraining Loss: 0.000251\n",
      "Epoch: 13 \tTraining Loss: 0.000251\n",
      "Epoch: 14 \tTraining Loss: 0.000251\n",
      "Epoch: 15 \tTraining Loss: 0.000251\n",
      "Epoch: 16 \tTraining Loss: 0.000251\n",
      "Epoch: 17 \tTraining Loss: 0.000251\n",
      "Epoch: 18 \tTraining Loss: 0.000251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000142\n",
      "Epoch: 2 \tTraining Loss: 0.000142\n",
      "Epoch: 3 \tTraining Loss: 0.000142\n",
      "Epoch: 4 \tTraining Loss: 0.000142\n",
      "Epoch: 5 \tTraining Loss: 0.000142\n",
      "Epoch: 6 \tTraining Loss: 0.000142\n",
      "Epoch: 7 \tTraining Loss: 0.000142\n",
      "Epoch: 8 \tTraining Loss: 0.000142\n",
      "Epoch: 9 \tTraining Loss: 0.000142\n",
      "Epoch: 10 \tTraining Loss: 0.000142\n",
      "Epoch: 11 \tTraining Loss: 0.000142\n",
      "Epoch: 12 \tTraining Loss: 0.000142\n",
      "Epoch: 13 \tTraining Loss: 0.000142\n",
      "Epoch: 14 \tTraining Loss: 0.000142\n",
      "Epoch: 15 \tTraining Loss: 0.000142\n",
      "Epoch: 16 \tTraining Loss: 0.000142\n",
      "Epoch: 17 \tTraining Loss: 0.000142\n",
      "Epoch: 18 \tTraining Loss: 0.000142\n",
      "Epoch: 1 \tTraining Loss: 0.000226\n",
      "Epoch: 2 \tTraining Loss: 0.000226\n",
      "Epoch: 3 \tTraining Loss: 0.000226\n",
      "Epoch: 4 \tTraining Loss: 0.000226\n",
      "Epoch: 5 \tTraining Loss: 0.000226\n",
      "Epoch: 6 \tTraining Loss: 0.000226\n",
      "Epoch: 7 \tTraining Loss: 0.000226\n",
      "Epoch: 8 \tTraining Loss: 0.000226\n",
      "Epoch: 9 \tTraining Loss: 0.000226\n",
      "Epoch: 10 \tTraining Loss: 0.000226\n",
      "Epoch: 11 \tTraining Loss: 0.000226\n",
      "Epoch: 12 \tTraining Loss: 0.000226\n",
      "Epoch: 13 \tTraining Loss: 0.000226\n",
      "Epoch: 14 \tTraining Loss: 0.000226\n",
      "Epoch: 15 \tTraining Loss: 0.000226\n",
      "Epoch: 16 \tTraining Loss: 0.000226\n",
      "Epoch: 17 \tTraining Loss: 0.000226\n",
      "Epoch: 18 \tTraining Loss: 0.000226\n",
      "Epoch: 1 \tTraining Loss: 0.000237\n",
      "Epoch: 2 \tTraining Loss: 0.000237\n",
      "Epoch: 3 \tTraining Loss: 0.000237\n",
      "Epoch: 4 \tTraining Loss: 0.000237\n",
      "Epoch: 5 \tTraining Loss: 0.000237\n",
      "Epoch: 6 \tTraining Loss: 0.000237\n",
      "Epoch: 7 \tTraining Loss: 0.000237\n",
      "Epoch: 8 \tTraining Loss: 0.000237\n",
      "Epoch: 9 \tTraining Loss: 0.000237\n",
      "Epoch: 10 \tTraining Loss: 0.000237\n",
      "Epoch: 11 \tTraining Loss: 0.000237\n",
      "Epoch: 12 \tTraining Loss: 0.000237\n",
      "Epoch: 13 \tTraining Loss: 0.000237\n",
      "Epoch: 14 \tTraining Loss: 0.000237\n",
      "Epoch: 15 \tTraining Loss: 0.000237\n",
      "Epoch: 16 \tTraining Loss: 0.000237\n",
      "Epoch: 17 \tTraining Loss: 0.000237\n",
      "Epoch: 18 \tTraining Loss: 0.000237\n",
      "Epoch: 1 \tTraining Loss: 0.000376\n",
      "Epoch: 2 \tTraining Loss: 0.000376\n",
      "Epoch: 3 \tTraining Loss: 0.000376\n",
      "Epoch: 4 \tTraining Loss: 0.000376\n",
      "Epoch: 5 \tTraining Loss: 0.000376\n",
      "Epoch: 6 \tTraining Loss: 0.000376\n",
      "Epoch: 7 \tTraining Loss: 0.000376\n",
      "Epoch: 8 \tTraining Loss: 0.000376\n",
      "Epoch: 9 \tTraining Loss: 0.000376\n",
      "Epoch: 10 \tTraining Loss: 0.000376\n",
      "Epoch: 11 \tTraining Loss: 0.000376\n",
      "Epoch: 12 \tTraining Loss: 0.000376\n",
      "Epoch: 13 \tTraining Loss: 0.000376\n",
      "Epoch: 14 \tTraining Loss: 0.000376\n",
      "Epoch: 15 \tTraining Loss: 0.000376\n",
      "Epoch: 16 \tTraining Loss: 0.000376\n",
      "Epoch: 17 \tTraining Loss: 0.000376\n",
      "Epoch: 18 \tTraining Loss: 0.000376\n",
      "Epoch: 1 \tTraining Loss: 0.000178\n",
      "Epoch: 2 \tTraining Loss: 0.000178\n",
      "Epoch: 3 \tTraining Loss: 0.000178\n",
      "Epoch: 4 \tTraining Loss: 0.000178\n",
      "Epoch: 5 \tTraining Loss: 0.000178\n",
      "Epoch: 6 \tTraining Loss: 0.000178\n",
      "Epoch: 7 \tTraining Loss: 0.000178\n",
      "Epoch: 8 \tTraining Loss: 0.000178\n",
      "Epoch: 9 \tTraining Loss: 0.000178\n",
      "Epoch: 10 \tTraining Loss: 0.000178\n",
      "Epoch: 11 \tTraining Loss: 0.000178\n",
      "Epoch: 12 \tTraining Loss: 0.000178\n",
      "Epoch: 13 \tTraining Loss: 0.000178\n",
      "Epoch: 14 \tTraining Loss: 0.000178\n",
      "Epoch: 15 \tTraining Loss: 0.000178\n",
      "Epoch: 16 \tTraining Loss: 0.000178\n",
      "Epoch: 17 \tTraining Loss: 0.000178\n",
      "Epoch: 18 \tTraining Loss: 0.000178\n",
      "Epoch: 1 \tTraining Loss: 0.000216\n",
      "Epoch: 2 \tTraining Loss: 0.000216\n",
      "Epoch: 3 \tTraining Loss: 0.000216\n",
      "Epoch: 4 \tTraining Loss: 0.000216\n",
      "Epoch: 5 \tTraining Loss: 0.000216\n",
      "Epoch: 6 \tTraining Loss: 0.000216\n",
      "Epoch: 7 \tTraining Loss: 0.000216\n",
      "Epoch: 8 \tTraining Loss: 0.000216\n",
      "Epoch: 9 \tTraining Loss: 0.000216\n",
      "Epoch: 10 \tTraining Loss: 0.000216\n",
      "Epoch: 11 \tTraining Loss: 0.000216\n",
      "Epoch: 12 \tTraining Loss: 0.000216\n",
      "Epoch: 13 \tTraining Loss: 0.000216\n",
      "Epoch: 14 \tTraining Loss: 0.000216\n",
      "Epoch: 15 \tTraining Loss: 0.000216\n",
      "Epoch: 16 \tTraining Loss: 0.000216\n",
      "Epoch: 17 \tTraining Loss: 0.000216\n",
      "Epoch: 18 \tTraining Loss: 0.000216\n",
      "Epoch: 1 \tTraining Loss: 0.000171\n",
      "Epoch: 2 \tTraining Loss: 0.000171\n",
      "Epoch: 3 \tTraining Loss: 0.000171\n",
      "Epoch: 4 \tTraining Loss: 0.000171\n",
      "Epoch: 5 \tTraining Loss: 0.000171\n",
      "Epoch: 6 \tTraining Loss: 0.000171\n",
      "Epoch: 7 \tTraining Loss: 0.000171\n",
      "Epoch: 8 \tTraining Loss: 0.000171\n",
      "Epoch: 9 \tTraining Loss: 0.000171\n",
      "Epoch: 10 \tTraining Loss: 0.000171\n",
      "Epoch: 11 \tTraining Loss: 0.000171\n",
      "Epoch: 12 \tTraining Loss: 0.000171\n",
      "Epoch: 13 \tTraining Loss: 0.000171\n",
      "Epoch: 14 \tTraining Loss: 0.000171\n",
      "Epoch: 15 \tTraining Loss: 0.000171\n",
      "Epoch: 16 \tTraining Loss: 0.000171\n",
      "Epoch: 17 \tTraining Loss: 0.000171\n",
      "Epoch: 18 \tTraining Loss: 0.000171\n",
      "Epoch: 1 \tTraining Loss: 0.000146\n",
      "Epoch: 2 \tTraining Loss: 0.000146\n",
      "Epoch: 3 \tTraining Loss: 0.000146\n",
      "Epoch: 4 \tTraining Loss: 0.000146\n",
      "Epoch: 5 \tTraining Loss: 0.000146\n",
      "Epoch: 6 \tTraining Loss: 0.000146\n",
      "Epoch: 7 \tTraining Loss: 0.000146\n",
      "Epoch: 8 \tTraining Loss: 0.000146\n",
      "Epoch: 9 \tTraining Loss: 0.000146\n",
      "Epoch: 10 \tTraining Loss: 0.000146\n",
      "Epoch: 11 \tTraining Loss: 0.000146\n",
      "Epoch: 12 \tTraining Loss: 0.000146\n",
      "Epoch: 13 \tTraining Loss: 0.000146\n",
      "Epoch: 14 \tTraining Loss: 0.000146\n",
      "Epoch: 15 \tTraining Loss: 0.000146\n",
      "Epoch: 16 \tTraining Loss: 0.000146\n",
      "Epoch: 17 \tTraining Loss: 0.000146\n",
      "Epoch: 18 \tTraining Loss: 0.000146\n",
      "Epoch: 1 \tTraining Loss: 0.000150\n",
      "Epoch: 2 \tTraining Loss: 0.000150\n",
      "Epoch: 3 \tTraining Loss: 0.000150\n",
      "Epoch: 4 \tTraining Loss: 0.000150\n",
      "Epoch: 5 \tTraining Loss: 0.000150\n",
      "Epoch: 6 \tTraining Loss: 0.000150\n",
      "Epoch: 7 \tTraining Loss: 0.000150\n",
      "Epoch: 8 \tTraining Loss: 0.000150\n",
      "Epoch: 9 \tTraining Loss: 0.000150\n",
      "Epoch: 10 \tTraining Loss: 0.000150\n",
      "Epoch: 11 \tTraining Loss: 0.000150\n",
      "Epoch: 12 \tTraining Loss: 0.000150\n",
      "Epoch: 13 \tTraining Loss: 0.000150\n",
      "Epoch: 14 \tTraining Loss: 0.000150\n",
      "Epoch: 15 \tTraining Loss: 0.000150\n",
      "Epoch: 16 \tTraining Loss: 0.000150\n",
      "Epoch: 17 \tTraining Loss: 0.000150\n",
      "Epoch: 18 \tTraining Loss: 0.000150\n",
      "Epoch: 1 \tTraining Loss: 0.000286\n",
      "Epoch: 2 \tTraining Loss: 0.000286\n",
      "Epoch: 3 \tTraining Loss: 0.000286\n",
      "Epoch: 4 \tTraining Loss: 0.000286\n",
      "Epoch: 5 \tTraining Loss: 0.000286\n",
      "Epoch: 6 \tTraining Loss: 0.000286\n",
      "Epoch: 7 \tTraining Loss: 0.000286\n",
      "Epoch: 8 \tTraining Loss: 0.000286\n",
      "Epoch: 9 \tTraining Loss: 0.000286\n",
      "Epoch: 10 \tTraining Loss: 0.000286\n",
      "Epoch: 11 \tTraining Loss: 0.000286\n",
      "Epoch: 12 \tTraining Loss: 0.000286\n",
      "Epoch: 13 \tTraining Loss: 0.000286\n",
      "Epoch: 14 \tTraining Loss: 0.000286\n",
      "Epoch: 15 \tTraining Loss: 0.000286\n",
      "Epoch: 16 \tTraining Loss: 0.000286\n",
      "Epoch: 17 \tTraining Loss: 0.000286\n",
      "Epoch: 18 \tTraining Loss: 0.000286\n",
      "Epoch: 1 \tTraining Loss: 0.000173\n",
      "Epoch: 2 \tTraining Loss: 0.000173\n",
      "Epoch: 3 \tTraining Loss: 0.000173\n",
      "Epoch: 4 \tTraining Loss: 0.000173\n",
      "Epoch: 5 \tTraining Loss: 0.000173\n",
      "Epoch: 6 \tTraining Loss: 0.000173\n",
      "Epoch: 7 \tTraining Loss: 0.000173\n",
      "Epoch: 8 \tTraining Loss: 0.000173\n",
      "Epoch: 9 \tTraining Loss: 0.000173\n",
      "Epoch: 10 \tTraining Loss: 0.000173\n",
      "Epoch: 11 \tTraining Loss: 0.000173\n",
      "Epoch: 12 \tTraining Loss: 0.000173\n",
      "Epoch: 13 \tTraining Loss: 0.000173\n",
      "Epoch: 14 \tTraining Loss: 0.000173\n",
      "Epoch: 15 \tTraining Loss: 0.000173\n",
      "Epoch: 16 \tTraining Loss: 0.000173\n",
      "Epoch: 17 \tTraining Loss: 0.000173\n",
      "Epoch: 18 \tTraining Loss: 0.000173\n",
      "Epoch: 1 \tTraining Loss: 0.000278\n",
      "Epoch: 2 \tTraining Loss: 0.000278\n",
      "Epoch: 3 \tTraining Loss: 0.000278\n",
      "Epoch: 4 \tTraining Loss: 0.000278\n",
      "Epoch: 5 \tTraining Loss: 0.000278\n",
      "Epoch: 6 \tTraining Loss: 0.000278\n",
      "Epoch: 7 \tTraining Loss: 0.000278\n",
      "Epoch: 8 \tTraining Loss: 0.000278\n",
      "Epoch: 9 \tTraining Loss: 0.000278\n",
      "Epoch: 10 \tTraining Loss: 0.000278\n",
      "Epoch: 11 \tTraining Loss: 0.000278\n",
      "Epoch: 12 \tTraining Loss: 0.000278\n",
      "Epoch: 13 \tTraining Loss: 0.000278\n",
      "Epoch: 14 \tTraining Loss: 0.000278\n",
      "Epoch: 15 \tTraining Loss: 0.000278\n",
      "Epoch: 16 \tTraining Loss: 0.000278\n",
      "Epoch: 17 \tTraining Loss: 0.000278\n",
      "Epoch: 18 \tTraining Loss: 0.000278\n",
      "Epoch: 1 \tTraining Loss: 0.000105\n",
      "Epoch: 2 \tTraining Loss: 0.000105\n",
      "Epoch: 3 \tTraining Loss: 0.000105\n",
      "Epoch: 4 \tTraining Loss: 0.000105\n",
      "Epoch: 5 \tTraining Loss: 0.000105\n",
      "Epoch: 6 \tTraining Loss: 0.000105\n",
      "Epoch: 7 \tTraining Loss: 0.000105\n",
      "Epoch: 8 \tTraining Loss: 0.000105\n",
      "Epoch: 9 \tTraining Loss: 0.000105\n",
      "Epoch: 10 \tTraining Loss: 0.000105\n",
      "Epoch: 11 \tTraining Loss: 0.000105\n",
      "Epoch: 12 \tTraining Loss: 0.000105\n",
      "Epoch: 13 \tTraining Loss: 0.000105\n",
      "Epoch: 14 \tTraining Loss: 0.000105\n",
      "Epoch: 15 \tTraining Loss: 0.000105\n",
      "Epoch: 16 \tTraining Loss: 0.000105\n",
      "Epoch: 17 \tTraining Loss: 0.000105\n",
      "Epoch: 18 \tTraining Loss: 0.000105\n",
      "Epoch: 1 \tTraining Loss: 0.000151\n",
      "Epoch: 2 \tTraining Loss: 0.000151\n",
      "Epoch: 3 \tTraining Loss: 0.000151\n",
      "Epoch: 4 \tTraining Loss: 0.000151\n",
      "Epoch: 5 \tTraining Loss: 0.000151\n",
      "Epoch: 6 \tTraining Loss: 0.000151\n",
      "Epoch: 7 \tTraining Loss: 0.000151\n",
      "Epoch: 8 \tTraining Loss: 0.000151\n",
      "Epoch: 9 \tTraining Loss: 0.000151\n",
      "Epoch: 10 \tTraining Loss: 0.000151\n",
      "Epoch: 11 \tTraining Loss: 0.000151\n",
      "Epoch: 12 \tTraining Loss: 0.000151\n",
      "Epoch: 13 \tTraining Loss: 0.000151\n",
      "Epoch: 14 \tTraining Loss: 0.000151\n",
      "Epoch: 15 \tTraining Loss: 0.000151\n",
      "Epoch: 16 \tTraining Loss: 0.000151\n",
      "Epoch: 17 \tTraining Loss: 0.000151\n",
      "Epoch: 18 \tTraining Loss: 0.000151\n",
      "Epoch: 1 \tTraining Loss: 0.000152\n",
      "Epoch: 2 \tTraining Loss: 0.000152\n",
      "Epoch: 3 \tTraining Loss: 0.000152\n",
      "Epoch: 4 \tTraining Loss: 0.000152\n",
      "Epoch: 5 \tTraining Loss: 0.000152\n",
      "Epoch: 6 \tTraining Loss: 0.000152\n",
      "Epoch: 7 \tTraining Loss: 0.000152\n",
      "Epoch: 8 \tTraining Loss: 0.000152\n",
      "Epoch: 9 \tTraining Loss: 0.000152\n",
      "Epoch: 10 \tTraining Loss: 0.000152\n",
      "Epoch: 11 \tTraining Loss: 0.000152\n",
      "Epoch: 12 \tTraining Loss: 0.000152\n",
      "Epoch: 13 \tTraining Loss: 0.000152\n",
      "Epoch: 14 \tTraining Loss: 0.000152\n",
      "Epoch: 15 \tTraining Loss: 0.000152\n",
      "Epoch: 16 \tTraining Loss: 0.000152\n",
      "Epoch: 17 \tTraining Loss: 0.000152\n",
      "Epoch: 18 \tTraining Loss: 0.000152\n",
      "Epoch: 1 \tTraining Loss: 0.000140\n",
      "Epoch: 2 \tTraining Loss: 0.000140\n",
      "Epoch: 3 \tTraining Loss: 0.000140\n",
      "Epoch: 4 \tTraining Loss: 0.000140\n",
      "Epoch: 5 \tTraining Loss: 0.000140\n",
      "Epoch: 6 \tTraining Loss: 0.000140\n",
      "Epoch: 7 \tTraining Loss: 0.000140\n",
      "Epoch: 8 \tTraining Loss: 0.000140\n",
      "Epoch: 9 \tTraining Loss: 0.000140\n",
      "Epoch: 10 \tTraining Loss: 0.000140\n",
      "Epoch: 11 \tTraining Loss: 0.000140\n",
      "Epoch: 12 \tTraining Loss: 0.000140\n",
      "Epoch: 13 \tTraining Loss: 0.000140\n",
      "Epoch: 14 \tTraining Loss: 0.000140\n",
      "Epoch: 15 \tTraining Loss: 0.000140\n",
      "Epoch: 16 \tTraining Loss: 0.000140\n",
      "Epoch: 17 \tTraining Loss: 0.000140\n",
      "Epoch: 18 \tTraining Loss: 0.000140\n",
      "Epoch: 1 \tTraining Loss: 0.000143\n",
      "Epoch: 2 \tTraining Loss: 0.000143\n",
      "Epoch: 3 \tTraining Loss: 0.000143\n",
      "Epoch: 4 \tTraining Loss: 0.000143\n",
      "Epoch: 5 \tTraining Loss: 0.000143\n",
      "Epoch: 6 \tTraining Loss: 0.000143\n",
      "Epoch: 7 \tTraining Loss: 0.000143\n",
      "Epoch: 8 \tTraining Loss: 0.000143\n",
      "Epoch: 9 \tTraining Loss: 0.000143\n",
      "Epoch: 10 \tTraining Loss: 0.000143\n",
      "Epoch: 11 \tTraining Loss: 0.000143\n",
      "Epoch: 12 \tTraining Loss: 0.000143\n",
      "Epoch: 13 \tTraining Loss: 0.000143\n",
      "Epoch: 14 \tTraining Loss: 0.000143\n",
      "Epoch: 15 \tTraining Loss: 0.000143\n",
      "Epoch: 16 \tTraining Loss: 0.000143\n",
      "Epoch: 17 \tTraining Loss: 0.000143\n",
      "Epoch: 18 \tTraining Loss: 0.000143\n",
      "Epoch: 1 \tTraining Loss: 0.000188\n",
      "Epoch: 2 \tTraining Loss: 0.000188\n",
      "Epoch: 3 \tTraining Loss: 0.000188\n",
      "Epoch: 4 \tTraining Loss: 0.000188\n",
      "Epoch: 5 \tTraining Loss: 0.000188\n",
      "Epoch: 6 \tTraining Loss: 0.000188\n",
      "Epoch: 7 \tTraining Loss: 0.000188\n",
      "Epoch: 8 \tTraining Loss: 0.000188\n",
      "Epoch: 9 \tTraining Loss: 0.000188\n",
      "Epoch: 10 \tTraining Loss: 0.000188\n",
      "Epoch: 11 \tTraining Loss: 0.000188\n",
      "Epoch: 12 \tTraining Loss: 0.000188\n",
      "Epoch: 13 \tTraining Loss: 0.000188\n",
      "Epoch: 14 \tTraining Loss: 0.000188\n",
      "Epoch: 15 \tTraining Loss: 0.000188\n",
      "Epoch: 16 \tTraining Loss: 0.000188\n",
      "Epoch: 17 \tTraining Loss: 0.000188\n",
      "Epoch: 18 \tTraining Loss: 0.000188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000265\n",
      "Epoch: 2 \tTraining Loss: 0.000265\n",
      "Epoch: 3 \tTraining Loss: 0.000265\n",
      "Epoch: 4 \tTraining Loss: 0.000265\n",
      "Epoch: 5 \tTraining Loss: 0.000265\n",
      "Epoch: 6 \tTraining Loss: 0.000265\n",
      "Epoch: 7 \tTraining Loss: 0.000265\n",
      "Epoch: 8 \tTraining Loss: 0.000265\n",
      "Epoch: 9 \tTraining Loss: 0.000265\n",
      "Epoch: 10 \tTraining Loss: 0.000265\n",
      "Epoch: 11 \tTraining Loss: 0.000265\n",
      "Epoch: 12 \tTraining Loss: 0.000265\n",
      "Epoch: 13 \tTraining Loss: 0.000265\n",
      "Epoch: 14 \tTraining Loss: 0.000265\n",
      "Epoch: 15 \tTraining Loss: 0.000265\n",
      "Epoch: 16 \tTraining Loss: 0.000265\n",
      "Epoch: 17 \tTraining Loss: 0.000265\n",
      "Epoch: 18 \tTraining Loss: 0.000265\n",
      "Epoch: 1 \tTraining Loss: 0.000123\n",
      "Epoch: 2 \tTraining Loss: 0.000123\n",
      "Epoch: 3 \tTraining Loss: 0.000123\n",
      "Epoch: 4 \tTraining Loss: 0.000123\n",
      "Epoch: 5 \tTraining Loss: 0.000123\n",
      "Epoch: 6 \tTraining Loss: 0.000123\n",
      "Epoch: 7 \tTraining Loss: 0.000123\n",
      "Epoch: 8 \tTraining Loss: 0.000123\n",
      "Epoch: 9 \tTraining Loss: 0.000123\n",
      "Epoch: 10 \tTraining Loss: 0.000123\n",
      "Epoch: 11 \tTraining Loss: 0.000123\n",
      "Epoch: 12 \tTraining Loss: 0.000123\n",
      "Epoch: 13 \tTraining Loss: 0.000123\n",
      "Epoch: 14 \tTraining Loss: 0.000123\n",
      "Epoch: 15 \tTraining Loss: 0.000123\n",
      "Epoch: 16 \tTraining Loss: 0.000123\n",
      "Epoch: 17 \tTraining Loss: 0.000123\n",
      "Epoch: 18 \tTraining Loss: 0.000123\n",
      "Epoch: 1 \tTraining Loss: 0.000129\n",
      "Epoch: 2 \tTraining Loss: 0.000129\n",
      "Epoch: 3 \tTraining Loss: 0.000129\n",
      "Epoch: 4 \tTraining Loss: 0.000129\n",
      "Epoch: 5 \tTraining Loss: 0.000129\n",
      "Epoch: 6 \tTraining Loss: 0.000129\n",
      "Epoch: 7 \tTraining Loss: 0.000129\n",
      "Epoch: 8 \tTraining Loss: 0.000129\n",
      "Epoch: 9 \tTraining Loss: 0.000129\n",
      "Epoch: 10 \tTraining Loss: 0.000129\n",
      "Epoch: 11 \tTraining Loss: 0.000129\n",
      "Epoch: 12 \tTraining Loss: 0.000129\n",
      "Epoch: 13 \tTraining Loss: 0.000129\n",
      "Epoch: 14 \tTraining Loss: 0.000129\n",
      "Epoch: 15 \tTraining Loss: 0.000129\n",
      "Epoch: 16 \tTraining Loss: 0.000129\n",
      "Epoch: 17 \tTraining Loss: 0.000129\n",
      "Epoch: 18 \tTraining Loss: 0.000129\n",
      "Epoch: 1 \tTraining Loss: 0.000250\n",
      "Epoch: 2 \tTraining Loss: 0.000250\n",
      "Epoch: 3 \tTraining Loss: 0.000250\n",
      "Epoch: 4 \tTraining Loss: 0.000250\n",
      "Epoch: 5 \tTraining Loss: 0.000250\n",
      "Epoch: 6 \tTraining Loss: 0.000250\n",
      "Epoch: 7 \tTraining Loss: 0.000250\n",
      "Epoch: 8 \tTraining Loss: 0.000250\n",
      "Epoch: 9 \tTraining Loss: 0.000250\n",
      "Epoch: 10 \tTraining Loss: 0.000250\n",
      "Epoch: 11 \tTraining Loss: 0.000250\n",
      "Epoch: 12 \tTraining Loss: 0.000250\n",
      "Epoch: 13 \tTraining Loss: 0.000250\n",
      "Epoch: 14 \tTraining Loss: 0.000250\n",
      "Epoch: 15 \tTraining Loss: 0.000250\n",
      "Epoch: 16 \tTraining Loss: 0.000250\n",
      "Epoch: 17 \tTraining Loss: 0.000250\n",
      "Epoch: 18 \tTraining Loss: 0.000250\n",
      "Epoch: 1 \tTraining Loss: 0.000199\n",
      "Epoch: 2 \tTraining Loss: 0.000199\n",
      "Epoch: 3 \tTraining Loss: 0.000199\n",
      "Epoch: 4 \tTraining Loss: 0.000199\n",
      "Epoch: 5 \tTraining Loss: 0.000199\n",
      "Epoch: 6 \tTraining Loss: 0.000199\n",
      "Epoch: 7 \tTraining Loss: 0.000199\n",
      "Epoch: 8 \tTraining Loss: 0.000199\n",
      "Epoch: 9 \tTraining Loss: 0.000199\n",
      "Epoch: 10 \tTraining Loss: 0.000199\n",
      "Epoch: 11 \tTraining Loss: 0.000199\n",
      "Epoch: 12 \tTraining Loss: 0.000199\n",
      "Epoch: 13 \tTraining Loss: 0.000199\n",
      "Epoch: 14 \tTraining Loss: 0.000199\n",
      "Epoch: 15 \tTraining Loss: 0.000199\n",
      "Epoch: 16 \tTraining Loss: 0.000199\n",
      "Epoch: 17 \tTraining Loss: 0.000199\n",
      "Epoch: 18 \tTraining Loss: 0.000199\n",
      "Epoch: 1 \tTraining Loss: 0.000201\n",
      "Epoch: 2 \tTraining Loss: 0.000201\n",
      "Epoch: 3 \tTraining Loss: 0.000201\n",
      "Epoch: 4 \tTraining Loss: 0.000201\n",
      "Epoch: 5 \tTraining Loss: 0.000201\n",
      "Epoch: 6 \tTraining Loss: 0.000201\n",
      "Epoch: 7 \tTraining Loss: 0.000201\n",
      "Epoch: 8 \tTraining Loss: 0.000201\n",
      "Epoch: 9 \tTraining Loss: 0.000201\n",
      "Epoch: 10 \tTraining Loss: 0.000201\n",
      "Epoch: 11 \tTraining Loss: 0.000201\n",
      "Epoch: 12 \tTraining Loss: 0.000201\n",
      "Epoch: 13 \tTraining Loss: 0.000201\n",
      "Epoch: 14 \tTraining Loss: 0.000201\n",
      "Epoch: 15 \tTraining Loss: 0.000201\n",
      "Epoch: 16 \tTraining Loss: 0.000201\n",
      "Epoch: 17 \tTraining Loss: 0.000201\n",
      "Epoch: 18 \tTraining Loss: 0.000201\n",
      "Epoch: 1 \tTraining Loss: 0.000352\n",
      "Epoch: 2 \tTraining Loss: 0.000352\n",
      "Epoch: 3 \tTraining Loss: 0.000352\n",
      "Epoch: 4 \tTraining Loss: 0.000352\n",
      "Epoch: 5 \tTraining Loss: 0.000352\n",
      "Epoch: 6 \tTraining Loss: 0.000352\n",
      "Epoch: 7 \tTraining Loss: 0.000352\n",
      "Epoch: 8 \tTraining Loss: 0.000352\n",
      "Epoch: 9 \tTraining Loss: 0.000352\n",
      "Epoch: 10 \tTraining Loss: 0.000352\n",
      "Epoch: 11 \tTraining Loss: 0.000352\n",
      "Epoch: 12 \tTraining Loss: 0.000352\n",
      "Epoch: 13 \tTraining Loss: 0.000352\n",
      "Epoch: 14 \tTraining Loss: 0.000352\n",
      "Epoch: 15 \tTraining Loss: 0.000352\n",
      "Epoch: 16 \tTraining Loss: 0.000352\n",
      "Epoch: 17 \tTraining Loss: 0.000352\n",
      "Epoch: 18 \tTraining Loss: 0.000352\n",
      "Epoch: 1 \tTraining Loss: 0.000131\n",
      "Epoch: 2 \tTraining Loss: 0.000131\n",
      "Epoch: 3 \tTraining Loss: 0.000131\n",
      "Epoch: 4 \tTraining Loss: 0.000131\n",
      "Epoch: 5 \tTraining Loss: 0.000131\n",
      "Epoch: 6 \tTraining Loss: 0.000131\n",
      "Epoch: 7 \tTraining Loss: 0.000131\n",
      "Epoch: 8 \tTraining Loss: 0.000131\n",
      "Epoch: 9 \tTraining Loss: 0.000131\n",
      "Epoch: 10 \tTraining Loss: 0.000131\n",
      "Epoch: 11 \tTraining Loss: 0.000131\n",
      "Epoch: 12 \tTraining Loss: 0.000131\n",
      "Epoch: 13 \tTraining Loss: 0.000131\n",
      "Epoch: 14 \tTraining Loss: 0.000131\n",
      "Epoch: 15 \tTraining Loss: 0.000131\n",
      "Epoch: 16 \tTraining Loss: 0.000131\n",
      "Epoch: 17 \tTraining Loss: 0.000131\n",
      "Epoch: 18 \tTraining Loss: 0.000131\n",
      "Epoch: 1 \tTraining Loss: 0.000110\n",
      "Epoch: 2 \tTraining Loss: 0.000110\n",
      "Epoch: 3 \tTraining Loss: 0.000110\n",
      "Epoch: 4 \tTraining Loss: 0.000110\n",
      "Epoch: 5 \tTraining Loss: 0.000110\n",
      "Epoch: 6 \tTraining Loss: 0.000110\n",
      "Epoch: 7 \tTraining Loss: 0.000110\n",
      "Epoch: 8 \tTraining Loss: 0.000110\n",
      "Epoch: 9 \tTraining Loss: 0.000110\n",
      "Epoch: 10 \tTraining Loss: 0.000110\n",
      "Epoch: 11 \tTraining Loss: 0.000110\n",
      "Epoch: 12 \tTraining Loss: 0.000110\n",
      "Epoch: 13 \tTraining Loss: 0.000110\n",
      "Epoch: 14 \tTraining Loss: 0.000110\n",
      "Epoch: 15 \tTraining Loss: 0.000110\n",
      "Epoch: 16 \tTraining Loss: 0.000110\n",
      "Epoch: 17 \tTraining Loss: 0.000110\n",
      "Epoch: 18 \tTraining Loss: 0.000110\n",
      "Epoch: 1 \tTraining Loss: 0.000073\n",
      "Epoch: 2 \tTraining Loss: 0.000073\n",
      "Epoch: 3 \tTraining Loss: 0.000073\n",
      "Epoch: 4 \tTraining Loss: 0.000073\n",
      "Epoch: 5 \tTraining Loss: 0.000073\n",
      "Epoch: 6 \tTraining Loss: 0.000073\n",
      "Epoch: 7 \tTraining Loss: 0.000073\n",
      "Epoch: 8 \tTraining Loss: 0.000073\n",
      "Epoch: 9 \tTraining Loss: 0.000073\n",
      "Epoch: 10 \tTraining Loss: 0.000073\n",
      "Epoch: 11 \tTraining Loss: 0.000073\n",
      "Epoch: 12 \tTraining Loss: 0.000073\n",
      "Epoch: 13 \tTraining Loss: 0.000073\n",
      "Epoch: 14 \tTraining Loss: 0.000073\n",
      "Epoch: 15 \tTraining Loss: 0.000073\n",
      "Epoch: 16 \tTraining Loss: 0.000073\n",
      "Epoch: 17 \tTraining Loss: 0.000073\n",
      "Epoch: 18 \tTraining Loss: 0.000073\n",
      "Epoch: 1 \tTraining Loss: 0.000239\n",
      "Epoch: 2 \tTraining Loss: 0.000239\n",
      "Epoch: 3 \tTraining Loss: 0.000239\n",
      "Epoch: 4 \tTraining Loss: 0.000239\n",
      "Epoch: 5 \tTraining Loss: 0.000239\n",
      "Epoch: 6 \tTraining Loss: 0.000239\n",
      "Epoch: 7 \tTraining Loss: 0.000239\n",
      "Epoch: 8 \tTraining Loss: 0.000239\n",
      "Epoch: 9 \tTraining Loss: 0.000239\n",
      "Epoch: 10 \tTraining Loss: 0.000239\n",
      "Epoch: 11 \tTraining Loss: 0.000239\n",
      "Epoch: 12 \tTraining Loss: 0.000239\n",
      "Epoch: 13 \tTraining Loss: 0.000239\n",
      "Epoch: 14 \tTraining Loss: 0.000239\n",
      "Epoch: 15 \tTraining Loss: 0.000239\n",
      "Epoch: 16 \tTraining Loss: 0.000239\n",
      "Epoch: 17 \tTraining Loss: 0.000239\n",
      "Epoch: 18 \tTraining Loss: 0.000239\n",
      "Epoch: 1 \tTraining Loss: 0.000152\n",
      "Epoch: 2 \tTraining Loss: 0.000152\n",
      "Epoch: 3 \tTraining Loss: 0.000152\n",
      "Epoch: 4 \tTraining Loss: 0.000152\n",
      "Epoch: 5 \tTraining Loss: 0.000152\n",
      "Epoch: 6 \tTraining Loss: 0.000152\n",
      "Epoch: 7 \tTraining Loss: 0.000152\n",
      "Epoch: 8 \tTraining Loss: 0.000152\n",
      "Epoch: 9 \tTraining Loss: 0.000152\n",
      "Epoch: 10 \tTraining Loss: 0.000152\n",
      "Epoch: 11 \tTraining Loss: 0.000152\n",
      "Epoch: 12 \tTraining Loss: 0.000152\n",
      "Epoch: 13 \tTraining Loss: 0.000152\n",
      "Epoch: 14 \tTraining Loss: 0.000152\n",
      "Epoch: 15 \tTraining Loss: 0.000152\n",
      "Epoch: 16 \tTraining Loss: 0.000152\n",
      "Epoch: 17 \tTraining Loss: 0.000152\n",
      "Epoch: 18 \tTraining Loss: 0.000152\n",
      "Epoch: 1 \tTraining Loss: 0.000111\n",
      "Epoch: 2 \tTraining Loss: 0.000111\n",
      "Epoch: 3 \tTraining Loss: 0.000111\n",
      "Epoch: 4 \tTraining Loss: 0.000111\n",
      "Epoch: 5 \tTraining Loss: 0.000111\n",
      "Epoch: 6 \tTraining Loss: 0.000111\n",
      "Epoch: 7 \tTraining Loss: 0.000111\n",
      "Epoch: 8 \tTraining Loss: 0.000111\n",
      "Epoch: 9 \tTraining Loss: 0.000111\n",
      "Epoch: 10 \tTraining Loss: 0.000111\n",
      "Epoch: 11 \tTraining Loss: 0.000111\n",
      "Epoch: 12 \tTraining Loss: 0.000111\n",
      "Epoch: 13 \tTraining Loss: 0.000111\n",
      "Epoch: 14 \tTraining Loss: 0.000111\n",
      "Epoch: 15 \tTraining Loss: 0.000111\n",
      "Epoch: 16 \tTraining Loss: 0.000111\n",
      "Epoch: 17 \tTraining Loss: 0.000111\n",
      "Epoch: 18 \tTraining Loss: 0.000111\n",
      "Epoch: 1 \tTraining Loss: 0.000235\n",
      "Epoch: 2 \tTraining Loss: 0.000235\n",
      "Epoch: 3 \tTraining Loss: 0.000235\n",
      "Epoch: 4 \tTraining Loss: 0.000235\n",
      "Epoch: 5 \tTraining Loss: 0.000235\n",
      "Epoch: 6 \tTraining Loss: 0.000235\n",
      "Epoch: 7 \tTraining Loss: 0.000235\n",
      "Epoch: 8 \tTraining Loss: 0.000235\n",
      "Epoch: 9 \tTraining Loss: 0.000235\n",
      "Epoch: 10 \tTraining Loss: 0.000235\n",
      "Epoch: 11 \tTraining Loss: 0.000235\n",
      "Epoch: 12 \tTraining Loss: 0.000235\n",
      "Epoch: 13 \tTraining Loss: 0.000235\n",
      "Epoch: 14 \tTraining Loss: 0.000235\n",
      "Epoch: 15 \tTraining Loss: 0.000235\n",
      "Epoch: 16 \tTraining Loss: 0.000235\n",
      "Epoch: 17 \tTraining Loss: 0.000235\n",
      "Epoch: 18 \tTraining Loss: 0.000235\n",
      "Epoch: 1 \tTraining Loss: 0.000239\n",
      "Epoch: 2 \tTraining Loss: 0.000239\n",
      "Epoch: 3 \tTraining Loss: 0.000239\n",
      "Epoch: 4 \tTraining Loss: 0.000239\n",
      "Epoch: 5 \tTraining Loss: 0.000239\n",
      "Epoch: 6 \tTraining Loss: 0.000239\n",
      "Epoch: 7 \tTraining Loss: 0.000239\n",
      "Epoch: 8 \tTraining Loss: 0.000239\n",
      "Epoch: 9 \tTraining Loss: 0.000239\n",
      "Epoch: 10 \tTraining Loss: 0.000239\n",
      "Epoch: 11 \tTraining Loss: 0.000239\n",
      "Epoch: 12 \tTraining Loss: 0.000239\n",
      "Epoch: 13 \tTraining Loss: 0.000239\n",
      "Epoch: 14 \tTraining Loss: 0.000239\n",
      "Epoch: 15 \tTraining Loss: 0.000239\n",
      "Epoch: 16 \tTraining Loss: 0.000239\n",
      "Epoch: 17 \tTraining Loss: 0.000239\n",
      "Epoch: 18 \tTraining Loss: 0.000239\n",
      "Epoch: 1 \tTraining Loss: 0.000143\n",
      "Epoch: 2 \tTraining Loss: 0.000143\n",
      "Epoch: 3 \tTraining Loss: 0.000143\n",
      "Epoch: 4 \tTraining Loss: 0.000143\n",
      "Epoch: 5 \tTraining Loss: 0.000143\n",
      "Epoch: 6 \tTraining Loss: 0.000143\n",
      "Epoch: 7 \tTraining Loss: 0.000143\n",
      "Epoch: 8 \tTraining Loss: 0.000143\n",
      "Epoch: 9 \tTraining Loss: 0.000143\n",
      "Epoch: 10 \tTraining Loss: 0.000143\n",
      "Epoch: 11 \tTraining Loss: 0.000143\n",
      "Epoch: 12 \tTraining Loss: 0.000143\n",
      "Epoch: 13 \tTraining Loss: 0.000143\n",
      "Epoch: 14 \tTraining Loss: 0.000143\n",
      "Epoch: 15 \tTraining Loss: 0.000143\n",
      "Epoch: 16 \tTraining Loss: 0.000143\n",
      "Epoch: 17 \tTraining Loss: 0.000143\n",
      "Epoch: 18 \tTraining Loss: 0.000143\n",
      "Epoch: 1 \tTraining Loss: 0.000147\n",
      "Epoch: 2 \tTraining Loss: 0.000147\n",
      "Epoch: 3 \tTraining Loss: 0.000147\n",
      "Epoch: 4 \tTraining Loss: 0.000147\n",
      "Epoch: 5 \tTraining Loss: 0.000147\n",
      "Epoch: 6 \tTraining Loss: 0.000147\n",
      "Epoch: 7 \tTraining Loss: 0.000147\n",
      "Epoch: 8 \tTraining Loss: 0.000147\n",
      "Epoch: 9 \tTraining Loss: 0.000147\n",
      "Epoch: 10 \tTraining Loss: 0.000147\n",
      "Epoch: 11 \tTraining Loss: 0.000147\n",
      "Epoch: 12 \tTraining Loss: 0.000147\n",
      "Epoch: 13 \tTraining Loss: 0.000147\n",
      "Epoch: 14 \tTraining Loss: 0.000147\n",
      "Epoch: 15 \tTraining Loss: 0.000147\n",
      "Epoch: 16 \tTraining Loss: 0.000147\n",
      "Epoch: 17 \tTraining Loss: 0.000147\n",
      "Epoch: 18 \tTraining Loss: 0.000147\n",
      "Epoch: 1 \tTraining Loss: 0.000159\n",
      "Epoch: 2 \tTraining Loss: 0.000159\n",
      "Epoch: 3 \tTraining Loss: 0.000159\n",
      "Epoch: 4 \tTraining Loss: 0.000159\n",
      "Epoch: 5 \tTraining Loss: 0.000159\n",
      "Epoch: 6 \tTraining Loss: 0.000159\n",
      "Epoch: 7 \tTraining Loss: 0.000159\n",
      "Epoch: 8 \tTraining Loss: 0.000159\n",
      "Epoch: 9 \tTraining Loss: 0.000159\n",
      "Epoch: 10 \tTraining Loss: 0.000159\n",
      "Epoch: 11 \tTraining Loss: 0.000159\n",
      "Epoch: 12 \tTraining Loss: 0.000159\n",
      "Epoch: 13 \tTraining Loss: 0.000159\n",
      "Epoch: 14 \tTraining Loss: 0.000159\n",
      "Epoch: 15 \tTraining Loss: 0.000159\n",
      "Epoch: 16 \tTraining Loss: 0.000159\n",
      "Epoch: 17 \tTraining Loss: 0.000159\n",
      "Epoch: 18 \tTraining Loss: 0.000159\n",
      "Epoch: 1 \tTraining Loss: 0.000401\n",
      "Epoch: 2 \tTraining Loss: 0.000401\n",
      "Epoch: 3 \tTraining Loss: 0.000401\n",
      "Epoch: 4 \tTraining Loss: 0.000401\n",
      "Epoch: 5 \tTraining Loss: 0.000401\n",
      "Epoch: 6 \tTraining Loss: 0.000401\n",
      "Epoch: 7 \tTraining Loss: 0.000401\n",
      "Epoch: 8 \tTraining Loss: 0.000401\n",
      "Epoch: 9 \tTraining Loss: 0.000401\n",
      "Epoch: 10 \tTraining Loss: 0.000401\n",
      "Epoch: 11 \tTraining Loss: 0.000401\n",
      "Epoch: 12 \tTraining Loss: 0.000401\n",
      "Epoch: 13 \tTraining Loss: 0.000401\n",
      "Epoch: 14 \tTraining Loss: 0.000401\n",
      "Epoch: 15 \tTraining Loss: 0.000401\n",
      "Epoch: 16 \tTraining Loss: 0.000401\n",
      "Epoch: 17 \tTraining Loss: 0.000401\n",
      "Epoch: 18 \tTraining Loss: 0.000401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000127\n",
      "Epoch: 2 \tTraining Loss: 0.000127\n",
      "Epoch: 3 \tTraining Loss: 0.000127\n",
      "Epoch: 4 \tTraining Loss: 0.000127\n",
      "Epoch: 5 \tTraining Loss: 0.000127\n",
      "Epoch: 6 \tTraining Loss: 0.000127\n",
      "Epoch: 7 \tTraining Loss: 0.000127\n",
      "Epoch: 8 \tTraining Loss: 0.000127\n",
      "Epoch: 9 \tTraining Loss: 0.000127\n",
      "Epoch: 10 \tTraining Loss: 0.000127\n",
      "Epoch: 11 \tTraining Loss: 0.000127\n",
      "Epoch: 12 \tTraining Loss: 0.000127\n",
      "Epoch: 13 \tTraining Loss: 0.000127\n",
      "Epoch: 14 \tTraining Loss: 0.000127\n",
      "Epoch: 15 \tTraining Loss: 0.000127\n",
      "Epoch: 16 \tTraining Loss: 0.000127\n",
      "Epoch: 17 \tTraining Loss: 0.000127\n",
      "Epoch: 18 \tTraining Loss: 0.000127\n",
      "Epoch: 1 \tTraining Loss: 0.000183\n",
      "Epoch: 2 \tTraining Loss: 0.000183\n",
      "Epoch: 3 \tTraining Loss: 0.000183\n",
      "Epoch: 4 \tTraining Loss: 0.000183\n",
      "Epoch: 5 \tTraining Loss: 0.000183\n",
      "Epoch: 6 \tTraining Loss: 0.000183\n",
      "Epoch: 7 \tTraining Loss: 0.000183\n",
      "Epoch: 8 \tTraining Loss: 0.000183\n",
      "Epoch: 9 \tTraining Loss: 0.000183\n",
      "Epoch: 10 \tTraining Loss: 0.000183\n",
      "Epoch: 11 \tTraining Loss: 0.000183\n",
      "Epoch: 12 \tTraining Loss: 0.000183\n",
      "Epoch: 13 \tTraining Loss: 0.000183\n",
      "Epoch: 14 \tTraining Loss: 0.000183\n",
      "Epoch: 15 \tTraining Loss: 0.000183\n",
      "Epoch: 16 \tTraining Loss: 0.000183\n",
      "Epoch: 17 \tTraining Loss: 0.000183\n",
      "Epoch: 18 \tTraining Loss: 0.000183\n",
      "Epoch: 1 \tTraining Loss: 0.000197\n",
      "Epoch: 2 \tTraining Loss: 0.000197\n",
      "Epoch: 3 \tTraining Loss: 0.000197\n",
      "Epoch: 4 \tTraining Loss: 0.000197\n",
      "Epoch: 5 \tTraining Loss: 0.000197\n",
      "Epoch: 6 \tTraining Loss: 0.000197\n",
      "Epoch: 7 \tTraining Loss: 0.000197\n",
      "Epoch: 8 \tTraining Loss: 0.000197\n",
      "Epoch: 9 \tTraining Loss: 0.000197\n",
      "Epoch: 10 \tTraining Loss: 0.000197\n",
      "Epoch: 11 \tTraining Loss: 0.000197\n",
      "Epoch: 12 \tTraining Loss: 0.000197\n",
      "Epoch: 13 \tTraining Loss: 0.000197\n",
      "Epoch: 14 \tTraining Loss: 0.000197\n",
      "Epoch: 15 \tTraining Loss: 0.000197\n",
      "Epoch: 16 \tTraining Loss: 0.000197\n",
      "Epoch: 17 \tTraining Loss: 0.000197\n",
      "Epoch: 18 \tTraining Loss: 0.000197\n",
      "Epoch: 1 \tTraining Loss: 0.000190\n",
      "Epoch: 2 \tTraining Loss: 0.000190\n",
      "Epoch: 3 \tTraining Loss: 0.000190\n",
      "Epoch: 4 \tTraining Loss: 0.000190\n",
      "Epoch: 5 \tTraining Loss: 0.000190\n",
      "Epoch: 6 \tTraining Loss: 0.000190\n",
      "Epoch: 7 \tTraining Loss: 0.000190\n",
      "Epoch: 8 \tTraining Loss: 0.000190\n",
      "Epoch: 9 \tTraining Loss: 0.000190\n",
      "Epoch: 10 \tTraining Loss: 0.000190\n",
      "Epoch: 11 \tTraining Loss: 0.000190\n",
      "Epoch: 12 \tTraining Loss: 0.000190\n",
      "Epoch: 13 \tTraining Loss: 0.000190\n",
      "Epoch: 14 \tTraining Loss: 0.000190\n",
      "Epoch: 15 \tTraining Loss: 0.000190\n",
      "Epoch: 16 \tTraining Loss: 0.000190\n",
      "Epoch: 17 \tTraining Loss: 0.000190\n",
      "Epoch: 18 \tTraining Loss: 0.000190\n",
      "Epoch: 1 \tTraining Loss: 0.000152\n",
      "Epoch: 2 \tTraining Loss: 0.000152\n",
      "Epoch: 3 \tTraining Loss: 0.000152\n",
      "Epoch: 4 \tTraining Loss: 0.000152\n",
      "Epoch: 5 \tTraining Loss: 0.000152\n",
      "Epoch: 6 \tTraining Loss: 0.000152\n",
      "Epoch: 7 \tTraining Loss: 0.000152\n",
      "Epoch: 8 \tTraining Loss: 0.000152\n",
      "Epoch: 9 \tTraining Loss: 0.000152\n",
      "Epoch: 10 \tTraining Loss: 0.000152\n",
      "Epoch: 11 \tTraining Loss: 0.000152\n",
      "Epoch: 12 \tTraining Loss: 0.000152\n",
      "Epoch: 13 \tTraining Loss: 0.000152\n",
      "Epoch: 14 \tTraining Loss: 0.000152\n",
      "Epoch: 15 \tTraining Loss: 0.000152\n",
      "Epoch: 16 \tTraining Loss: 0.000152\n",
      "Epoch: 17 \tTraining Loss: 0.000152\n",
      "Epoch: 18 \tTraining Loss: 0.000152\n",
      "Epoch: 1 \tTraining Loss: 0.000216\n",
      "Epoch: 2 \tTraining Loss: 0.000216\n",
      "Epoch: 3 \tTraining Loss: 0.000216\n",
      "Epoch: 4 \tTraining Loss: 0.000216\n",
      "Epoch: 5 \tTraining Loss: 0.000216\n",
      "Epoch: 6 \tTraining Loss: 0.000216\n",
      "Epoch: 7 \tTraining Loss: 0.000216\n",
      "Epoch: 8 \tTraining Loss: 0.000216\n",
      "Epoch: 9 \tTraining Loss: 0.000216\n",
      "Epoch: 10 \tTraining Loss: 0.000216\n",
      "Epoch: 11 \tTraining Loss: 0.000216\n",
      "Epoch: 12 \tTraining Loss: 0.000216\n",
      "Epoch: 13 \tTraining Loss: 0.000216\n",
      "Epoch: 14 \tTraining Loss: 0.000216\n",
      "Epoch: 15 \tTraining Loss: 0.000216\n",
      "Epoch: 16 \tTraining Loss: 0.000216\n",
      "Epoch: 17 \tTraining Loss: 0.000216\n",
      "Epoch: 18 \tTraining Loss: 0.000216\n",
      "Epoch: 1 \tTraining Loss: 0.000348\n",
      "Epoch: 2 \tTraining Loss: 0.000348\n",
      "Epoch: 3 \tTraining Loss: 0.000348\n",
      "Epoch: 4 \tTraining Loss: 0.000348\n",
      "Epoch: 5 \tTraining Loss: 0.000348\n",
      "Epoch: 6 \tTraining Loss: 0.000348\n",
      "Epoch: 7 \tTraining Loss: 0.000348\n",
      "Epoch: 8 \tTraining Loss: 0.000348\n",
      "Epoch: 9 \tTraining Loss: 0.000348\n",
      "Epoch: 10 \tTraining Loss: 0.000348\n",
      "Epoch: 11 \tTraining Loss: 0.000348\n",
      "Epoch: 12 \tTraining Loss: 0.000348\n",
      "Epoch: 13 \tTraining Loss: 0.000348\n",
      "Epoch: 14 \tTraining Loss: 0.000348\n",
      "Epoch: 15 \tTraining Loss: 0.000348\n",
      "Epoch: 16 \tTraining Loss: 0.000348\n",
      "Epoch: 17 \tTraining Loss: 0.000348\n",
      "Epoch: 18 \tTraining Loss: 0.000348\n",
      "Epoch: 1 \tTraining Loss: 0.000177\n",
      "Epoch: 2 \tTraining Loss: 0.000177\n",
      "Epoch: 3 \tTraining Loss: 0.000177\n",
      "Epoch: 4 \tTraining Loss: 0.000177\n",
      "Epoch: 5 \tTraining Loss: 0.000177\n",
      "Epoch: 6 \tTraining Loss: 0.000177\n",
      "Epoch: 7 \tTraining Loss: 0.000177\n",
      "Epoch: 8 \tTraining Loss: 0.000177\n",
      "Epoch: 9 \tTraining Loss: 0.000177\n",
      "Epoch: 10 \tTraining Loss: 0.000177\n",
      "Epoch: 11 \tTraining Loss: 0.000177\n",
      "Epoch: 12 \tTraining Loss: 0.000177\n",
      "Epoch: 13 \tTraining Loss: 0.000177\n",
      "Epoch: 14 \tTraining Loss: 0.000177\n",
      "Epoch: 15 \tTraining Loss: 0.000177\n",
      "Epoch: 16 \tTraining Loss: 0.000177\n",
      "Epoch: 17 \tTraining Loss: 0.000177\n",
      "Epoch: 18 \tTraining Loss: 0.000177\n",
      "Epoch: 1 \tTraining Loss: 0.000147\n",
      "Epoch: 2 \tTraining Loss: 0.000147\n",
      "Epoch: 3 \tTraining Loss: 0.000147\n",
      "Epoch: 4 \tTraining Loss: 0.000147\n",
      "Epoch: 5 \tTraining Loss: 0.000147\n",
      "Epoch: 6 \tTraining Loss: 0.000147\n",
      "Epoch: 7 \tTraining Loss: 0.000147\n",
      "Epoch: 8 \tTraining Loss: 0.000147\n",
      "Epoch: 9 \tTraining Loss: 0.000147\n",
      "Epoch: 10 \tTraining Loss: 0.000147\n",
      "Epoch: 11 \tTraining Loss: 0.000147\n",
      "Epoch: 12 \tTraining Loss: 0.000147\n",
      "Epoch: 13 \tTraining Loss: 0.000147\n",
      "Epoch: 14 \tTraining Loss: 0.000147\n",
      "Epoch: 15 \tTraining Loss: 0.000147\n",
      "Epoch: 16 \tTraining Loss: 0.000147\n",
      "Epoch: 17 \tTraining Loss: 0.000147\n",
      "Epoch: 18 \tTraining Loss: 0.000147\n",
      "Epoch: 1 \tTraining Loss: 0.000312\n",
      "Epoch: 2 \tTraining Loss: 0.000312\n",
      "Epoch: 3 \tTraining Loss: 0.000312\n",
      "Epoch: 4 \tTraining Loss: 0.000312\n",
      "Epoch: 5 \tTraining Loss: 0.000312\n",
      "Epoch: 6 \tTraining Loss: 0.000312\n",
      "Epoch: 7 \tTraining Loss: 0.000312\n",
      "Epoch: 8 \tTraining Loss: 0.000312\n",
      "Epoch: 9 \tTraining Loss: 0.000312\n",
      "Epoch: 10 \tTraining Loss: 0.000312\n",
      "Epoch: 11 \tTraining Loss: 0.000312\n",
      "Epoch: 12 \tTraining Loss: 0.000312\n",
      "Epoch: 13 \tTraining Loss: 0.000312\n",
      "Epoch: 14 \tTraining Loss: 0.000312\n",
      "Epoch: 15 \tTraining Loss: 0.000312\n",
      "Epoch: 16 \tTraining Loss: 0.000312\n",
      "Epoch: 17 \tTraining Loss: 0.000312\n",
      "Epoch: 18 \tTraining Loss: 0.000312\n",
      "Epoch: 1 \tTraining Loss: 0.000205\n",
      "Epoch: 2 \tTraining Loss: 0.000205\n",
      "Epoch: 3 \tTraining Loss: 0.000205\n",
      "Epoch: 4 \tTraining Loss: 0.000205\n",
      "Epoch: 5 \tTraining Loss: 0.000205\n",
      "Epoch: 6 \tTraining Loss: 0.000205\n",
      "Epoch: 7 \tTraining Loss: 0.000205\n",
      "Epoch: 8 \tTraining Loss: 0.000205\n",
      "Epoch: 9 \tTraining Loss: 0.000205\n",
      "Epoch: 10 \tTraining Loss: 0.000205\n",
      "Epoch: 11 \tTraining Loss: 0.000205\n",
      "Epoch: 12 \tTraining Loss: 0.000205\n",
      "Epoch: 13 \tTraining Loss: 0.000205\n",
      "Epoch: 14 \tTraining Loss: 0.000205\n",
      "Epoch: 15 \tTraining Loss: 0.000205\n",
      "Epoch: 16 \tTraining Loss: 0.000205\n",
      "Epoch: 17 \tTraining Loss: 0.000205\n",
      "Epoch: 18 \tTraining Loss: 0.000205\n",
      "Epoch: 1 \tTraining Loss: 0.000235\n",
      "Epoch: 2 \tTraining Loss: 0.000235\n",
      "Epoch: 3 \tTraining Loss: 0.000235\n",
      "Epoch: 4 \tTraining Loss: 0.000235\n",
      "Epoch: 5 \tTraining Loss: 0.000235\n",
      "Epoch: 6 \tTraining Loss: 0.000235\n",
      "Epoch: 7 \tTraining Loss: 0.000235\n",
      "Epoch: 8 \tTraining Loss: 0.000235\n",
      "Epoch: 9 \tTraining Loss: 0.000235\n",
      "Epoch: 10 \tTraining Loss: 0.000235\n",
      "Epoch: 11 \tTraining Loss: 0.000235\n",
      "Epoch: 12 \tTraining Loss: 0.000235\n",
      "Epoch: 13 \tTraining Loss: 0.000235\n",
      "Epoch: 14 \tTraining Loss: 0.000235\n",
      "Epoch: 15 \tTraining Loss: 0.000235\n",
      "Epoch: 16 \tTraining Loss: 0.000235\n",
      "Epoch: 17 \tTraining Loss: 0.000235\n",
      "Epoch: 18 \tTraining Loss: 0.000235\n",
      "Epoch: 1 \tTraining Loss: 0.000165\n",
      "Epoch: 2 \tTraining Loss: 0.000165\n",
      "Epoch: 3 \tTraining Loss: 0.000165\n",
      "Epoch: 4 \tTraining Loss: 0.000165\n",
      "Epoch: 5 \tTraining Loss: 0.000165\n",
      "Epoch: 6 \tTraining Loss: 0.000165\n",
      "Epoch: 7 \tTraining Loss: 0.000165\n",
      "Epoch: 8 \tTraining Loss: 0.000165\n",
      "Epoch: 9 \tTraining Loss: 0.000165\n",
      "Epoch: 10 \tTraining Loss: 0.000165\n",
      "Epoch: 11 \tTraining Loss: 0.000165\n",
      "Epoch: 12 \tTraining Loss: 0.000165\n",
      "Epoch: 13 \tTraining Loss: 0.000165\n",
      "Epoch: 14 \tTraining Loss: 0.000165\n",
      "Epoch: 15 \tTraining Loss: 0.000165\n",
      "Epoch: 16 \tTraining Loss: 0.000165\n",
      "Epoch: 17 \tTraining Loss: 0.000165\n",
      "Epoch: 18 \tTraining Loss: 0.000165\n",
      "Epoch: 1 \tTraining Loss: 0.000338\n",
      "Epoch: 2 \tTraining Loss: 0.000338\n",
      "Epoch: 3 \tTraining Loss: 0.000338\n",
      "Epoch: 4 \tTraining Loss: 0.000338\n",
      "Epoch: 5 \tTraining Loss: 0.000338\n",
      "Epoch: 6 \tTraining Loss: 0.000338\n",
      "Epoch: 7 \tTraining Loss: 0.000338\n",
      "Epoch: 8 \tTraining Loss: 0.000338\n",
      "Epoch: 9 \tTraining Loss: 0.000338\n",
      "Epoch: 10 \tTraining Loss: 0.000338\n",
      "Epoch: 11 \tTraining Loss: 0.000338\n",
      "Epoch: 12 \tTraining Loss: 0.000338\n",
      "Epoch: 13 \tTraining Loss: 0.000338\n",
      "Epoch: 14 \tTraining Loss: 0.000338\n",
      "Epoch: 15 \tTraining Loss: 0.000338\n",
      "Epoch: 16 \tTraining Loss: 0.000338\n",
      "Epoch: 17 \tTraining Loss: 0.000338\n",
      "Epoch: 18 \tTraining Loss: 0.000338\n",
      "Epoch: 1 \tTraining Loss: 0.000256\n",
      "Epoch: 2 \tTraining Loss: 0.000256\n",
      "Epoch: 3 \tTraining Loss: 0.000256\n",
      "Epoch: 4 \tTraining Loss: 0.000256\n",
      "Epoch: 5 \tTraining Loss: 0.000256\n",
      "Epoch: 6 \tTraining Loss: 0.000256\n",
      "Epoch: 7 \tTraining Loss: 0.000256\n",
      "Epoch: 8 \tTraining Loss: 0.000256\n",
      "Epoch: 9 \tTraining Loss: 0.000256\n",
      "Epoch: 10 \tTraining Loss: 0.000256\n",
      "Epoch: 11 \tTraining Loss: 0.000256\n",
      "Epoch: 12 \tTraining Loss: 0.000256\n",
      "Epoch: 13 \tTraining Loss: 0.000256\n",
      "Epoch: 14 \tTraining Loss: 0.000256\n",
      "Epoch: 15 \tTraining Loss: 0.000256\n",
      "Epoch: 16 \tTraining Loss: 0.000256\n",
      "Epoch: 17 \tTraining Loss: 0.000256\n",
      "Epoch: 18 \tTraining Loss: 0.000256\n",
      "Epoch: 1 \tTraining Loss: 0.000194\n",
      "Epoch: 2 \tTraining Loss: 0.000194\n",
      "Epoch: 3 \tTraining Loss: 0.000194\n",
      "Epoch: 4 \tTraining Loss: 0.000194\n",
      "Epoch: 5 \tTraining Loss: 0.000194\n",
      "Epoch: 6 \tTraining Loss: 0.000194\n",
      "Epoch: 7 \tTraining Loss: 0.000194\n",
      "Epoch: 8 \tTraining Loss: 0.000194\n",
      "Epoch: 9 \tTraining Loss: 0.000194\n",
      "Epoch: 10 \tTraining Loss: 0.000194\n",
      "Epoch: 11 \tTraining Loss: 0.000194\n",
      "Epoch: 12 \tTraining Loss: 0.000194\n",
      "Epoch: 13 \tTraining Loss: 0.000194\n",
      "Epoch: 14 \tTraining Loss: 0.000194\n",
      "Epoch: 15 \tTraining Loss: 0.000194\n",
      "Epoch: 16 \tTraining Loss: 0.000194\n",
      "Epoch: 17 \tTraining Loss: 0.000194\n",
      "Epoch: 18 \tTraining Loss: 0.000194\n",
      "Epoch: 1 \tTraining Loss: 0.000245\n",
      "Epoch: 2 \tTraining Loss: 0.000245\n",
      "Epoch: 3 \tTraining Loss: 0.000245\n",
      "Epoch: 4 \tTraining Loss: 0.000245\n",
      "Epoch: 5 \tTraining Loss: 0.000245\n",
      "Epoch: 6 \tTraining Loss: 0.000245\n",
      "Epoch: 7 \tTraining Loss: 0.000245\n",
      "Epoch: 8 \tTraining Loss: 0.000245\n",
      "Epoch: 9 \tTraining Loss: 0.000245\n",
      "Epoch: 10 \tTraining Loss: 0.000245\n",
      "Epoch: 11 \tTraining Loss: 0.000245\n",
      "Epoch: 12 \tTraining Loss: 0.000245\n",
      "Epoch: 13 \tTraining Loss: 0.000245\n",
      "Epoch: 14 \tTraining Loss: 0.000245\n",
      "Epoch: 15 \tTraining Loss: 0.000245\n",
      "Epoch: 16 \tTraining Loss: 0.000245\n",
      "Epoch: 17 \tTraining Loss: 0.000245\n",
      "Epoch: 18 \tTraining Loss: 0.000245\n",
      "Epoch: 1 \tTraining Loss: 0.000202\n",
      "Epoch: 2 \tTraining Loss: 0.000202\n",
      "Epoch: 3 \tTraining Loss: 0.000202\n",
      "Epoch: 4 \tTraining Loss: 0.000202\n",
      "Epoch: 5 \tTraining Loss: 0.000202\n",
      "Epoch: 6 \tTraining Loss: 0.000202\n",
      "Epoch: 7 \tTraining Loss: 0.000202\n",
      "Epoch: 8 \tTraining Loss: 0.000202\n",
      "Epoch: 9 \tTraining Loss: 0.000202\n",
      "Epoch: 10 \tTraining Loss: 0.000202\n",
      "Epoch: 11 \tTraining Loss: 0.000202\n",
      "Epoch: 12 \tTraining Loss: 0.000202\n",
      "Epoch: 13 \tTraining Loss: 0.000202\n",
      "Epoch: 14 \tTraining Loss: 0.000202\n",
      "Epoch: 15 \tTraining Loss: 0.000202\n",
      "Epoch: 16 \tTraining Loss: 0.000202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 \tTraining Loss: 0.000202\n",
      "Epoch: 18 \tTraining Loss: 0.000202\n",
      "Epoch: 1 \tTraining Loss: 0.000112\n",
      "Epoch: 2 \tTraining Loss: 0.000112\n",
      "Epoch: 3 \tTraining Loss: 0.000112\n",
      "Epoch: 4 \tTraining Loss: 0.000112\n",
      "Epoch: 5 \tTraining Loss: 0.000112\n",
      "Epoch: 6 \tTraining Loss: 0.000112\n",
      "Epoch: 7 \tTraining Loss: 0.000112\n",
      "Epoch: 8 \tTraining Loss: 0.000112\n",
      "Epoch: 9 \tTraining Loss: 0.000112\n",
      "Epoch: 10 \tTraining Loss: 0.000112\n",
      "Epoch: 11 \tTraining Loss: 0.000112\n",
      "Epoch: 12 \tTraining Loss: 0.000112\n",
      "Epoch: 13 \tTraining Loss: 0.000112\n",
      "Epoch: 14 \tTraining Loss: 0.000112\n",
      "Epoch: 15 \tTraining Loss: 0.000112\n",
      "Epoch: 16 \tTraining Loss: 0.000112\n",
      "Epoch: 17 \tTraining Loss: 0.000112\n",
      "Epoch: 18 \tTraining Loss: 0.000112\n",
      "Epoch: 1 \tTraining Loss: 0.000356\n",
      "Epoch: 2 \tTraining Loss: 0.000356\n",
      "Epoch: 3 \tTraining Loss: 0.000356\n",
      "Epoch: 4 \tTraining Loss: 0.000356\n",
      "Epoch: 5 \tTraining Loss: 0.000356\n",
      "Epoch: 6 \tTraining Loss: 0.000356\n",
      "Epoch: 7 \tTraining Loss: 0.000356\n",
      "Epoch: 8 \tTraining Loss: 0.000356\n",
      "Epoch: 9 \tTraining Loss: 0.000356\n",
      "Epoch: 10 \tTraining Loss: 0.000356\n",
      "Epoch: 11 \tTraining Loss: 0.000356\n",
      "Epoch: 12 \tTraining Loss: 0.000356\n",
      "Epoch: 13 \tTraining Loss: 0.000356\n",
      "Epoch: 14 \tTraining Loss: 0.000356\n",
      "Epoch: 15 \tTraining Loss: 0.000356\n",
      "Epoch: 16 \tTraining Loss: 0.000356\n",
      "Epoch: 17 \tTraining Loss: 0.000356\n",
      "Epoch: 18 \tTraining Loss: 0.000356\n",
      "Epoch: 1 \tTraining Loss: 0.000172\n",
      "Epoch: 2 \tTraining Loss: 0.000172\n",
      "Epoch: 3 \tTraining Loss: 0.000172\n",
      "Epoch: 4 \tTraining Loss: 0.000172\n",
      "Epoch: 5 \tTraining Loss: 0.000172\n",
      "Epoch: 6 \tTraining Loss: 0.000172\n",
      "Epoch: 7 \tTraining Loss: 0.000172\n",
      "Epoch: 8 \tTraining Loss: 0.000172\n",
      "Epoch: 9 \tTraining Loss: 0.000172\n",
      "Epoch: 10 \tTraining Loss: 0.000172\n",
      "Epoch: 11 \tTraining Loss: 0.000172\n",
      "Epoch: 12 \tTraining Loss: 0.000172\n",
      "Epoch: 13 \tTraining Loss: 0.000172\n",
      "Epoch: 14 \tTraining Loss: 0.000172\n",
      "Epoch: 15 \tTraining Loss: 0.000172\n",
      "Epoch: 16 \tTraining Loss: 0.000172\n",
      "Epoch: 17 \tTraining Loss: 0.000172\n",
      "Epoch: 18 \tTraining Loss: 0.000172\n",
      "Epoch: 1 \tTraining Loss: 0.000241\n",
      "Epoch: 2 \tTraining Loss: 0.000241\n",
      "Epoch: 3 \tTraining Loss: 0.000241\n",
      "Epoch: 4 \tTraining Loss: 0.000241\n",
      "Epoch: 5 \tTraining Loss: 0.000241\n",
      "Epoch: 6 \tTraining Loss: 0.000241\n",
      "Epoch: 7 \tTraining Loss: 0.000241\n",
      "Epoch: 8 \tTraining Loss: 0.000241\n",
      "Epoch: 9 \tTraining Loss: 0.000241\n",
      "Epoch: 10 \tTraining Loss: 0.000241\n",
      "Epoch: 11 \tTraining Loss: 0.000241\n",
      "Epoch: 12 \tTraining Loss: 0.000241\n",
      "Epoch: 13 \tTraining Loss: 0.000241\n",
      "Epoch: 14 \tTraining Loss: 0.000241\n",
      "Epoch: 15 \tTraining Loss: 0.000241\n",
      "Epoch: 16 \tTraining Loss: 0.000241\n",
      "Epoch: 17 \tTraining Loss: 0.000241\n",
      "Epoch: 18 \tTraining Loss: 0.000241\n",
      "Epoch: 1 \tTraining Loss: 0.000147\n",
      "Epoch: 2 \tTraining Loss: 0.000147\n",
      "Epoch: 3 \tTraining Loss: 0.000147\n",
      "Epoch: 4 \tTraining Loss: 0.000147\n",
      "Epoch: 5 \tTraining Loss: 0.000147\n",
      "Epoch: 6 \tTraining Loss: 0.000147\n",
      "Epoch: 7 \tTraining Loss: 0.000147\n",
      "Epoch: 8 \tTraining Loss: 0.000147\n",
      "Epoch: 9 \tTraining Loss: 0.000147\n",
      "Epoch: 10 \tTraining Loss: 0.000147\n",
      "Epoch: 11 \tTraining Loss: 0.000147\n",
      "Epoch: 12 \tTraining Loss: 0.000147\n",
      "Epoch: 13 \tTraining Loss: 0.000147\n",
      "Epoch: 14 \tTraining Loss: 0.000147\n",
      "Epoch: 15 \tTraining Loss: 0.000147\n",
      "Epoch: 16 \tTraining Loss: 0.000147\n",
      "Epoch: 17 \tTraining Loss: 0.000147\n",
      "Epoch: 18 \tTraining Loss: 0.000147\n",
      "Epoch: 1 \tTraining Loss: 0.000339\n",
      "Epoch: 2 \tTraining Loss: 0.000339\n",
      "Epoch: 3 \tTraining Loss: 0.000339\n",
      "Epoch: 4 \tTraining Loss: 0.000339\n",
      "Epoch: 5 \tTraining Loss: 0.000339\n",
      "Epoch: 6 \tTraining Loss: 0.000339\n",
      "Epoch: 7 \tTraining Loss: 0.000339\n",
      "Epoch: 8 \tTraining Loss: 0.000339\n",
      "Epoch: 9 \tTraining Loss: 0.000339\n",
      "Epoch: 10 \tTraining Loss: 0.000339\n",
      "Epoch: 11 \tTraining Loss: 0.000339\n",
      "Epoch: 12 \tTraining Loss: 0.000339\n",
      "Epoch: 13 \tTraining Loss: 0.000339\n",
      "Epoch: 14 \tTraining Loss: 0.000339\n",
      "Epoch: 15 \tTraining Loss: 0.000339\n",
      "Epoch: 16 \tTraining Loss: 0.000339\n",
      "Epoch: 17 \tTraining Loss: 0.000339\n",
      "Epoch: 18 \tTraining Loss: 0.000339\n",
      "Epoch: 1 \tTraining Loss: 0.000168\n",
      "Epoch: 2 \tTraining Loss: 0.000168\n",
      "Epoch: 3 \tTraining Loss: 0.000168\n",
      "Epoch: 4 \tTraining Loss: 0.000168\n",
      "Epoch: 5 \tTraining Loss: 0.000168\n",
      "Epoch: 6 \tTraining Loss: 0.000168\n",
      "Epoch: 7 \tTraining Loss: 0.000168\n",
      "Epoch: 8 \tTraining Loss: 0.000168\n",
      "Epoch: 9 \tTraining Loss: 0.000168\n",
      "Epoch: 10 \tTraining Loss: 0.000168\n",
      "Epoch: 11 \tTraining Loss: 0.000168\n",
      "Epoch: 12 \tTraining Loss: 0.000168\n",
      "Epoch: 13 \tTraining Loss: 0.000168\n",
      "Epoch: 14 \tTraining Loss: 0.000168\n",
      "Epoch: 15 \tTraining Loss: 0.000168\n",
      "Epoch: 16 \tTraining Loss: 0.000168\n",
      "Epoch: 17 \tTraining Loss: 0.000168\n",
      "Epoch: 18 \tTraining Loss: 0.000168\n",
      "Epoch: 1 \tTraining Loss: 0.000239\n",
      "Epoch: 2 \tTraining Loss: 0.000239\n",
      "Epoch: 3 \tTraining Loss: 0.000239\n",
      "Epoch: 4 \tTraining Loss: 0.000239\n",
      "Epoch: 5 \tTraining Loss: 0.000239\n",
      "Epoch: 6 \tTraining Loss: 0.000239\n",
      "Epoch: 7 \tTraining Loss: 0.000239\n",
      "Epoch: 8 \tTraining Loss: 0.000239\n",
      "Epoch: 9 \tTraining Loss: 0.000239\n",
      "Epoch: 10 \tTraining Loss: 0.000239\n",
      "Epoch: 11 \tTraining Loss: 0.000239\n",
      "Epoch: 12 \tTraining Loss: 0.000239\n",
      "Epoch: 13 \tTraining Loss: 0.000239\n",
      "Epoch: 14 \tTraining Loss: 0.000239\n",
      "Epoch: 15 \tTraining Loss: 0.000239\n",
      "Epoch: 16 \tTraining Loss: 0.000239\n",
      "Epoch: 17 \tTraining Loss: 0.000239\n",
      "Epoch: 18 \tTraining Loss: 0.000239\n",
      "Epoch: 1 \tTraining Loss: 0.000317\n",
      "Epoch: 2 \tTraining Loss: 0.000317\n",
      "Epoch: 3 \tTraining Loss: 0.000317\n",
      "Epoch: 4 \tTraining Loss: 0.000317\n",
      "Epoch: 5 \tTraining Loss: 0.000317\n",
      "Epoch: 6 \tTraining Loss: 0.000317\n",
      "Epoch: 7 \tTraining Loss: 0.000317\n",
      "Epoch: 8 \tTraining Loss: 0.000317\n",
      "Epoch: 9 \tTraining Loss: 0.000317\n",
      "Epoch: 10 \tTraining Loss: 0.000317\n",
      "Epoch: 11 \tTraining Loss: 0.000317\n",
      "Epoch: 12 \tTraining Loss: 0.000317\n",
      "Epoch: 13 \tTraining Loss: 0.000317\n",
      "Epoch: 14 \tTraining Loss: 0.000317\n",
      "Epoch: 15 \tTraining Loss: 0.000317\n",
      "Epoch: 16 \tTraining Loss: 0.000317\n",
      "Epoch: 17 \tTraining Loss: 0.000317\n",
      "Epoch: 18 \tTraining Loss: 0.000317\n",
      "Epoch: 1 \tTraining Loss: 0.000222\n",
      "Epoch: 2 \tTraining Loss: 0.000222\n",
      "Epoch: 3 \tTraining Loss: 0.000222\n",
      "Epoch: 4 \tTraining Loss: 0.000222\n",
      "Epoch: 5 \tTraining Loss: 0.000222\n",
      "Epoch: 6 \tTraining Loss: 0.000222\n",
      "Epoch: 7 \tTraining Loss: 0.000222\n",
      "Epoch: 8 \tTraining Loss: 0.000222\n",
      "Epoch: 9 \tTraining Loss: 0.000222\n",
      "Epoch: 10 \tTraining Loss: 0.000222\n",
      "Epoch: 11 \tTraining Loss: 0.000222\n",
      "Epoch: 12 \tTraining Loss: 0.000222\n",
      "Epoch: 13 \tTraining Loss: 0.000222\n",
      "Epoch: 14 \tTraining Loss: 0.000222\n",
      "Epoch: 15 \tTraining Loss: 0.000222\n",
      "Epoch: 16 \tTraining Loss: 0.000222\n",
      "Epoch: 17 \tTraining Loss: 0.000222\n",
      "Epoch: 18 \tTraining Loss: 0.000222\n",
      "Epoch: 1 \tTraining Loss: 0.000110\n",
      "Epoch: 2 \tTraining Loss: 0.000110\n",
      "Epoch: 3 \tTraining Loss: 0.000110\n",
      "Epoch: 4 \tTraining Loss: 0.000110\n",
      "Epoch: 5 \tTraining Loss: 0.000110\n",
      "Epoch: 6 \tTraining Loss: 0.000110\n",
      "Epoch: 7 \tTraining Loss: 0.000110\n",
      "Epoch: 8 \tTraining Loss: 0.000110\n",
      "Epoch: 9 \tTraining Loss: 0.000110\n",
      "Epoch: 10 \tTraining Loss: 0.000110\n",
      "Epoch: 11 \tTraining Loss: 0.000110\n",
      "Epoch: 12 \tTraining Loss: 0.000110\n",
      "Epoch: 13 \tTraining Loss: 0.000110\n",
      "Epoch: 14 \tTraining Loss: 0.000110\n",
      "Epoch: 15 \tTraining Loss: 0.000110\n",
      "Epoch: 16 \tTraining Loss: 0.000110\n",
      "Epoch: 17 \tTraining Loss: 0.000110\n",
      "Epoch: 18 \tTraining Loss: 0.000110\n",
      "Epoch: 1 \tTraining Loss: 0.000222\n",
      "Epoch: 2 \tTraining Loss: 0.000222\n",
      "Epoch: 3 \tTraining Loss: 0.000222\n",
      "Epoch: 4 \tTraining Loss: 0.000222\n",
      "Epoch: 5 \tTraining Loss: 0.000222\n",
      "Epoch: 6 \tTraining Loss: 0.000222\n",
      "Epoch: 7 \tTraining Loss: 0.000222\n",
      "Epoch: 8 \tTraining Loss: 0.000222\n",
      "Epoch: 9 \tTraining Loss: 0.000222\n",
      "Epoch: 10 \tTraining Loss: 0.000222\n",
      "Epoch: 11 \tTraining Loss: 0.000222\n",
      "Epoch: 12 \tTraining Loss: 0.000222\n",
      "Epoch: 13 \tTraining Loss: 0.000222\n",
      "Epoch: 14 \tTraining Loss: 0.000222\n",
      "Epoch: 15 \tTraining Loss: 0.000222\n",
      "Epoch: 16 \tTraining Loss: 0.000222\n",
      "Epoch: 17 \tTraining Loss: 0.000222\n",
      "Epoch: 18 \tTraining Loss: 0.000222\n",
      "Epoch: 1 \tTraining Loss: 0.000224\n",
      "Epoch: 2 \tTraining Loss: 0.000224\n",
      "Epoch: 3 \tTraining Loss: 0.000224\n",
      "Epoch: 4 \tTraining Loss: 0.000224\n",
      "Epoch: 5 \tTraining Loss: 0.000224\n",
      "Epoch: 6 \tTraining Loss: 0.000224\n",
      "Epoch: 7 \tTraining Loss: 0.000224\n",
      "Epoch: 8 \tTraining Loss: 0.000224\n",
      "Epoch: 9 \tTraining Loss: 0.000224\n",
      "Epoch: 10 \tTraining Loss: 0.000224\n",
      "Epoch: 11 \tTraining Loss: 0.000224\n",
      "Epoch: 12 \tTraining Loss: 0.000224\n",
      "Epoch: 13 \tTraining Loss: 0.000224\n",
      "Epoch: 14 \tTraining Loss: 0.000224\n",
      "Epoch: 15 \tTraining Loss: 0.000224\n",
      "Epoch: 16 \tTraining Loss: 0.000224\n",
      "Epoch: 17 \tTraining Loss: 0.000224\n",
      "Epoch: 18 \tTraining Loss: 0.000224\n",
      "Epoch: 1 \tTraining Loss: 0.000269\n",
      "Epoch: 2 \tTraining Loss: 0.000269\n",
      "Epoch: 3 \tTraining Loss: 0.000269\n",
      "Epoch: 4 \tTraining Loss: 0.000269\n",
      "Epoch: 5 \tTraining Loss: 0.000269\n",
      "Epoch: 6 \tTraining Loss: 0.000269\n",
      "Epoch: 7 \tTraining Loss: 0.000269\n",
      "Epoch: 8 \tTraining Loss: 0.000269\n",
      "Epoch: 9 \tTraining Loss: 0.000269\n",
      "Epoch: 10 \tTraining Loss: 0.000269\n",
      "Epoch: 11 \tTraining Loss: 0.000269\n",
      "Epoch: 12 \tTraining Loss: 0.000269\n",
      "Epoch: 13 \tTraining Loss: 0.000269\n",
      "Epoch: 14 \tTraining Loss: 0.000269\n",
      "Epoch: 15 \tTraining Loss: 0.000269\n",
      "Epoch: 16 \tTraining Loss: 0.000269\n",
      "Epoch: 17 \tTraining Loss: 0.000269\n",
      "Epoch: 18 \tTraining Loss: 0.000269\n",
      "Epoch: 1 \tTraining Loss: 0.000189\n",
      "Epoch: 2 \tTraining Loss: 0.000189\n",
      "Epoch: 3 \tTraining Loss: 0.000189\n",
      "Epoch: 4 \tTraining Loss: 0.000189\n",
      "Epoch: 5 \tTraining Loss: 0.000189\n",
      "Epoch: 6 \tTraining Loss: 0.000189\n",
      "Epoch: 7 \tTraining Loss: 0.000189\n",
      "Epoch: 8 \tTraining Loss: 0.000189\n",
      "Epoch: 9 \tTraining Loss: 0.000189\n",
      "Epoch: 10 \tTraining Loss: 0.000189\n",
      "Epoch: 11 \tTraining Loss: 0.000189\n",
      "Epoch: 12 \tTraining Loss: 0.000189\n",
      "Epoch: 13 \tTraining Loss: 0.000189\n",
      "Epoch: 14 \tTraining Loss: 0.000189\n",
      "Epoch: 15 \tTraining Loss: 0.000189\n",
      "Epoch: 16 \tTraining Loss: 0.000189\n",
      "Epoch: 17 \tTraining Loss: 0.000189\n",
      "Epoch: 18 \tTraining Loss: 0.000189\n",
      "Epoch: 1 \tTraining Loss: 0.000201\n",
      "Epoch: 2 \tTraining Loss: 0.000201\n",
      "Epoch: 3 \tTraining Loss: 0.000201\n",
      "Epoch: 4 \tTraining Loss: 0.000201\n",
      "Epoch: 5 \tTraining Loss: 0.000201\n",
      "Epoch: 6 \tTraining Loss: 0.000201\n",
      "Epoch: 7 \tTraining Loss: 0.000201\n",
      "Epoch: 8 \tTraining Loss: 0.000201\n",
      "Epoch: 9 \tTraining Loss: 0.000201\n",
      "Epoch: 10 \tTraining Loss: 0.000201\n",
      "Epoch: 11 \tTraining Loss: 0.000201\n",
      "Epoch: 12 \tTraining Loss: 0.000201\n",
      "Epoch: 13 \tTraining Loss: 0.000201\n",
      "Epoch: 14 \tTraining Loss: 0.000201\n",
      "Epoch: 15 \tTraining Loss: 0.000201\n",
      "Epoch: 16 \tTraining Loss: 0.000201\n",
      "Epoch: 17 \tTraining Loss: 0.000201\n",
      "Epoch: 18 \tTraining Loss: 0.000201\n",
      "Epoch: 1 \tTraining Loss: 0.000173\n",
      "Epoch: 2 \tTraining Loss: 0.000173\n",
      "Epoch: 3 \tTraining Loss: 0.000173\n",
      "Epoch: 4 \tTraining Loss: 0.000173\n",
      "Epoch: 5 \tTraining Loss: 0.000173\n",
      "Epoch: 6 \tTraining Loss: 0.000173\n",
      "Epoch: 7 \tTraining Loss: 0.000173\n",
      "Epoch: 8 \tTraining Loss: 0.000173\n",
      "Epoch: 9 \tTraining Loss: 0.000173\n",
      "Epoch: 10 \tTraining Loss: 0.000173\n",
      "Epoch: 11 \tTraining Loss: 0.000173\n",
      "Epoch: 12 \tTraining Loss: 0.000173\n",
      "Epoch: 13 \tTraining Loss: 0.000173\n",
      "Epoch: 14 \tTraining Loss: 0.000173\n",
      "Epoch: 15 \tTraining Loss: 0.000173\n",
      "Epoch: 16 \tTraining Loss: 0.000173\n",
      "Epoch: 17 \tTraining Loss: 0.000173\n",
      "Epoch: 18 \tTraining Loss: 0.000173\n",
      "Epoch: 1 \tTraining Loss: 0.000201\n",
      "Epoch: 2 \tTraining Loss: 0.000201\n",
      "Epoch: 3 \tTraining Loss: 0.000201\n",
      "Epoch: 4 \tTraining Loss: 0.000201\n",
      "Epoch: 5 \tTraining Loss: 0.000201\n",
      "Epoch: 6 \tTraining Loss: 0.000201\n",
      "Epoch: 7 \tTraining Loss: 0.000201\n",
      "Epoch: 8 \tTraining Loss: 0.000201\n",
      "Epoch: 9 \tTraining Loss: 0.000201\n",
      "Epoch: 10 \tTraining Loss: 0.000201\n",
      "Epoch: 11 \tTraining Loss: 0.000201\n",
      "Epoch: 12 \tTraining Loss: 0.000201\n",
      "Epoch: 13 \tTraining Loss: 0.000201\n",
      "Epoch: 14 \tTraining Loss: 0.000201\n",
      "Epoch: 15 \tTraining Loss: 0.000201\n",
      "Epoch: 16 \tTraining Loss: 0.000201\n",
      "Epoch: 17 \tTraining Loss: 0.000201\n",
      "Epoch: 18 \tTraining Loss: 0.000201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000258\n",
      "Epoch: 2 \tTraining Loss: 0.000258\n",
      "Epoch: 3 \tTraining Loss: 0.000258\n",
      "Epoch: 4 \tTraining Loss: 0.000258\n",
      "Epoch: 5 \tTraining Loss: 0.000258\n",
      "Epoch: 6 \tTraining Loss: 0.000258\n",
      "Epoch: 7 \tTraining Loss: 0.000258\n",
      "Epoch: 8 \tTraining Loss: 0.000258\n",
      "Epoch: 9 \tTraining Loss: 0.000258\n",
      "Epoch: 10 \tTraining Loss: 0.000258\n",
      "Epoch: 11 \tTraining Loss: 0.000258\n",
      "Epoch: 12 \tTraining Loss: 0.000258\n",
      "Epoch: 13 \tTraining Loss: 0.000258\n",
      "Epoch: 14 \tTraining Loss: 0.000258\n",
      "Epoch: 15 \tTraining Loss: 0.000258\n",
      "Epoch: 16 \tTraining Loss: 0.000258\n",
      "Epoch: 17 \tTraining Loss: 0.000258\n",
      "Epoch: 18 \tTraining Loss: 0.000258\n",
      "Epoch: 1 \tTraining Loss: 0.000105\n",
      "Epoch: 2 \tTraining Loss: 0.000105\n",
      "Epoch: 3 \tTraining Loss: 0.000105\n",
      "Epoch: 4 \tTraining Loss: 0.000105\n",
      "Epoch: 5 \tTraining Loss: 0.000105\n",
      "Epoch: 6 \tTraining Loss: 0.000105\n",
      "Epoch: 7 \tTraining Loss: 0.000105\n",
      "Epoch: 8 \tTraining Loss: 0.000105\n",
      "Epoch: 9 \tTraining Loss: 0.000105\n",
      "Epoch: 10 \tTraining Loss: 0.000105\n",
      "Epoch: 11 \tTraining Loss: 0.000105\n",
      "Epoch: 12 \tTraining Loss: 0.000105\n",
      "Epoch: 13 \tTraining Loss: 0.000105\n",
      "Epoch: 14 \tTraining Loss: 0.000105\n",
      "Epoch: 15 \tTraining Loss: 0.000105\n",
      "Epoch: 16 \tTraining Loss: 0.000105\n",
      "Epoch: 17 \tTraining Loss: 0.000105\n",
      "Epoch: 18 \tTraining Loss: 0.000105\n",
      "Epoch: 1 \tTraining Loss: 0.000490\n",
      "Epoch: 2 \tTraining Loss: 0.000490\n",
      "Epoch: 3 \tTraining Loss: 0.000490\n",
      "Epoch: 4 \tTraining Loss: 0.000490\n",
      "Epoch: 5 \tTraining Loss: 0.000490\n",
      "Epoch: 6 \tTraining Loss: 0.000490\n",
      "Epoch: 7 \tTraining Loss: 0.000490\n",
      "Epoch: 8 \tTraining Loss: 0.000490\n",
      "Epoch: 9 \tTraining Loss: 0.000490\n",
      "Epoch: 10 \tTraining Loss: 0.000490\n",
      "Epoch: 11 \tTraining Loss: 0.000490\n",
      "Epoch: 12 \tTraining Loss: 0.000490\n",
      "Epoch: 13 \tTraining Loss: 0.000490\n",
      "Epoch: 14 \tTraining Loss: 0.000490\n",
      "Epoch: 15 \tTraining Loss: 0.000490\n",
      "Epoch: 16 \tTraining Loss: 0.000490\n",
      "Epoch: 17 \tTraining Loss: 0.000490\n",
      "Epoch: 18 \tTraining Loss: 0.000490\n",
      "Epoch: 1 \tTraining Loss: 0.000218\n",
      "Epoch: 2 \tTraining Loss: 0.000218\n",
      "Epoch: 3 \tTraining Loss: 0.000218\n",
      "Epoch: 4 \tTraining Loss: 0.000218\n",
      "Epoch: 5 \tTraining Loss: 0.000218\n",
      "Epoch: 6 \tTraining Loss: 0.000218\n",
      "Epoch: 7 \tTraining Loss: 0.000218\n",
      "Epoch: 8 \tTraining Loss: 0.000218\n",
      "Epoch: 9 \tTraining Loss: 0.000218\n",
      "Epoch: 10 \tTraining Loss: 0.000218\n",
      "Epoch: 11 \tTraining Loss: 0.000218\n",
      "Epoch: 12 \tTraining Loss: 0.000218\n",
      "Epoch: 13 \tTraining Loss: 0.000218\n",
      "Epoch: 14 \tTraining Loss: 0.000218\n",
      "Epoch: 15 \tTraining Loss: 0.000218\n",
      "Epoch: 16 \tTraining Loss: 0.000218\n",
      "Epoch: 17 \tTraining Loss: 0.000218\n",
      "Epoch: 18 \tTraining Loss: 0.000218\n",
      "Epoch: 1 \tTraining Loss: 0.000190\n",
      "Epoch: 2 \tTraining Loss: 0.000190\n",
      "Epoch: 3 \tTraining Loss: 0.000190\n",
      "Epoch: 4 \tTraining Loss: 0.000190\n",
      "Epoch: 5 \tTraining Loss: 0.000190\n",
      "Epoch: 6 \tTraining Loss: 0.000190\n",
      "Epoch: 7 \tTraining Loss: 0.000190\n",
      "Epoch: 8 \tTraining Loss: 0.000190\n",
      "Epoch: 9 \tTraining Loss: 0.000190\n",
      "Epoch: 10 \tTraining Loss: 0.000190\n",
      "Epoch: 11 \tTraining Loss: 0.000190\n",
      "Epoch: 12 \tTraining Loss: 0.000190\n",
      "Epoch: 13 \tTraining Loss: 0.000190\n",
      "Epoch: 14 \tTraining Loss: 0.000190\n",
      "Epoch: 15 \tTraining Loss: 0.000190\n",
      "Epoch: 16 \tTraining Loss: 0.000190\n",
      "Epoch: 17 \tTraining Loss: 0.000190\n",
      "Epoch: 18 \tTraining Loss: 0.000190\n",
      "Epoch: 1 \tTraining Loss: 0.000191\n",
      "Epoch: 2 \tTraining Loss: 0.000191\n",
      "Epoch: 3 \tTraining Loss: 0.000191\n",
      "Epoch: 4 \tTraining Loss: 0.000191\n",
      "Epoch: 5 \tTraining Loss: 0.000191\n",
      "Epoch: 6 \tTraining Loss: 0.000191\n",
      "Epoch: 7 \tTraining Loss: 0.000191\n",
      "Epoch: 8 \tTraining Loss: 0.000191\n",
      "Epoch: 9 \tTraining Loss: 0.000191\n",
      "Epoch: 10 \tTraining Loss: 0.000191\n",
      "Epoch: 11 \tTraining Loss: 0.000191\n",
      "Epoch: 12 \tTraining Loss: 0.000191\n",
      "Epoch: 13 \tTraining Loss: 0.000191\n",
      "Epoch: 14 \tTraining Loss: 0.000191\n",
      "Epoch: 15 \tTraining Loss: 0.000191\n",
      "Epoch: 16 \tTraining Loss: 0.000191\n",
      "Epoch: 17 \tTraining Loss: 0.000191\n",
      "Epoch: 18 \tTraining Loss: 0.000191\n",
      "Epoch: 1 \tTraining Loss: 0.000211\n",
      "Epoch: 2 \tTraining Loss: 0.000211\n",
      "Epoch: 3 \tTraining Loss: 0.000211\n",
      "Epoch: 4 \tTraining Loss: 0.000211\n",
      "Epoch: 5 \tTraining Loss: 0.000211\n",
      "Epoch: 6 \tTraining Loss: 0.000211\n",
      "Epoch: 7 \tTraining Loss: 0.000211\n",
      "Epoch: 8 \tTraining Loss: 0.000211\n",
      "Epoch: 9 \tTraining Loss: 0.000211\n",
      "Epoch: 10 \tTraining Loss: 0.000211\n",
      "Epoch: 11 \tTraining Loss: 0.000211\n",
      "Epoch: 12 \tTraining Loss: 0.000211\n",
      "Epoch: 13 \tTraining Loss: 0.000211\n",
      "Epoch: 14 \tTraining Loss: 0.000211\n",
      "Epoch: 15 \tTraining Loss: 0.000211\n",
      "Epoch: 16 \tTraining Loss: 0.000211\n",
      "Epoch: 17 \tTraining Loss: 0.000211\n",
      "Epoch: 18 \tTraining Loss: 0.000211\n",
      "Epoch: 1 \tTraining Loss: 0.000303\n",
      "Epoch: 2 \tTraining Loss: 0.000303\n",
      "Epoch: 3 \tTraining Loss: 0.000303\n",
      "Epoch: 4 \tTraining Loss: 0.000303\n",
      "Epoch: 5 \tTraining Loss: 0.000303\n",
      "Epoch: 6 \tTraining Loss: 0.000303\n",
      "Epoch: 7 \tTraining Loss: 0.000303\n",
      "Epoch: 8 \tTraining Loss: 0.000303\n",
      "Epoch: 9 \tTraining Loss: 0.000303\n",
      "Epoch: 10 \tTraining Loss: 0.000303\n",
      "Epoch: 11 \tTraining Loss: 0.000303\n",
      "Epoch: 12 \tTraining Loss: 0.000303\n",
      "Epoch: 13 \tTraining Loss: 0.000303\n",
      "Epoch: 14 \tTraining Loss: 0.000303\n",
      "Epoch: 15 \tTraining Loss: 0.000303\n",
      "Epoch: 16 \tTraining Loss: 0.000303\n",
      "Epoch: 17 \tTraining Loss: 0.000303\n",
      "Epoch: 18 \tTraining Loss: 0.000303\n",
      "Epoch: 1 \tTraining Loss: 0.000139\n",
      "Epoch: 2 \tTraining Loss: 0.000139\n",
      "Epoch: 3 \tTraining Loss: 0.000139\n",
      "Epoch: 4 \tTraining Loss: 0.000139\n",
      "Epoch: 5 \tTraining Loss: 0.000139\n",
      "Epoch: 6 \tTraining Loss: 0.000139\n",
      "Epoch: 7 \tTraining Loss: 0.000139\n",
      "Epoch: 8 \tTraining Loss: 0.000139\n",
      "Epoch: 9 \tTraining Loss: 0.000139\n",
      "Epoch: 10 \tTraining Loss: 0.000139\n",
      "Epoch: 11 \tTraining Loss: 0.000139\n",
      "Epoch: 12 \tTraining Loss: 0.000139\n",
      "Epoch: 13 \tTraining Loss: 0.000139\n",
      "Epoch: 14 \tTraining Loss: 0.000139\n",
      "Epoch: 15 \tTraining Loss: 0.000139\n",
      "Epoch: 16 \tTraining Loss: 0.000139\n",
      "Epoch: 17 \tTraining Loss: 0.000139\n",
      "Epoch: 18 \tTraining Loss: 0.000139\n",
      "Epoch: 1 \tTraining Loss: 0.000122\n",
      "Epoch: 2 \tTraining Loss: 0.000122\n",
      "Epoch: 3 \tTraining Loss: 0.000122\n",
      "Epoch: 4 \tTraining Loss: 0.000122\n",
      "Epoch: 5 \tTraining Loss: 0.000122\n",
      "Epoch: 6 \tTraining Loss: 0.000122\n",
      "Epoch: 7 \tTraining Loss: 0.000122\n",
      "Epoch: 8 \tTraining Loss: 0.000122\n",
      "Epoch: 9 \tTraining Loss: 0.000122\n",
      "Epoch: 10 \tTraining Loss: 0.000122\n",
      "Epoch: 11 \tTraining Loss: 0.000122\n",
      "Epoch: 12 \tTraining Loss: 0.000122\n",
      "Epoch: 13 \tTraining Loss: 0.000122\n",
      "Epoch: 14 \tTraining Loss: 0.000122\n",
      "Epoch: 15 \tTraining Loss: 0.000122\n",
      "Epoch: 16 \tTraining Loss: 0.000122\n",
      "Epoch: 17 \tTraining Loss: 0.000122\n",
      "Epoch: 18 \tTraining Loss: 0.000122\n",
      "Epoch: 1 \tTraining Loss: 0.000306\n",
      "Epoch: 2 \tTraining Loss: 0.000306\n",
      "Epoch: 3 \tTraining Loss: 0.000306\n",
      "Epoch: 4 \tTraining Loss: 0.000306\n",
      "Epoch: 5 \tTraining Loss: 0.000306\n",
      "Epoch: 6 \tTraining Loss: 0.000306\n",
      "Epoch: 7 \tTraining Loss: 0.000306\n",
      "Epoch: 8 \tTraining Loss: 0.000306\n",
      "Epoch: 9 \tTraining Loss: 0.000306\n",
      "Epoch: 10 \tTraining Loss: 0.000306\n",
      "Epoch: 11 \tTraining Loss: 0.000306\n",
      "Epoch: 12 \tTraining Loss: 0.000306\n",
      "Epoch: 13 \tTraining Loss: 0.000306\n",
      "Epoch: 14 \tTraining Loss: 0.000306\n",
      "Epoch: 15 \tTraining Loss: 0.000306\n",
      "Epoch: 16 \tTraining Loss: 0.000306\n",
      "Epoch: 17 \tTraining Loss: 0.000306\n",
      "Epoch: 18 \tTraining Loss: 0.000306\n",
      "Epoch: 1 \tTraining Loss: 0.000193\n",
      "Epoch: 2 \tTraining Loss: 0.000193\n",
      "Epoch: 3 \tTraining Loss: 0.000193\n",
      "Epoch: 4 \tTraining Loss: 0.000193\n",
      "Epoch: 5 \tTraining Loss: 0.000193\n",
      "Epoch: 6 \tTraining Loss: 0.000193\n",
      "Epoch: 7 \tTraining Loss: 0.000193\n",
      "Epoch: 8 \tTraining Loss: 0.000193\n",
      "Epoch: 9 \tTraining Loss: 0.000193\n",
      "Epoch: 10 \tTraining Loss: 0.000193\n",
      "Epoch: 11 \tTraining Loss: 0.000193\n",
      "Epoch: 12 \tTraining Loss: 0.000193\n",
      "Epoch: 13 \tTraining Loss: 0.000193\n",
      "Epoch: 14 \tTraining Loss: 0.000193\n",
      "Epoch: 15 \tTraining Loss: 0.000193\n",
      "Epoch: 16 \tTraining Loss: 0.000193\n",
      "Epoch: 17 \tTraining Loss: 0.000193\n",
      "Epoch: 18 \tTraining Loss: 0.000193\n",
      "Epoch: 1 \tTraining Loss: 0.000196\n",
      "Epoch: 2 \tTraining Loss: 0.000196\n",
      "Epoch: 3 \tTraining Loss: 0.000196\n",
      "Epoch: 4 \tTraining Loss: 0.000196\n",
      "Epoch: 5 \tTraining Loss: 0.000196\n",
      "Epoch: 6 \tTraining Loss: 0.000196\n",
      "Epoch: 7 \tTraining Loss: 0.000196\n",
      "Epoch: 8 \tTraining Loss: 0.000196\n",
      "Epoch: 9 \tTraining Loss: 0.000196\n",
      "Epoch: 10 \tTraining Loss: 0.000196\n",
      "Epoch: 11 \tTraining Loss: 0.000196\n",
      "Epoch: 12 \tTraining Loss: 0.000196\n",
      "Epoch: 13 \tTraining Loss: 0.000196\n",
      "Epoch: 14 \tTraining Loss: 0.000196\n",
      "Epoch: 15 \tTraining Loss: 0.000196\n",
      "Epoch: 16 \tTraining Loss: 0.000196\n",
      "Epoch: 17 \tTraining Loss: 0.000196\n",
      "Epoch: 18 \tTraining Loss: 0.000196\n",
      "Epoch: 1 \tTraining Loss: 0.000340\n",
      "Epoch: 2 \tTraining Loss: 0.000340\n",
      "Epoch: 3 \tTraining Loss: 0.000340\n",
      "Epoch: 4 \tTraining Loss: 0.000340\n",
      "Epoch: 5 \tTraining Loss: 0.000340\n",
      "Epoch: 6 \tTraining Loss: 0.000340\n",
      "Epoch: 7 \tTraining Loss: 0.000340\n",
      "Epoch: 8 \tTraining Loss: 0.000340\n",
      "Epoch: 9 \tTraining Loss: 0.000340\n",
      "Epoch: 10 \tTraining Loss: 0.000340\n",
      "Epoch: 11 \tTraining Loss: 0.000340\n",
      "Epoch: 12 \tTraining Loss: 0.000340\n",
      "Epoch: 13 \tTraining Loss: 0.000340\n",
      "Epoch: 14 \tTraining Loss: 0.000340\n",
      "Epoch: 15 \tTraining Loss: 0.000340\n",
      "Epoch: 16 \tTraining Loss: 0.000340\n",
      "Epoch: 17 \tTraining Loss: 0.000340\n",
      "Epoch: 18 \tTraining Loss: 0.000340\n",
      "Epoch: 1 \tTraining Loss: 0.000229\n",
      "Epoch: 2 \tTraining Loss: 0.000229\n",
      "Epoch: 3 \tTraining Loss: 0.000229\n",
      "Epoch: 4 \tTraining Loss: 0.000229\n",
      "Epoch: 5 \tTraining Loss: 0.000229\n",
      "Epoch: 6 \tTraining Loss: 0.000229\n",
      "Epoch: 7 \tTraining Loss: 0.000229\n",
      "Epoch: 8 \tTraining Loss: 0.000229\n",
      "Epoch: 9 \tTraining Loss: 0.000229\n",
      "Epoch: 10 \tTraining Loss: 0.000229\n",
      "Epoch: 11 \tTraining Loss: 0.000229\n",
      "Epoch: 12 \tTraining Loss: 0.000229\n",
      "Epoch: 13 \tTraining Loss: 0.000229\n",
      "Epoch: 14 \tTraining Loss: 0.000229\n",
      "Epoch: 15 \tTraining Loss: 0.000229\n",
      "Epoch: 16 \tTraining Loss: 0.000229\n",
      "Epoch: 17 \tTraining Loss: 0.000229\n",
      "Epoch: 18 \tTraining Loss: 0.000229\n",
      "Epoch: 1 \tTraining Loss: 0.000297\n",
      "Epoch: 2 \tTraining Loss: 0.000297\n",
      "Epoch: 3 \tTraining Loss: 0.000297\n",
      "Epoch: 4 \tTraining Loss: 0.000297\n",
      "Epoch: 5 \tTraining Loss: 0.000297\n",
      "Epoch: 6 \tTraining Loss: 0.000297\n",
      "Epoch: 7 \tTraining Loss: 0.000297\n",
      "Epoch: 8 \tTraining Loss: 0.000297\n",
      "Epoch: 9 \tTraining Loss: 0.000297\n",
      "Epoch: 10 \tTraining Loss: 0.000297\n",
      "Epoch: 11 \tTraining Loss: 0.000297\n",
      "Epoch: 12 \tTraining Loss: 0.000297\n",
      "Epoch: 13 \tTraining Loss: 0.000297\n",
      "Epoch: 14 \tTraining Loss: 0.000297\n",
      "Epoch: 15 \tTraining Loss: 0.000297\n",
      "Epoch: 16 \tTraining Loss: 0.000297\n",
      "Epoch: 17 \tTraining Loss: 0.000297\n",
      "Epoch: 18 \tTraining Loss: 0.000297\n",
      "Epoch: 1 \tTraining Loss: 0.000236\n",
      "Epoch: 2 \tTraining Loss: 0.000236\n",
      "Epoch: 3 \tTraining Loss: 0.000236\n",
      "Epoch: 4 \tTraining Loss: 0.000236\n",
      "Epoch: 5 \tTraining Loss: 0.000236\n",
      "Epoch: 6 \tTraining Loss: 0.000236\n",
      "Epoch: 7 \tTraining Loss: 0.000236\n",
      "Epoch: 8 \tTraining Loss: 0.000236\n",
      "Epoch: 9 \tTraining Loss: 0.000236\n",
      "Epoch: 10 \tTraining Loss: 0.000236\n",
      "Epoch: 11 \tTraining Loss: 0.000236\n",
      "Epoch: 12 \tTraining Loss: 0.000236\n",
      "Epoch: 13 \tTraining Loss: 0.000236\n",
      "Epoch: 14 \tTraining Loss: 0.000236\n",
      "Epoch: 15 \tTraining Loss: 0.000236\n",
      "Epoch: 16 \tTraining Loss: 0.000236\n",
      "Epoch: 17 \tTraining Loss: 0.000236\n",
      "Epoch: 18 \tTraining Loss: 0.000236\n",
      "Epoch: 1 \tTraining Loss: 0.000248\n",
      "Epoch: 2 \tTraining Loss: 0.000248\n",
      "Epoch: 3 \tTraining Loss: 0.000248\n",
      "Epoch: 4 \tTraining Loss: 0.000248\n",
      "Epoch: 5 \tTraining Loss: 0.000248\n",
      "Epoch: 6 \tTraining Loss: 0.000248\n",
      "Epoch: 7 \tTraining Loss: 0.000248\n",
      "Epoch: 8 \tTraining Loss: 0.000248\n",
      "Epoch: 9 \tTraining Loss: 0.000248\n",
      "Epoch: 10 \tTraining Loss: 0.000248\n",
      "Epoch: 11 \tTraining Loss: 0.000248\n",
      "Epoch: 12 \tTraining Loss: 0.000248\n",
      "Epoch: 13 \tTraining Loss: 0.000248\n",
      "Epoch: 14 \tTraining Loss: 0.000248\n",
      "Epoch: 15 \tTraining Loss: 0.000248\n",
      "Epoch: 16 \tTraining Loss: 0.000248\n",
      "Epoch: 17 \tTraining Loss: 0.000248\n",
      "Epoch: 18 \tTraining Loss: 0.000248\n",
      "Epoch: 1 \tTraining Loss: 0.000308\n",
      "Epoch: 2 \tTraining Loss: 0.000308\n",
      "Epoch: 3 \tTraining Loss: 0.000308\n",
      "Epoch: 4 \tTraining Loss: 0.000308\n",
      "Epoch: 5 \tTraining Loss: 0.000308\n",
      "Epoch: 6 \tTraining Loss: 0.000308\n",
      "Epoch: 7 \tTraining Loss: 0.000308\n",
      "Epoch: 8 \tTraining Loss: 0.000308\n",
      "Epoch: 9 \tTraining Loss: 0.000308\n",
      "Epoch: 10 \tTraining Loss: 0.000308\n",
      "Epoch: 11 \tTraining Loss: 0.000308\n",
      "Epoch: 12 \tTraining Loss: 0.000308\n",
      "Epoch: 13 \tTraining Loss: 0.000308\n",
      "Epoch: 14 \tTraining Loss: 0.000308\n",
      "Epoch: 15 \tTraining Loss: 0.000308\n",
      "Epoch: 16 \tTraining Loss: 0.000308\n",
      "Epoch: 17 \tTraining Loss: 0.000308\n",
      "Epoch: 18 \tTraining Loss: 0.000308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000227\n",
      "Epoch: 2 \tTraining Loss: 0.000227\n",
      "Epoch: 3 \tTraining Loss: 0.000227\n",
      "Epoch: 4 \tTraining Loss: 0.000227\n",
      "Epoch: 5 \tTraining Loss: 0.000227\n",
      "Epoch: 6 \tTraining Loss: 0.000227\n",
      "Epoch: 7 \tTraining Loss: 0.000227\n",
      "Epoch: 8 \tTraining Loss: 0.000227\n",
      "Epoch: 9 \tTraining Loss: 0.000227\n",
      "Epoch: 10 \tTraining Loss: 0.000227\n",
      "Epoch: 11 \tTraining Loss: 0.000227\n",
      "Epoch: 12 \tTraining Loss: 0.000227\n",
      "Epoch: 13 \tTraining Loss: 0.000227\n",
      "Epoch: 14 \tTraining Loss: 0.000227\n",
      "Epoch: 15 \tTraining Loss: 0.000227\n",
      "Epoch: 16 \tTraining Loss: 0.000227\n",
      "Epoch: 17 \tTraining Loss: 0.000227\n",
      "Epoch: 18 \tTraining Loss: 0.000227\n",
      "Epoch: 1 \tTraining Loss: 0.000239\n",
      "Epoch: 2 \tTraining Loss: 0.000239\n",
      "Epoch: 3 \tTraining Loss: 0.000239\n",
      "Epoch: 4 \tTraining Loss: 0.000239\n",
      "Epoch: 5 \tTraining Loss: 0.000239\n",
      "Epoch: 6 \tTraining Loss: 0.000239\n",
      "Epoch: 7 \tTraining Loss: 0.000239\n",
      "Epoch: 8 \tTraining Loss: 0.000239\n",
      "Epoch: 9 \tTraining Loss: 0.000239\n",
      "Epoch: 10 \tTraining Loss: 0.000239\n",
      "Epoch: 11 \tTraining Loss: 0.000239\n",
      "Epoch: 12 \tTraining Loss: 0.000239\n",
      "Epoch: 13 \tTraining Loss: 0.000239\n",
      "Epoch: 14 \tTraining Loss: 0.000239\n",
      "Epoch: 15 \tTraining Loss: 0.000239\n",
      "Epoch: 16 \tTraining Loss: 0.000239\n",
      "Epoch: 17 \tTraining Loss: 0.000239\n",
      "Epoch: 18 \tTraining Loss: 0.000239\n",
      "Epoch: 1 \tTraining Loss: 0.000174\n",
      "Epoch: 2 \tTraining Loss: 0.000174\n",
      "Epoch: 3 \tTraining Loss: 0.000174\n",
      "Epoch: 4 \tTraining Loss: 0.000174\n",
      "Epoch: 5 \tTraining Loss: 0.000174\n",
      "Epoch: 6 \tTraining Loss: 0.000174\n",
      "Epoch: 7 \tTraining Loss: 0.000174\n",
      "Epoch: 8 \tTraining Loss: 0.000174\n",
      "Epoch: 9 \tTraining Loss: 0.000174\n",
      "Epoch: 10 \tTraining Loss: 0.000174\n",
      "Epoch: 11 \tTraining Loss: 0.000174\n",
      "Epoch: 12 \tTraining Loss: 0.000174\n",
      "Epoch: 13 \tTraining Loss: 0.000174\n",
      "Epoch: 14 \tTraining Loss: 0.000174\n",
      "Epoch: 15 \tTraining Loss: 0.000174\n",
      "Epoch: 16 \tTraining Loss: 0.000174\n",
      "Epoch: 17 \tTraining Loss: 0.000174\n",
      "Epoch: 18 \tTraining Loss: 0.000174\n",
      "Epoch: 1 \tTraining Loss: 0.000392\n",
      "Epoch: 2 \tTraining Loss: 0.000392\n",
      "Epoch: 3 \tTraining Loss: 0.000392\n",
      "Epoch: 4 \tTraining Loss: 0.000392\n",
      "Epoch: 5 \tTraining Loss: 0.000392\n",
      "Epoch: 6 \tTraining Loss: 0.000392\n",
      "Epoch: 7 \tTraining Loss: 0.000392\n",
      "Epoch: 8 \tTraining Loss: 0.000392\n",
      "Epoch: 9 \tTraining Loss: 0.000392\n",
      "Epoch: 10 \tTraining Loss: 0.000392\n",
      "Epoch: 11 \tTraining Loss: 0.000392\n",
      "Epoch: 12 \tTraining Loss: 0.000392\n",
      "Epoch: 13 \tTraining Loss: 0.000392\n",
      "Epoch: 14 \tTraining Loss: 0.000392\n",
      "Epoch: 15 \tTraining Loss: 0.000392\n",
      "Epoch: 16 \tTraining Loss: 0.000392\n",
      "Epoch: 17 \tTraining Loss: 0.000392\n",
      "Epoch: 18 \tTraining Loss: 0.000392\n",
      "Epoch: 1 \tTraining Loss: 0.000276\n",
      "Epoch: 2 \tTraining Loss: 0.000276\n",
      "Epoch: 3 \tTraining Loss: 0.000276\n",
      "Epoch: 4 \tTraining Loss: 0.000276\n",
      "Epoch: 5 \tTraining Loss: 0.000276\n",
      "Epoch: 6 \tTraining Loss: 0.000276\n",
      "Epoch: 7 \tTraining Loss: 0.000276\n",
      "Epoch: 8 \tTraining Loss: 0.000276\n",
      "Epoch: 9 \tTraining Loss: 0.000276\n",
      "Epoch: 10 \tTraining Loss: 0.000276\n",
      "Epoch: 11 \tTraining Loss: 0.000276\n",
      "Epoch: 12 \tTraining Loss: 0.000276\n",
      "Epoch: 13 \tTraining Loss: 0.000276\n",
      "Epoch: 14 \tTraining Loss: 0.000276\n",
      "Epoch: 15 \tTraining Loss: 0.000276\n",
      "Epoch: 16 \tTraining Loss: 0.000276\n",
      "Epoch: 17 \tTraining Loss: 0.000276\n",
      "Epoch: 18 \tTraining Loss: 0.000276\n",
      "Epoch: 1 \tTraining Loss: 0.000140\n",
      "Epoch: 2 \tTraining Loss: 0.000140\n",
      "Epoch: 3 \tTraining Loss: 0.000140\n",
      "Epoch: 4 \tTraining Loss: 0.000140\n",
      "Epoch: 5 \tTraining Loss: 0.000140\n",
      "Epoch: 6 \tTraining Loss: 0.000140\n",
      "Epoch: 7 \tTraining Loss: 0.000140\n",
      "Epoch: 8 \tTraining Loss: 0.000140\n",
      "Epoch: 9 \tTraining Loss: 0.000140\n",
      "Epoch: 10 \tTraining Loss: 0.000140\n",
      "Epoch: 11 \tTraining Loss: 0.000140\n",
      "Epoch: 12 \tTraining Loss: 0.000140\n",
      "Epoch: 13 \tTraining Loss: 0.000140\n",
      "Epoch: 14 \tTraining Loss: 0.000140\n",
      "Epoch: 15 \tTraining Loss: 0.000140\n",
      "Epoch: 16 \tTraining Loss: 0.000140\n",
      "Epoch: 17 \tTraining Loss: 0.000140\n",
      "Epoch: 18 \tTraining Loss: 0.000140\n",
      "Epoch: 1 \tTraining Loss: 0.000231\n",
      "Epoch: 2 \tTraining Loss: 0.000231\n",
      "Epoch: 3 \tTraining Loss: 0.000231\n",
      "Epoch: 4 \tTraining Loss: 0.000231\n",
      "Epoch: 5 \tTraining Loss: 0.000231\n",
      "Epoch: 6 \tTraining Loss: 0.000231\n",
      "Epoch: 7 \tTraining Loss: 0.000231\n",
      "Epoch: 8 \tTraining Loss: 0.000231\n",
      "Epoch: 9 \tTraining Loss: 0.000231\n",
      "Epoch: 10 \tTraining Loss: 0.000231\n",
      "Epoch: 11 \tTraining Loss: 0.000231\n",
      "Epoch: 12 \tTraining Loss: 0.000231\n",
      "Epoch: 13 \tTraining Loss: 0.000231\n",
      "Epoch: 14 \tTraining Loss: 0.000231\n",
      "Epoch: 15 \tTraining Loss: 0.000231\n",
      "Epoch: 16 \tTraining Loss: 0.000231\n",
      "Epoch: 17 \tTraining Loss: 0.000231\n",
      "Epoch: 18 \tTraining Loss: 0.000231\n",
      "Epoch: 1 \tTraining Loss: 0.000398\n",
      "Epoch: 2 \tTraining Loss: 0.000398\n",
      "Epoch: 3 \tTraining Loss: 0.000398\n",
      "Epoch: 4 \tTraining Loss: 0.000398\n",
      "Epoch: 5 \tTraining Loss: 0.000398\n",
      "Epoch: 6 \tTraining Loss: 0.000398\n",
      "Epoch: 7 \tTraining Loss: 0.000398\n",
      "Epoch: 8 \tTraining Loss: 0.000398\n",
      "Epoch: 9 \tTraining Loss: 0.000398\n",
      "Epoch: 10 \tTraining Loss: 0.000398\n",
      "Epoch: 11 \tTraining Loss: 0.000398\n",
      "Epoch: 12 \tTraining Loss: 0.000398\n",
      "Epoch: 13 \tTraining Loss: 0.000398\n",
      "Epoch: 14 \tTraining Loss: 0.000398\n",
      "Epoch: 15 \tTraining Loss: 0.000398\n",
      "Epoch: 16 \tTraining Loss: 0.000398\n",
      "Epoch: 17 \tTraining Loss: 0.000398\n",
      "Epoch: 18 \tTraining Loss: 0.000398\n",
      "Epoch: 1 \tTraining Loss: 0.000142\n",
      "Epoch: 2 \tTraining Loss: 0.000142\n",
      "Epoch: 3 \tTraining Loss: 0.000142\n",
      "Epoch: 4 \tTraining Loss: 0.000142\n",
      "Epoch: 5 \tTraining Loss: 0.000142\n",
      "Epoch: 6 \tTraining Loss: 0.000142\n",
      "Epoch: 7 \tTraining Loss: 0.000142\n",
      "Epoch: 8 \tTraining Loss: 0.000142\n",
      "Epoch: 9 \tTraining Loss: 0.000142\n",
      "Epoch: 10 \tTraining Loss: 0.000142\n",
      "Epoch: 11 \tTraining Loss: 0.000142\n",
      "Epoch: 12 \tTraining Loss: 0.000142\n",
      "Epoch: 13 \tTraining Loss: 0.000142\n",
      "Epoch: 14 \tTraining Loss: 0.000142\n",
      "Epoch: 15 \tTraining Loss: 0.000142\n",
      "Epoch: 16 \tTraining Loss: 0.000142\n",
      "Epoch: 17 \tTraining Loss: 0.000142\n",
      "Epoch: 18 \tTraining Loss: 0.000142\n",
      "Epoch: 1 \tTraining Loss: 0.000230\n",
      "Epoch: 2 \tTraining Loss: 0.000230\n",
      "Epoch: 3 \tTraining Loss: 0.000230\n",
      "Epoch: 4 \tTraining Loss: 0.000230\n",
      "Epoch: 5 \tTraining Loss: 0.000230\n",
      "Epoch: 6 \tTraining Loss: 0.000230\n",
      "Epoch: 7 \tTraining Loss: 0.000230\n",
      "Epoch: 8 \tTraining Loss: 0.000230\n",
      "Epoch: 9 \tTraining Loss: 0.000230\n",
      "Epoch: 10 \tTraining Loss: 0.000230\n",
      "Epoch: 11 \tTraining Loss: 0.000230\n",
      "Epoch: 12 \tTraining Loss: 0.000230\n",
      "Epoch: 13 \tTraining Loss: 0.000230\n",
      "Epoch: 14 \tTraining Loss: 0.000230\n",
      "Epoch: 15 \tTraining Loss: 0.000230\n",
      "Epoch: 16 \tTraining Loss: 0.000230\n",
      "Epoch: 17 \tTraining Loss: 0.000230\n",
      "Epoch: 18 \tTraining Loss: 0.000230\n",
      "Epoch: 1 \tTraining Loss: 0.000270\n",
      "Epoch: 2 \tTraining Loss: 0.000270\n",
      "Epoch: 3 \tTraining Loss: 0.000270\n",
      "Epoch: 4 \tTraining Loss: 0.000270\n",
      "Epoch: 5 \tTraining Loss: 0.000270\n",
      "Epoch: 6 \tTraining Loss: 0.000270\n",
      "Epoch: 7 \tTraining Loss: 0.000270\n",
      "Epoch: 8 \tTraining Loss: 0.000270\n",
      "Epoch: 9 \tTraining Loss: 0.000270\n",
      "Epoch: 10 \tTraining Loss: 0.000270\n",
      "Epoch: 11 \tTraining Loss: 0.000270\n",
      "Epoch: 12 \tTraining Loss: 0.000270\n",
      "Epoch: 13 \tTraining Loss: 0.000270\n",
      "Epoch: 14 \tTraining Loss: 0.000270\n",
      "Epoch: 15 \tTraining Loss: 0.000270\n",
      "Epoch: 16 \tTraining Loss: 0.000270\n",
      "Epoch: 17 \tTraining Loss: 0.000270\n",
      "Epoch: 18 \tTraining Loss: 0.000270\n",
      "Epoch: 1 \tTraining Loss: 0.000246\n",
      "Epoch: 2 \tTraining Loss: 0.000246\n",
      "Epoch: 3 \tTraining Loss: 0.000246\n",
      "Epoch: 4 \tTraining Loss: 0.000246\n",
      "Epoch: 5 \tTraining Loss: 0.000246\n",
      "Epoch: 6 \tTraining Loss: 0.000246\n",
      "Epoch: 7 \tTraining Loss: 0.000246\n",
      "Epoch: 8 \tTraining Loss: 0.000246\n",
      "Epoch: 9 \tTraining Loss: 0.000246\n",
      "Epoch: 10 \tTraining Loss: 0.000246\n",
      "Epoch: 11 \tTraining Loss: 0.000246\n",
      "Epoch: 12 \tTraining Loss: 0.000246\n",
      "Epoch: 13 \tTraining Loss: 0.000246\n",
      "Epoch: 14 \tTraining Loss: 0.000246\n",
      "Epoch: 15 \tTraining Loss: 0.000246\n",
      "Epoch: 16 \tTraining Loss: 0.000246\n",
      "Epoch: 17 \tTraining Loss: 0.000246\n",
      "Epoch: 18 \tTraining Loss: 0.000246\n",
      "Epoch: 1 \tTraining Loss: 0.000167\n",
      "Epoch: 2 \tTraining Loss: 0.000167\n",
      "Epoch: 3 \tTraining Loss: 0.000167\n",
      "Epoch: 4 \tTraining Loss: 0.000167\n",
      "Epoch: 5 \tTraining Loss: 0.000167\n",
      "Epoch: 6 \tTraining Loss: 0.000167\n",
      "Epoch: 7 \tTraining Loss: 0.000167\n",
      "Epoch: 8 \tTraining Loss: 0.000167\n",
      "Epoch: 9 \tTraining Loss: 0.000167\n",
      "Epoch: 10 \tTraining Loss: 0.000167\n",
      "Epoch: 11 \tTraining Loss: 0.000167\n",
      "Epoch: 12 \tTraining Loss: 0.000167\n",
      "Epoch: 13 \tTraining Loss: 0.000167\n",
      "Epoch: 14 \tTraining Loss: 0.000167\n",
      "Epoch: 15 \tTraining Loss: 0.000167\n",
      "Epoch: 16 \tTraining Loss: 0.000167\n",
      "Epoch: 17 \tTraining Loss: 0.000167\n",
      "Epoch: 18 \tTraining Loss: 0.000167\n",
      "Epoch: 1 \tTraining Loss: 0.000232\n",
      "Epoch: 2 \tTraining Loss: 0.000232\n",
      "Epoch: 3 \tTraining Loss: 0.000232\n",
      "Epoch: 4 \tTraining Loss: 0.000232\n",
      "Epoch: 5 \tTraining Loss: 0.000232\n",
      "Epoch: 6 \tTraining Loss: 0.000232\n",
      "Epoch: 7 \tTraining Loss: 0.000232\n",
      "Epoch: 8 \tTraining Loss: 0.000232\n",
      "Epoch: 9 \tTraining Loss: 0.000232\n",
      "Epoch: 10 \tTraining Loss: 0.000232\n",
      "Epoch: 11 \tTraining Loss: 0.000232\n",
      "Epoch: 12 \tTraining Loss: 0.000232\n",
      "Epoch: 13 \tTraining Loss: 0.000232\n",
      "Epoch: 14 \tTraining Loss: 0.000232\n",
      "Epoch: 15 \tTraining Loss: 0.000232\n",
      "Epoch: 16 \tTraining Loss: 0.000232\n",
      "Epoch: 17 \tTraining Loss: 0.000232\n",
      "Epoch: 18 \tTraining Loss: 0.000232\n",
      "Epoch: 1 \tTraining Loss: 0.000330\n",
      "Epoch: 2 \tTraining Loss: 0.000330\n",
      "Epoch: 3 \tTraining Loss: 0.000330\n",
      "Epoch: 4 \tTraining Loss: 0.000330\n",
      "Epoch: 5 \tTraining Loss: 0.000330\n",
      "Epoch: 6 \tTraining Loss: 0.000330\n",
      "Epoch: 7 \tTraining Loss: 0.000330\n",
      "Epoch: 8 \tTraining Loss: 0.000330\n",
      "Epoch: 9 \tTraining Loss: 0.000330\n",
      "Epoch: 10 \tTraining Loss: 0.000330\n",
      "Epoch: 11 \tTraining Loss: 0.000330\n",
      "Epoch: 12 \tTraining Loss: 0.000330\n",
      "Epoch: 13 \tTraining Loss: 0.000330\n",
      "Epoch: 14 \tTraining Loss: 0.000330\n",
      "Epoch: 15 \tTraining Loss: 0.000330\n",
      "Epoch: 16 \tTraining Loss: 0.000330\n",
      "Epoch: 17 \tTraining Loss: 0.000330\n",
      "Epoch: 18 \tTraining Loss: 0.000330\n",
      "Epoch: 1 \tTraining Loss: 0.000118\n",
      "Epoch: 2 \tTraining Loss: 0.000118\n",
      "Epoch: 3 \tTraining Loss: 0.000118\n",
      "Epoch: 4 \tTraining Loss: 0.000118\n",
      "Epoch: 5 \tTraining Loss: 0.000118\n",
      "Epoch: 6 \tTraining Loss: 0.000118\n",
      "Epoch: 7 \tTraining Loss: 0.000118\n",
      "Epoch: 8 \tTraining Loss: 0.000118\n",
      "Epoch: 9 \tTraining Loss: 0.000118\n",
      "Epoch: 10 \tTraining Loss: 0.000118\n",
      "Epoch: 11 \tTraining Loss: 0.000118\n",
      "Epoch: 12 \tTraining Loss: 0.000118\n",
      "Epoch: 13 \tTraining Loss: 0.000118\n",
      "Epoch: 14 \tTraining Loss: 0.000118\n",
      "Epoch: 15 \tTraining Loss: 0.000118\n",
      "Epoch: 16 \tTraining Loss: 0.000118\n",
      "Epoch: 17 \tTraining Loss: 0.000118\n",
      "Epoch: 18 \tTraining Loss: 0.000118\n",
      "Epoch: 1 \tTraining Loss: 0.000188\n",
      "Epoch: 2 \tTraining Loss: 0.000188\n",
      "Epoch: 3 \tTraining Loss: 0.000188\n",
      "Epoch: 4 \tTraining Loss: 0.000188\n",
      "Epoch: 5 \tTraining Loss: 0.000188\n",
      "Epoch: 6 \tTraining Loss: 0.000188\n",
      "Epoch: 7 \tTraining Loss: 0.000188\n",
      "Epoch: 8 \tTraining Loss: 0.000188\n",
      "Epoch: 9 \tTraining Loss: 0.000188\n",
      "Epoch: 10 \tTraining Loss: 0.000188\n",
      "Epoch: 11 \tTraining Loss: 0.000188\n",
      "Epoch: 12 \tTraining Loss: 0.000188\n",
      "Epoch: 13 \tTraining Loss: 0.000188\n",
      "Epoch: 14 \tTraining Loss: 0.000188\n",
      "Epoch: 15 \tTraining Loss: 0.000188\n",
      "Epoch: 16 \tTraining Loss: 0.000188\n",
      "Epoch: 17 \tTraining Loss: 0.000188\n",
      "Epoch: 18 \tTraining Loss: 0.000188\n",
      "Epoch: 1 \tTraining Loss: 0.000125\n",
      "Epoch: 2 \tTraining Loss: 0.000125\n",
      "Epoch: 3 \tTraining Loss: 0.000125\n",
      "Epoch: 4 \tTraining Loss: 0.000125\n",
      "Epoch: 5 \tTraining Loss: 0.000125\n",
      "Epoch: 6 \tTraining Loss: 0.000125\n",
      "Epoch: 7 \tTraining Loss: 0.000125\n",
      "Epoch: 8 \tTraining Loss: 0.000125\n",
      "Epoch: 9 \tTraining Loss: 0.000125\n",
      "Epoch: 10 \tTraining Loss: 0.000125\n",
      "Epoch: 11 \tTraining Loss: 0.000125\n",
      "Epoch: 12 \tTraining Loss: 0.000125\n",
      "Epoch: 13 \tTraining Loss: 0.000125\n",
      "Epoch: 14 \tTraining Loss: 0.000125\n",
      "Epoch: 15 \tTraining Loss: 0.000125\n",
      "Epoch: 16 \tTraining Loss: 0.000125\n",
      "Epoch: 17 \tTraining Loss: 0.000125\n",
      "Epoch: 18 \tTraining Loss: 0.000125\n",
      "Epoch: 1 \tTraining Loss: 0.000433\n",
      "Epoch: 2 \tTraining Loss: 0.000433\n",
      "Epoch: 3 \tTraining Loss: 0.000433\n",
      "Epoch: 4 \tTraining Loss: 0.000433\n",
      "Epoch: 5 \tTraining Loss: 0.000433\n",
      "Epoch: 6 \tTraining Loss: 0.000433\n",
      "Epoch: 7 \tTraining Loss: 0.000433\n",
      "Epoch: 8 \tTraining Loss: 0.000433\n",
      "Epoch: 9 \tTraining Loss: 0.000433\n",
      "Epoch: 10 \tTraining Loss: 0.000433\n",
      "Epoch: 11 \tTraining Loss: 0.000433\n",
      "Epoch: 12 \tTraining Loss: 0.000433\n",
      "Epoch: 13 \tTraining Loss: 0.000433\n",
      "Epoch: 14 \tTraining Loss: 0.000433\n",
      "Epoch: 15 \tTraining Loss: 0.000433\n",
      "Epoch: 16 \tTraining Loss: 0.000433\n",
      "Epoch: 17 \tTraining Loss: 0.000433\n",
      "Epoch: 18 \tTraining Loss: 0.000433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000141\n",
      "Epoch: 2 \tTraining Loss: 0.000141\n",
      "Epoch: 3 \tTraining Loss: 0.000141\n",
      "Epoch: 4 \tTraining Loss: 0.000141\n",
      "Epoch: 5 \tTraining Loss: 0.000141\n",
      "Epoch: 6 \tTraining Loss: 0.000141\n",
      "Epoch: 7 \tTraining Loss: 0.000141\n",
      "Epoch: 8 \tTraining Loss: 0.000141\n",
      "Epoch: 9 \tTraining Loss: 0.000141\n",
      "Epoch: 10 \tTraining Loss: 0.000141\n",
      "Epoch: 11 \tTraining Loss: 0.000141\n",
      "Epoch: 12 \tTraining Loss: 0.000141\n",
      "Epoch: 13 \tTraining Loss: 0.000141\n",
      "Epoch: 14 \tTraining Loss: 0.000141\n",
      "Epoch: 15 \tTraining Loss: 0.000141\n",
      "Epoch: 16 \tTraining Loss: 0.000141\n",
      "Epoch: 17 \tTraining Loss: 0.000141\n",
      "Epoch: 18 \tTraining Loss: 0.000141\n",
      "Epoch: 1 \tTraining Loss: 0.000225\n",
      "Epoch: 2 \tTraining Loss: 0.000225\n",
      "Epoch: 3 \tTraining Loss: 0.000225\n",
      "Epoch: 4 \tTraining Loss: 0.000225\n",
      "Epoch: 5 \tTraining Loss: 0.000225\n",
      "Epoch: 6 \tTraining Loss: 0.000225\n",
      "Epoch: 7 \tTraining Loss: 0.000225\n",
      "Epoch: 8 \tTraining Loss: 0.000225\n",
      "Epoch: 9 \tTraining Loss: 0.000225\n",
      "Epoch: 10 \tTraining Loss: 0.000225\n",
      "Epoch: 11 \tTraining Loss: 0.000225\n",
      "Epoch: 12 \tTraining Loss: 0.000225\n",
      "Epoch: 13 \tTraining Loss: 0.000225\n",
      "Epoch: 14 \tTraining Loss: 0.000225\n",
      "Epoch: 15 \tTraining Loss: 0.000225\n",
      "Epoch: 16 \tTraining Loss: 0.000225\n",
      "Epoch: 17 \tTraining Loss: 0.000225\n",
      "Epoch: 18 \tTraining Loss: 0.000225\n",
      "Epoch: 1 \tTraining Loss: 0.000195\n",
      "Epoch: 2 \tTraining Loss: 0.000195\n",
      "Epoch: 3 \tTraining Loss: 0.000195\n",
      "Epoch: 4 \tTraining Loss: 0.000195\n",
      "Epoch: 5 \tTraining Loss: 0.000195\n",
      "Epoch: 6 \tTraining Loss: 0.000195\n",
      "Epoch: 7 \tTraining Loss: 0.000195\n",
      "Epoch: 8 \tTraining Loss: 0.000195\n",
      "Epoch: 9 \tTraining Loss: 0.000195\n",
      "Epoch: 10 \tTraining Loss: 0.000195\n",
      "Epoch: 11 \tTraining Loss: 0.000195\n",
      "Epoch: 12 \tTraining Loss: 0.000195\n",
      "Epoch: 13 \tTraining Loss: 0.000195\n",
      "Epoch: 14 \tTraining Loss: 0.000195\n",
      "Epoch: 15 \tTraining Loss: 0.000195\n",
      "Epoch: 16 \tTraining Loss: 0.000195\n",
      "Epoch: 17 \tTraining Loss: 0.000195\n",
      "Epoch: 18 \tTraining Loss: 0.000195\n",
      "Epoch: 1 \tTraining Loss: 0.000207\n",
      "Epoch: 2 \tTraining Loss: 0.000207\n",
      "Epoch: 3 \tTraining Loss: 0.000207\n",
      "Epoch: 4 \tTraining Loss: 0.000207\n",
      "Epoch: 5 \tTraining Loss: 0.000207\n",
      "Epoch: 6 \tTraining Loss: 0.000207\n",
      "Epoch: 7 \tTraining Loss: 0.000207\n",
      "Epoch: 8 \tTraining Loss: 0.000207\n",
      "Epoch: 9 \tTraining Loss: 0.000207\n",
      "Epoch: 10 \tTraining Loss: 0.000207\n",
      "Epoch: 11 \tTraining Loss: 0.000207\n",
      "Epoch: 12 \tTraining Loss: 0.000207\n",
      "Epoch: 13 \tTraining Loss: 0.000207\n",
      "Epoch: 14 \tTraining Loss: 0.000207\n",
      "Epoch: 15 \tTraining Loss: 0.000207\n",
      "Epoch: 16 \tTraining Loss: 0.000207\n",
      "Epoch: 17 \tTraining Loss: 0.000207\n",
      "Epoch: 18 \tTraining Loss: 0.000207\n",
      "Epoch: 1 \tTraining Loss: 0.000397\n",
      "Epoch: 2 \tTraining Loss: 0.000397\n",
      "Epoch: 3 \tTraining Loss: 0.000397\n",
      "Epoch: 4 \tTraining Loss: 0.000397\n",
      "Epoch: 5 \tTraining Loss: 0.000397\n",
      "Epoch: 6 \tTraining Loss: 0.000397\n",
      "Epoch: 7 \tTraining Loss: 0.000397\n",
      "Epoch: 8 \tTraining Loss: 0.000397\n",
      "Epoch: 9 \tTraining Loss: 0.000397\n",
      "Epoch: 10 \tTraining Loss: 0.000397\n",
      "Epoch: 11 \tTraining Loss: 0.000397\n",
      "Epoch: 12 \tTraining Loss: 0.000397\n",
      "Epoch: 13 \tTraining Loss: 0.000397\n",
      "Epoch: 14 \tTraining Loss: 0.000397\n",
      "Epoch: 15 \tTraining Loss: 0.000397\n",
      "Epoch: 16 \tTraining Loss: 0.000397\n",
      "Epoch: 17 \tTraining Loss: 0.000397\n",
      "Epoch: 18 \tTraining Loss: 0.000397\n",
      "Epoch: 1 \tTraining Loss: 0.000091\n",
      "Epoch: 2 \tTraining Loss: 0.000091\n",
      "Epoch: 3 \tTraining Loss: 0.000091\n",
      "Epoch: 4 \tTraining Loss: 0.000091\n",
      "Epoch: 5 \tTraining Loss: 0.000091\n",
      "Epoch: 6 \tTraining Loss: 0.000091\n",
      "Epoch: 7 \tTraining Loss: 0.000091\n",
      "Epoch: 8 \tTraining Loss: 0.000091\n",
      "Epoch: 9 \tTraining Loss: 0.000091\n",
      "Epoch: 10 \tTraining Loss: 0.000091\n",
      "Epoch: 11 \tTraining Loss: 0.000091\n",
      "Epoch: 12 \tTraining Loss: 0.000091\n",
      "Epoch: 13 \tTraining Loss: 0.000091\n",
      "Epoch: 14 \tTraining Loss: 0.000091\n",
      "Epoch: 15 \tTraining Loss: 0.000091\n",
      "Epoch: 16 \tTraining Loss: 0.000091\n",
      "Epoch: 17 \tTraining Loss: 0.000091\n",
      "Epoch: 18 \tTraining Loss: 0.000091\n",
      "Epoch: 1 \tTraining Loss: 0.000149\n",
      "Epoch: 2 \tTraining Loss: 0.000149\n",
      "Epoch: 3 \tTraining Loss: 0.000149\n",
      "Epoch: 4 \tTraining Loss: 0.000149\n",
      "Epoch: 5 \tTraining Loss: 0.000149\n",
      "Epoch: 6 \tTraining Loss: 0.000149\n",
      "Epoch: 7 \tTraining Loss: 0.000149\n",
      "Epoch: 8 \tTraining Loss: 0.000149\n",
      "Epoch: 9 \tTraining Loss: 0.000149\n",
      "Epoch: 10 \tTraining Loss: 0.000149\n",
      "Epoch: 11 \tTraining Loss: 0.000149\n",
      "Epoch: 12 \tTraining Loss: 0.000149\n",
      "Epoch: 13 \tTraining Loss: 0.000149\n",
      "Epoch: 14 \tTraining Loss: 0.000149\n",
      "Epoch: 15 \tTraining Loss: 0.000149\n",
      "Epoch: 16 \tTraining Loss: 0.000149\n",
      "Epoch: 17 \tTraining Loss: 0.000149\n",
      "Epoch: 18 \tTraining Loss: 0.000149\n",
      "Epoch: 1 \tTraining Loss: 0.000227\n",
      "Epoch: 2 \tTraining Loss: 0.000227\n",
      "Epoch: 3 \tTraining Loss: 0.000227\n",
      "Epoch: 4 \tTraining Loss: 0.000227\n",
      "Epoch: 5 \tTraining Loss: 0.000227\n",
      "Epoch: 6 \tTraining Loss: 0.000227\n",
      "Epoch: 7 \tTraining Loss: 0.000227\n",
      "Epoch: 8 \tTraining Loss: 0.000227\n",
      "Epoch: 9 \tTraining Loss: 0.000227\n",
      "Epoch: 10 \tTraining Loss: 0.000227\n",
      "Epoch: 11 \tTraining Loss: 0.000227\n",
      "Epoch: 12 \tTraining Loss: 0.000227\n",
      "Epoch: 13 \tTraining Loss: 0.000227\n",
      "Epoch: 14 \tTraining Loss: 0.000227\n",
      "Epoch: 15 \tTraining Loss: 0.000227\n",
      "Epoch: 16 \tTraining Loss: 0.000227\n",
      "Epoch: 17 \tTraining Loss: 0.000227\n",
      "Epoch: 18 \tTraining Loss: 0.000227\n",
      "Epoch: 1 \tTraining Loss: 0.000319\n",
      "Epoch: 2 \tTraining Loss: 0.000319\n",
      "Epoch: 3 \tTraining Loss: 0.000319\n",
      "Epoch: 4 \tTraining Loss: 0.000319\n",
      "Epoch: 5 \tTraining Loss: 0.000319\n",
      "Epoch: 6 \tTraining Loss: 0.000319\n",
      "Epoch: 7 \tTraining Loss: 0.000319\n",
      "Epoch: 8 \tTraining Loss: 0.000319\n",
      "Epoch: 9 \tTraining Loss: 0.000319\n",
      "Epoch: 10 \tTraining Loss: 0.000319\n",
      "Epoch: 11 \tTraining Loss: 0.000319\n",
      "Epoch: 12 \tTraining Loss: 0.000319\n",
      "Epoch: 13 \tTraining Loss: 0.000319\n",
      "Epoch: 14 \tTraining Loss: 0.000319\n",
      "Epoch: 15 \tTraining Loss: 0.000319\n",
      "Epoch: 16 \tTraining Loss: 0.000319\n",
      "Epoch: 17 \tTraining Loss: 0.000319\n",
      "Epoch: 18 \tTraining Loss: 0.000319\n",
      "Epoch: 1 \tTraining Loss: 0.000278\n",
      "Epoch: 2 \tTraining Loss: 0.000278\n",
      "Epoch: 3 \tTraining Loss: 0.000278\n",
      "Epoch: 4 \tTraining Loss: 0.000278\n",
      "Epoch: 5 \tTraining Loss: 0.000278\n",
      "Epoch: 6 \tTraining Loss: 0.000278\n",
      "Epoch: 7 \tTraining Loss: 0.000278\n",
      "Epoch: 8 \tTraining Loss: 0.000278\n",
      "Epoch: 9 \tTraining Loss: 0.000278\n",
      "Epoch: 10 \tTraining Loss: 0.000278\n",
      "Epoch: 11 \tTraining Loss: 0.000278\n",
      "Epoch: 12 \tTraining Loss: 0.000278\n",
      "Epoch: 13 \tTraining Loss: 0.000278\n",
      "Epoch: 14 \tTraining Loss: 0.000278\n",
      "Epoch: 15 \tTraining Loss: 0.000278\n",
      "Epoch: 16 \tTraining Loss: 0.000278\n",
      "Epoch: 17 \tTraining Loss: 0.000278\n",
      "Epoch: 18 \tTraining Loss: 0.000278\n",
      "Epoch: 1 \tTraining Loss: 0.000114\n",
      "Epoch: 2 \tTraining Loss: 0.000114\n",
      "Epoch: 3 \tTraining Loss: 0.000114\n",
      "Epoch: 4 \tTraining Loss: 0.000114\n",
      "Epoch: 5 \tTraining Loss: 0.000114\n",
      "Epoch: 6 \tTraining Loss: 0.000114\n",
      "Epoch: 7 \tTraining Loss: 0.000114\n",
      "Epoch: 8 \tTraining Loss: 0.000114\n",
      "Epoch: 9 \tTraining Loss: 0.000114\n",
      "Epoch: 10 \tTraining Loss: 0.000114\n",
      "Epoch: 11 \tTraining Loss: 0.000114\n",
      "Epoch: 12 \tTraining Loss: 0.000114\n",
      "Epoch: 13 \tTraining Loss: 0.000114\n",
      "Epoch: 14 \tTraining Loss: 0.000114\n",
      "Epoch: 15 \tTraining Loss: 0.000114\n",
      "Epoch: 16 \tTraining Loss: 0.000114\n",
      "Epoch: 17 \tTraining Loss: 0.000114\n",
      "Epoch: 18 \tTraining Loss: 0.000114\n",
      "Epoch: 1 \tTraining Loss: 0.000230\n",
      "Epoch: 2 \tTraining Loss: 0.000230\n",
      "Epoch: 3 \tTraining Loss: 0.000230\n",
      "Epoch: 4 \tTraining Loss: 0.000230\n",
      "Epoch: 5 \tTraining Loss: 0.000230\n",
      "Epoch: 6 \tTraining Loss: 0.000230\n",
      "Epoch: 7 \tTraining Loss: 0.000230\n",
      "Epoch: 8 \tTraining Loss: 0.000230\n",
      "Epoch: 9 \tTraining Loss: 0.000230\n",
      "Epoch: 10 \tTraining Loss: 0.000230\n",
      "Epoch: 11 \tTraining Loss: 0.000230\n",
      "Epoch: 12 \tTraining Loss: 0.000230\n",
      "Epoch: 13 \tTraining Loss: 0.000230\n",
      "Epoch: 14 \tTraining Loss: 0.000230\n",
      "Epoch: 15 \tTraining Loss: 0.000230\n",
      "Epoch: 16 \tTraining Loss: 0.000230\n",
      "Epoch: 17 \tTraining Loss: 0.000230\n",
      "Epoch: 18 \tTraining Loss: 0.000230\n",
      "Epoch: 1 \tTraining Loss: 0.000149\n",
      "Epoch: 2 \tTraining Loss: 0.000149\n",
      "Epoch: 3 \tTraining Loss: 0.000149\n",
      "Epoch: 4 \tTraining Loss: 0.000149\n",
      "Epoch: 5 \tTraining Loss: 0.000149\n",
      "Epoch: 6 \tTraining Loss: 0.000149\n",
      "Epoch: 7 \tTraining Loss: 0.000149\n",
      "Epoch: 8 \tTraining Loss: 0.000149\n",
      "Epoch: 9 \tTraining Loss: 0.000149\n",
      "Epoch: 10 \tTraining Loss: 0.000149\n",
      "Epoch: 11 \tTraining Loss: 0.000149\n",
      "Epoch: 12 \tTraining Loss: 0.000149\n",
      "Epoch: 13 \tTraining Loss: 0.000149\n",
      "Epoch: 14 \tTraining Loss: 0.000149\n",
      "Epoch: 15 \tTraining Loss: 0.000149\n",
      "Epoch: 16 \tTraining Loss: 0.000149\n",
      "Epoch: 17 \tTraining Loss: 0.000149\n",
      "Epoch: 18 \tTraining Loss: 0.000149\n",
      "Epoch: 1 \tTraining Loss: 0.000245\n",
      "Epoch: 2 \tTraining Loss: 0.000245\n",
      "Epoch: 3 \tTraining Loss: 0.000245\n",
      "Epoch: 4 \tTraining Loss: 0.000245\n",
      "Epoch: 5 \tTraining Loss: 0.000245\n",
      "Epoch: 6 \tTraining Loss: 0.000245\n",
      "Epoch: 7 \tTraining Loss: 0.000245\n",
      "Epoch: 8 \tTraining Loss: 0.000245\n",
      "Epoch: 9 \tTraining Loss: 0.000245\n",
      "Epoch: 10 \tTraining Loss: 0.000245\n",
      "Epoch: 11 \tTraining Loss: 0.000245\n",
      "Epoch: 12 \tTraining Loss: 0.000245\n",
      "Epoch: 13 \tTraining Loss: 0.000245\n",
      "Epoch: 14 \tTraining Loss: 0.000245\n",
      "Epoch: 15 \tTraining Loss: 0.000245\n",
      "Epoch: 16 \tTraining Loss: 0.000245\n",
      "Epoch: 17 \tTraining Loss: 0.000245\n",
      "Epoch: 18 \tTraining Loss: 0.000245\n",
      "Epoch: 1 \tTraining Loss: 0.000155\n",
      "Epoch: 2 \tTraining Loss: 0.000155\n",
      "Epoch: 3 \tTraining Loss: 0.000155\n",
      "Epoch: 4 \tTraining Loss: 0.000155\n",
      "Epoch: 5 \tTraining Loss: 0.000155\n",
      "Epoch: 6 \tTraining Loss: 0.000155\n",
      "Epoch: 7 \tTraining Loss: 0.000155\n",
      "Epoch: 8 \tTraining Loss: 0.000155\n",
      "Epoch: 9 \tTraining Loss: 0.000155\n",
      "Epoch: 10 \tTraining Loss: 0.000155\n",
      "Epoch: 11 \tTraining Loss: 0.000155\n",
      "Epoch: 12 \tTraining Loss: 0.000155\n",
      "Epoch: 13 \tTraining Loss: 0.000155\n",
      "Epoch: 14 \tTraining Loss: 0.000155\n",
      "Epoch: 15 \tTraining Loss: 0.000155\n",
      "Epoch: 16 \tTraining Loss: 0.000155\n",
      "Epoch: 17 \tTraining Loss: 0.000155\n",
      "Epoch: 18 \tTraining Loss: 0.000155\n",
      "Epoch: 1 \tTraining Loss: 0.000173\n",
      "Epoch: 2 \tTraining Loss: 0.000173\n",
      "Epoch: 3 \tTraining Loss: 0.000173\n",
      "Epoch: 4 \tTraining Loss: 0.000173\n",
      "Epoch: 5 \tTraining Loss: 0.000173\n",
      "Epoch: 6 \tTraining Loss: 0.000173\n",
      "Epoch: 7 \tTraining Loss: 0.000173\n",
      "Epoch: 8 \tTraining Loss: 0.000173\n",
      "Epoch: 9 \tTraining Loss: 0.000173\n",
      "Epoch: 10 \tTraining Loss: 0.000173\n",
      "Epoch: 11 \tTraining Loss: 0.000173\n",
      "Epoch: 12 \tTraining Loss: 0.000173\n",
      "Epoch: 13 \tTraining Loss: 0.000173\n",
      "Epoch: 14 \tTraining Loss: 0.000173\n",
      "Epoch: 15 \tTraining Loss: 0.000173\n",
      "Epoch: 16 \tTraining Loss: 0.000173\n",
      "Epoch: 17 \tTraining Loss: 0.000173\n",
      "Epoch: 18 \tTraining Loss: 0.000173\n",
      "Epoch: 1 \tTraining Loss: 0.000345\n",
      "Epoch: 2 \tTraining Loss: 0.000345\n",
      "Epoch: 3 \tTraining Loss: 0.000345\n",
      "Epoch: 4 \tTraining Loss: 0.000345\n",
      "Epoch: 5 \tTraining Loss: 0.000345\n",
      "Epoch: 6 \tTraining Loss: 0.000345\n",
      "Epoch: 7 \tTraining Loss: 0.000345\n",
      "Epoch: 8 \tTraining Loss: 0.000345\n",
      "Epoch: 9 \tTraining Loss: 0.000345\n",
      "Epoch: 10 \tTraining Loss: 0.000345\n",
      "Epoch: 11 \tTraining Loss: 0.000345\n",
      "Epoch: 12 \tTraining Loss: 0.000345\n",
      "Epoch: 13 \tTraining Loss: 0.000345\n",
      "Epoch: 14 \tTraining Loss: 0.000345\n",
      "Epoch: 15 \tTraining Loss: 0.000345\n",
      "Epoch: 16 \tTraining Loss: 0.000345\n",
      "Epoch: 17 \tTraining Loss: 0.000345\n",
      "Epoch: 18 \tTraining Loss: 0.000345\n",
      "Epoch: 1 \tTraining Loss: 0.000389\n",
      "Epoch: 2 \tTraining Loss: 0.000389\n",
      "Epoch: 3 \tTraining Loss: 0.000389\n",
      "Epoch: 4 \tTraining Loss: 0.000389\n",
      "Epoch: 5 \tTraining Loss: 0.000389\n",
      "Epoch: 6 \tTraining Loss: 0.000389\n",
      "Epoch: 7 \tTraining Loss: 0.000389\n",
      "Epoch: 8 \tTraining Loss: 0.000389\n",
      "Epoch: 9 \tTraining Loss: 0.000389\n",
      "Epoch: 10 \tTraining Loss: 0.000389\n",
      "Epoch: 11 \tTraining Loss: 0.000389\n",
      "Epoch: 12 \tTraining Loss: 0.000389\n",
      "Epoch: 13 \tTraining Loss: 0.000389\n",
      "Epoch: 14 \tTraining Loss: 0.000389\n",
      "Epoch: 15 \tTraining Loss: 0.000389\n",
      "Epoch: 16 \tTraining Loss: 0.000389\n",
      "Epoch: 17 \tTraining Loss: 0.000389\n",
      "Epoch: 18 \tTraining Loss: 0.000389\n",
      "Epoch: 1 \tTraining Loss: 0.000117\n",
      "Epoch: 2 \tTraining Loss: 0.000117\n",
      "Epoch: 3 \tTraining Loss: 0.000117\n",
      "Epoch: 4 \tTraining Loss: 0.000117\n",
      "Epoch: 5 \tTraining Loss: 0.000117\n",
      "Epoch: 6 \tTraining Loss: 0.000117\n",
      "Epoch: 7 \tTraining Loss: 0.000117\n",
      "Epoch: 8 \tTraining Loss: 0.000117\n",
      "Epoch: 9 \tTraining Loss: 0.000117\n",
      "Epoch: 10 \tTraining Loss: 0.000117\n",
      "Epoch: 11 \tTraining Loss: 0.000117\n",
      "Epoch: 12 \tTraining Loss: 0.000117\n",
      "Epoch: 13 \tTraining Loss: 0.000117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 \tTraining Loss: 0.000117\n",
      "Epoch: 15 \tTraining Loss: 0.000117\n",
      "Epoch: 16 \tTraining Loss: 0.000117\n",
      "Epoch: 17 \tTraining Loss: 0.000117\n",
      "Epoch: 18 \tTraining Loss: 0.000117\n",
      "Epoch: 1 \tTraining Loss: 0.000218\n",
      "Epoch: 2 \tTraining Loss: 0.000218\n",
      "Epoch: 3 \tTraining Loss: 0.000218\n",
      "Epoch: 4 \tTraining Loss: 0.000218\n",
      "Epoch: 5 \tTraining Loss: 0.000218\n",
      "Epoch: 6 \tTraining Loss: 0.000218\n",
      "Epoch: 7 \tTraining Loss: 0.000218\n",
      "Epoch: 8 \tTraining Loss: 0.000218\n",
      "Epoch: 9 \tTraining Loss: 0.000218\n",
      "Epoch: 10 \tTraining Loss: 0.000218\n",
      "Epoch: 11 \tTraining Loss: 0.000218\n",
      "Epoch: 12 \tTraining Loss: 0.000218\n",
      "Epoch: 13 \tTraining Loss: 0.000218\n",
      "Epoch: 14 \tTraining Loss: 0.000218\n",
      "Epoch: 15 \tTraining Loss: 0.000218\n",
      "Epoch: 16 \tTraining Loss: 0.000218\n",
      "Epoch: 17 \tTraining Loss: 0.000218\n",
      "Epoch: 18 \tTraining Loss: 0.000218\n",
      "Epoch: 1 \tTraining Loss: 0.000236\n",
      "Epoch: 2 \tTraining Loss: 0.000236\n",
      "Epoch: 3 \tTraining Loss: 0.000236\n",
      "Epoch: 4 \tTraining Loss: 0.000236\n",
      "Epoch: 5 \tTraining Loss: 0.000236\n",
      "Epoch: 6 \tTraining Loss: 0.000236\n",
      "Epoch: 7 \tTraining Loss: 0.000236\n",
      "Epoch: 8 \tTraining Loss: 0.000236\n",
      "Epoch: 9 \tTraining Loss: 0.000236\n",
      "Epoch: 10 \tTraining Loss: 0.000236\n",
      "Epoch: 11 \tTraining Loss: 0.000236\n",
      "Epoch: 12 \tTraining Loss: 0.000236\n",
      "Epoch: 13 \tTraining Loss: 0.000236\n",
      "Epoch: 14 \tTraining Loss: 0.000236\n",
      "Epoch: 15 \tTraining Loss: 0.000236\n",
      "Epoch: 16 \tTraining Loss: 0.000236\n",
      "Epoch: 17 \tTraining Loss: 0.000236\n",
      "Epoch: 18 \tTraining Loss: 0.000236\n",
      "Epoch: 1 \tTraining Loss: 0.000272\n",
      "Epoch: 2 \tTraining Loss: 0.000272\n",
      "Epoch: 3 \tTraining Loss: 0.000272\n",
      "Epoch: 4 \tTraining Loss: 0.000272\n",
      "Epoch: 5 \tTraining Loss: 0.000272\n",
      "Epoch: 6 \tTraining Loss: 0.000272\n",
      "Epoch: 7 \tTraining Loss: 0.000272\n",
      "Epoch: 8 \tTraining Loss: 0.000272\n",
      "Epoch: 9 \tTraining Loss: 0.000272\n",
      "Epoch: 10 \tTraining Loss: 0.000272\n",
      "Epoch: 11 \tTraining Loss: 0.000272\n",
      "Epoch: 12 \tTraining Loss: 0.000272\n",
      "Epoch: 13 \tTraining Loss: 0.000272\n",
      "Epoch: 14 \tTraining Loss: 0.000272\n",
      "Epoch: 15 \tTraining Loss: 0.000272\n",
      "Epoch: 16 \tTraining Loss: 0.000272\n",
      "Epoch: 17 \tTraining Loss: 0.000272\n",
      "Epoch: 18 \tTraining Loss: 0.000272\n",
      "Epoch: 1 \tTraining Loss: 0.000072\n",
      "Epoch: 2 \tTraining Loss: 0.000072\n",
      "Epoch: 3 \tTraining Loss: 0.000072\n",
      "Epoch: 4 \tTraining Loss: 0.000072\n",
      "Epoch: 5 \tTraining Loss: 0.000072\n",
      "Epoch: 6 \tTraining Loss: 0.000072\n",
      "Epoch: 7 \tTraining Loss: 0.000072\n",
      "Epoch: 8 \tTraining Loss: 0.000072\n",
      "Epoch: 9 \tTraining Loss: 0.000072\n",
      "Epoch: 10 \tTraining Loss: 0.000072\n",
      "Epoch: 11 \tTraining Loss: 0.000072\n",
      "Epoch: 12 \tTraining Loss: 0.000072\n",
      "Epoch: 13 \tTraining Loss: 0.000072\n",
      "Epoch: 14 \tTraining Loss: 0.000072\n",
      "Epoch: 15 \tTraining Loss: 0.000072\n",
      "Epoch: 16 \tTraining Loss: 0.000072\n",
      "Epoch: 17 \tTraining Loss: 0.000072\n",
      "Epoch: 18 \tTraining Loss: 0.000072\n",
      "Epoch: 1 \tTraining Loss: 0.000184\n",
      "Epoch: 2 \tTraining Loss: 0.000184\n",
      "Epoch: 3 \tTraining Loss: 0.000184\n",
      "Epoch: 4 \tTraining Loss: 0.000184\n",
      "Epoch: 5 \tTraining Loss: 0.000184\n",
      "Epoch: 6 \tTraining Loss: 0.000184\n",
      "Epoch: 7 \tTraining Loss: 0.000184\n",
      "Epoch: 8 \tTraining Loss: 0.000184\n",
      "Epoch: 9 \tTraining Loss: 0.000184\n",
      "Epoch: 10 \tTraining Loss: 0.000184\n",
      "Epoch: 11 \tTraining Loss: 0.000184\n",
      "Epoch: 12 \tTraining Loss: 0.000184\n",
      "Epoch: 13 \tTraining Loss: 0.000184\n",
      "Epoch: 14 \tTraining Loss: 0.000184\n",
      "Epoch: 15 \tTraining Loss: 0.000184\n",
      "Epoch: 16 \tTraining Loss: 0.000184\n",
      "Epoch: 17 \tTraining Loss: 0.000184\n",
      "Epoch: 18 \tTraining Loss: 0.000184\n",
      "Epoch: 1 \tTraining Loss: 0.000207\n",
      "Epoch: 2 \tTraining Loss: 0.000207\n",
      "Epoch: 3 \tTraining Loss: 0.000207\n",
      "Epoch: 4 \tTraining Loss: 0.000207\n",
      "Epoch: 5 \tTraining Loss: 0.000207\n",
      "Epoch: 6 \tTraining Loss: 0.000207\n",
      "Epoch: 7 \tTraining Loss: 0.000207\n",
      "Epoch: 8 \tTraining Loss: 0.000207\n",
      "Epoch: 9 \tTraining Loss: 0.000207\n",
      "Epoch: 10 \tTraining Loss: 0.000207\n",
      "Epoch: 11 \tTraining Loss: 0.000207\n",
      "Epoch: 12 \tTraining Loss: 0.000207\n",
      "Epoch: 13 \tTraining Loss: 0.000207\n",
      "Epoch: 14 \tTraining Loss: 0.000207\n",
      "Epoch: 15 \tTraining Loss: 0.000207\n",
      "Epoch: 16 \tTraining Loss: 0.000207\n",
      "Epoch: 17 \tTraining Loss: 0.000207\n",
      "Epoch: 18 \tTraining Loss: 0.000207\n",
      "Epoch: 1 \tTraining Loss: 0.000460\n",
      "Epoch: 2 \tTraining Loss: 0.000460\n",
      "Epoch: 3 \tTraining Loss: 0.000460\n",
      "Epoch: 4 \tTraining Loss: 0.000460\n",
      "Epoch: 5 \tTraining Loss: 0.000460\n",
      "Epoch: 6 \tTraining Loss: 0.000460\n",
      "Epoch: 7 \tTraining Loss: 0.000460\n",
      "Epoch: 8 \tTraining Loss: 0.000460\n",
      "Epoch: 9 \tTraining Loss: 0.000460\n",
      "Epoch: 10 \tTraining Loss: 0.000460\n",
      "Epoch: 11 \tTraining Loss: 0.000460\n",
      "Epoch: 12 \tTraining Loss: 0.000460\n",
      "Epoch: 13 \tTraining Loss: 0.000460\n",
      "Epoch: 14 \tTraining Loss: 0.000460\n",
      "Epoch: 15 \tTraining Loss: 0.000460\n",
      "Epoch: 16 \tTraining Loss: 0.000460\n",
      "Epoch: 17 \tTraining Loss: 0.000460\n",
      "Epoch: 18 \tTraining Loss: 0.000460\n",
      "Epoch: 1 \tTraining Loss: 0.000226\n",
      "Epoch: 2 \tTraining Loss: 0.000226\n",
      "Epoch: 3 \tTraining Loss: 0.000226\n",
      "Epoch: 4 \tTraining Loss: 0.000226\n",
      "Epoch: 5 \tTraining Loss: 0.000226\n",
      "Epoch: 6 \tTraining Loss: 0.000226\n",
      "Epoch: 7 \tTraining Loss: 0.000226\n",
      "Epoch: 8 \tTraining Loss: 0.000226\n",
      "Epoch: 9 \tTraining Loss: 0.000226\n",
      "Epoch: 10 \tTraining Loss: 0.000226\n",
      "Epoch: 11 \tTraining Loss: 0.000226\n",
      "Epoch: 12 \tTraining Loss: 0.000226\n",
      "Epoch: 13 \tTraining Loss: 0.000226\n",
      "Epoch: 14 \tTraining Loss: 0.000226\n",
      "Epoch: 15 \tTraining Loss: 0.000226\n",
      "Epoch: 16 \tTraining Loss: 0.000226\n",
      "Epoch: 17 \tTraining Loss: 0.000226\n",
      "Epoch: 18 \tTraining Loss: 0.000226\n",
      "Epoch: 1 \tTraining Loss: 0.000133\n",
      "Epoch: 2 \tTraining Loss: 0.000133\n",
      "Epoch: 3 \tTraining Loss: 0.000133\n",
      "Epoch: 4 \tTraining Loss: 0.000133\n",
      "Epoch: 5 \tTraining Loss: 0.000133\n",
      "Epoch: 6 \tTraining Loss: 0.000133\n",
      "Epoch: 7 \tTraining Loss: 0.000133\n",
      "Epoch: 8 \tTraining Loss: 0.000133\n",
      "Epoch: 9 \tTraining Loss: 0.000133\n",
      "Epoch: 10 \tTraining Loss: 0.000133\n",
      "Epoch: 11 \tTraining Loss: 0.000133\n",
      "Epoch: 12 \tTraining Loss: 0.000133\n",
      "Epoch: 13 \tTraining Loss: 0.000133\n",
      "Epoch: 14 \tTraining Loss: 0.000133\n",
      "Epoch: 15 \tTraining Loss: 0.000133\n",
      "Epoch: 16 \tTraining Loss: 0.000133\n",
      "Epoch: 17 \tTraining Loss: 0.000133\n",
      "Epoch: 18 \tTraining Loss: 0.000133\n",
      "Epoch: 1 \tTraining Loss: 0.000256\n",
      "Epoch: 2 \tTraining Loss: 0.000256\n",
      "Epoch: 3 \tTraining Loss: 0.000256\n",
      "Epoch: 4 \tTraining Loss: 0.000256\n",
      "Epoch: 5 \tTraining Loss: 0.000256\n",
      "Epoch: 6 \tTraining Loss: 0.000256\n",
      "Epoch: 7 \tTraining Loss: 0.000256\n",
      "Epoch: 8 \tTraining Loss: 0.000256\n",
      "Epoch: 9 \tTraining Loss: 0.000256\n",
      "Epoch: 10 \tTraining Loss: 0.000256\n",
      "Epoch: 11 \tTraining Loss: 0.000256\n",
      "Epoch: 12 \tTraining Loss: 0.000256\n",
      "Epoch: 13 \tTraining Loss: 0.000256\n",
      "Epoch: 14 \tTraining Loss: 0.000256\n",
      "Epoch: 15 \tTraining Loss: 0.000256\n",
      "Epoch: 16 \tTraining Loss: 0.000256\n",
      "Epoch: 17 \tTraining Loss: 0.000256\n",
      "Epoch: 18 \tTraining Loss: 0.000256\n",
      "Epoch: 1 \tTraining Loss: 0.000075\n",
      "Epoch: 2 \tTraining Loss: 0.000075\n",
      "Epoch: 3 \tTraining Loss: 0.000075\n",
      "Epoch: 4 \tTraining Loss: 0.000075\n",
      "Epoch: 5 \tTraining Loss: 0.000075\n",
      "Epoch: 6 \tTraining Loss: 0.000075\n",
      "Epoch: 7 \tTraining Loss: 0.000075\n",
      "Epoch: 8 \tTraining Loss: 0.000075\n",
      "Epoch: 9 \tTraining Loss: 0.000075\n",
      "Epoch: 10 \tTraining Loss: 0.000075\n",
      "Epoch: 11 \tTraining Loss: 0.000075\n",
      "Epoch: 12 \tTraining Loss: 0.000075\n",
      "Epoch: 13 \tTraining Loss: 0.000075\n",
      "Epoch: 14 \tTraining Loss: 0.000075\n",
      "Epoch: 15 \tTraining Loss: 0.000075\n",
      "Epoch: 16 \tTraining Loss: 0.000075\n",
      "Epoch: 17 \tTraining Loss: 0.000075\n",
      "Epoch: 18 \tTraining Loss: 0.000075\n",
      "Epoch: 1 \tTraining Loss: 0.000086\n",
      "Epoch: 2 \tTraining Loss: 0.000086\n",
      "Epoch: 3 \tTraining Loss: 0.000086\n",
      "Epoch: 4 \tTraining Loss: 0.000086\n",
      "Epoch: 5 \tTraining Loss: 0.000086\n",
      "Epoch: 6 \tTraining Loss: 0.000086\n",
      "Epoch: 7 \tTraining Loss: 0.000086\n",
      "Epoch: 8 \tTraining Loss: 0.000086\n",
      "Epoch: 9 \tTraining Loss: 0.000086\n",
      "Epoch: 10 \tTraining Loss: 0.000086\n",
      "Epoch: 11 \tTraining Loss: 0.000086\n",
      "Epoch: 12 \tTraining Loss: 0.000086\n",
      "Epoch: 13 \tTraining Loss: 0.000086\n",
      "Epoch: 14 \tTraining Loss: 0.000086\n",
      "Epoch: 15 \tTraining Loss: 0.000086\n",
      "Epoch: 16 \tTraining Loss: 0.000086\n",
      "Epoch: 17 \tTraining Loss: 0.000086\n",
      "Epoch: 18 \tTraining Loss: 0.000086\n",
      "Epoch: 1 \tTraining Loss: 0.000335\n",
      "Epoch: 2 \tTraining Loss: 0.000335\n",
      "Epoch: 3 \tTraining Loss: 0.000335\n",
      "Epoch: 4 \tTraining Loss: 0.000335\n",
      "Epoch: 5 \tTraining Loss: 0.000335\n",
      "Epoch: 6 \tTraining Loss: 0.000335\n",
      "Epoch: 7 \tTraining Loss: 0.000335\n",
      "Epoch: 8 \tTraining Loss: 0.000335\n",
      "Epoch: 9 \tTraining Loss: 0.000335\n",
      "Epoch: 10 \tTraining Loss: 0.000335\n",
      "Epoch: 11 \tTraining Loss: 0.000335\n",
      "Epoch: 12 \tTraining Loss: 0.000335\n",
      "Epoch: 13 \tTraining Loss: 0.000335\n",
      "Epoch: 14 \tTraining Loss: 0.000335\n",
      "Epoch: 15 \tTraining Loss: 0.000335\n",
      "Epoch: 16 \tTraining Loss: 0.000335\n",
      "Epoch: 17 \tTraining Loss: 0.000335\n",
      "Epoch: 18 \tTraining Loss: 0.000335\n",
      "Epoch: 1 \tTraining Loss: 0.000200\n",
      "Epoch: 2 \tTraining Loss: 0.000200\n",
      "Epoch: 3 \tTraining Loss: 0.000200\n",
      "Epoch: 4 \tTraining Loss: 0.000200\n",
      "Epoch: 5 \tTraining Loss: 0.000200\n",
      "Epoch: 6 \tTraining Loss: 0.000200\n",
      "Epoch: 7 \tTraining Loss: 0.000200\n",
      "Epoch: 8 \tTraining Loss: 0.000200\n",
      "Epoch: 9 \tTraining Loss: 0.000200\n",
      "Epoch: 10 \tTraining Loss: 0.000200\n",
      "Epoch: 11 \tTraining Loss: 0.000200\n",
      "Epoch: 12 \tTraining Loss: 0.000200\n",
      "Epoch: 13 \tTraining Loss: 0.000200\n",
      "Epoch: 14 \tTraining Loss: 0.000200\n",
      "Epoch: 15 \tTraining Loss: 0.000200\n",
      "Epoch: 16 \tTraining Loss: 0.000200\n",
      "Epoch: 17 \tTraining Loss: 0.000200\n",
      "Epoch: 18 \tTraining Loss: 0.000200\n",
      "Epoch: 1 \tTraining Loss: 0.000214\n",
      "Epoch: 2 \tTraining Loss: 0.000214\n",
      "Epoch: 3 \tTraining Loss: 0.000214\n",
      "Epoch: 4 \tTraining Loss: 0.000214\n",
      "Epoch: 5 \tTraining Loss: 0.000214\n",
      "Epoch: 6 \tTraining Loss: 0.000214\n",
      "Epoch: 7 \tTraining Loss: 0.000214\n",
      "Epoch: 8 \tTraining Loss: 0.000214\n",
      "Epoch: 9 \tTraining Loss: 0.000214\n",
      "Epoch: 10 \tTraining Loss: 0.000214\n",
      "Epoch: 11 \tTraining Loss: 0.000214\n",
      "Epoch: 12 \tTraining Loss: 0.000214\n",
      "Epoch: 13 \tTraining Loss: 0.000214\n",
      "Epoch: 14 \tTraining Loss: 0.000214\n",
      "Epoch: 15 \tTraining Loss: 0.000214\n",
      "Epoch: 16 \tTraining Loss: 0.000214\n",
      "Epoch: 17 \tTraining Loss: 0.000214\n",
      "Epoch: 18 \tTraining Loss: 0.000214\n",
      "Epoch: 1 \tTraining Loss: 0.000153\n",
      "Epoch: 2 \tTraining Loss: 0.000153\n",
      "Epoch: 3 \tTraining Loss: 0.000153\n",
      "Epoch: 4 \tTraining Loss: 0.000153\n",
      "Epoch: 5 \tTraining Loss: 0.000153\n",
      "Epoch: 6 \tTraining Loss: 0.000153\n",
      "Epoch: 7 \tTraining Loss: 0.000153\n",
      "Epoch: 8 \tTraining Loss: 0.000153\n",
      "Epoch: 9 \tTraining Loss: 0.000153\n",
      "Epoch: 10 \tTraining Loss: 0.000153\n",
      "Epoch: 11 \tTraining Loss: 0.000153\n",
      "Epoch: 12 \tTraining Loss: 0.000153\n",
      "Epoch: 13 \tTraining Loss: 0.000153\n",
      "Epoch: 14 \tTraining Loss: 0.000153\n",
      "Epoch: 15 \tTraining Loss: 0.000153\n",
      "Epoch: 16 \tTraining Loss: 0.000153\n",
      "Epoch: 17 \tTraining Loss: 0.000153\n",
      "Epoch: 18 \tTraining Loss: 0.000153\n",
      "Epoch: 1 \tTraining Loss: 0.000334\n",
      "Epoch: 2 \tTraining Loss: 0.000334\n",
      "Epoch: 3 \tTraining Loss: 0.000334\n",
      "Epoch: 4 \tTraining Loss: 0.000334\n",
      "Epoch: 5 \tTraining Loss: 0.000334\n",
      "Epoch: 6 \tTraining Loss: 0.000334\n",
      "Epoch: 7 \tTraining Loss: 0.000334\n",
      "Epoch: 8 \tTraining Loss: 0.000334\n",
      "Epoch: 9 \tTraining Loss: 0.000334\n",
      "Epoch: 10 \tTraining Loss: 0.000334\n",
      "Epoch: 11 \tTraining Loss: 0.000334\n",
      "Epoch: 12 \tTraining Loss: 0.000334\n",
      "Epoch: 13 \tTraining Loss: 0.000334\n",
      "Epoch: 14 \tTraining Loss: 0.000334\n",
      "Epoch: 15 \tTraining Loss: 0.000334\n",
      "Epoch: 16 \tTraining Loss: 0.000334\n",
      "Epoch: 17 \tTraining Loss: 0.000334\n",
      "Epoch: 18 \tTraining Loss: 0.000334\n",
      "Epoch: 1 \tTraining Loss: 0.000139\n",
      "Epoch: 2 \tTraining Loss: 0.000139\n",
      "Epoch: 3 \tTraining Loss: 0.000139\n",
      "Epoch: 4 \tTraining Loss: 0.000139\n",
      "Epoch: 5 \tTraining Loss: 0.000139\n",
      "Epoch: 6 \tTraining Loss: 0.000139\n",
      "Epoch: 7 \tTraining Loss: 0.000139\n",
      "Epoch: 8 \tTraining Loss: 0.000139\n",
      "Epoch: 9 \tTraining Loss: 0.000139\n",
      "Epoch: 10 \tTraining Loss: 0.000139\n",
      "Epoch: 11 \tTraining Loss: 0.000139\n",
      "Epoch: 12 \tTraining Loss: 0.000139\n",
      "Epoch: 13 \tTraining Loss: 0.000139\n",
      "Epoch: 14 \tTraining Loss: 0.000139\n",
      "Epoch: 15 \tTraining Loss: 0.000139\n",
      "Epoch: 16 \tTraining Loss: 0.000139\n",
      "Epoch: 17 \tTraining Loss: 0.000139\n",
      "Epoch: 18 \tTraining Loss: 0.000139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000347\n",
      "Epoch: 2 \tTraining Loss: 0.000347\n",
      "Epoch: 3 \tTraining Loss: 0.000347\n",
      "Epoch: 4 \tTraining Loss: 0.000347\n",
      "Epoch: 5 \tTraining Loss: 0.000347\n",
      "Epoch: 6 \tTraining Loss: 0.000347\n",
      "Epoch: 7 \tTraining Loss: 0.000347\n",
      "Epoch: 8 \tTraining Loss: 0.000347\n",
      "Epoch: 9 \tTraining Loss: 0.000347\n",
      "Epoch: 10 \tTraining Loss: 0.000347\n",
      "Epoch: 11 \tTraining Loss: 0.000347\n",
      "Epoch: 12 \tTraining Loss: 0.000347\n",
      "Epoch: 13 \tTraining Loss: 0.000347\n",
      "Epoch: 14 \tTraining Loss: 0.000347\n",
      "Epoch: 15 \tTraining Loss: 0.000347\n",
      "Epoch: 16 \tTraining Loss: 0.000347\n",
      "Epoch: 17 \tTraining Loss: 0.000347\n",
      "Epoch: 18 \tTraining Loss: 0.000347\n",
      "Epoch: 1 \tTraining Loss: 0.000316\n",
      "Epoch: 2 \tTraining Loss: 0.000316\n",
      "Epoch: 3 \tTraining Loss: 0.000316\n",
      "Epoch: 4 \tTraining Loss: 0.000316\n",
      "Epoch: 5 \tTraining Loss: 0.000316\n",
      "Epoch: 6 \tTraining Loss: 0.000316\n",
      "Epoch: 7 \tTraining Loss: 0.000316\n",
      "Epoch: 8 \tTraining Loss: 0.000316\n",
      "Epoch: 9 \tTraining Loss: 0.000316\n",
      "Epoch: 10 \tTraining Loss: 0.000316\n",
      "Epoch: 11 \tTraining Loss: 0.000316\n",
      "Epoch: 12 \tTraining Loss: 0.000316\n",
      "Epoch: 13 \tTraining Loss: 0.000316\n",
      "Epoch: 14 \tTraining Loss: 0.000316\n",
      "Epoch: 15 \tTraining Loss: 0.000316\n",
      "Epoch: 16 \tTraining Loss: 0.000316\n",
      "Epoch: 17 \tTraining Loss: 0.000316\n",
      "Epoch: 18 \tTraining Loss: 0.000316\n",
      "Epoch: 1 \tTraining Loss: 0.000187\n",
      "Epoch: 2 \tTraining Loss: 0.000187\n",
      "Epoch: 3 \tTraining Loss: 0.000187\n",
      "Epoch: 4 \tTraining Loss: 0.000187\n",
      "Epoch: 5 \tTraining Loss: 0.000187\n",
      "Epoch: 6 \tTraining Loss: 0.000187\n",
      "Epoch: 7 \tTraining Loss: 0.000187\n",
      "Epoch: 8 \tTraining Loss: 0.000187\n",
      "Epoch: 9 \tTraining Loss: 0.000187\n",
      "Epoch: 10 \tTraining Loss: 0.000187\n",
      "Epoch: 11 \tTraining Loss: 0.000187\n",
      "Epoch: 12 \tTraining Loss: 0.000187\n",
      "Epoch: 13 \tTraining Loss: 0.000187\n",
      "Epoch: 14 \tTraining Loss: 0.000187\n",
      "Epoch: 15 \tTraining Loss: 0.000187\n",
      "Epoch: 16 \tTraining Loss: 0.000187\n",
      "Epoch: 17 \tTraining Loss: 0.000187\n",
      "Epoch: 18 \tTraining Loss: 0.000187\n",
      "Epoch: 1 \tTraining Loss: 0.000246\n",
      "Epoch: 2 \tTraining Loss: 0.000246\n",
      "Epoch: 3 \tTraining Loss: 0.000246\n",
      "Epoch: 4 \tTraining Loss: 0.000246\n",
      "Epoch: 5 \tTraining Loss: 0.000246\n",
      "Epoch: 6 \tTraining Loss: 0.000246\n",
      "Epoch: 7 \tTraining Loss: 0.000246\n",
      "Epoch: 8 \tTraining Loss: 0.000246\n",
      "Epoch: 9 \tTraining Loss: 0.000246\n",
      "Epoch: 10 \tTraining Loss: 0.000246\n",
      "Epoch: 11 \tTraining Loss: 0.000246\n",
      "Epoch: 12 \tTraining Loss: 0.000246\n",
      "Epoch: 13 \tTraining Loss: 0.000246\n",
      "Epoch: 14 \tTraining Loss: 0.000246\n",
      "Epoch: 15 \tTraining Loss: 0.000246\n",
      "Epoch: 16 \tTraining Loss: 0.000246\n",
      "Epoch: 17 \tTraining Loss: 0.000246\n",
      "Epoch: 18 \tTraining Loss: 0.000246\n",
      "Epoch: 1 \tTraining Loss: 0.000372\n",
      "Epoch: 2 \tTraining Loss: 0.000372\n",
      "Epoch: 3 \tTraining Loss: 0.000372\n",
      "Epoch: 4 \tTraining Loss: 0.000372\n",
      "Epoch: 5 \tTraining Loss: 0.000372\n",
      "Epoch: 6 \tTraining Loss: 0.000372\n",
      "Epoch: 7 \tTraining Loss: 0.000372\n",
      "Epoch: 8 \tTraining Loss: 0.000372\n",
      "Epoch: 9 \tTraining Loss: 0.000372\n",
      "Epoch: 10 \tTraining Loss: 0.000372\n",
      "Epoch: 11 \tTraining Loss: 0.000372\n",
      "Epoch: 12 \tTraining Loss: 0.000372\n",
      "Epoch: 13 \tTraining Loss: 0.000372\n",
      "Epoch: 14 \tTraining Loss: 0.000372\n",
      "Epoch: 15 \tTraining Loss: 0.000372\n",
      "Epoch: 16 \tTraining Loss: 0.000372\n",
      "Epoch: 17 \tTraining Loss: 0.000372\n",
      "Epoch: 18 \tTraining Loss: 0.000372\n",
      "Epoch: 1 \tTraining Loss: 0.000105\n",
      "Epoch: 2 \tTraining Loss: 0.000105\n",
      "Epoch: 3 \tTraining Loss: 0.000105\n",
      "Epoch: 4 \tTraining Loss: 0.000105\n",
      "Epoch: 5 \tTraining Loss: 0.000105\n",
      "Epoch: 6 \tTraining Loss: 0.000105\n",
      "Epoch: 7 \tTraining Loss: 0.000105\n",
      "Epoch: 8 \tTraining Loss: 0.000105\n",
      "Epoch: 9 \tTraining Loss: 0.000105\n",
      "Epoch: 10 \tTraining Loss: 0.000105\n",
      "Epoch: 11 \tTraining Loss: 0.000105\n",
      "Epoch: 12 \tTraining Loss: 0.000105\n",
      "Epoch: 13 \tTraining Loss: 0.000105\n",
      "Epoch: 14 \tTraining Loss: 0.000105\n",
      "Epoch: 15 \tTraining Loss: 0.000105\n",
      "Epoch: 16 \tTraining Loss: 0.000105\n",
      "Epoch: 17 \tTraining Loss: 0.000105\n",
      "Epoch: 18 \tTraining Loss: 0.000105\n",
      "Epoch: 1 \tTraining Loss: 0.000298\n",
      "Epoch: 2 \tTraining Loss: 0.000298\n",
      "Epoch: 3 \tTraining Loss: 0.000298\n",
      "Epoch: 4 \tTraining Loss: 0.000298\n",
      "Epoch: 5 \tTraining Loss: 0.000298\n",
      "Epoch: 6 \tTraining Loss: 0.000298\n",
      "Epoch: 7 \tTraining Loss: 0.000298\n",
      "Epoch: 8 \tTraining Loss: 0.000298\n",
      "Epoch: 9 \tTraining Loss: 0.000298\n",
      "Epoch: 10 \tTraining Loss: 0.000298\n",
      "Epoch: 11 \tTraining Loss: 0.000298\n",
      "Epoch: 12 \tTraining Loss: 0.000298\n",
      "Epoch: 13 \tTraining Loss: 0.000298\n",
      "Epoch: 14 \tTraining Loss: 0.000298\n",
      "Epoch: 15 \tTraining Loss: 0.000298\n",
      "Epoch: 16 \tTraining Loss: 0.000298\n",
      "Epoch: 17 \tTraining Loss: 0.000298\n",
      "Epoch: 18 \tTraining Loss: 0.000298\n",
      "Epoch: 1 \tTraining Loss: 0.000176\n",
      "Epoch: 2 \tTraining Loss: 0.000176\n",
      "Epoch: 3 \tTraining Loss: 0.000176\n",
      "Epoch: 4 \tTraining Loss: 0.000176\n",
      "Epoch: 5 \tTraining Loss: 0.000176\n",
      "Epoch: 6 \tTraining Loss: 0.000176\n",
      "Epoch: 7 \tTraining Loss: 0.000176\n",
      "Epoch: 8 \tTraining Loss: 0.000176\n",
      "Epoch: 9 \tTraining Loss: 0.000176\n",
      "Epoch: 10 \tTraining Loss: 0.000176\n",
      "Epoch: 11 \tTraining Loss: 0.000176\n",
      "Epoch: 12 \tTraining Loss: 0.000176\n",
      "Epoch: 13 \tTraining Loss: 0.000176\n",
      "Epoch: 14 \tTraining Loss: 0.000176\n",
      "Epoch: 15 \tTraining Loss: 0.000176\n",
      "Epoch: 16 \tTraining Loss: 0.000176\n",
      "Epoch: 17 \tTraining Loss: 0.000176\n",
      "Epoch: 18 \tTraining Loss: 0.000176\n",
      "Epoch: 1 \tTraining Loss: 0.000171\n",
      "Epoch: 2 \tTraining Loss: 0.000171\n",
      "Epoch: 3 \tTraining Loss: 0.000171\n",
      "Epoch: 4 \tTraining Loss: 0.000171\n",
      "Epoch: 5 \tTraining Loss: 0.000171\n",
      "Epoch: 6 \tTraining Loss: 0.000171\n",
      "Epoch: 7 \tTraining Loss: 0.000171\n",
      "Epoch: 8 \tTraining Loss: 0.000171\n",
      "Epoch: 9 \tTraining Loss: 0.000171\n",
      "Epoch: 10 \tTraining Loss: 0.000171\n",
      "Epoch: 11 \tTraining Loss: 0.000171\n",
      "Epoch: 12 \tTraining Loss: 0.000171\n",
      "Epoch: 13 \tTraining Loss: 0.000171\n",
      "Epoch: 14 \tTraining Loss: 0.000171\n",
      "Epoch: 15 \tTraining Loss: 0.000171\n",
      "Epoch: 16 \tTraining Loss: 0.000171\n",
      "Epoch: 17 \tTraining Loss: 0.000171\n",
      "Epoch: 18 \tTraining Loss: 0.000171\n",
      "Epoch: 1 \tTraining Loss: 0.000073\n",
      "Epoch: 2 \tTraining Loss: 0.000073\n",
      "Epoch: 3 \tTraining Loss: 0.000073\n",
      "Epoch: 4 \tTraining Loss: 0.000073\n",
      "Epoch: 5 \tTraining Loss: 0.000073\n",
      "Epoch: 6 \tTraining Loss: 0.000073\n",
      "Epoch: 7 \tTraining Loss: 0.000073\n",
      "Epoch: 8 \tTraining Loss: 0.000073\n",
      "Epoch: 9 \tTraining Loss: 0.000073\n",
      "Epoch: 10 \tTraining Loss: 0.000073\n",
      "Epoch: 11 \tTraining Loss: 0.000073\n",
      "Epoch: 12 \tTraining Loss: 0.000073\n",
      "Epoch: 13 \tTraining Loss: 0.000073\n",
      "Epoch: 14 \tTraining Loss: 0.000073\n",
      "Epoch: 15 \tTraining Loss: 0.000073\n",
      "Epoch: 16 \tTraining Loss: 0.000073\n",
      "Epoch: 17 \tTraining Loss: 0.000073\n",
      "Epoch: 18 \tTraining Loss: 0.000073\n",
      "Epoch: 1 \tTraining Loss: 0.000222\n",
      "Epoch: 2 \tTraining Loss: 0.000222\n",
      "Epoch: 3 \tTraining Loss: 0.000222\n",
      "Epoch: 4 \tTraining Loss: 0.000222\n",
      "Epoch: 5 \tTraining Loss: 0.000222\n",
      "Epoch: 6 \tTraining Loss: 0.000222\n",
      "Epoch: 7 \tTraining Loss: 0.000222\n",
      "Epoch: 8 \tTraining Loss: 0.000222\n",
      "Epoch: 9 \tTraining Loss: 0.000222\n",
      "Epoch: 10 \tTraining Loss: 0.000222\n",
      "Epoch: 11 \tTraining Loss: 0.000222\n",
      "Epoch: 12 \tTraining Loss: 0.000222\n",
      "Epoch: 13 \tTraining Loss: 0.000222\n",
      "Epoch: 14 \tTraining Loss: 0.000222\n",
      "Epoch: 15 \tTraining Loss: 0.000222\n",
      "Epoch: 16 \tTraining Loss: 0.000222\n",
      "Epoch: 17 \tTraining Loss: 0.000222\n",
      "Epoch: 18 \tTraining Loss: 0.000222\n",
      "Epoch: 1 \tTraining Loss: 0.000360\n",
      "Epoch: 2 \tTraining Loss: 0.000360\n",
      "Epoch: 3 \tTraining Loss: 0.000360\n",
      "Epoch: 4 \tTraining Loss: 0.000360\n",
      "Epoch: 5 \tTraining Loss: 0.000360\n",
      "Epoch: 6 \tTraining Loss: 0.000360\n",
      "Epoch: 7 \tTraining Loss: 0.000360\n",
      "Epoch: 8 \tTraining Loss: 0.000360\n",
      "Epoch: 9 \tTraining Loss: 0.000360\n",
      "Epoch: 10 \tTraining Loss: 0.000360\n",
      "Epoch: 11 \tTraining Loss: 0.000360\n",
      "Epoch: 12 \tTraining Loss: 0.000360\n",
      "Epoch: 13 \tTraining Loss: 0.000360\n",
      "Epoch: 14 \tTraining Loss: 0.000360\n",
      "Epoch: 15 \tTraining Loss: 0.000360\n",
      "Epoch: 16 \tTraining Loss: 0.000360\n",
      "Epoch: 17 \tTraining Loss: 0.000360\n",
      "Epoch: 18 \tTraining Loss: 0.000360\n",
      "Epoch: 1 \tTraining Loss: 0.000247\n",
      "Epoch: 2 \tTraining Loss: 0.000247\n",
      "Epoch: 3 \tTraining Loss: 0.000247\n",
      "Epoch: 4 \tTraining Loss: 0.000247\n",
      "Epoch: 5 \tTraining Loss: 0.000247\n",
      "Epoch: 6 \tTraining Loss: 0.000247\n",
      "Epoch: 7 \tTraining Loss: 0.000247\n",
      "Epoch: 8 \tTraining Loss: 0.000247\n",
      "Epoch: 9 \tTraining Loss: 0.000247\n",
      "Epoch: 10 \tTraining Loss: 0.000247\n",
      "Epoch: 11 \tTraining Loss: 0.000247\n",
      "Epoch: 12 \tTraining Loss: 0.000247\n",
      "Epoch: 13 \tTraining Loss: 0.000247\n",
      "Epoch: 14 \tTraining Loss: 0.000247\n",
      "Epoch: 15 \tTraining Loss: 0.000247\n",
      "Epoch: 16 \tTraining Loss: 0.000247\n",
      "Epoch: 17 \tTraining Loss: 0.000247\n",
      "Epoch: 18 \tTraining Loss: 0.000247\n",
      "Epoch: 1 \tTraining Loss: 0.000207\n",
      "Epoch: 2 \tTraining Loss: 0.000207\n",
      "Epoch: 3 \tTraining Loss: 0.000207\n",
      "Epoch: 4 \tTraining Loss: 0.000207\n",
      "Epoch: 5 \tTraining Loss: 0.000207\n",
      "Epoch: 6 \tTraining Loss: 0.000207\n",
      "Epoch: 7 \tTraining Loss: 0.000207\n",
      "Epoch: 8 \tTraining Loss: 0.000207\n",
      "Epoch: 9 \tTraining Loss: 0.000207\n",
      "Epoch: 10 \tTraining Loss: 0.000207\n",
      "Epoch: 11 \tTraining Loss: 0.000207\n",
      "Epoch: 12 \tTraining Loss: 0.000207\n",
      "Epoch: 13 \tTraining Loss: 0.000207\n",
      "Epoch: 14 \tTraining Loss: 0.000207\n",
      "Epoch: 15 \tTraining Loss: 0.000207\n",
      "Epoch: 16 \tTraining Loss: 0.000207\n",
      "Epoch: 17 \tTraining Loss: 0.000207\n",
      "Epoch: 18 \tTraining Loss: 0.000207\n",
      "Epoch: 1 \tTraining Loss: 0.000132\n",
      "Epoch: 2 \tTraining Loss: 0.000132\n",
      "Epoch: 3 \tTraining Loss: 0.000132\n",
      "Epoch: 4 \tTraining Loss: 0.000132\n",
      "Epoch: 5 \tTraining Loss: 0.000132\n",
      "Epoch: 6 \tTraining Loss: 0.000132\n",
      "Epoch: 7 \tTraining Loss: 0.000132\n",
      "Epoch: 8 \tTraining Loss: 0.000132\n",
      "Epoch: 9 \tTraining Loss: 0.000132\n",
      "Epoch: 10 \tTraining Loss: 0.000132\n",
      "Epoch: 11 \tTraining Loss: 0.000132\n",
      "Epoch: 12 \tTraining Loss: 0.000132\n",
      "Epoch: 13 \tTraining Loss: 0.000132\n",
      "Epoch: 14 \tTraining Loss: 0.000132\n",
      "Epoch: 15 \tTraining Loss: 0.000132\n",
      "Epoch: 16 \tTraining Loss: 0.000132\n",
      "Epoch: 17 \tTraining Loss: 0.000132\n",
      "Epoch: 18 \tTraining Loss: 0.000132\n",
      "Epoch: 1 \tTraining Loss: 0.000350\n",
      "Epoch: 2 \tTraining Loss: 0.000350\n",
      "Epoch: 3 \tTraining Loss: 0.000350\n",
      "Epoch: 4 \tTraining Loss: 0.000350\n",
      "Epoch: 5 \tTraining Loss: 0.000350\n",
      "Epoch: 6 \tTraining Loss: 0.000350\n",
      "Epoch: 7 \tTraining Loss: 0.000350\n",
      "Epoch: 8 \tTraining Loss: 0.000350\n",
      "Epoch: 9 \tTraining Loss: 0.000350\n",
      "Epoch: 10 \tTraining Loss: 0.000350\n",
      "Epoch: 11 \tTraining Loss: 0.000350\n",
      "Epoch: 12 \tTraining Loss: 0.000350\n",
      "Epoch: 13 \tTraining Loss: 0.000350\n",
      "Epoch: 14 \tTraining Loss: 0.000350\n",
      "Epoch: 15 \tTraining Loss: 0.000350\n",
      "Epoch: 16 \tTraining Loss: 0.000350\n",
      "Epoch: 17 \tTraining Loss: 0.000350\n",
      "Epoch: 18 \tTraining Loss: 0.000350\n",
      "Epoch: 1 \tTraining Loss: 0.000117\n",
      "Epoch: 2 \tTraining Loss: 0.000117\n",
      "Epoch: 3 \tTraining Loss: 0.000117\n",
      "Epoch: 4 \tTraining Loss: 0.000117\n",
      "Epoch: 5 \tTraining Loss: 0.000117\n",
      "Epoch: 6 \tTraining Loss: 0.000117\n",
      "Epoch: 7 \tTraining Loss: 0.000117\n",
      "Epoch: 8 \tTraining Loss: 0.000117\n",
      "Epoch: 9 \tTraining Loss: 0.000117\n",
      "Epoch: 10 \tTraining Loss: 0.000117\n",
      "Epoch: 11 \tTraining Loss: 0.000117\n",
      "Epoch: 12 \tTraining Loss: 0.000117\n",
      "Epoch: 13 \tTraining Loss: 0.000117\n",
      "Epoch: 14 \tTraining Loss: 0.000117\n",
      "Epoch: 15 \tTraining Loss: 0.000117\n",
      "Epoch: 16 \tTraining Loss: 0.000117\n",
      "Epoch: 17 \tTraining Loss: 0.000117\n",
      "Epoch: 18 \tTraining Loss: 0.000117\n",
      "Epoch: 1 \tTraining Loss: 0.000178\n",
      "Epoch: 2 \tTraining Loss: 0.000178\n",
      "Epoch: 3 \tTraining Loss: 0.000178\n",
      "Epoch: 4 \tTraining Loss: 0.000178\n",
      "Epoch: 5 \tTraining Loss: 0.000178\n",
      "Epoch: 6 \tTraining Loss: 0.000178\n",
      "Epoch: 7 \tTraining Loss: 0.000178\n",
      "Epoch: 8 \tTraining Loss: 0.000178\n",
      "Epoch: 9 \tTraining Loss: 0.000178\n",
      "Epoch: 10 \tTraining Loss: 0.000178\n",
      "Epoch: 11 \tTraining Loss: 0.000178\n",
      "Epoch: 12 \tTraining Loss: 0.000178\n",
      "Epoch: 13 \tTraining Loss: 0.000178\n",
      "Epoch: 14 \tTraining Loss: 0.000178\n",
      "Epoch: 15 \tTraining Loss: 0.000178\n",
      "Epoch: 16 \tTraining Loss: 0.000178\n",
      "Epoch: 17 \tTraining Loss: 0.000178\n",
      "Epoch: 18 \tTraining Loss: 0.000178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000143\n",
      "Epoch: 2 \tTraining Loss: 0.000143\n",
      "Epoch: 3 \tTraining Loss: 0.000143\n",
      "Epoch: 4 \tTraining Loss: 0.000143\n",
      "Epoch: 5 \tTraining Loss: 0.000143\n",
      "Epoch: 6 \tTraining Loss: 0.000143\n",
      "Epoch: 7 \tTraining Loss: 0.000143\n",
      "Epoch: 8 \tTraining Loss: 0.000143\n",
      "Epoch: 9 \tTraining Loss: 0.000143\n",
      "Epoch: 10 \tTraining Loss: 0.000143\n",
      "Epoch: 11 \tTraining Loss: 0.000143\n",
      "Epoch: 12 \tTraining Loss: 0.000143\n",
      "Epoch: 13 \tTraining Loss: 0.000143\n",
      "Epoch: 14 \tTraining Loss: 0.000143\n",
      "Epoch: 15 \tTraining Loss: 0.000143\n",
      "Epoch: 16 \tTraining Loss: 0.000143\n",
      "Epoch: 17 \tTraining Loss: 0.000143\n",
      "Epoch: 18 \tTraining Loss: 0.000143\n",
      "Epoch: 1 \tTraining Loss: 0.000131\n",
      "Epoch: 2 \tTraining Loss: 0.000131\n",
      "Epoch: 3 \tTraining Loss: 0.000131\n",
      "Epoch: 4 \tTraining Loss: 0.000131\n",
      "Epoch: 5 \tTraining Loss: 0.000131\n",
      "Epoch: 6 \tTraining Loss: 0.000131\n",
      "Epoch: 7 \tTraining Loss: 0.000131\n",
      "Epoch: 8 \tTraining Loss: 0.000131\n",
      "Epoch: 9 \tTraining Loss: 0.000131\n",
      "Epoch: 10 \tTraining Loss: 0.000131\n",
      "Epoch: 11 \tTraining Loss: 0.000131\n",
      "Epoch: 12 \tTraining Loss: 0.000131\n",
      "Epoch: 13 \tTraining Loss: 0.000131\n",
      "Epoch: 14 \tTraining Loss: 0.000131\n",
      "Epoch: 15 \tTraining Loss: 0.000131\n",
      "Epoch: 16 \tTraining Loss: 0.000131\n",
      "Epoch: 17 \tTraining Loss: 0.000131\n",
      "Epoch: 18 \tTraining Loss: 0.000131\n",
      "Epoch: 1 \tTraining Loss: 0.000194\n",
      "Epoch: 2 \tTraining Loss: 0.000194\n",
      "Epoch: 3 \tTraining Loss: 0.000194\n",
      "Epoch: 4 \tTraining Loss: 0.000194\n",
      "Epoch: 5 \tTraining Loss: 0.000194\n",
      "Epoch: 6 \tTraining Loss: 0.000194\n",
      "Epoch: 7 \tTraining Loss: 0.000194\n",
      "Epoch: 8 \tTraining Loss: 0.000194\n",
      "Epoch: 9 \tTraining Loss: 0.000194\n",
      "Epoch: 10 \tTraining Loss: 0.000194\n",
      "Epoch: 11 \tTraining Loss: 0.000194\n",
      "Epoch: 12 \tTraining Loss: 0.000194\n",
      "Epoch: 13 \tTraining Loss: 0.000194\n",
      "Epoch: 14 \tTraining Loss: 0.000194\n",
      "Epoch: 15 \tTraining Loss: 0.000194\n",
      "Epoch: 16 \tTraining Loss: 0.000194\n",
      "Epoch: 17 \tTraining Loss: 0.000194\n",
      "Epoch: 18 \tTraining Loss: 0.000194\n",
      "Epoch: 1 \tTraining Loss: 0.000225\n",
      "Epoch: 2 \tTraining Loss: 0.000225\n",
      "Epoch: 3 \tTraining Loss: 0.000225\n",
      "Epoch: 4 \tTraining Loss: 0.000225\n",
      "Epoch: 5 \tTraining Loss: 0.000225\n",
      "Epoch: 6 \tTraining Loss: 0.000225\n",
      "Epoch: 7 \tTraining Loss: 0.000225\n",
      "Epoch: 8 \tTraining Loss: 0.000225\n",
      "Epoch: 9 \tTraining Loss: 0.000225\n",
      "Epoch: 10 \tTraining Loss: 0.000225\n",
      "Epoch: 11 \tTraining Loss: 0.000225\n",
      "Epoch: 12 \tTraining Loss: 0.000225\n",
      "Epoch: 13 \tTraining Loss: 0.000225\n",
      "Epoch: 14 \tTraining Loss: 0.000225\n",
      "Epoch: 15 \tTraining Loss: 0.000225\n",
      "Epoch: 16 \tTraining Loss: 0.000225\n",
      "Epoch: 17 \tTraining Loss: 0.000225\n",
      "Epoch: 18 \tTraining Loss: 0.000225\n",
      "Epoch: 1 \tTraining Loss: 0.000318\n",
      "Epoch: 2 \tTraining Loss: 0.000318\n",
      "Epoch: 3 \tTraining Loss: 0.000318\n",
      "Epoch: 4 \tTraining Loss: 0.000318\n",
      "Epoch: 5 \tTraining Loss: 0.000318\n",
      "Epoch: 6 \tTraining Loss: 0.000318\n",
      "Epoch: 7 \tTraining Loss: 0.000318\n",
      "Epoch: 8 \tTraining Loss: 0.000318\n",
      "Epoch: 9 \tTraining Loss: 0.000318\n",
      "Epoch: 10 \tTraining Loss: 0.000318\n",
      "Epoch: 11 \tTraining Loss: 0.000318\n",
      "Epoch: 12 \tTraining Loss: 0.000318\n",
      "Epoch: 13 \tTraining Loss: 0.000318\n",
      "Epoch: 14 \tTraining Loss: 0.000318\n",
      "Epoch: 15 \tTraining Loss: 0.000318\n",
      "Epoch: 16 \tTraining Loss: 0.000318\n",
      "Epoch: 17 \tTraining Loss: 0.000318\n",
      "Epoch: 18 \tTraining Loss: 0.000318\n",
      "Epoch: 1 \tTraining Loss: 0.000137\n",
      "Epoch: 2 \tTraining Loss: 0.000137\n",
      "Epoch: 3 \tTraining Loss: 0.000137\n",
      "Epoch: 4 \tTraining Loss: 0.000137\n",
      "Epoch: 5 \tTraining Loss: 0.000137\n",
      "Epoch: 6 \tTraining Loss: 0.000137\n",
      "Epoch: 7 \tTraining Loss: 0.000137\n",
      "Epoch: 8 \tTraining Loss: 0.000137\n",
      "Epoch: 9 \tTraining Loss: 0.000137\n",
      "Epoch: 10 \tTraining Loss: 0.000137\n",
      "Epoch: 11 \tTraining Loss: 0.000137\n",
      "Epoch: 12 \tTraining Loss: 0.000137\n",
      "Epoch: 13 \tTraining Loss: 0.000137\n",
      "Epoch: 14 \tTraining Loss: 0.000137\n",
      "Epoch: 15 \tTraining Loss: 0.000137\n",
      "Epoch: 16 \tTraining Loss: 0.000137\n",
      "Epoch: 17 \tTraining Loss: 0.000137\n",
      "Epoch: 18 \tTraining Loss: 0.000137\n",
      "Epoch: 1 \tTraining Loss: 0.000155\n",
      "Epoch: 2 \tTraining Loss: 0.000155\n",
      "Epoch: 3 \tTraining Loss: 0.000155\n",
      "Epoch: 4 \tTraining Loss: 0.000155\n",
      "Epoch: 5 \tTraining Loss: 0.000155\n",
      "Epoch: 6 \tTraining Loss: 0.000155\n",
      "Epoch: 7 \tTraining Loss: 0.000155\n",
      "Epoch: 8 \tTraining Loss: 0.000155\n",
      "Epoch: 9 \tTraining Loss: 0.000155\n",
      "Epoch: 10 \tTraining Loss: 0.000155\n",
      "Epoch: 11 \tTraining Loss: 0.000155\n",
      "Epoch: 12 \tTraining Loss: 0.000155\n",
      "Epoch: 13 \tTraining Loss: 0.000155\n",
      "Epoch: 14 \tTraining Loss: 0.000155\n",
      "Epoch: 15 \tTraining Loss: 0.000155\n",
      "Epoch: 16 \tTraining Loss: 0.000155\n",
      "Epoch: 17 \tTraining Loss: 0.000155\n",
      "Epoch: 18 \tTraining Loss: 0.000155\n",
      "Epoch: 1 \tTraining Loss: 0.000112\n",
      "Epoch: 2 \tTraining Loss: 0.000112\n",
      "Epoch: 3 \tTraining Loss: 0.000112\n",
      "Epoch: 4 \tTraining Loss: 0.000112\n",
      "Epoch: 5 \tTraining Loss: 0.000112\n",
      "Epoch: 6 \tTraining Loss: 0.000112\n",
      "Epoch: 7 \tTraining Loss: 0.000112\n",
      "Epoch: 8 \tTraining Loss: 0.000112\n",
      "Epoch: 9 \tTraining Loss: 0.000112\n",
      "Epoch: 10 \tTraining Loss: 0.000112\n",
      "Epoch: 11 \tTraining Loss: 0.000112\n",
      "Epoch: 12 \tTraining Loss: 0.000112\n",
      "Epoch: 13 \tTraining Loss: 0.000112\n",
      "Epoch: 14 \tTraining Loss: 0.000112\n",
      "Epoch: 15 \tTraining Loss: 0.000112\n",
      "Epoch: 16 \tTraining Loss: 0.000112\n",
      "Epoch: 17 \tTraining Loss: 0.000112\n",
      "Epoch: 18 \tTraining Loss: 0.000112\n",
      "Epoch: 1 \tTraining Loss: 0.000173\n",
      "Epoch: 2 \tTraining Loss: 0.000173\n",
      "Epoch: 3 \tTraining Loss: 0.000173\n",
      "Epoch: 4 \tTraining Loss: 0.000173\n",
      "Epoch: 5 \tTraining Loss: 0.000173\n",
      "Epoch: 6 \tTraining Loss: 0.000173\n",
      "Epoch: 7 \tTraining Loss: 0.000173\n",
      "Epoch: 8 \tTraining Loss: 0.000173\n",
      "Epoch: 9 \tTraining Loss: 0.000173\n",
      "Epoch: 10 \tTraining Loss: 0.000173\n",
      "Epoch: 11 \tTraining Loss: 0.000173\n",
      "Epoch: 12 \tTraining Loss: 0.000173\n",
      "Epoch: 13 \tTraining Loss: 0.000173\n",
      "Epoch: 14 \tTraining Loss: 0.000173\n",
      "Epoch: 15 \tTraining Loss: 0.000173\n",
      "Epoch: 16 \tTraining Loss: 0.000173\n",
      "Epoch: 17 \tTraining Loss: 0.000173\n",
      "Epoch: 18 \tTraining Loss: 0.000173\n",
      "Epoch: 1 \tTraining Loss: 0.000318\n",
      "Epoch: 2 \tTraining Loss: 0.000318\n",
      "Epoch: 3 \tTraining Loss: 0.000318\n",
      "Epoch: 4 \tTraining Loss: 0.000318\n",
      "Epoch: 5 \tTraining Loss: 0.000318\n",
      "Epoch: 6 \tTraining Loss: 0.000318\n",
      "Epoch: 7 \tTraining Loss: 0.000318\n",
      "Epoch: 8 \tTraining Loss: 0.000318\n",
      "Epoch: 9 \tTraining Loss: 0.000318\n",
      "Epoch: 10 \tTraining Loss: 0.000318\n",
      "Epoch: 11 \tTraining Loss: 0.000318\n",
      "Epoch: 12 \tTraining Loss: 0.000318\n",
      "Epoch: 13 \tTraining Loss: 0.000318\n",
      "Epoch: 14 \tTraining Loss: 0.000318\n",
      "Epoch: 15 \tTraining Loss: 0.000318\n",
      "Epoch: 16 \tTraining Loss: 0.000318\n",
      "Epoch: 17 \tTraining Loss: 0.000318\n",
      "Epoch: 18 \tTraining Loss: 0.000318\n",
      "Epoch: 1 \tTraining Loss: 0.000139\n",
      "Epoch: 2 \tTraining Loss: 0.000139\n",
      "Epoch: 3 \tTraining Loss: 0.000139\n",
      "Epoch: 4 \tTraining Loss: 0.000139\n",
      "Epoch: 5 \tTraining Loss: 0.000139\n",
      "Epoch: 6 \tTraining Loss: 0.000139\n",
      "Epoch: 7 \tTraining Loss: 0.000139\n",
      "Epoch: 8 \tTraining Loss: 0.000139\n",
      "Epoch: 9 \tTraining Loss: 0.000139\n",
      "Epoch: 10 \tTraining Loss: 0.000139\n",
      "Epoch: 11 \tTraining Loss: 0.000139\n",
      "Epoch: 12 \tTraining Loss: 0.000139\n",
      "Epoch: 13 \tTraining Loss: 0.000139\n",
      "Epoch: 14 \tTraining Loss: 0.000139\n",
      "Epoch: 15 \tTraining Loss: 0.000139\n",
      "Epoch: 16 \tTraining Loss: 0.000139\n",
      "Epoch: 17 \tTraining Loss: 0.000139\n",
      "Epoch: 18 \tTraining Loss: 0.000139\n",
      "Epoch: 1 \tTraining Loss: 0.000172\n",
      "Epoch: 2 \tTraining Loss: 0.000172\n",
      "Epoch: 3 \tTraining Loss: 0.000172\n",
      "Epoch: 4 \tTraining Loss: 0.000172\n",
      "Epoch: 5 \tTraining Loss: 0.000172\n",
      "Epoch: 6 \tTraining Loss: 0.000172\n",
      "Epoch: 7 \tTraining Loss: 0.000172\n",
      "Epoch: 8 \tTraining Loss: 0.000172\n",
      "Epoch: 9 \tTraining Loss: 0.000172\n",
      "Epoch: 10 \tTraining Loss: 0.000172\n",
      "Epoch: 11 \tTraining Loss: 0.000172\n",
      "Epoch: 12 \tTraining Loss: 0.000172\n",
      "Epoch: 13 \tTraining Loss: 0.000172\n",
      "Epoch: 14 \tTraining Loss: 0.000172\n",
      "Epoch: 15 \tTraining Loss: 0.000172\n",
      "Epoch: 16 \tTraining Loss: 0.000172\n",
      "Epoch: 17 \tTraining Loss: 0.000172\n",
      "Epoch: 18 \tTraining Loss: 0.000172\n",
      "Epoch: 1 \tTraining Loss: 0.000179\n",
      "Epoch: 2 \tTraining Loss: 0.000179\n",
      "Epoch: 3 \tTraining Loss: 0.000179\n",
      "Epoch: 4 \tTraining Loss: 0.000179\n",
      "Epoch: 5 \tTraining Loss: 0.000179\n",
      "Epoch: 6 \tTraining Loss: 0.000179\n",
      "Epoch: 7 \tTraining Loss: 0.000179\n",
      "Epoch: 8 \tTraining Loss: 0.000179\n",
      "Epoch: 9 \tTraining Loss: 0.000179\n",
      "Epoch: 10 \tTraining Loss: 0.000179\n",
      "Epoch: 11 \tTraining Loss: 0.000179\n",
      "Epoch: 12 \tTraining Loss: 0.000179\n",
      "Epoch: 13 \tTraining Loss: 0.000179\n",
      "Epoch: 14 \tTraining Loss: 0.000179\n",
      "Epoch: 15 \tTraining Loss: 0.000179\n",
      "Epoch: 16 \tTraining Loss: 0.000179\n",
      "Epoch: 17 \tTraining Loss: 0.000179\n",
      "Epoch: 18 \tTraining Loss: 0.000179\n",
      "Epoch: 1 \tTraining Loss: 0.000356\n",
      "Epoch: 2 \tTraining Loss: 0.000356\n",
      "Epoch: 3 \tTraining Loss: 0.000356\n",
      "Epoch: 4 \tTraining Loss: 0.000356\n",
      "Epoch: 5 \tTraining Loss: 0.000356\n",
      "Epoch: 6 \tTraining Loss: 0.000356\n",
      "Epoch: 7 \tTraining Loss: 0.000356\n",
      "Epoch: 8 \tTraining Loss: 0.000356\n",
      "Epoch: 9 \tTraining Loss: 0.000356\n",
      "Epoch: 10 \tTraining Loss: 0.000356\n",
      "Epoch: 11 \tTraining Loss: 0.000356\n",
      "Epoch: 12 \tTraining Loss: 0.000356\n",
      "Epoch: 13 \tTraining Loss: 0.000356\n",
      "Epoch: 14 \tTraining Loss: 0.000356\n",
      "Epoch: 15 \tTraining Loss: 0.000356\n",
      "Epoch: 16 \tTraining Loss: 0.000356\n",
      "Epoch: 17 \tTraining Loss: 0.000356\n",
      "Epoch: 18 \tTraining Loss: 0.000356\n",
      "Epoch: 1 \tTraining Loss: 0.000131\n",
      "Epoch: 2 \tTraining Loss: 0.000131\n",
      "Epoch: 3 \tTraining Loss: 0.000131\n",
      "Epoch: 4 \tTraining Loss: 0.000131\n",
      "Epoch: 5 \tTraining Loss: 0.000131\n",
      "Epoch: 6 \tTraining Loss: 0.000131\n",
      "Epoch: 7 \tTraining Loss: 0.000131\n",
      "Epoch: 8 \tTraining Loss: 0.000131\n",
      "Epoch: 9 \tTraining Loss: 0.000131\n",
      "Epoch: 10 \tTraining Loss: 0.000131\n",
      "Epoch: 11 \tTraining Loss: 0.000131\n",
      "Epoch: 12 \tTraining Loss: 0.000131\n",
      "Epoch: 13 \tTraining Loss: 0.000131\n",
      "Epoch: 14 \tTraining Loss: 0.000131\n",
      "Epoch: 15 \tTraining Loss: 0.000131\n",
      "Epoch: 16 \tTraining Loss: 0.000131\n",
      "Epoch: 17 \tTraining Loss: 0.000131\n",
      "Epoch: 18 \tTraining Loss: 0.000131\n",
      "Epoch: 1 \tTraining Loss: 0.000150\n",
      "Epoch: 2 \tTraining Loss: 0.000150\n",
      "Epoch: 3 \tTraining Loss: 0.000150\n",
      "Epoch: 4 \tTraining Loss: 0.000150\n",
      "Epoch: 5 \tTraining Loss: 0.000150\n",
      "Epoch: 6 \tTraining Loss: 0.000150\n",
      "Epoch: 7 \tTraining Loss: 0.000150\n",
      "Epoch: 8 \tTraining Loss: 0.000150\n",
      "Epoch: 9 \tTraining Loss: 0.000150\n",
      "Epoch: 10 \tTraining Loss: 0.000150\n",
      "Epoch: 11 \tTraining Loss: 0.000150\n",
      "Epoch: 12 \tTraining Loss: 0.000150\n",
      "Epoch: 13 \tTraining Loss: 0.000150\n",
      "Epoch: 14 \tTraining Loss: 0.000150\n",
      "Epoch: 15 \tTraining Loss: 0.000150\n",
      "Epoch: 16 \tTraining Loss: 0.000150\n",
      "Epoch: 17 \tTraining Loss: 0.000150\n",
      "Epoch: 18 \tTraining Loss: 0.000150\n",
      "Epoch: 1 \tTraining Loss: 0.000106\n",
      "Epoch: 2 \tTraining Loss: 0.000106\n",
      "Epoch: 3 \tTraining Loss: 0.000106\n",
      "Epoch: 4 \tTraining Loss: 0.000106\n",
      "Epoch: 5 \tTraining Loss: 0.000106\n",
      "Epoch: 6 \tTraining Loss: 0.000106\n",
      "Epoch: 7 \tTraining Loss: 0.000106\n",
      "Epoch: 8 \tTraining Loss: 0.000106\n",
      "Epoch: 9 \tTraining Loss: 0.000106\n",
      "Epoch: 10 \tTraining Loss: 0.000106\n",
      "Epoch: 11 \tTraining Loss: 0.000106\n",
      "Epoch: 12 \tTraining Loss: 0.000106\n",
      "Epoch: 13 \tTraining Loss: 0.000106\n",
      "Epoch: 14 \tTraining Loss: 0.000106\n",
      "Epoch: 15 \tTraining Loss: 0.000106\n",
      "Epoch: 16 \tTraining Loss: 0.000106\n",
      "Epoch: 17 \tTraining Loss: 0.000106\n",
      "Epoch: 18 \tTraining Loss: 0.000106\n",
      "Epoch: 1 \tTraining Loss: 0.000243\n",
      "Epoch: 2 \tTraining Loss: 0.000243\n",
      "Epoch: 3 \tTraining Loss: 0.000243\n",
      "Epoch: 4 \tTraining Loss: 0.000243\n",
      "Epoch: 5 \tTraining Loss: 0.000243\n",
      "Epoch: 6 \tTraining Loss: 0.000243\n",
      "Epoch: 7 \tTraining Loss: 0.000243\n",
      "Epoch: 8 \tTraining Loss: 0.000243\n",
      "Epoch: 9 \tTraining Loss: 0.000243\n",
      "Epoch: 10 \tTraining Loss: 0.000243\n",
      "Epoch: 11 \tTraining Loss: 0.000243\n",
      "Epoch: 12 \tTraining Loss: 0.000243\n",
      "Epoch: 13 \tTraining Loss: 0.000243\n",
      "Epoch: 14 \tTraining Loss: 0.000243\n",
      "Epoch: 15 \tTraining Loss: 0.000243\n",
      "Epoch: 16 \tTraining Loss: 0.000243\n",
      "Epoch: 17 \tTraining Loss: 0.000243\n",
      "Epoch: 18 \tTraining Loss: 0.000243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000165\n",
      "Epoch: 2 \tTraining Loss: 0.000165\n",
      "Epoch: 3 \tTraining Loss: 0.000165\n",
      "Epoch: 4 \tTraining Loss: 0.000165\n",
      "Epoch: 5 \tTraining Loss: 0.000165\n",
      "Epoch: 6 \tTraining Loss: 0.000165\n",
      "Epoch: 7 \tTraining Loss: 0.000165\n",
      "Epoch: 8 \tTraining Loss: 0.000165\n",
      "Epoch: 9 \tTraining Loss: 0.000165\n",
      "Epoch: 10 \tTraining Loss: 0.000165\n",
      "Epoch: 11 \tTraining Loss: 0.000165\n",
      "Epoch: 12 \tTraining Loss: 0.000165\n",
      "Epoch: 13 \tTraining Loss: 0.000165\n",
      "Epoch: 14 \tTraining Loss: 0.000165\n",
      "Epoch: 15 \tTraining Loss: 0.000165\n",
      "Epoch: 16 \tTraining Loss: 0.000165\n",
      "Epoch: 17 \tTraining Loss: 0.000165\n",
      "Epoch: 18 \tTraining Loss: 0.000165\n",
      "Epoch: 1 \tTraining Loss: 0.000184\n",
      "Epoch: 2 \tTraining Loss: 0.000184\n",
      "Epoch: 3 \tTraining Loss: 0.000184\n",
      "Epoch: 4 \tTraining Loss: 0.000184\n",
      "Epoch: 5 \tTraining Loss: 0.000184\n",
      "Epoch: 6 \tTraining Loss: 0.000184\n",
      "Epoch: 7 \tTraining Loss: 0.000184\n",
      "Epoch: 8 \tTraining Loss: 0.000184\n",
      "Epoch: 9 \tTraining Loss: 0.000184\n",
      "Epoch: 10 \tTraining Loss: 0.000184\n",
      "Epoch: 11 \tTraining Loss: 0.000184\n",
      "Epoch: 12 \tTraining Loss: 0.000184\n",
      "Epoch: 13 \tTraining Loss: 0.000184\n",
      "Epoch: 14 \tTraining Loss: 0.000184\n",
      "Epoch: 15 \tTraining Loss: 0.000184\n",
      "Epoch: 16 \tTraining Loss: 0.000184\n",
      "Epoch: 17 \tTraining Loss: 0.000184\n",
      "Epoch: 18 \tTraining Loss: 0.000184\n",
      "Epoch: 1 \tTraining Loss: 0.000133\n",
      "Epoch: 2 \tTraining Loss: 0.000133\n",
      "Epoch: 3 \tTraining Loss: 0.000133\n",
      "Epoch: 4 \tTraining Loss: 0.000133\n",
      "Epoch: 5 \tTraining Loss: 0.000133\n",
      "Epoch: 6 \tTraining Loss: 0.000133\n",
      "Epoch: 7 \tTraining Loss: 0.000133\n",
      "Epoch: 8 \tTraining Loss: 0.000133\n",
      "Epoch: 9 \tTraining Loss: 0.000133\n",
      "Epoch: 10 \tTraining Loss: 0.000133\n",
      "Epoch: 11 \tTraining Loss: 0.000133\n",
      "Epoch: 12 \tTraining Loss: 0.000133\n",
      "Epoch: 13 \tTraining Loss: 0.000133\n",
      "Epoch: 14 \tTraining Loss: 0.000133\n",
      "Epoch: 15 \tTraining Loss: 0.000133\n",
      "Epoch: 16 \tTraining Loss: 0.000133\n",
      "Epoch: 17 \tTraining Loss: 0.000133\n",
      "Epoch: 18 \tTraining Loss: 0.000133\n",
      "Epoch: 1 \tTraining Loss: 0.000129\n",
      "Epoch: 2 \tTraining Loss: 0.000129\n",
      "Epoch: 3 \tTraining Loss: 0.000129\n",
      "Epoch: 4 \tTraining Loss: 0.000129\n",
      "Epoch: 5 \tTraining Loss: 0.000129\n",
      "Epoch: 6 \tTraining Loss: 0.000129\n",
      "Epoch: 7 \tTraining Loss: 0.000129\n",
      "Epoch: 8 \tTraining Loss: 0.000129\n",
      "Epoch: 9 \tTraining Loss: 0.000129\n",
      "Epoch: 10 \tTraining Loss: 0.000129\n",
      "Epoch: 11 \tTraining Loss: 0.000129\n",
      "Epoch: 12 \tTraining Loss: 0.000129\n",
      "Epoch: 13 \tTraining Loss: 0.000129\n",
      "Epoch: 14 \tTraining Loss: 0.000129\n",
      "Epoch: 15 \tTraining Loss: 0.000129\n",
      "Epoch: 16 \tTraining Loss: 0.000129\n",
      "Epoch: 17 \tTraining Loss: 0.000129\n",
      "Epoch: 18 \tTraining Loss: 0.000129\n",
      "Epoch: 1 \tTraining Loss: 0.000232\n",
      "Epoch: 2 \tTraining Loss: 0.000232\n",
      "Epoch: 3 \tTraining Loss: 0.000232\n",
      "Epoch: 4 \tTraining Loss: 0.000232\n",
      "Epoch: 5 \tTraining Loss: 0.000232\n",
      "Epoch: 6 \tTraining Loss: 0.000232\n",
      "Epoch: 7 \tTraining Loss: 0.000232\n",
      "Epoch: 8 \tTraining Loss: 0.000232\n",
      "Epoch: 9 \tTraining Loss: 0.000232\n",
      "Epoch: 10 \tTraining Loss: 0.000232\n",
      "Epoch: 11 \tTraining Loss: 0.000232\n",
      "Epoch: 12 \tTraining Loss: 0.000232\n",
      "Epoch: 13 \tTraining Loss: 0.000232\n",
      "Epoch: 14 \tTraining Loss: 0.000232\n",
      "Epoch: 15 \tTraining Loss: 0.000232\n",
      "Epoch: 16 \tTraining Loss: 0.000232\n",
      "Epoch: 17 \tTraining Loss: 0.000232\n",
      "Epoch: 18 \tTraining Loss: 0.000232\n",
      "Epoch: 1 \tTraining Loss: 0.000236\n",
      "Epoch: 2 \tTraining Loss: 0.000236\n",
      "Epoch: 3 \tTraining Loss: 0.000236\n",
      "Epoch: 4 \tTraining Loss: 0.000236\n",
      "Epoch: 5 \tTraining Loss: 0.000236\n",
      "Epoch: 6 \tTraining Loss: 0.000236\n",
      "Epoch: 7 \tTraining Loss: 0.000236\n",
      "Epoch: 8 \tTraining Loss: 0.000236\n",
      "Epoch: 9 \tTraining Loss: 0.000236\n",
      "Epoch: 10 \tTraining Loss: 0.000236\n",
      "Epoch: 11 \tTraining Loss: 0.000236\n",
      "Epoch: 12 \tTraining Loss: 0.000236\n",
      "Epoch: 13 \tTraining Loss: 0.000236\n",
      "Epoch: 14 \tTraining Loss: 0.000236\n",
      "Epoch: 15 \tTraining Loss: 0.000236\n",
      "Epoch: 16 \tTraining Loss: 0.000236\n",
      "Epoch: 17 \tTraining Loss: 0.000236\n",
      "Epoch: 18 \tTraining Loss: 0.000236\n",
      "Epoch: 1 \tTraining Loss: 0.000323\n",
      "Epoch: 2 \tTraining Loss: 0.000323\n",
      "Epoch: 3 \tTraining Loss: 0.000323\n",
      "Epoch: 4 \tTraining Loss: 0.000323\n",
      "Epoch: 5 \tTraining Loss: 0.000323\n",
      "Epoch: 6 \tTraining Loss: 0.000323\n",
      "Epoch: 7 \tTraining Loss: 0.000323\n",
      "Epoch: 8 \tTraining Loss: 0.000323\n",
      "Epoch: 9 \tTraining Loss: 0.000323\n",
      "Epoch: 10 \tTraining Loss: 0.000323\n",
      "Epoch: 11 \tTraining Loss: 0.000323\n",
      "Epoch: 12 \tTraining Loss: 0.000323\n",
      "Epoch: 13 \tTraining Loss: 0.000323\n",
      "Epoch: 14 \tTraining Loss: 0.000323\n",
      "Epoch: 15 \tTraining Loss: 0.000323\n",
      "Epoch: 16 \tTraining Loss: 0.000323\n",
      "Epoch: 17 \tTraining Loss: 0.000323\n",
      "Epoch: 18 \tTraining Loss: 0.000323\n",
      "Epoch: 1 \tTraining Loss: 0.000321\n",
      "Epoch: 2 \tTraining Loss: 0.000321\n",
      "Epoch: 3 \tTraining Loss: 0.000321\n",
      "Epoch: 4 \tTraining Loss: 0.000321\n",
      "Epoch: 5 \tTraining Loss: 0.000321\n",
      "Epoch: 6 \tTraining Loss: 0.000321\n",
      "Epoch: 7 \tTraining Loss: 0.000321\n",
      "Epoch: 8 \tTraining Loss: 0.000321\n",
      "Epoch: 9 \tTraining Loss: 0.000321\n",
      "Epoch: 10 \tTraining Loss: 0.000321\n",
      "Epoch: 11 \tTraining Loss: 0.000321\n",
      "Epoch: 12 \tTraining Loss: 0.000321\n",
      "Epoch: 13 \tTraining Loss: 0.000321\n",
      "Epoch: 14 \tTraining Loss: 0.000321\n",
      "Epoch: 15 \tTraining Loss: 0.000321\n",
      "Epoch: 16 \tTraining Loss: 0.000321\n",
      "Epoch: 17 \tTraining Loss: 0.000321\n",
      "Epoch: 18 \tTraining Loss: 0.000321\n",
      "Epoch: 1 \tTraining Loss: 0.000201\n",
      "Epoch: 2 \tTraining Loss: 0.000201\n",
      "Epoch: 3 \tTraining Loss: 0.000201\n",
      "Epoch: 4 \tTraining Loss: 0.000201\n",
      "Epoch: 5 \tTraining Loss: 0.000201\n",
      "Epoch: 6 \tTraining Loss: 0.000201\n",
      "Epoch: 7 \tTraining Loss: 0.000201\n",
      "Epoch: 8 \tTraining Loss: 0.000201\n",
      "Epoch: 9 \tTraining Loss: 0.000201\n",
      "Epoch: 10 \tTraining Loss: 0.000201\n",
      "Epoch: 11 \tTraining Loss: 0.000201\n",
      "Epoch: 12 \tTraining Loss: 0.000201\n",
      "Epoch: 13 \tTraining Loss: 0.000201\n",
      "Epoch: 14 \tTraining Loss: 0.000201\n",
      "Epoch: 15 \tTraining Loss: 0.000201\n",
      "Epoch: 16 \tTraining Loss: 0.000201\n",
      "Epoch: 17 \tTraining Loss: 0.000201\n",
      "Epoch: 18 \tTraining Loss: 0.000201\n",
      "Epoch: 1 \tTraining Loss: 0.000261\n",
      "Epoch: 2 \tTraining Loss: 0.000261\n",
      "Epoch: 3 \tTraining Loss: 0.000261\n",
      "Epoch: 4 \tTraining Loss: 0.000261\n",
      "Epoch: 5 \tTraining Loss: 0.000261\n",
      "Epoch: 6 \tTraining Loss: 0.000261\n",
      "Epoch: 7 \tTraining Loss: 0.000261\n",
      "Epoch: 8 \tTraining Loss: 0.000261\n",
      "Epoch: 9 \tTraining Loss: 0.000261\n",
      "Epoch: 10 \tTraining Loss: 0.000261\n",
      "Epoch: 11 \tTraining Loss: 0.000261\n",
      "Epoch: 12 \tTraining Loss: 0.000261\n",
      "Epoch: 13 \tTraining Loss: 0.000261\n",
      "Epoch: 14 \tTraining Loss: 0.000261\n",
      "Epoch: 15 \tTraining Loss: 0.000261\n",
      "Epoch: 16 \tTraining Loss: 0.000261\n",
      "Epoch: 17 \tTraining Loss: 0.000261\n",
      "Epoch: 18 \tTraining Loss: 0.000261\n",
      "Epoch: 1 \tTraining Loss: 0.000179\n",
      "Epoch: 2 \tTraining Loss: 0.000179\n",
      "Epoch: 3 \tTraining Loss: 0.000179\n",
      "Epoch: 4 \tTraining Loss: 0.000179\n",
      "Epoch: 5 \tTraining Loss: 0.000179\n",
      "Epoch: 6 \tTraining Loss: 0.000179\n",
      "Epoch: 7 \tTraining Loss: 0.000179\n",
      "Epoch: 8 \tTraining Loss: 0.000179\n",
      "Epoch: 9 \tTraining Loss: 0.000179\n",
      "Epoch: 10 \tTraining Loss: 0.000179\n",
      "Epoch: 11 \tTraining Loss: 0.000179\n",
      "Epoch: 12 \tTraining Loss: 0.000179\n",
      "Epoch: 13 \tTraining Loss: 0.000179\n",
      "Epoch: 14 \tTraining Loss: 0.000179\n",
      "Epoch: 15 \tTraining Loss: 0.000179\n",
      "Epoch: 16 \tTraining Loss: 0.000179\n",
      "Epoch: 17 \tTraining Loss: 0.000179\n",
      "Epoch: 18 \tTraining Loss: 0.000179\n",
      "Epoch: 1 \tTraining Loss: 0.000044\n",
      "Epoch: 2 \tTraining Loss: 0.000044\n",
      "Epoch: 3 \tTraining Loss: 0.000044\n",
      "Epoch: 4 \tTraining Loss: 0.000044\n",
      "Epoch: 5 \tTraining Loss: 0.000044\n",
      "Epoch: 6 \tTraining Loss: 0.000044\n",
      "Epoch: 7 \tTraining Loss: 0.000044\n",
      "Epoch: 8 \tTraining Loss: 0.000044\n",
      "Epoch: 9 \tTraining Loss: 0.000044\n",
      "Epoch: 10 \tTraining Loss: 0.000044\n",
      "Epoch: 11 \tTraining Loss: 0.000044\n",
      "Epoch: 12 \tTraining Loss: 0.000044\n",
      "Epoch: 13 \tTraining Loss: 0.000044\n",
      "Epoch: 14 \tTraining Loss: 0.000044\n",
      "Epoch: 15 \tTraining Loss: 0.000044\n",
      "Epoch: 16 \tTraining Loss: 0.000044\n",
      "Epoch: 17 \tTraining Loss: 0.000044\n",
      "Epoch: 18 \tTraining Loss: 0.000044\n",
      "Epoch: 1 \tTraining Loss: 0.000146\n",
      "Epoch: 2 \tTraining Loss: 0.000146\n",
      "Epoch: 3 \tTraining Loss: 0.000146\n",
      "Epoch: 4 \tTraining Loss: 0.000146\n",
      "Epoch: 5 \tTraining Loss: 0.000146\n",
      "Epoch: 6 \tTraining Loss: 0.000146\n",
      "Epoch: 7 \tTraining Loss: 0.000146\n",
      "Epoch: 8 \tTraining Loss: 0.000146\n",
      "Epoch: 9 \tTraining Loss: 0.000146\n",
      "Epoch: 10 \tTraining Loss: 0.000146\n",
      "Epoch: 11 \tTraining Loss: 0.000146\n",
      "Epoch: 12 \tTraining Loss: 0.000146\n",
      "Epoch: 13 \tTraining Loss: 0.000146\n",
      "Epoch: 14 \tTraining Loss: 0.000146\n",
      "Epoch: 15 \tTraining Loss: 0.000146\n",
      "Epoch: 16 \tTraining Loss: 0.000146\n",
      "Epoch: 17 \tTraining Loss: 0.000146\n",
      "Epoch: 18 \tTraining Loss: 0.000146\n",
      "Epoch: 1 \tTraining Loss: 0.000334\n",
      "Epoch: 2 \tTraining Loss: 0.000334\n",
      "Epoch: 3 \tTraining Loss: 0.000334\n",
      "Epoch: 4 \tTraining Loss: 0.000334\n",
      "Epoch: 5 \tTraining Loss: 0.000334\n",
      "Epoch: 6 \tTraining Loss: 0.000334\n",
      "Epoch: 7 \tTraining Loss: 0.000334\n",
      "Epoch: 8 \tTraining Loss: 0.000334\n",
      "Epoch: 9 \tTraining Loss: 0.000334\n",
      "Epoch: 10 \tTraining Loss: 0.000334\n",
      "Epoch: 11 \tTraining Loss: 0.000334\n",
      "Epoch: 12 \tTraining Loss: 0.000334\n",
      "Epoch: 13 \tTraining Loss: 0.000334\n",
      "Epoch: 14 \tTraining Loss: 0.000334\n",
      "Epoch: 15 \tTraining Loss: 0.000334\n",
      "Epoch: 16 \tTraining Loss: 0.000334\n",
      "Epoch: 17 \tTraining Loss: 0.000334\n",
      "Epoch: 18 \tTraining Loss: 0.000334\n",
      "Epoch: 1 \tTraining Loss: 0.000271\n",
      "Epoch: 2 \tTraining Loss: 0.000271\n",
      "Epoch: 3 \tTraining Loss: 0.000271\n",
      "Epoch: 4 \tTraining Loss: 0.000271\n",
      "Epoch: 5 \tTraining Loss: 0.000271\n",
      "Epoch: 6 \tTraining Loss: 0.000271\n",
      "Epoch: 7 \tTraining Loss: 0.000271\n",
      "Epoch: 8 \tTraining Loss: 0.000271\n",
      "Epoch: 9 \tTraining Loss: 0.000271\n",
      "Epoch: 10 \tTraining Loss: 0.000271\n",
      "Epoch: 11 \tTraining Loss: 0.000271\n",
      "Epoch: 12 \tTraining Loss: 0.000271\n",
      "Epoch: 13 \tTraining Loss: 0.000271\n",
      "Epoch: 14 \tTraining Loss: 0.000271\n",
      "Epoch: 15 \tTraining Loss: 0.000271\n",
      "Epoch: 16 \tTraining Loss: 0.000271\n",
      "Epoch: 17 \tTraining Loss: 0.000271\n",
      "Epoch: 18 \tTraining Loss: 0.000271\n",
      "Epoch: 1 \tTraining Loss: 0.000218\n",
      "Epoch: 2 \tTraining Loss: 0.000218\n",
      "Epoch: 3 \tTraining Loss: 0.000218\n",
      "Epoch: 4 \tTraining Loss: 0.000218\n",
      "Epoch: 5 \tTraining Loss: 0.000218\n",
      "Epoch: 6 \tTraining Loss: 0.000218\n",
      "Epoch: 7 \tTraining Loss: 0.000218\n",
      "Epoch: 8 \tTraining Loss: 0.000218\n",
      "Epoch: 9 \tTraining Loss: 0.000218\n",
      "Epoch: 10 \tTraining Loss: 0.000218\n",
      "Epoch: 11 \tTraining Loss: 0.000218\n",
      "Epoch: 12 \tTraining Loss: 0.000218\n",
      "Epoch: 13 \tTraining Loss: 0.000218\n",
      "Epoch: 14 \tTraining Loss: 0.000218\n",
      "Epoch: 15 \tTraining Loss: 0.000218\n",
      "Epoch: 16 \tTraining Loss: 0.000218\n",
      "Epoch: 17 \tTraining Loss: 0.000218\n",
      "Epoch: 18 \tTraining Loss: 0.000218\n",
      "Epoch: 1 \tTraining Loss: 0.000181\n",
      "Epoch: 2 \tTraining Loss: 0.000181\n",
      "Epoch: 3 \tTraining Loss: 0.000181\n",
      "Epoch: 4 \tTraining Loss: 0.000181\n",
      "Epoch: 5 \tTraining Loss: 0.000181\n",
      "Epoch: 6 \tTraining Loss: 0.000181\n",
      "Epoch: 7 \tTraining Loss: 0.000181\n",
      "Epoch: 8 \tTraining Loss: 0.000181\n",
      "Epoch: 9 \tTraining Loss: 0.000181\n",
      "Epoch: 10 \tTraining Loss: 0.000181\n",
      "Epoch: 11 \tTraining Loss: 0.000181\n",
      "Epoch: 12 \tTraining Loss: 0.000181\n",
      "Epoch: 13 \tTraining Loss: 0.000181\n",
      "Epoch: 14 \tTraining Loss: 0.000181\n",
      "Epoch: 15 \tTraining Loss: 0.000181\n",
      "Epoch: 16 \tTraining Loss: 0.000181\n",
      "Epoch: 17 \tTraining Loss: 0.000181\n",
      "Epoch: 18 \tTraining Loss: 0.000181\n",
      "Epoch: 1 \tTraining Loss: 0.000221\n",
      "Epoch: 2 \tTraining Loss: 0.000221\n",
      "Epoch: 3 \tTraining Loss: 0.000221\n",
      "Epoch: 4 \tTraining Loss: 0.000221\n",
      "Epoch: 5 \tTraining Loss: 0.000221\n",
      "Epoch: 6 \tTraining Loss: 0.000221\n",
      "Epoch: 7 \tTraining Loss: 0.000221\n",
      "Epoch: 8 \tTraining Loss: 0.000221\n",
      "Epoch: 9 \tTraining Loss: 0.000221\n",
      "Epoch: 10 \tTraining Loss: 0.000221\n",
      "Epoch: 11 \tTraining Loss: 0.000221\n",
      "Epoch: 12 \tTraining Loss: 0.000221\n",
      "Epoch: 13 \tTraining Loss: 0.000221\n",
      "Epoch: 14 \tTraining Loss: 0.000221\n",
      "Epoch: 15 \tTraining Loss: 0.000221\n",
      "Epoch: 16 \tTraining Loss: 0.000221\n",
      "Epoch: 17 \tTraining Loss: 0.000221\n",
      "Epoch: 18 \tTraining Loss: 0.000221\n",
      "Epoch: 1 \tTraining Loss: 0.000154\n",
      "Epoch: 2 \tTraining Loss: 0.000154\n",
      "Epoch: 3 \tTraining Loss: 0.000154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \tTraining Loss: 0.000154\n",
      "Epoch: 5 \tTraining Loss: 0.000154\n",
      "Epoch: 6 \tTraining Loss: 0.000154\n",
      "Epoch: 7 \tTraining Loss: 0.000154\n",
      "Epoch: 8 \tTraining Loss: 0.000154\n",
      "Epoch: 9 \tTraining Loss: 0.000154\n",
      "Epoch: 10 \tTraining Loss: 0.000154\n",
      "Epoch: 11 \tTraining Loss: 0.000154\n",
      "Epoch: 12 \tTraining Loss: 0.000154\n",
      "Epoch: 13 \tTraining Loss: 0.000154\n",
      "Epoch: 14 \tTraining Loss: 0.000154\n",
      "Epoch: 15 \tTraining Loss: 0.000154\n",
      "Epoch: 16 \tTraining Loss: 0.000154\n",
      "Epoch: 17 \tTraining Loss: 0.000154\n",
      "Epoch: 18 \tTraining Loss: 0.000154\n",
      "Epoch: 1 \tTraining Loss: 0.000216\n",
      "Epoch: 2 \tTraining Loss: 0.000216\n",
      "Epoch: 3 \tTraining Loss: 0.000216\n",
      "Epoch: 4 \tTraining Loss: 0.000216\n",
      "Epoch: 5 \tTraining Loss: 0.000216\n",
      "Epoch: 6 \tTraining Loss: 0.000216\n",
      "Epoch: 7 \tTraining Loss: 0.000216\n",
      "Epoch: 8 \tTraining Loss: 0.000216\n",
      "Epoch: 9 \tTraining Loss: 0.000216\n",
      "Epoch: 10 \tTraining Loss: 0.000216\n",
      "Epoch: 11 \tTraining Loss: 0.000216\n",
      "Epoch: 12 \tTraining Loss: 0.000216\n",
      "Epoch: 13 \tTraining Loss: 0.000216\n",
      "Epoch: 14 \tTraining Loss: 0.000216\n",
      "Epoch: 15 \tTraining Loss: 0.000216\n",
      "Epoch: 16 \tTraining Loss: 0.000216\n",
      "Epoch: 17 \tTraining Loss: 0.000216\n",
      "Epoch: 18 \tTraining Loss: 0.000216\n",
      "Epoch: 1 \tTraining Loss: 0.000184\n",
      "Epoch: 2 \tTraining Loss: 0.000184\n",
      "Epoch: 3 \tTraining Loss: 0.000184\n",
      "Epoch: 4 \tTraining Loss: 0.000184\n",
      "Epoch: 5 \tTraining Loss: 0.000184\n",
      "Epoch: 6 \tTraining Loss: 0.000184\n",
      "Epoch: 7 \tTraining Loss: 0.000184\n",
      "Epoch: 8 \tTraining Loss: 0.000184\n",
      "Epoch: 9 \tTraining Loss: 0.000184\n",
      "Epoch: 10 \tTraining Loss: 0.000184\n",
      "Epoch: 11 \tTraining Loss: 0.000184\n",
      "Epoch: 12 \tTraining Loss: 0.000184\n",
      "Epoch: 13 \tTraining Loss: 0.000184\n",
      "Epoch: 14 \tTraining Loss: 0.000184\n",
      "Epoch: 15 \tTraining Loss: 0.000184\n",
      "Epoch: 16 \tTraining Loss: 0.000184\n",
      "Epoch: 17 \tTraining Loss: 0.000184\n",
      "Epoch: 18 \tTraining Loss: 0.000184\n",
      "Epoch: 1 \tTraining Loss: 0.000150\n",
      "Epoch: 2 \tTraining Loss: 0.000150\n",
      "Epoch: 3 \tTraining Loss: 0.000150\n",
      "Epoch: 4 \tTraining Loss: 0.000150\n",
      "Epoch: 5 \tTraining Loss: 0.000150\n",
      "Epoch: 6 \tTraining Loss: 0.000150\n",
      "Epoch: 7 \tTraining Loss: 0.000150\n",
      "Epoch: 8 \tTraining Loss: 0.000150\n",
      "Epoch: 9 \tTraining Loss: 0.000150\n",
      "Epoch: 10 \tTraining Loss: 0.000150\n",
      "Epoch: 11 \tTraining Loss: 0.000150\n",
      "Epoch: 12 \tTraining Loss: 0.000150\n",
      "Epoch: 13 \tTraining Loss: 0.000150\n",
      "Epoch: 14 \tTraining Loss: 0.000150\n",
      "Epoch: 15 \tTraining Loss: 0.000150\n",
      "Epoch: 16 \tTraining Loss: 0.000150\n",
      "Epoch: 17 \tTraining Loss: 0.000150\n",
      "Epoch: 18 \tTraining Loss: 0.000150\n",
      "Epoch: 1 \tTraining Loss: 0.000135\n",
      "Epoch: 2 \tTraining Loss: 0.000135\n",
      "Epoch: 3 \tTraining Loss: 0.000135\n",
      "Epoch: 4 \tTraining Loss: 0.000135\n",
      "Epoch: 5 \tTraining Loss: 0.000135\n",
      "Epoch: 6 \tTraining Loss: 0.000135\n",
      "Epoch: 7 \tTraining Loss: 0.000135\n",
      "Epoch: 8 \tTraining Loss: 0.000135\n",
      "Epoch: 9 \tTraining Loss: 0.000135\n",
      "Epoch: 10 \tTraining Loss: 0.000135\n",
      "Epoch: 11 \tTraining Loss: 0.000135\n",
      "Epoch: 12 \tTraining Loss: 0.000135\n",
      "Epoch: 13 \tTraining Loss: 0.000135\n",
      "Epoch: 14 \tTraining Loss: 0.000135\n",
      "Epoch: 15 \tTraining Loss: 0.000135\n",
      "Epoch: 16 \tTraining Loss: 0.000135\n",
      "Epoch: 17 \tTraining Loss: 0.000135\n",
      "Epoch: 18 \tTraining Loss: 0.000135\n",
      "Epoch: 1 \tTraining Loss: 0.000242\n",
      "Epoch: 2 \tTraining Loss: 0.000242\n",
      "Epoch: 3 \tTraining Loss: 0.000242\n",
      "Epoch: 4 \tTraining Loss: 0.000242\n",
      "Epoch: 5 \tTraining Loss: 0.000242\n",
      "Epoch: 6 \tTraining Loss: 0.000242\n",
      "Epoch: 7 \tTraining Loss: 0.000242\n",
      "Epoch: 8 \tTraining Loss: 0.000242\n",
      "Epoch: 9 \tTraining Loss: 0.000242\n",
      "Epoch: 10 \tTraining Loss: 0.000242\n",
      "Epoch: 11 \tTraining Loss: 0.000242\n",
      "Epoch: 12 \tTraining Loss: 0.000242\n",
      "Epoch: 13 \tTraining Loss: 0.000242\n",
      "Epoch: 14 \tTraining Loss: 0.000242\n",
      "Epoch: 15 \tTraining Loss: 0.000242\n",
      "Epoch: 16 \tTraining Loss: 0.000242\n",
      "Epoch: 17 \tTraining Loss: 0.000242\n",
      "Epoch: 18 \tTraining Loss: 0.000242\n",
      "Epoch: 1 \tTraining Loss: 0.000196\n",
      "Epoch: 2 \tTraining Loss: 0.000196\n",
      "Epoch: 3 \tTraining Loss: 0.000196\n",
      "Epoch: 4 \tTraining Loss: 0.000196\n",
      "Epoch: 5 \tTraining Loss: 0.000196\n",
      "Epoch: 6 \tTraining Loss: 0.000196\n",
      "Epoch: 7 \tTraining Loss: 0.000196\n",
      "Epoch: 8 \tTraining Loss: 0.000196\n",
      "Epoch: 9 \tTraining Loss: 0.000196\n",
      "Epoch: 10 \tTraining Loss: 0.000196\n",
      "Epoch: 11 \tTraining Loss: 0.000196\n",
      "Epoch: 12 \tTraining Loss: 0.000196\n",
      "Epoch: 13 \tTraining Loss: 0.000196\n",
      "Epoch: 14 \tTraining Loss: 0.000196\n",
      "Epoch: 15 \tTraining Loss: 0.000196\n",
      "Epoch: 16 \tTraining Loss: 0.000196\n",
      "Epoch: 17 \tTraining Loss: 0.000196\n",
      "Epoch: 18 \tTraining Loss: 0.000196\n",
      "Epoch: 1 \tTraining Loss: 0.000191\n",
      "Epoch: 2 \tTraining Loss: 0.000191\n",
      "Epoch: 3 \tTraining Loss: 0.000191\n",
      "Epoch: 4 \tTraining Loss: 0.000191\n",
      "Epoch: 5 \tTraining Loss: 0.000191\n",
      "Epoch: 6 \tTraining Loss: 0.000191\n",
      "Epoch: 7 \tTraining Loss: 0.000191\n",
      "Epoch: 8 \tTraining Loss: 0.000191\n",
      "Epoch: 9 \tTraining Loss: 0.000191\n",
      "Epoch: 10 \tTraining Loss: 0.000191\n",
      "Epoch: 11 \tTraining Loss: 0.000191\n",
      "Epoch: 12 \tTraining Loss: 0.000191\n",
      "Epoch: 13 \tTraining Loss: 0.000191\n",
      "Epoch: 14 \tTraining Loss: 0.000191\n",
      "Epoch: 15 \tTraining Loss: 0.000191\n",
      "Epoch: 16 \tTraining Loss: 0.000191\n",
      "Epoch: 17 \tTraining Loss: 0.000191\n",
      "Epoch: 18 \tTraining Loss: 0.000191\n",
      "Epoch: 1 \tTraining Loss: 0.000154\n",
      "Epoch: 2 \tTraining Loss: 0.000154\n",
      "Epoch: 3 \tTraining Loss: 0.000154\n",
      "Epoch: 4 \tTraining Loss: 0.000154\n",
      "Epoch: 5 \tTraining Loss: 0.000154\n",
      "Epoch: 6 \tTraining Loss: 0.000154\n",
      "Epoch: 7 \tTraining Loss: 0.000154\n",
      "Epoch: 8 \tTraining Loss: 0.000154\n",
      "Epoch: 9 \tTraining Loss: 0.000154\n",
      "Epoch: 10 \tTraining Loss: 0.000154\n",
      "Epoch: 11 \tTraining Loss: 0.000154\n",
      "Epoch: 12 \tTraining Loss: 0.000154\n",
      "Epoch: 13 \tTraining Loss: 0.000154\n",
      "Epoch: 14 \tTraining Loss: 0.000154\n",
      "Epoch: 15 \tTraining Loss: 0.000154\n",
      "Epoch: 16 \tTraining Loss: 0.000154\n",
      "Epoch: 17 \tTraining Loss: 0.000154\n",
      "Epoch: 18 \tTraining Loss: 0.000154\n",
      "Epoch: 1 \tTraining Loss: 0.000160\n",
      "Epoch: 2 \tTraining Loss: 0.000160\n",
      "Epoch: 3 \tTraining Loss: 0.000160\n",
      "Epoch: 4 \tTraining Loss: 0.000160\n",
      "Epoch: 5 \tTraining Loss: 0.000160\n",
      "Epoch: 6 \tTraining Loss: 0.000160\n",
      "Epoch: 7 \tTraining Loss: 0.000160\n",
      "Epoch: 8 \tTraining Loss: 0.000160\n",
      "Epoch: 9 \tTraining Loss: 0.000160\n",
      "Epoch: 10 \tTraining Loss: 0.000160\n",
      "Epoch: 11 \tTraining Loss: 0.000160\n",
      "Epoch: 12 \tTraining Loss: 0.000160\n",
      "Epoch: 13 \tTraining Loss: 0.000160\n",
      "Epoch: 14 \tTraining Loss: 0.000160\n",
      "Epoch: 15 \tTraining Loss: 0.000160\n",
      "Epoch: 16 \tTraining Loss: 0.000160\n",
      "Epoch: 17 \tTraining Loss: 0.000160\n",
      "Epoch: 18 \tTraining Loss: 0.000160\n",
      "Epoch: 1 \tTraining Loss: 0.000190\n",
      "Epoch: 2 \tTraining Loss: 0.000190\n",
      "Epoch: 3 \tTraining Loss: 0.000190\n",
      "Epoch: 4 \tTraining Loss: 0.000190\n",
      "Epoch: 5 \tTraining Loss: 0.000190\n",
      "Epoch: 6 \tTraining Loss: 0.000190\n",
      "Epoch: 7 \tTraining Loss: 0.000190\n",
      "Epoch: 8 \tTraining Loss: 0.000190\n",
      "Epoch: 9 \tTraining Loss: 0.000190\n",
      "Epoch: 10 \tTraining Loss: 0.000190\n",
      "Epoch: 11 \tTraining Loss: 0.000190\n",
      "Epoch: 12 \tTraining Loss: 0.000190\n",
      "Epoch: 13 \tTraining Loss: 0.000190\n",
      "Epoch: 14 \tTraining Loss: 0.000190\n",
      "Epoch: 15 \tTraining Loss: 0.000190\n",
      "Epoch: 16 \tTraining Loss: 0.000190\n",
      "Epoch: 17 \tTraining Loss: 0.000190\n",
      "Epoch: 18 \tTraining Loss: 0.000190\n",
      "Epoch: 1 \tTraining Loss: 0.000221\n",
      "Epoch: 2 \tTraining Loss: 0.000221\n",
      "Epoch: 3 \tTraining Loss: 0.000221\n",
      "Epoch: 4 \tTraining Loss: 0.000221\n",
      "Epoch: 5 \tTraining Loss: 0.000221\n",
      "Epoch: 6 \tTraining Loss: 0.000221\n",
      "Epoch: 7 \tTraining Loss: 0.000221\n",
      "Epoch: 8 \tTraining Loss: 0.000221\n",
      "Epoch: 9 \tTraining Loss: 0.000221\n",
      "Epoch: 10 \tTraining Loss: 0.000221\n",
      "Epoch: 11 \tTraining Loss: 0.000221\n",
      "Epoch: 12 \tTraining Loss: 0.000221\n",
      "Epoch: 13 \tTraining Loss: 0.000221\n",
      "Epoch: 14 \tTraining Loss: 0.000221\n",
      "Epoch: 15 \tTraining Loss: 0.000221\n",
      "Epoch: 16 \tTraining Loss: 0.000221\n",
      "Epoch: 17 \tTraining Loss: 0.000221\n",
      "Epoch: 18 \tTraining Loss: 0.000221\n",
      "Epoch: 1 \tTraining Loss: 0.000137\n",
      "Epoch: 2 \tTraining Loss: 0.000137\n",
      "Epoch: 3 \tTraining Loss: 0.000137\n",
      "Epoch: 4 \tTraining Loss: 0.000137\n",
      "Epoch: 5 \tTraining Loss: 0.000137\n",
      "Epoch: 6 \tTraining Loss: 0.000137\n",
      "Epoch: 7 \tTraining Loss: 0.000137\n",
      "Epoch: 8 \tTraining Loss: 0.000137\n",
      "Epoch: 9 \tTraining Loss: 0.000137\n",
      "Epoch: 10 \tTraining Loss: 0.000137\n",
      "Epoch: 11 \tTraining Loss: 0.000137\n",
      "Epoch: 12 \tTraining Loss: 0.000137\n",
      "Epoch: 13 \tTraining Loss: 0.000137\n",
      "Epoch: 14 \tTraining Loss: 0.000137\n",
      "Epoch: 15 \tTraining Loss: 0.000137\n",
      "Epoch: 16 \tTraining Loss: 0.000137\n",
      "Epoch: 17 \tTraining Loss: 0.000137\n",
      "Epoch: 18 \tTraining Loss: 0.000137\n",
      "Epoch: 1 \tTraining Loss: 0.000191\n",
      "Epoch: 2 \tTraining Loss: 0.000191\n",
      "Epoch: 3 \tTraining Loss: 0.000191\n",
      "Epoch: 4 \tTraining Loss: 0.000191\n",
      "Epoch: 5 \tTraining Loss: 0.000191\n",
      "Epoch: 6 \tTraining Loss: 0.000191\n",
      "Epoch: 7 \tTraining Loss: 0.000191\n",
      "Epoch: 8 \tTraining Loss: 0.000191\n",
      "Epoch: 9 \tTraining Loss: 0.000191\n",
      "Epoch: 10 \tTraining Loss: 0.000191\n",
      "Epoch: 11 \tTraining Loss: 0.000191\n",
      "Epoch: 12 \tTraining Loss: 0.000191\n",
      "Epoch: 13 \tTraining Loss: 0.000191\n",
      "Epoch: 14 \tTraining Loss: 0.000191\n",
      "Epoch: 15 \tTraining Loss: 0.000191\n",
      "Epoch: 16 \tTraining Loss: 0.000191\n",
      "Epoch: 17 \tTraining Loss: 0.000191\n",
      "Epoch: 18 \tTraining Loss: 0.000191\n",
      "Epoch: 1 \tTraining Loss: 0.000197\n",
      "Epoch: 2 \tTraining Loss: 0.000197\n",
      "Epoch: 3 \tTraining Loss: 0.000197\n",
      "Epoch: 4 \tTraining Loss: 0.000197\n",
      "Epoch: 5 \tTraining Loss: 0.000197\n",
      "Epoch: 6 \tTraining Loss: 0.000197\n",
      "Epoch: 7 \tTraining Loss: 0.000197\n",
      "Epoch: 8 \tTraining Loss: 0.000197\n",
      "Epoch: 9 \tTraining Loss: 0.000197\n",
      "Epoch: 10 \tTraining Loss: 0.000197\n",
      "Epoch: 11 \tTraining Loss: 0.000197\n",
      "Epoch: 12 \tTraining Loss: 0.000197\n",
      "Epoch: 13 \tTraining Loss: 0.000197\n",
      "Epoch: 14 \tTraining Loss: 0.000197\n",
      "Epoch: 15 \tTraining Loss: 0.000197\n",
      "Epoch: 16 \tTraining Loss: 0.000197\n",
      "Epoch: 17 \tTraining Loss: 0.000197\n",
      "Epoch: 18 \tTraining Loss: 0.000197\n",
      "Epoch: 1 \tTraining Loss: 0.000185\n",
      "Epoch: 2 \tTraining Loss: 0.000185\n",
      "Epoch: 3 \tTraining Loss: 0.000185\n",
      "Epoch: 4 \tTraining Loss: 0.000185\n",
      "Epoch: 5 \tTraining Loss: 0.000185\n",
      "Epoch: 6 \tTraining Loss: 0.000185\n",
      "Epoch: 7 \tTraining Loss: 0.000185\n",
      "Epoch: 8 \tTraining Loss: 0.000185\n",
      "Epoch: 9 \tTraining Loss: 0.000185\n",
      "Epoch: 10 \tTraining Loss: 0.000185\n",
      "Epoch: 11 \tTraining Loss: 0.000185\n",
      "Epoch: 12 \tTraining Loss: 0.000185\n",
      "Epoch: 13 \tTraining Loss: 0.000185\n",
      "Epoch: 14 \tTraining Loss: 0.000185\n",
      "Epoch: 15 \tTraining Loss: 0.000185\n",
      "Epoch: 16 \tTraining Loss: 0.000185\n",
      "Epoch: 17 \tTraining Loss: 0.000185\n",
      "Epoch: 18 \tTraining Loss: 0.000185\n",
      "Epoch: 1 \tTraining Loss: 0.000138\n",
      "Epoch: 2 \tTraining Loss: 0.000138\n",
      "Epoch: 3 \tTraining Loss: 0.000138\n",
      "Epoch: 4 \tTraining Loss: 0.000138\n",
      "Epoch: 5 \tTraining Loss: 0.000138\n",
      "Epoch: 6 \tTraining Loss: 0.000138\n",
      "Epoch: 7 \tTraining Loss: 0.000138\n",
      "Epoch: 8 \tTraining Loss: 0.000138\n",
      "Epoch: 9 \tTraining Loss: 0.000138\n",
      "Epoch: 10 \tTraining Loss: 0.000138\n",
      "Epoch: 11 \tTraining Loss: 0.000138\n",
      "Epoch: 12 \tTraining Loss: 0.000138\n",
      "Epoch: 13 \tTraining Loss: 0.000138\n",
      "Epoch: 14 \tTraining Loss: 0.000138\n",
      "Epoch: 15 \tTraining Loss: 0.000138\n",
      "Epoch: 16 \tTraining Loss: 0.000138\n",
      "Epoch: 17 \tTraining Loss: 0.000138\n",
      "Epoch: 18 \tTraining Loss: 0.000138\n",
      "Epoch: 1 \tTraining Loss: 0.000177\n",
      "Epoch: 2 \tTraining Loss: 0.000177\n",
      "Epoch: 3 \tTraining Loss: 0.000177\n",
      "Epoch: 4 \tTraining Loss: 0.000177\n",
      "Epoch: 5 \tTraining Loss: 0.000177\n",
      "Epoch: 6 \tTraining Loss: 0.000177\n",
      "Epoch: 7 \tTraining Loss: 0.000177\n",
      "Epoch: 8 \tTraining Loss: 0.000177\n",
      "Epoch: 9 \tTraining Loss: 0.000177\n",
      "Epoch: 10 \tTraining Loss: 0.000177\n",
      "Epoch: 11 \tTraining Loss: 0.000177\n",
      "Epoch: 12 \tTraining Loss: 0.000177\n",
      "Epoch: 13 \tTraining Loss: 0.000177\n",
      "Epoch: 14 \tTraining Loss: 0.000177\n",
      "Epoch: 15 \tTraining Loss: 0.000177\n",
      "Epoch: 16 \tTraining Loss: 0.000177\n",
      "Epoch: 17 \tTraining Loss: 0.000177\n",
      "Epoch: 18 \tTraining Loss: 0.000177\n",
      "Epoch: 1 \tTraining Loss: 0.000333\n",
      "Epoch: 2 \tTraining Loss: 0.000333\n",
      "Epoch: 3 \tTraining Loss: 0.000333\n",
      "Epoch: 4 \tTraining Loss: 0.000333\n",
      "Epoch: 5 \tTraining Loss: 0.000333\n",
      "Epoch: 6 \tTraining Loss: 0.000333\n",
      "Epoch: 7 \tTraining Loss: 0.000333\n",
      "Epoch: 8 \tTraining Loss: 0.000333\n",
      "Epoch: 9 \tTraining Loss: 0.000333\n",
      "Epoch: 10 \tTraining Loss: 0.000333\n",
      "Epoch: 11 \tTraining Loss: 0.000333\n",
      "Epoch: 12 \tTraining Loss: 0.000333\n",
      "Epoch: 13 \tTraining Loss: 0.000333\n",
      "Epoch: 14 \tTraining Loss: 0.000333\n",
      "Epoch: 15 \tTraining Loss: 0.000333\n",
      "Epoch: 16 \tTraining Loss: 0.000333\n",
      "Epoch: 17 \tTraining Loss: 0.000333\n",
      "Epoch: 18 \tTraining Loss: 0.000333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000104\n",
      "Epoch: 2 \tTraining Loss: 0.000104\n",
      "Epoch: 3 \tTraining Loss: 0.000104\n",
      "Epoch: 4 \tTraining Loss: 0.000104\n",
      "Epoch: 5 \tTraining Loss: 0.000104\n",
      "Epoch: 6 \tTraining Loss: 0.000104\n",
      "Epoch: 7 \tTraining Loss: 0.000104\n",
      "Epoch: 8 \tTraining Loss: 0.000104\n",
      "Epoch: 9 \tTraining Loss: 0.000104\n",
      "Epoch: 10 \tTraining Loss: 0.000104\n",
      "Epoch: 11 \tTraining Loss: 0.000104\n",
      "Epoch: 12 \tTraining Loss: 0.000104\n",
      "Epoch: 13 \tTraining Loss: 0.000104\n",
      "Epoch: 14 \tTraining Loss: 0.000104\n",
      "Epoch: 15 \tTraining Loss: 0.000104\n",
      "Epoch: 16 \tTraining Loss: 0.000104\n",
      "Epoch: 17 \tTraining Loss: 0.000104\n",
      "Epoch: 18 \tTraining Loss: 0.000104\n",
      "Epoch: 1 \tTraining Loss: 0.000074\n",
      "Epoch: 2 \tTraining Loss: 0.000074\n",
      "Epoch: 3 \tTraining Loss: 0.000074\n",
      "Epoch: 4 \tTraining Loss: 0.000074\n",
      "Epoch: 5 \tTraining Loss: 0.000074\n",
      "Epoch: 6 \tTraining Loss: 0.000074\n",
      "Epoch: 7 \tTraining Loss: 0.000074\n",
      "Epoch: 8 \tTraining Loss: 0.000074\n",
      "Epoch: 9 \tTraining Loss: 0.000074\n",
      "Epoch: 10 \tTraining Loss: 0.000074\n",
      "Epoch: 11 \tTraining Loss: 0.000074\n",
      "Epoch: 12 \tTraining Loss: 0.000074\n",
      "Epoch: 13 \tTraining Loss: 0.000074\n",
      "Epoch: 14 \tTraining Loss: 0.000074\n",
      "Epoch: 15 \tTraining Loss: 0.000074\n",
      "Epoch: 16 \tTraining Loss: 0.000074\n",
      "Epoch: 17 \tTraining Loss: 0.000074\n",
      "Epoch: 18 \tTraining Loss: 0.000074\n",
      "Epoch: 1 \tTraining Loss: 0.000301\n",
      "Epoch: 2 \tTraining Loss: 0.000301\n",
      "Epoch: 3 \tTraining Loss: 0.000301\n",
      "Epoch: 4 \tTraining Loss: 0.000301\n",
      "Epoch: 5 \tTraining Loss: 0.000301\n",
      "Epoch: 6 \tTraining Loss: 0.000301\n",
      "Epoch: 7 \tTraining Loss: 0.000301\n",
      "Epoch: 8 \tTraining Loss: 0.000301\n",
      "Epoch: 9 \tTraining Loss: 0.000301\n",
      "Epoch: 10 \tTraining Loss: 0.000301\n",
      "Epoch: 11 \tTraining Loss: 0.000301\n",
      "Epoch: 12 \tTraining Loss: 0.000301\n",
      "Epoch: 13 \tTraining Loss: 0.000301\n",
      "Epoch: 14 \tTraining Loss: 0.000301\n",
      "Epoch: 15 \tTraining Loss: 0.000301\n",
      "Epoch: 16 \tTraining Loss: 0.000301\n",
      "Epoch: 17 \tTraining Loss: 0.000301\n",
      "Epoch: 18 \tTraining Loss: 0.000301\n",
      "Epoch: 1 \tTraining Loss: 0.000178\n",
      "Epoch: 2 \tTraining Loss: 0.000178\n",
      "Epoch: 3 \tTraining Loss: 0.000178\n",
      "Epoch: 4 \tTraining Loss: 0.000178\n",
      "Epoch: 5 \tTraining Loss: 0.000178\n",
      "Epoch: 6 \tTraining Loss: 0.000178\n",
      "Epoch: 7 \tTraining Loss: 0.000178\n",
      "Epoch: 8 \tTraining Loss: 0.000178\n",
      "Epoch: 9 \tTraining Loss: 0.000178\n",
      "Epoch: 10 \tTraining Loss: 0.000178\n",
      "Epoch: 11 \tTraining Loss: 0.000178\n",
      "Epoch: 12 \tTraining Loss: 0.000178\n",
      "Epoch: 13 \tTraining Loss: 0.000178\n",
      "Epoch: 14 \tTraining Loss: 0.000178\n",
      "Epoch: 15 \tTraining Loss: 0.000178\n",
      "Epoch: 16 \tTraining Loss: 0.000178\n",
      "Epoch: 17 \tTraining Loss: 0.000178\n",
      "Epoch: 18 \tTraining Loss: 0.000178\n",
      "Epoch: 1 \tTraining Loss: 0.000324\n",
      "Epoch: 2 \tTraining Loss: 0.000324\n",
      "Epoch: 3 \tTraining Loss: 0.000324\n",
      "Epoch: 4 \tTraining Loss: 0.000324\n",
      "Epoch: 5 \tTraining Loss: 0.000324\n",
      "Epoch: 6 \tTraining Loss: 0.000324\n",
      "Epoch: 7 \tTraining Loss: 0.000324\n",
      "Epoch: 8 \tTraining Loss: 0.000324\n",
      "Epoch: 9 \tTraining Loss: 0.000324\n",
      "Epoch: 10 \tTraining Loss: 0.000324\n",
      "Epoch: 11 \tTraining Loss: 0.000324\n",
      "Epoch: 12 \tTraining Loss: 0.000324\n",
      "Epoch: 13 \tTraining Loss: 0.000324\n",
      "Epoch: 14 \tTraining Loss: 0.000324\n",
      "Epoch: 15 \tTraining Loss: 0.000324\n",
      "Epoch: 16 \tTraining Loss: 0.000324\n",
      "Epoch: 17 \tTraining Loss: 0.000324\n",
      "Epoch: 18 \tTraining Loss: 0.000324\n",
      "Epoch: 1 \tTraining Loss: 0.000149\n",
      "Epoch: 2 \tTraining Loss: 0.000149\n",
      "Epoch: 3 \tTraining Loss: 0.000149\n",
      "Epoch: 4 \tTraining Loss: 0.000149\n",
      "Epoch: 5 \tTraining Loss: 0.000149\n",
      "Epoch: 6 \tTraining Loss: 0.000149\n",
      "Epoch: 7 \tTraining Loss: 0.000149\n",
      "Epoch: 8 \tTraining Loss: 0.000149\n",
      "Epoch: 9 \tTraining Loss: 0.000149\n",
      "Epoch: 10 \tTraining Loss: 0.000149\n",
      "Epoch: 11 \tTraining Loss: 0.000149\n",
      "Epoch: 12 \tTraining Loss: 0.000149\n",
      "Epoch: 13 \tTraining Loss: 0.000149\n",
      "Epoch: 14 \tTraining Loss: 0.000149\n",
      "Epoch: 15 \tTraining Loss: 0.000149\n",
      "Epoch: 16 \tTraining Loss: 0.000149\n",
      "Epoch: 17 \tTraining Loss: 0.000149\n",
      "Epoch: 18 \tTraining Loss: 0.000149\n",
      "Epoch: 1 \tTraining Loss: 0.000218\n",
      "Epoch: 2 \tTraining Loss: 0.000218\n",
      "Epoch: 3 \tTraining Loss: 0.000218\n",
      "Epoch: 4 \tTraining Loss: 0.000218\n",
      "Epoch: 5 \tTraining Loss: 0.000218\n",
      "Epoch: 6 \tTraining Loss: 0.000218\n",
      "Epoch: 7 \tTraining Loss: 0.000218\n",
      "Epoch: 8 \tTraining Loss: 0.000218\n",
      "Epoch: 9 \tTraining Loss: 0.000218\n",
      "Epoch: 10 \tTraining Loss: 0.000218\n",
      "Epoch: 11 \tTraining Loss: 0.000218\n",
      "Epoch: 12 \tTraining Loss: 0.000218\n",
      "Epoch: 13 \tTraining Loss: 0.000218\n",
      "Epoch: 14 \tTraining Loss: 0.000218\n",
      "Epoch: 15 \tTraining Loss: 0.000218\n",
      "Epoch: 16 \tTraining Loss: 0.000218\n",
      "Epoch: 17 \tTraining Loss: 0.000218\n",
      "Epoch: 18 \tTraining Loss: 0.000218\n",
      "Epoch: 1 \tTraining Loss: 0.000299\n",
      "Epoch: 2 \tTraining Loss: 0.000299\n",
      "Epoch: 3 \tTraining Loss: 0.000299\n",
      "Epoch: 4 \tTraining Loss: 0.000299\n",
      "Epoch: 5 \tTraining Loss: 0.000299\n",
      "Epoch: 6 \tTraining Loss: 0.000299\n",
      "Epoch: 7 \tTraining Loss: 0.000299\n",
      "Epoch: 8 \tTraining Loss: 0.000299\n",
      "Epoch: 9 \tTraining Loss: 0.000299\n",
      "Epoch: 10 \tTraining Loss: 0.000299\n",
      "Epoch: 11 \tTraining Loss: 0.000299\n",
      "Epoch: 12 \tTraining Loss: 0.000299\n",
      "Epoch: 13 \tTraining Loss: 0.000299\n",
      "Epoch: 14 \tTraining Loss: 0.000299\n",
      "Epoch: 15 \tTraining Loss: 0.000299\n",
      "Epoch: 16 \tTraining Loss: 0.000299\n",
      "Epoch: 17 \tTraining Loss: 0.000299\n",
      "Epoch: 18 \tTraining Loss: 0.000299\n",
      "Epoch: 1 \tTraining Loss: 0.000108\n",
      "Epoch: 2 \tTraining Loss: 0.000108\n",
      "Epoch: 3 \tTraining Loss: 0.000108\n",
      "Epoch: 4 \tTraining Loss: 0.000108\n",
      "Epoch: 5 \tTraining Loss: 0.000108\n",
      "Epoch: 6 \tTraining Loss: 0.000108\n",
      "Epoch: 7 \tTraining Loss: 0.000108\n",
      "Epoch: 8 \tTraining Loss: 0.000108\n",
      "Epoch: 9 \tTraining Loss: 0.000108\n",
      "Epoch: 10 \tTraining Loss: 0.000108\n",
      "Epoch: 11 \tTraining Loss: 0.000108\n",
      "Epoch: 12 \tTraining Loss: 0.000108\n",
      "Epoch: 13 \tTraining Loss: 0.000108\n",
      "Epoch: 14 \tTraining Loss: 0.000108\n",
      "Epoch: 15 \tTraining Loss: 0.000108\n",
      "Epoch: 16 \tTraining Loss: 0.000108\n",
      "Epoch: 17 \tTraining Loss: 0.000108\n",
      "Epoch: 18 \tTraining Loss: 0.000108\n",
      "Epoch: 1 \tTraining Loss: 0.000201\n",
      "Epoch: 2 \tTraining Loss: 0.000201\n",
      "Epoch: 3 \tTraining Loss: 0.000201\n",
      "Epoch: 4 \tTraining Loss: 0.000201\n",
      "Epoch: 5 \tTraining Loss: 0.000201\n",
      "Epoch: 6 \tTraining Loss: 0.000201\n",
      "Epoch: 7 \tTraining Loss: 0.000201\n",
      "Epoch: 8 \tTraining Loss: 0.000201\n",
      "Epoch: 9 \tTraining Loss: 0.000201\n",
      "Epoch: 10 \tTraining Loss: 0.000201\n",
      "Epoch: 11 \tTraining Loss: 0.000201\n",
      "Epoch: 12 \tTraining Loss: 0.000201\n",
      "Epoch: 13 \tTraining Loss: 0.000201\n",
      "Epoch: 14 \tTraining Loss: 0.000201\n",
      "Epoch: 15 \tTraining Loss: 0.000201\n",
      "Epoch: 16 \tTraining Loss: 0.000201\n",
      "Epoch: 17 \tTraining Loss: 0.000201\n",
      "Epoch: 18 \tTraining Loss: 0.000201\n",
      "Epoch: 1 \tTraining Loss: 0.000100\n",
      "Epoch: 2 \tTraining Loss: 0.000100\n",
      "Epoch: 3 \tTraining Loss: 0.000100\n",
      "Epoch: 4 \tTraining Loss: 0.000100\n",
      "Epoch: 5 \tTraining Loss: 0.000100\n",
      "Epoch: 6 \tTraining Loss: 0.000100\n",
      "Epoch: 7 \tTraining Loss: 0.000100\n",
      "Epoch: 8 \tTraining Loss: 0.000100\n",
      "Epoch: 9 \tTraining Loss: 0.000100\n",
      "Epoch: 10 \tTraining Loss: 0.000100\n",
      "Epoch: 11 \tTraining Loss: 0.000100\n",
      "Epoch: 12 \tTraining Loss: 0.000100\n",
      "Epoch: 13 \tTraining Loss: 0.000100\n",
      "Epoch: 14 \tTraining Loss: 0.000100\n",
      "Epoch: 15 \tTraining Loss: 0.000100\n",
      "Epoch: 16 \tTraining Loss: 0.000100\n",
      "Epoch: 17 \tTraining Loss: 0.000100\n",
      "Epoch: 18 \tTraining Loss: 0.000100\n",
      "Epoch: 1 \tTraining Loss: 0.000258\n",
      "Epoch: 2 \tTraining Loss: 0.000258\n",
      "Epoch: 3 \tTraining Loss: 0.000258\n",
      "Epoch: 4 \tTraining Loss: 0.000258\n",
      "Epoch: 5 \tTraining Loss: 0.000258\n",
      "Epoch: 6 \tTraining Loss: 0.000258\n",
      "Epoch: 7 \tTraining Loss: 0.000258\n",
      "Epoch: 8 \tTraining Loss: 0.000258\n",
      "Epoch: 9 \tTraining Loss: 0.000258\n",
      "Epoch: 10 \tTraining Loss: 0.000258\n",
      "Epoch: 11 \tTraining Loss: 0.000258\n",
      "Epoch: 12 \tTraining Loss: 0.000258\n",
      "Epoch: 13 \tTraining Loss: 0.000258\n",
      "Epoch: 14 \tTraining Loss: 0.000258\n",
      "Epoch: 15 \tTraining Loss: 0.000258\n",
      "Epoch: 16 \tTraining Loss: 0.000258\n",
      "Epoch: 17 \tTraining Loss: 0.000258\n",
      "Epoch: 18 \tTraining Loss: 0.000258\n",
      "Epoch: 1 \tTraining Loss: 0.000293\n",
      "Epoch: 2 \tTraining Loss: 0.000293\n",
      "Epoch: 3 \tTraining Loss: 0.000293\n",
      "Epoch: 4 \tTraining Loss: 0.000293\n",
      "Epoch: 5 \tTraining Loss: 0.000293\n",
      "Epoch: 6 \tTraining Loss: 0.000293\n",
      "Epoch: 7 \tTraining Loss: 0.000293\n",
      "Epoch: 8 \tTraining Loss: 0.000293\n",
      "Epoch: 9 \tTraining Loss: 0.000293\n",
      "Epoch: 10 \tTraining Loss: 0.000293\n",
      "Epoch: 11 \tTraining Loss: 0.000293\n",
      "Epoch: 12 \tTraining Loss: 0.000293\n",
      "Epoch: 13 \tTraining Loss: 0.000293\n",
      "Epoch: 14 \tTraining Loss: 0.000293\n",
      "Epoch: 15 \tTraining Loss: 0.000293\n",
      "Epoch: 16 \tTraining Loss: 0.000293\n",
      "Epoch: 17 \tTraining Loss: 0.000293\n",
      "Epoch: 18 \tTraining Loss: 0.000293\n",
      "Epoch: 1 \tTraining Loss: 0.000231\n",
      "Epoch: 2 \tTraining Loss: 0.000231\n",
      "Epoch: 3 \tTraining Loss: 0.000231\n",
      "Epoch: 4 \tTraining Loss: 0.000231\n",
      "Epoch: 5 \tTraining Loss: 0.000231\n",
      "Epoch: 6 \tTraining Loss: 0.000231\n",
      "Epoch: 7 \tTraining Loss: 0.000231\n",
      "Epoch: 8 \tTraining Loss: 0.000231\n",
      "Epoch: 9 \tTraining Loss: 0.000231\n",
      "Epoch: 10 \tTraining Loss: 0.000231\n",
      "Epoch: 11 \tTraining Loss: 0.000231\n",
      "Epoch: 12 \tTraining Loss: 0.000231\n",
      "Epoch: 13 \tTraining Loss: 0.000231\n",
      "Epoch: 14 \tTraining Loss: 0.000231\n",
      "Epoch: 15 \tTraining Loss: 0.000231\n",
      "Epoch: 16 \tTraining Loss: 0.000231\n",
      "Epoch: 17 \tTraining Loss: 0.000231\n",
      "Epoch: 18 \tTraining Loss: 0.000231\n",
      "Epoch: 1 \tTraining Loss: 0.000182\n",
      "Epoch: 2 \tTraining Loss: 0.000182\n",
      "Epoch: 3 \tTraining Loss: 0.000182\n",
      "Epoch: 4 \tTraining Loss: 0.000182\n",
      "Epoch: 5 \tTraining Loss: 0.000182\n",
      "Epoch: 6 \tTraining Loss: 0.000182\n",
      "Epoch: 7 \tTraining Loss: 0.000182\n",
      "Epoch: 8 \tTraining Loss: 0.000182\n",
      "Epoch: 9 \tTraining Loss: 0.000182\n",
      "Epoch: 10 \tTraining Loss: 0.000182\n",
      "Epoch: 11 \tTraining Loss: 0.000182\n",
      "Epoch: 12 \tTraining Loss: 0.000182\n",
      "Epoch: 13 \tTraining Loss: 0.000182\n",
      "Epoch: 14 \tTraining Loss: 0.000182\n",
      "Epoch: 15 \tTraining Loss: 0.000182\n",
      "Epoch: 16 \tTraining Loss: 0.000182\n",
      "Epoch: 17 \tTraining Loss: 0.000182\n",
      "Epoch: 18 \tTraining Loss: 0.000182\n",
      "Epoch: 1 \tTraining Loss: 0.000188\n",
      "Epoch: 2 \tTraining Loss: 0.000188\n",
      "Epoch: 3 \tTraining Loss: 0.000188\n",
      "Epoch: 4 \tTraining Loss: 0.000188\n",
      "Epoch: 5 \tTraining Loss: 0.000188\n",
      "Epoch: 6 \tTraining Loss: 0.000188\n",
      "Epoch: 7 \tTraining Loss: 0.000188\n",
      "Epoch: 8 \tTraining Loss: 0.000188\n",
      "Epoch: 9 \tTraining Loss: 0.000188\n",
      "Epoch: 10 \tTraining Loss: 0.000188\n",
      "Epoch: 11 \tTraining Loss: 0.000188\n",
      "Epoch: 12 \tTraining Loss: 0.000188\n",
      "Epoch: 13 \tTraining Loss: 0.000188\n",
      "Epoch: 14 \tTraining Loss: 0.000188\n",
      "Epoch: 15 \tTraining Loss: 0.000188\n",
      "Epoch: 16 \tTraining Loss: 0.000188\n",
      "Epoch: 17 \tTraining Loss: 0.000188\n",
      "Epoch: 18 \tTraining Loss: 0.000188\n",
      "Epoch: 1 \tTraining Loss: 0.000180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \tTraining Loss: 0.000180\n",
      "Epoch: 3 \tTraining Loss: 0.000180\n",
      "Epoch: 4 \tTraining Loss: 0.000180\n",
      "Epoch: 5 \tTraining Loss: 0.000180\n",
      "Epoch: 6 \tTraining Loss: 0.000180\n",
      "Epoch: 7 \tTraining Loss: 0.000180\n",
      "Epoch: 8 \tTraining Loss: 0.000180\n",
      "Epoch: 9 \tTraining Loss: 0.000180\n",
      "Epoch: 10 \tTraining Loss: 0.000180\n",
      "Epoch: 11 \tTraining Loss: 0.000180\n",
      "Epoch: 12 \tTraining Loss: 0.000180\n",
      "Epoch: 13 \tTraining Loss: 0.000180\n",
      "Epoch: 14 \tTraining Loss: 0.000180\n",
      "Epoch: 15 \tTraining Loss: 0.000180\n",
      "Epoch: 16 \tTraining Loss: 0.000180\n",
      "Epoch: 17 \tTraining Loss: 0.000180\n",
      "Epoch: 18 \tTraining Loss: 0.000180\n",
      "Epoch: 1 \tTraining Loss: 0.000161\n",
      "Epoch: 2 \tTraining Loss: 0.000161\n",
      "Epoch: 3 \tTraining Loss: 0.000161\n",
      "Epoch: 4 \tTraining Loss: 0.000161\n",
      "Epoch: 5 \tTraining Loss: 0.000161\n",
      "Epoch: 6 \tTraining Loss: 0.000161\n",
      "Epoch: 7 \tTraining Loss: 0.000161\n",
      "Epoch: 8 \tTraining Loss: 0.000161\n",
      "Epoch: 9 \tTraining Loss: 0.000161\n",
      "Epoch: 10 \tTraining Loss: 0.000161\n",
      "Epoch: 11 \tTraining Loss: 0.000161\n",
      "Epoch: 12 \tTraining Loss: 0.000161\n",
      "Epoch: 13 \tTraining Loss: 0.000161\n",
      "Epoch: 14 \tTraining Loss: 0.000161\n",
      "Epoch: 15 \tTraining Loss: 0.000161\n",
      "Epoch: 16 \tTraining Loss: 0.000161\n",
      "Epoch: 17 \tTraining Loss: 0.000161\n",
      "Epoch: 18 \tTraining Loss: 0.000161\n",
      "Epoch: 1 \tTraining Loss: 0.000167\n",
      "Epoch: 2 \tTraining Loss: 0.000167\n",
      "Epoch: 3 \tTraining Loss: 0.000167\n",
      "Epoch: 4 \tTraining Loss: 0.000167\n",
      "Epoch: 5 \tTraining Loss: 0.000167\n",
      "Epoch: 6 \tTraining Loss: 0.000167\n",
      "Epoch: 7 \tTraining Loss: 0.000167\n",
      "Epoch: 8 \tTraining Loss: 0.000167\n",
      "Epoch: 9 \tTraining Loss: 0.000167\n",
      "Epoch: 10 \tTraining Loss: 0.000167\n",
      "Epoch: 11 \tTraining Loss: 0.000167\n",
      "Epoch: 12 \tTraining Loss: 0.000167\n",
      "Epoch: 13 \tTraining Loss: 0.000167\n",
      "Epoch: 14 \tTraining Loss: 0.000167\n",
      "Epoch: 15 \tTraining Loss: 0.000167\n",
      "Epoch: 16 \tTraining Loss: 0.000167\n",
      "Epoch: 17 \tTraining Loss: 0.000167\n",
      "Epoch: 18 \tTraining Loss: 0.000167\n",
      "Epoch: 1 \tTraining Loss: 0.000239\n",
      "Epoch: 2 \tTraining Loss: 0.000239\n",
      "Epoch: 3 \tTraining Loss: 0.000239\n",
      "Epoch: 4 \tTraining Loss: 0.000239\n",
      "Epoch: 5 \tTraining Loss: 0.000239\n",
      "Epoch: 6 \tTraining Loss: 0.000239\n",
      "Epoch: 7 \tTraining Loss: 0.000239\n",
      "Epoch: 8 \tTraining Loss: 0.000239\n",
      "Epoch: 9 \tTraining Loss: 0.000239\n",
      "Epoch: 10 \tTraining Loss: 0.000239\n",
      "Epoch: 11 \tTraining Loss: 0.000239\n",
      "Epoch: 12 \tTraining Loss: 0.000239\n",
      "Epoch: 13 \tTraining Loss: 0.000239\n",
      "Epoch: 14 \tTraining Loss: 0.000239\n",
      "Epoch: 15 \tTraining Loss: 0.000239\n",
      "Epoch: 16 \tTraining Loss: 0.000239\n",
      "Epoch: 17 \tTraining Loss: 0.000239\n",
      "Epoch: 18 \tTraining Loss: 0.000239\n",
      "Epoch: 1 \tTraining Loss: 0.000228\n",
      "Epoch: 2 \tTraining Loss: 0.000228\n",
      "Epoch: 3 \tTraining Loss: 0.000228\n",
      "Epoch: 4 \tTraining Loss: 0.000228\n",
      "Epoch: 5 \tTraining Loss: 0.000228\n",
      "Epoch: 6 \tTraining Loss: 0.000228\n",
      "Epoch: 7 \tTraining Loss: 0.000228\n",
      "Epoch: 8 \tTraining Loss: 0.000228\n",
      "Epoch: 9 \tTraining Loss: 0.000228\n",
      "Epoch: 10 \tTraining Loss: 0.000228\n",
      "Epoch: 11 \tTraining Loss: 0.000228\n",
      "Epoch: 12 \tTraining Loss: 0.000228\n",
      "Epoch: 13 \tTraining Loss: 0.000228\n",
      "Epoch: 14 \tTraining Loss: 0.000228\n",
      "Epoch: 15 \tTraining Loss: 0.000228\n",
      "Epoch: 16 \tTraining Loss: 0.000228\n",
      "Epoch: 17 \tTraining Loss: 0.000228\n",
      "Epoch: 18 \tTraining Loss: 0.000228\n",
      "Epoch: 1 \tTraining Loss: 0.000198\n",
      "Epoch: 2 \tTraining Loss: 0.000198\n",
      "Epoch: 3 \tTraining Loss: 0.000198\n",
      "Epoch: 4 \tTraining Loss: 0.000198\n",
      "Epoch: 5 \tTraining Loss: 0.000198\n",
      "Epoch: 6 \tTraining Loss: 0.000198\n",
      "Epoch: 7 \tTraining Loss: 0.000198\n",
      "Epoch: 8 \tTraining Loss: 0.000198\n",
      "Epoch: 9 \tTraining Loss: 0.000198\n",
      "Epoch: 10 \tTraining Loss: 0.000198\n",
      "Epoch: 11 \tTraining Loss: 0.000198\n",
      "Epoch: 12 \tTraining Loss: 0.000198\n",
      "Epoch: 13 \tTraining Loss: 0.000198\n",
      "Epoch: 14 \tTraining Loss: 0.000198\n",
      "Epoch: 15 \tTraining Loss: 0.000198\n",
      "Epoch: 16 \tTraining Loss: 0.000198\n",
      "Epoch: 17 \tTraining Loss: 0.000198\n",
      "Epoch: 18 \tTraining Loss: 0.000198\n",
      "Epoch: 1 \tTraining Loss: 0.000233\n",
      "Epoch: 2 \tTraining Loss: 0.000233\n",
      "Epoch: 3 \tTraining Loss: 0.000233\n",
      "Epoch: 4 \tTraining Loss: 0.000233\n",
      "Epoch: 5 \tTraining Loss: 0.000233\n",
      "Epoch: 6 \tTraining Loss: 0.000233\n",
      "Epoch: 7 \tTraining Loss: 0.000233\n",
      "Epoch: 8 \tTraining Loss: 0.000233\n",
      "Epoch: 9 \tTraining Loss: 0.000233\n",
      "Epoch: 10 \tTraining Loss: 0.000233\n",
      "Epoch: 11 \tTraining Loss: 0.000233\n",
      "Epoch: 12 \tTraining Loss: 0.000233\n",
      "Epoch: 13 \tTraining Loss: 0.000233\n",
      "Epoch: 14 \tTraining Loss: 0.000233\n",
      "Epoch: 15 \tTraining Loss: 0.000233\n",
      "Epoch: 16 \tTraining Loss: 0.000233\n",
      "Epoch: 17 \tTraining Loss: 0.000233\n",
      "Epoch: 18 \tTraining Loss: 0.000233\n",
      "Epoch: 1 \tTraining Loss: 0.000190\n",
      "Epoch: 2 \tTraining Loss: 0.000190\n",
      "Epoch: 3 \tTraining Loss: 0.000190\n",
      "Epoch: 4 \tTraining Loss: 0.000190\n",
      "Epoch: 5 \tTraining Loss: 0.000190\n",
      "Epoch: 6 \tTraining Loss: 0.000190\n",
      "Epoch: 7 \tTraining Loss: 0.000190\n",
      "Epoch: 8 \tTraining Loss: 0.000190\n",
      "Epoch: 9 \tTraining Loss: 0.000190\n",
      "Epoch: 10 \tTraining Loss: 0.000190\n",
      "Epoch: 11 \tTraining Loss: 0.000190\n",
      "Epoch: 12 \tTraining Loss: 0.000190\n",
      "Epoch: 13 \tTraining Loss: 0.000190\n",
      "Epoch: 14 \tTraining Loss: 0.000190\n",
      "Epoch: 15 \tTraining Loss: 0.000190\n",
      "Epoch: 16 \tTraining Loss: 0.000190\n",
      "Epoch: 17 \tTraining Loss: 0.000190\n",
      "Epoch: 18 \tTraining Loss: 0.000190\n",
      "Epoch: 1 \tTraining Loss: 0.000172\n",
      "Epoch: 2 \tTraining Loss: 0.000172\n",
      "Epoch: 3 \tTraining Loss: 0.000172\n",
      "Epoch: 4 \tTraining Loss: 0.000172\n",
      "Epoch: 5 \tTraining Loss: 0.000172\n",
      "Epoch: 6 \tTraining Loss: 0.000172\n",
      "Epoch: 7 \tTraining Loss: 0.000172\n",
      "Epoch: 8 \tTraining Loss: 0.000172\n",
      "Epoch: 9 \tTraining Loss: 0.000172\n",
      "Epoch: 10 \tTraining Loss: 0.000172\n",
      "Epoch: 11 \tTraining Loss: 0.000172\n",
      "Epoch: 12 \tTraining Loss: 0.000172\n",
      "Epoch: 13 \tTraining Loss: 0.000172\n",
      "Epoch: 14 \tTraining Loss: 0.000172\n",
      "Epoch: 15 \tTraining Loss: 0.000172\n",
      "Epoch: 16 \tTraining Loss: 0.000172\n",
      "Epoch: 17 \tTraining Loss: 0.000172\n",
      "Epoch: 18 \tTraining Loss: 0.000172\n",
      "Epoch: 1 \tTraining Loss: 0.000114\n",
      "Epoch: 2 \tTraining Loss: 0.000114\n",
      "Epoch: 3 \tTraining Loss: 0.000114\n",
      "Epoch: 4 \tTraining Loss: 0.000114\n",
      "Epoch: 5 \tTraining Loss: 0.000114\n",
      "Epoch: 6 \tTraining Loss: 0.000114\n",
      "Epoch: 7 \tTraining Loss: 0.000114\n",
      "Epoch: 8 \tTraining Loss: 0.000114\n",
      "Epoch: 9 \tTraining Loss: 0.000114\n",
      "Epoch: 10 \tTraining Loss: 0.000114\n",
      "Epoch: 11 \tTraining Loss: 0.000114\n",
      "Epoch: 12 \tTraining Loss: 0.000114\n",
      "Epoch: 13 \tTraining Loss: 0.000114\n",
      "Epoch: 14 \tTraining Loss: 0.000114\n",
      "Epoch: 15 \tTraining Loss: 0.000114\n",
      "Epoch: 16 \tTraining Loss: 0.000114\n",
      "Epoch: 17 \tTraining Loss: 0.000114\n",
      "Epoch: 18 \tTraining Loss: 0.000114\n",
      "Epoch: 1 \tTraining Loss: 0.000170\n",
      "Epoch: 2 \tTraining Loss: 0.000170\n",
      "Epoch: 3 \tTraining Loss: 0.000170\n",
      "Epoch: 4 \tTraining Loss: 0.000170\n",
      "Epoch: 5 \tTraining Loss: 0.000170\n",
      "Epoch: 6 \tTraining Loss: 0.000170\n",
      "Epoch: 7 \tTraining Loss: 0.000170\n",
      "Epoch: 8 \tTraining Loss: 0.000170\n",
      "Epoch: 9 \tTraining Loss: 0.000170\n",
      "Epoch: 10 \tTraining Loss: 0.000170\n",
      "Epoch: 11 \tTraining Loss: 0.000170\n",
      "Epoch: 12 \tTraining Loss: 0.000170\n",
      "Epoch: 13 \tTraining Loss: 0.000170\n",
      "Epoch: 14 \tTraining Loss: 0.000170\n",
      "Epoch: 15 \tTraining Loss: 0.000170\n",
      "Epoch: 16 \tTraining Loss: 0.000170\n",
      "Epoch: 17 \tTraining Loss: 0.000170\n",
      "Epoch: 18 \tTraining Loss: 0.000170\n",
      "Epoch: 1 \tTraining Loss: 0.000269\n",
      "Epoch: 2 \tTraining Loss: 0.000269\n",
      "Epoch: 3 \tTraining Loss: 0.000269\n",
      "Epoch: 4 \tTraining Loss: 0.000269\n",
      "Epoch: 5 \tTraining Loss: 0.000269\n",
      "Epoch: 6 \tTraining Loss: 0.000269\n",
      "Epoch: 7 \tTraining Loss: 0.000269\n",
      "Epoch: 8 \tTraining Loss: 0.000269\n",
      "Epoch: 9 \tTraining Loss: 0.000269\n",
      "Epoch: 10 \tTraining Loss: 0.000269\n",
      "Epoch: 11 \tTraining Loss: 0.000269\n",
      "Epoch: 12 \tTraining Loss: 0.000269\n",
      "Epoch: 13 \tTraining Loss: 0.000269\n",
      "Epoch: 14 \tTraining Loss: 0.000269\n",
      "Epoch: 15 \tTraining Loss: 0.000269\n",
      "Epoch: 16 \tTraining Loss: 0.000269\n",
      "Epoch: 17 \tTraining Loss: 0.000269\n",
      "Epoch: 18 \tTraining Loss: 0.000269\n",
      "Epoch: 1 \tTraining Loss: 0.000099\n",
      "Epoch: 2 \tTraining Loss: 0.000099\n",
      "Epoch: 3 \tTraining Loss: 0.000099\n",
      "Epoch: 4 \tTraining Loss: 0.000099\n",
      "Epoch: 5 \tTraining Loss: 0.000099\n",
      "Epoch: 6 \tTraining Loss: 0.000099\n",
      "Epoch: 7 \tTraining Loss: 0.000099\n",
      "Epoch: 8 \tTraining Loss: 0.000099\n",
      "Epoch: 9 \tTraining Loss: 0.000099\n",
      "Epoch: 10 \tTraining Loss: 0.000099\n",
      "Epoch: 11 \tTraining Loss: 0.000099\n",
      "Epoch: 12 \tTraining Loss: 0.000099\n",
      "Epoch: 13 \tTraining Loss: 0.000099\n",
      "Epoch: 14 \tTraining Loss: 0.000099\n",
      "Epoch: 15 \tTraining Loss: 0.000099\n",
      "Epoch: 16 \tTraining Loss: 0.000099\n",
      "Epoch: 17 \tTraining Loss: 0.000099\n",
      "Epoch: 18 \tTraining Loss: 0.000099\n",
      "Epoch: 1 \tTraining Loss: 0.000276\n",
      "Epoch: 2 \tTraining Loss: 0.000276\n",
      "Epoch: 3 \tTraining Loss: 0.000276\n",
      "Epoch: 4 \tTraining Loss: 0.000276\n",
      "Epoch: 5 \tTraining Loss: 0.000276\n",
      "Epoch: 6 \tTraining Loss: 0.000276\n",
      "Epoch: 7 \tTraining Loss: 0.000276\n",
      "Epoch: 8 \tTraining Loss: 0.000276\n",
      "Epoch: 9 \tTraining Loss: 0.000276\n",
      "Epoch: 10 \tTraining Loss: 0.000276\n",
      "Epoch: 11 \tTraining Loss: 0.000276\n",
      "Epoch: 12 \tTraining Loss: 0.000276\n",
      "Epoch: 13 \tTraining Loss: 0.000276\n",
      "Epoch: 14 \tTraining Loss: 0.000276\n",
      "Epoch: 15 \tTraining Loss: 0.000276\n",
      "Epoch: 16 \tTraining Loss: 0.000276\n",
      "Epoch: 17 \tTraining Loss: 0.000276\n",
      "Epoch: 18 \tTraining Loss: 0.000276\n",
      "Epoch: 1 \tTraining Loss: 0.000232\n",
      "Epoch: 2 \tTraining Loss: 0.000232\n",
      "Epoch: 3 \tTraining Loss: 0.000232\n",
      "Epoch: 4 \tTraining Loss: 0.000232\n",
      "Epoch: 5 \tTraining Loss: 0.000232\n",
      "Epoch: 6 \tTraining Loss: 0.000232\n",
      "Epoch: 7 \tTraining Loss: 0.000232\n",
      "Epoch: 8 \tTraining Loss: 0.000232\n",
      "Epoch: 9 \tTraining Loss: 0.000232\n",
      "Epoch: 10 \tTraining Loss: 0.000232\n",
      "Epoch: 11 \tTraining Loss: 0.000232\n",
      "Epoch: 12 \tTraining Loss: 0.000232\n",
      "Epoch: 13 \tTraining Loss: 0.000232\n",
      "Epoch: 14 \tTraining Loss: 0.000232\n",
      "Epoch: 15 \tTraining Loss: 0.000232\n",
      "Epoch: 16 \tTraining Loss: 0.000232\n",
      "Epoch: 17 \tTraining Loss: 0.000232\n",
      "Epoch: 18 \tTraining Loss: 0.000232\n",
      "Epoch: 1 \tTraining Loss: 0.000288\n",
      "Epoch: 2 \tTraining Loss: 0.000288\n",
      "Epoch: 3 \tTraining Loss: 0.000288\n",
      "Epoch: 4 \tTraining Loss: 0.000288\n",
      "Epoch: 5 \tTraining Loss: 0.000288\n",
      "Epoch: 6 \tTraining Loss: 0.000288\n",
      "Epoch: 7 \tTraining Loss: 0.000288\n",
      "Epoch: 8 \tTraining Loss: 0.000288\n",
      "Epoch: 9 \tTraining Loss: 0.000288\n",
      "Epoch: 10 \tTraining Loss: 0.000288\n",
      "Epoch: 11 \tTraining Loss: 0.000288\n",
      "Epoch: 12 \tTraining Loss: 0.000288\n",
      "Epoch: 13 \tTraining Loss: 0.000288\n",
      "Epoch: 14 \tTraining Loss: 0.000288\n",
      "Epoch: 15 \tTraining Loss: 0.000288\n",
      "Epoch: 16 \tTraining Loss: 0.000288\n",
      "Epoch: 17 \tTraining Loss: 0.000288\n",
      "Epoch: 18 \tTraining Loss: 0.000288\n",
      "Epoch: 1 \tTraining Loss: 0.000161\n",
      "Epoch: 2 \tTraining Loss: 0.000161\n",
      "Epoch: 3 \tTraining Loss: 0.000161\n",
      "Epoch: 4 \tTraining Loss: 0.000161\n",
      "Epoch: 5 \tTraining Loss: 0.000161\n",
      "Epoch: 6 \tTraining Loss: 0.000161\n",
      "Epoch: 7 \tTraining Loss: 0.000161\n",
      "Epoch: 8 \tTraining Loss: 0.000161\n",
      "Epoch: 9 \tTraining Loss: 0.000161\n",
      "Epoch: 10 \tTraining Loss: 0.000161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 \tTraining Loss: 0.000161\n",
      "Epoch: 12 \tTraining Loss: 0.000161\n",
      "Epoch: 13 \tTraining Loss: 0.000161\n",
      "Epoch: 14 \tTraining Loss: 0.000161\n",
      "Epoch: 15 \tTraining Loss: 0.000161\n",
      "Epoch: 16 \tTraining Loss: 0.000161\n",
      "Epoch: 17 \tTraining Loss: 0.000161\n",
      "Epoch: 18 \tTraining Loss: 0.000161\n",
      "Epoch: 1 \tTraining Loss: 0.000173\n",
      "Epoch: 2 \tTraining Loss: 0.000173\n",
      "Epoch: 3 \tTraining Loss: 0.000173\n",
      "Epoch: 4 \tTraining Loss: 0.000173\n",
      "Epoch: 5 \tTraining Loss: 0.000173\n",
      "Epoch: 6 \tTraining Loss: 0.000173\n",
      "Epoch: 7 \tTraining Loss: 0.000173\n",
      "Epoch: 8 \tTraining Loss: 0.000173\n",
      "Epoch: 9 \tTraining Loss: 0.000173\n",
      "Epoch: 10 \tTraining Loss: 0.000173\n",
      "Epoch: 11 \tTraining Loss: 0.000173\n",
      "Epoch: 12 \tTraining Loss: 0.000173\n",
      "Epoch: 13 \tTraining Loss: 0.000173\n",
      "Epoch: 14 \tTraining Loss: 0.000173\n",
      "Epoch: 15 \tTraining Loss: 0.000173\n",
      "Epoch: 16 \tTraining Loss: 0.000173\n",
      "Epoch: 17 \tTraining Loss: 0.000173\n",
      "Epoch: 18 \tTraining Loss: 0.000173\n",
      "Epoch: 1 \tTraining Loss: 0.000277\n",
      "Epoch: 2 \tTraining Loss: 0.000277\n",
      "Epoch: 3 \tTraining Loss: 0.000277\n",
      "Epoch: 4 \tTraining Loss: 0.000277\n",
      "Epoch: 5 \tTraining Loss: 0.000277\n",
      "Epoch: 6 \tTraining Loss: 0.000277\n",
      "Epoch: 7 \tTraining Loss: 0.000277\n",
      "Epoch: 8 \tTraining Loss: 0.000277\n",
      "Epoch: 9 \tTraining Loss: 0.000277\n",
      "Epoch: 10 \tTraining Loss: 0.000277\n",
      "Epoch: 11 \tTraining Loss: 0.000277\n",
      "Epoch: 12 \tTraining Loss: 0.000277\n",
      "Epoch: 13 \tTraining Loss: 0.000277\n",
      "Epoch: 14 \tTraining Loss: 0.000277\n",
      "Epoch: 15 \tTraining Loss: 0.000277\n",
      "Epoch: 16 \tTraining Loss: 0.000277\n",
      "Epoch: 17 \tTraining Loss: 0.000277\n",
      "Epoch: 18 \tTraining Loss: 0.000277\n",
      "Epoch: 1 \tTraining Loss: 0.000220\n",
      "Epoch: 2 \tTraining Loss: 0.000220\n",
      "Epoch: 3 \tTraining Loss: 0.000220\n",
      "Epoch: 4 \tTraining Loss: 0.000220\n",
      "Epoch: 5 \tTraining Loss: 0.000220\n",
      "Epoch: 6 \tTraining Loss: 0.000220\n",
      "Epoch: 7 \tTraining Loss: 0.000220\n",
      "Epoch: 8 \tTraining Loss: 0.000220\n",
      "Epoch: 9 \tTraining Loss: 0.000220\n",
      "Epoch: 10 \tTraining Loss: 0.000220\n",
      "Epoch: 11 \tTraining Loss: 0.000220\n",
      "Epoch: 12 \tTraining Loss: 0.000220\n",
      "Epoch: 13 \tTraining Loss: 0.000220\n",
      "Epoch: 14 \tTraining Loss: 0.000220\n",
      "Epoch: 15 \tTraining Loss: 0.000220\n",
      "Epoch: 16 \tTraining Loss: 0.000220\n",
      "Epoch: 17 \tTraining Loss: 0.000220\n",
      "Epoch: 18 \tTraining Loss: 0.000220\n",
      "Epoch: 1 \tTraining Loss: 0.000321\n",
      "Epoch: 2 \tTraining Loss: 0.000321\n",
      "Epoch: 3 \tTraining Loss: 0.000321\n",
      "Epoch: 4 \tTraining Loss: 0.000321\n",
      "Epoch: 5 \tTraining Loss: 0.000321\n",
      "Epoch: 6 \tTraining Loss: 0.000321\n",
      "Epoch: 7 \tTraining Loss: 0.000321\n",
      "Epoch: 8 \tTraining Loss: 0.000321\n",
      "Epoch: 9 \tTraining Loss: 0.000321\n",
      "Epoch: 10 \tTraining Loss: 0.000321\n",
      "Epoch: 11 \tTraining Loss: 0.000321\n",
      "Epoch: 12 \tTraining Loss: 0.000321\n",
      "Epoch: 13 \tTraining Loss: 0.000321\n",
      "Epoch: 14 \tTraining Loss: 0.000321\n",
      "Epoch: 15 \tTraining Loss: 0.000321\n",
      "Epoch: 16 \tTraining Loss: 0.000321\n",
      "Epoch: 17 \tTraining Loss: 0.000321\n",
      "Epoch: 18 \tTraining Loss: 0.000321\n",
      "Epoch: 1 \tTraining Loss: 0.000199\n",
      "Epoch: 2 \tTraining Loss: 0.000199\n",
      "Epoch: 3 \tTraining Loss: 0.000199\n",
      "Epoch: 4 \tTraining Loss: 0.000199\n",
      "Epoch: 5 \tTraining Loss: 0.000199\n",
      "Epoch: 6 \tTraining Loss: 0.000199\n",
      "Epoch: 7 \tTraining Loss: 0.000199\n",
      "Epoch: 8 \tTraining Loss: 0.000199\n",
      "Epoch: 9 \tTraining Loss: 0.000199\n",
      "Epoch: 10 \tTraining Loss: 0.000199\n",
      "Epoch: 11 \tTraining Loss: 0.000199\n",
      "Epoch: 12 \tTraining Loss: 0.000199\n",
      "Epoch: 13 \tTraining Loss: 0.000199\n",
      "Epoch: 14 \tTraining Loss: 0.000199\n",
      "Epoch: 15 \tTraining Loss: 0.000199\n",
      "Epoch: 16 \tTraining Loss: 0.000199\n",
      "Epoch: 17 \tTraining Loss: 0.000199\n",
      "Epoch: 18 \tTraining Loss: 0.000199\n",
      "Epoch: 1 \tTraining Loss: 0.000190\n",
      "Epoch: 2 \tTraining Loss: 0.000190\n",
      "Epoch: 3 \tTraining Loss: 0.000190\n",
      "Epoch: 4 \tTraining Loss: 0.000190\n",
      "Epoch: 5 \tTraining Loss: 0.000190\n",
      "Epoch: 6 \tTraining Loss: 0.000190\n",
      "Epoch: 7 \tTraining Loss: 0.000190\n",
      "Epoch: 8 \tTraining Loss: 0.000190\n",
      "Epoch: 9 \tTraining Loss: 0.000190\n",
      "Epoch: 10 \tTraining Loss: 0.000190\n",
      "Epoch: 11 \tTraining Loss: 0.000190\n",
      "Epoch: 12 \tTraining Loss: 0.000190\n",
      "Epoch: 13 \tTraining Loss: 0.000190\n",
      "Epoch: 14 \tTraining Loss: 0.000190\n",
      "Epoch: 15 \tTraining Loss: 0.000190\n",
      "Epoch: 16 \tTraining Loss: 0.000190\n",
      "Epoch: 17 \tTraining Loss: 0.000190\n",
      "Epoch: 18 \tTraining Loss: 0.000190\n",
      "Epoch: 1 \tTraining Loss: 0.000262\n",
      "Epoch: 2 \tTraining Loss: 0.000262\n",
      "Epoch: 3 \tTraining Loss: 0.000262\n",
      "Epoch: 4 \tTraining Loss: 0.000262\n",
      "Epoch: 5 \tTraining Loss: 0.000262\n",
      "Epoch: 6 \tTraining Loss: 0.000262\n",
      "Epoch: 7 \tTraining Loss: 0.000262\n",
      "Epoch: 8 \tTraining Loss: 0.000262\n",
      "Epoch: 9 \tTraining Loss: 0.000262\n",
      "Epoch: 10 \tTraining Loss: 0.000262\n",
      "Epoch: 11 \tTraining Loss: 0.000262\n",
      "Epoch: 12 \tTraining Loss: 0.000262\n",
      "Epoch: 13 \tTraining Loss: 0.000262\n",
      "Epoch: 14 \tTraining Loss: 0.000262\n",
      "Epoch: 15 \tTraining Loss: 0.000262\n",
      "Epoch: 16 \tTraining Loss: 0.000262\n",
      "Epoch: 17 \tTraining Loss: 0.000262\n",
      "Epoch: 18 \tTraining Loss: 0.000262\n",
      "Epoch: 1 \tTraining Loss: 0.000170\n",
      "Epoch: 2 \tTraining Loss: 0.000170\n",
      "Epoch: 3 \tTraining Loss: 0.000170\n",
      "Epoch: 4 \tTraining Loss: 0.000170\n",
      "Epoch: 5 \tTraining Loss: 0.000170\n",
      "Epoch: 6 \tTraining Loss: 0.000170\n",
      "Epoch: 7 \tTraining Loss: 0.000170\n",
      "Epoch: 8 \tTraining Loss: 0.000170\n",
      "Epoch: 9 \tTraining Loss: 0.000170\n",
      "Epoch: 10 \tTraining Loss: 0.000170\n",
      "Epoch: 11 \tTraining Loss: 0.000170\n",
      "Epoch: 12 \tTraining Loss: 0.000170\n",
      "Epoch: 13 \tTraining Loss: 0.000170\n",
      "Epoch: 14 \tTraining Loss: 0.000170\n",
      "Epoch: 15 \tTraining Loss: 0.000170\n",
      "Epoch: 16 \tTraining Loss: 0.000170\n",
      "Epoch: 17 \tTraining Loss: 0.000170\n",
      "Epoch: 18 \tTraining Loss: 0.000170\n",
      "Epoch: 1 \tTraining Loss: 0.000136\n",
      "Epoch: 2 \tTraining Loss: 0.000136\n",
      "Epoch: 3 \tTraining Loss: 0.000136\n",
      "Epoch: 4 \tTraining Loss: 0.000136\n",
      "Epoch: 5 \tTraining Loss: 0.000136\n",
      "Epoch: 6 \tTraining Loss: 0.000136\n",
      "Epoch: 7 \tTraining Loss: 0.000136\n",
      "Epoch: 8 \tTraining Loss: 0.000136\n",
      "Epoch: 9 \tTraining Loss: 0.000136\n",
      "Epoch: 10 \tTraining Loss: 0.000136\n",
      "Epoch: 11 \tTraining Loss: 0.000136\n",
      "Epoch: 12 \tTraining Loss: 0.000136\n",
      "Epoch: 13 \tTraining Loss: 0.000136\n",
      "Epoch: 14 \tTraining Loss: 0.000136\n",
      "Epoch: 15 \tTraining Loss: 0.000136\n",
      "Epoch: 16 \tTraining Loss: 0.000136\n",
      "Epoch: 17 \tTraining Loss: 0.000136\n",
      "Epoch: 18 \tTraining Loss: 0.000136\n",
      "Epoch: 1 \tTraining Loss: 0.000091\n",
      "Epoch: 2 \tTraining Loss: 0.000091\n",
      "Epoch: 3 \tTraining Loss: 0.000091\n",
      "Epoch: 4 \tTraining Loss: 0.000091\n",
      "Epoch: 5 \tTraining Loss: 0.000091\n",
      "Epoch: 6 \tTraining Loss: 0.000091\n",
      "Epoch: 7 \tTraining Loss: 0.000091\n",
      "Epoch: 8 \tTraining Loss: 0.000091\n",
      "Epoch: 9 \tTraining Loss: 0.000091\n",
      "Epoch: 10 \tTraining Loss: 0.000091\n",
      "Epoch: 11 \tTraining Loss: 0.000091\n",
      "Epoch: 12 \tTraining Loss: 0.000091\n",
      "Epoch: 13 \tTraining Loss: 0.000091\n",
      "Epoch: 14 \tTraining Loss: 0.000091\n",
      "Epoch: 15 \tTraining Loss: 0.000091\n",
      "Epoch: 16 \tTraining Loss: 0.000091\n",
      "Epoch: 17 \tTraining Loss: 0.000091\n",
      "Epoch: 18 \tTraining Loss: 0.000091\n",
      "Epoch: 1 \tTraining Loss: 0.000141\n",
      "Epoch: 2 \tTraining Loss: 0.000141\n",
      "Epoch: 3 \tTraining Loss: 0.000141\n",
      "Epoch: 4 \tTraining Loss: 0.000141\n",
      "Epoch: 5 \tTraining Loss: 0.000141\n",
      "Epoch: 6 \tTraining Loss: 0.000141\n",
      "Epoch: 7 \tTraining Loss: 0.000141\n",
      "Epoch: 8 \tTraining Loss: 0.000141\n",
      "Epoch: 9 \tTraining Loss: 0.000141\n",
      "Epoch: 10 \tTraining Loss: 0.000141\n",
      "Epoch: 11 \tTraining Loss: 0.000141\n",
      "Epoch: 12 \tTraining Loss: 0.000141\n",
      "Epoch: 13 \tTraining Loss: 0.000141\n",
      "Epoch: 14 \tTraining Loss: 0.000141\n",
      "Epoch: 15 \tTraining Loss: 0.000141\n",
      "Epoch: 16 \tTraining Loss: 0.000141\n",
      "Epoch: 17 \tTraining Loss: 0.000141\n",
      "Epoch: 18 \tTraining Loss: 0.000141\n",
      "Epoch: 1 \tTraining Loss: 0.000075\n",
      "Epoch: 2 \tTraining Loss: 0.000075\n",
      "Epoch: 3 \tTraining Loss: 0.000075\n",
      "Epoch: 4 \tTraining Loss: 0.000075\n",
      "Epoch: 5 \tTraining Loss: 0.000075\n",
      "Epoch: 6 \tTraining Loss: 0.000075\n",
      "Epoch: 7 \tTraining Loss: 0.000075\n",
      "Epoch: 8 \tTraining Loss: 0.000075\n",
      "Epoch: 9 \tTraining Loss: 0.000075\n",
      "Epoch: 10 \tTraining Loss: 0.000075\n",
      "Epoch: 11 \tTraining Loss: 0.000075\n",
      "Epoch: 12 \tTraining Loss: 0.000075\n",
      "Epoch: 13 \tTraining Loss: 0.000075\n",
      "Epoch: 14 \tTraining Loss: 0.000075\n",
      "Epoch: 15 \tTraining Loss: 0.000075\n",
      "Epoch: 16 \tTraining Loss: 0.000075\n",
      "Epoch: 17 \tTraining Loss: 0.000075\n",
      "Epoch: 18 \tTraining Loss: 0.000075\n",
      "Epoch: 1 \tTraining Loss: 0.000145\n",
      "Epoch: 2 \tTraining Loss: 0.000145\n",
      "Epoch: 3 \tTraining Loss: 0.000145\n",
      "Epoch: 4 \tTraining Loss: 0.000145\n",
      "Epoch: 5 \tTraining Loss: 0.000145\n",
      "Epoch: 6 \tTraining Loss: 0.000145\n",
      "Epoch: 7 \tTraining Loss: 0.000145\n",
      "Epoch: 8 \tTraining Loss: 0.000145\n",
      "Epoch: 9 \tTraining Loss: 0.000145\n",
      "Epoch: 10 \tTraining Loss: 0.000145\n",
      "Epoch: 11 \tTraining Loss: 0.000145\n",
      "Epoch: 12 \tTraining Loss: 0.000145\n",
      "Epoch: 13 \tTraining Loss: 0.000145\n",
      "Epoch: 14 \tTraining Loss: 0.000145\n",
      "Epoch: 15 \tTraining Loss: 0.000145\n",
      "Epoch: 16 \tTraining Loss: 0.000145\n",
      "Epoch: 17 \tTraining Loss: 0.000145\n",
      "Epoch: 18 \tTraining Loss: 0.000145\n",
      "Epoch: 1 \tTraining Loss: 0.000172\n",
      "Epoch: 2 \tTraining Loss: 0.000172\n",
      "Epoch: 3 \tTraining Loss: 0.000172\n",
      "Epoch: 4 \tTraining Loss: 0.000172\n",
      "Epoch: 5 \tTraining Loss: 0.000172\n",
      "Epoch: 6 \tTraining Loss: 0.000172\n",
      "Epoch: 7 \tTraining Loss: 0.000172\n",
      "Epoch: 8 \tTraining Loss: 0.000172\n",
      "Epoch: 9 \tTraining Loss: 0.000172\n",
      "Epoch: 10 \tTraining Loss: 0.000172\n",
      "Epoch: 11 \tTraining Loss: 0.000172\n",
      "Epoch: 12 \tTraining Loss: 0.000172\n",
      "Epoch: 13 \tTraining Loss: 0.000172\n",
      "Epoch: 14 \tTraining Loss: 0.000172\n",
      "Epoch: 15 \tTraining Loss: 0.000172\n",
      "Epoch: 16 \tTraining Loss: 0.000172\n",
      "Epoch: 17 \tTraining Loss: 0.000172\n",
      "Epoch: 18 \tTraining Loss: 0.000172\n",
      "Epoch: 1 \tTraining Loss: 0.000179\n",
      "Epoch: 2 \tTraining Loss: 0.000179\n",
      "Epoch: 3 \tTraining Loss: 0.000179\n",
      "Epoch: 4 \tTraining Loss: 0.000179\n",
      "Epoch: 5 \tTraining Loss: 0.000179\n",
      "Epoch: 6 \tTraining Loss: 0.000179\n",
      "Epoch: 7 \tTraining Loss: 0.000179\n",
      "Epoch: 8 \tTraining Loss: 0.000179\n",
      "Epoch: 9 \tTraining Loss: 0.000179\n",
      "Epoch: 10 \tTraining Loss: 0.000179\n",
      "Epoch: 11 \tTraining Loss: 0.000179\n",
      "Epoch: 12 \tTraining Loss: 0.000179\n",
      "Epoch: 13 \tTraining Loss: 0.000179\n",
      "Epoch: 14 \tTraining Loss: 0.000179\n",
      "Epoch: 15 \tTraining Loss: 0.000179\n",
      "Epoch: 16 \tTraining Loss: 0.000179\n",
      "Epoch: 17 \tTraining Loss: 0.000179\n",
      "Epoch: 18 \tTraining Loss: 0.000179\n",
      "Epoch: 1 \tTraining Loss: 0.000125\n",
      "Epoch: 2 \tTraining Loss: 0.000125\n",
      "Epoch: 3 \tTraining Loss: 0.000125\n",
      "Epoch: 4 \tTraining Loss: 0.000125\n",
      "Epoch: 5 \tTraining Loss: 0.000125\n",
      "Epoch: 6 \tTraining Loss: 0.000125\n",
      "Epoch: 7 \tTraining Loss: 0.000125\n",
      "Epoch: 8 \tTraining Loss: 0.000125\n",
      "Epoch: 9 \tTraining Loss: 0.000125\n",
      "Epoch: 10 \tTraining Loss: 0.000125\n",
      "Epoch: 11 \tTraining Loss: 0.000125\n",
      "Epoch: 12 \tTraining Loss: 0.000125\n",
      "Epoch: 13 \tTraining Loss: 0.000125\n",
      "Epoch: 14 \tTraining Loss: 0.000125\n",
      "Epoch: 15 \tTraining Loss: 0.000125\n",
      "Epoch: 16 \tTraining Loss: 0.000125\n",
      "Epoch: 17 \tTraining Loss: 0.000125\n",
      "Epoch: 18 \tTraining Loss: 0.000125\n",
      "Epoch: 1 \tTraining Loss: 0.000382\n",
      "Epoch: 2 \tTraining Loss: 0.000382\n",
      "Epoch: 3 \tTraining Loss: 0.000382\n",
      "Epoch: 4 \tTraining Loss: 0.000382\n",
      "Epoch: 5 \tTraining Loss: 0.000382\n",
      "Epoch: 6 \tTraining Loss: 0.000382\n",
      "Epoch: 7 \tTraining Loss: 0.000382\n",
      "Epoch: 8 \tTraining Loss: 0.000382\n",
      "Epoch: 9 \tTraining Loss: 0.000382\n",
      "Epoch: 10 \tTraining Loss: 0.000382\n",
      "Epoch: 11 \tTraining Loss: 0.000382\n",
      "Epoch: 12 \tTraining Loss: 0.000382\n",
      "Epoch: 13 \tTraining Loss: 0.000382\n",
      "Epoch: 14 \tTraining Loss: 0.000382\n",
      "Epoch: 15 \tTraining Loss: 0.000382\n",
      "Epoch: 16 \tTraining Loss: 0.000382\n",
      "Epoch: 17 \tTraining Loss: 0.000382\n",
      "Epoch: 18 \tTraining Loss: 0.000382\n",
      "Epoch: 1 \tTraining Loss: 0.000160\n",
      "Epoch: 2 \tTraining Loss: 0.000160\n",
      "Epoch: 3 \tTraining Loss: 0.000160\n",
      "Epoch: 4 \tTraining Loss: 0.000160\n",
      "Epoch: 5 \tTraining Loss: 0.000160\n",
      "Epoch: 6 \tTraining Loss: 0.000160\n",
      "Epoch: 7 \tTraining Loss: 0.000160\n",
      "Epoch: 8 \tTraining Loss: 0.000160\n",
      "Epoch: 9 \tTraining Loss: 0.000160\n",
      "Epoch: 10 \tTraining Loss: 0.000160\n",
      "Epoch: 11 \tTraining Loss: 0.000160\n",
      "Epoch: 12 \tTraining Loss: 0.000160\n",
      "Epoch: 13 \tTraining Loss: 0.000160\n",
      "Epoch: 14 \tTraining Loss: 0.000160\n",
      "Epoch: 15 \tTraining Loss: 0.000160\n",
      "Epoch: 16 \tTraining Loss: 0.000160\n",
      "Epoch: 17 \tTraining Loss: 0.000160\n",
      "Epoch: 18 \tTraining Loss: 0.000160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000169\n",
      "Epoch: 2 \tTraining Loss: 0.000169\n",
      "Epoch: 3 \tTraining Loss: 0.000169\n",
      "Epoch: 4 \tTraining Loss: 0.000169\n",
      "Epoch: 5 \tTraining Loss: 0.000169\n",
      "Epoch: 6 \tTraining Loss: 0.000169\n",
      "Epoch: 7 \tTraining Loss: 0.000169\n",
      "Epoch: 8 \tTraining Loss: 0.000169\n",
      "Epoch: 9 \tTraining Loss: 0.000169\n",
      "Epoch: 10 \tTraining Loss: 0.000169\n",
      "Epoch: 11 \tTraining Loss: 0.000169\n",
      "Epoch: 12 \tTraining Loss: 0.000169\n",
      "Epoch: 13 \tTraining Loss: 0.000169\n",
      "Epoch: 14 \tTraining Loss: 0.000169\n",
      "Epoch: 15 \tTraining Loss: 0.000169\n",
      "Epoch: 16 \tTraining Loss: 0.000169\n",
      "Epoch: 17 \tTraining Loss: 0.000169\n",
      "Epoch: 18 \tTraining Loss: 0.000169\n",
      "Epoch: 1 \tTraining Loss: 0.000226\n",
      "Epoch: 2 \tTraining Loss: 0.000226\n",
      "Epoch: 3 \tTraining Loss: 0.000226\n",
      "Epoch: 4 \tTraining Loss: 0.000226\n",
      "Epoch: 5 \tTraining Loss: 0.000226\n",
      "Epoch: 6 \tTraining Loss: 0.000226\n",
      "Epoch: 7 \tTraining Loss: 0.000226\n",
      "Epoch: 8 \tTraining Loss: 0.000226\n",
      "Epoch: 9 \tTraining Loss: 0.000226\n",
      "Epoch: 10 \tTraining Loss: 0.000226\n",
      "Epoch: 11 \tTraining Loss: 0.000226\n",
      "Epoch: 12 \tTraining Loss: 0.000226\n",
      "Epoch: 13 \tTraining Loss: 0.000226\n",
      "Epoch: 14 \tTraining Loss: 0.000226\n",
      "Epoch: 15 \tTraining Loss: 0.000226\n",
      "Epoch: 16 \tTraining Loss: 0.000226\n",
      "Epoch: 17 \tTraining Loss: 0.000226\n",
      "Epoch: 18 \tTraining Loss: 0.000226\n",
      "Epoch: 1 \tTraining Loss: 0.000208\n",
      "Epoch: 2 \tTraining Loss: 0.000208\n",
      "Epoch: 3 \tTraining Loss: 0.000208\n",
      "Epoch: 4 \tTraining Loss: 0.000208\n",
      "Epoch: 5 \tTraining Loss: 0.000208\n",
      "Epoch: 6 \tTraining Loss: 0.000208\n",
      "Epoch: 7 \tTraining Loss: 0.000208\n",
      "Epoch: 8 \tTraining Loss: 0.000208\n",
      "Epoch: 9 \tTraining Loss: 0.000208\n",
      "Epoch: 10 \tTraining Loss: 0.000208\n",
      "Epoch: 11 \tTraining Loss: 0.000208\n",
      "Epoch: 12 \tTraining Loss: 0.000208\n",
      "Epoch: 13 \tTraining Loss: 0.000208\n",
      "Epoch: 14 \tTraining Loss: 0.000208\n",
      "Epoch: 15 \tTraining Loss: 0.000208\n",
      "Epoch: 16 \tTraining Loss: 0.000208\n",
      "Epoch: 17 \tTraining Loss: 0.000208\n",
      "Epoch: 18 \tTraining Loss: 0.000208\n",
      "Epoch: 1 \tTraining Loss: 0.000244\n",
      "Epoch: 2 \tTraining Loss: 0.000244\n",
      "Epoch: 3 \tTraining Loss: 0.000244\n",
      "Epoch: 4 \tTraining Loss: 0.000244\n",
      "Epoch: 5 \tTraining Loss: 0.000244\n",
      "Epoch: 6 \tTraining Loss: 0.000244\n",
      "Epoch: 7 \tTraining Loss: 0.000244\n",
      "Epoch: 8 \tTraining Loss: 0.000244\n",
      "Epoch: 9 \tTraining Loss: 0.000244\n",
      "Epoch: 10 \tTraining Loss: 0.000244\n",
      "Epoch: 11 \tTraining Loss: 0.000244\n",
      "Epoch: 12 \tTraining Loss: 0.000244\n",
      "Epoch: 13 \tTraining Loss: 0.000244\n",
      "Epoch: 14 \tTraining Loss: 0.000244\n",
      "Epoch: 15 \tTraining Loss: 0.000244\n",
      "Epoch: 16 \tTraining Loss: 0.000244\n",
      "Epoch: 17 \tTraining Loss: 0.000244\n",
      "Epoch: 18 \tTraining Loss: 0.000244\n",
      "Epoch: 1 \tTraining Loss: 0.000120\n",
      "Epoch: 2 \tTraining Loss: 0.000120\n",
      "Epoch: 3 \tTraining Loss: 0.000120\n",
      "Epoch: 4 \tTraining Loss: 0.000120\n",
      "Epoch: 5 \tTraining Loss: 0.000120\n",
      "Epoch: 6 \tTraining Loss: 0.000120\n",
      "Epoch: 7 \tTraining Loss: 0.000120\n",
      "Epoch: 8 \tTraining Loss: 0.000120\n",
      "Epoch: 9 \tTraining Loss: 0.000120\n",
      "Epoch: 10 \tTraining Loss: 0.000120\n",
      "Epoch: 11 \tTraining Loss: 0.000120\n",
      "Epoch: 12 \tTraining Loss: 0.000120\n",
      "Epoch: 13 \tTraining Loss: 0.000120\n",
      "Epoch: 14 \tTraining Loss: 0.000120\n",
      "Epoch: 15 \tTraining Loss: 0.000120\n",
      "Epoch: 16 \tTraining Loss: 0.000120\n",
      "Epoch: 17 \tTraining Loss: 0.000120\n",
      "Epoch: 18 \tTraining Loss: 0.000120\n",
      "Epoch: 1 \tTraining Loss: 0.000267\n",
      "Epoch: 2 \tTraining Loss: 0.000267\n",
      "Epoch: 3 \tTraining Loss: 0.000267\n",
      "Epoch: 4 \tTraining Loss: 0.000267\n",
      "Epoch: 5 \tTraining Loss: 0.000267\n",
      "Epoch: 6 \tTraining Loss: 0.000267\n",
      "Epoch: 7 \tTraining Loss: 0.000267\n",
      "Epoch: 8 \tTraining Loss: 0.000267\n",
      "Epoch: 9 \tTraining Loss: 0.000267\n",
      "Epoch: 10 \tTraining Loss: 0.000267\n",
      "Epoch: 11 \tTraining Loss: 0.000267\n",
      "Epoch: 12 \tTraining Loss: 0.000267\n",
      "Epoch: 13 \tTraining Loss: 0.000267\n",
      "Epoch: 14 \tTraining Loss: 0.000267\n",
      "Epoch: 15 \tTraining Loss: 0.000267\n",
      "Epoch: 16 \tTraining Loss: 0.000267\n",
      "Epoch: 17 \tTraining Loss: 0.000267\n",
      "Epoch: 18 \tTraining Loss: 0.000267\n",
      "Epoch: 1 \tTraining Loss: 0.000200\n",
      "Epoch: 2 \tTraining Loss: 0.000200\n",
      "Epoch: 3 \tTraining Loss: 0.000200\n",
      "Epoch: 4 \tTraining Loss: 0.000200\n",
      "Epoch: 5 \tTraining Loss: 0.000200\n",
      "Epoch: 6 \tTraining Loss: 0.000200\n",
      "Epoch: 7 \tTraining Loss: 0.000200\n",
      "Epoch: 8 \tTraining Loss: 0.000200\n",
      "Epoch: 9 \tTraining Loss: 0.000200\n",
      "Epoch: 10 \tTraining Loss: 0.000200\n",
      "Epoch: 11 \tTraining Loss: 0.000200\n",
      "Epoch: 12 \tTraining Loss: 0.000200\n",
      "Epoch: 13 \tTraining Loss: 0.000200\n",
      "Epoch: 14 \tTraining Loss: 0.000200\n",
      "Epoch: 15 \tTraining Loss: 0.000200\n",
      "Epoch: 16 \tTraining Loss: 0.000200\n",
      "Epoch: 17 \tTraining Loss: 0.000200\n",
      "Epoch: 18 \tTraining Loss: 0.000200\n",
      "Epoch: 1 \tTraining Loss: 0.000130\n",
      "Epoch: 2 \tTraining Loss: 0.000130\n",
      "Epoch: 3 \tTraining Loss: 0.000130\n",
      "Epoch: 4 \tTraining Loss: 0.000130\n",
      "Epoch: 5 \tTraining Loss: 0.000130\n",
      "Epoch: 6 \tTraining Loss: 0.000130\n",
      "Epoch: 7 \tTraining Loss: 0.000130\n",
      "Epoch: 8 \tTraining Loss: 0.000130\n",
      "Epoch: 9 \tTraining Loss: 0.000130\n",
      "Epoch: 10 \tTraining Loss: 0.000130\n",
      "Epoch: 11 \tTraining Loss: 0.000130\n",
      "Epoch: 12 \tTraining Loss: 0.000130\n",
      "Epoch: 13 \tTraining Loss: 0.000130\n",
      "Epoch: 14 \tTraining Loss: 0.000130\n",
      "Epoch: 15 \tTraining Loss: 0.000130\n",
      "Epoch: 16 \tTraining Loss: 0.000130\n",
      "Epoch: 17 \tTraining Loss: 0.000130\n",
      "Epoch: 18 \tTraining Loss: 0.000130\n",
      "Epoch: 1 \tTraining Loss: 0.000259\n",
      "Epoch: 2 \tTraining Loss: 0.000259\n",
      "Epoch: 3 \tTraining Loss: 0.000259\n",
      "Epoch: 4 \tTraining Loss: 0.000259\n",
      "Epoch: 5 \tTraining Loss: 0.000259\n",
      "Epoch: 6 \tTraining Loss: 0.000259\n",
      "Epoch: 7 \tTraining Loss: 0.000259\n",
      "Epoch: 8 \tTraining Loss: 0.000259\n",
      "Epoch: 9 \tTraining Loss: 0.000259\n",
      "Epoch: 10 \tTraining Loss: 0.000259\n",
      "Epoch: 11 \tTraining Loss: 0.000259\n",
      "Epoch: 12 \tTraining Loss: 0.000259\n",
      "Epoch: 13 \tTraining Loss: 0.000259\n",
      "Epoch: 14 \tTraining Loss: 0.000259\n",
      "Epoch: 15 \tTraining Loss: 0.000259\n",
      "Epoch: 16 \tTraining Loss: 0.000259\n",
      "Epoch: 17 \tTraining Loss: 0.000259\n",
      "Epoch: 18 \tTraining Loss: 0.000259\n",
      "Epoch: 1 \tTraining Loss: 0.000218\n",
      "Epoch: 2 \tTraining Loss: 0.000218\n",
      "Epoch: 3 \tTraining Loss: 0.000218\n",
      "Epoch: 4 \tTraining Loss: 0.000218\n",
      "Epoch: 5 \tTraining Loss: 0.000218\n",
      "Epoch: 6 \tTraining Loss: 0.000218\n",
      "Epoch: 7 \tTraining Loss: 0.000218\n",
      "Epoch: 8 \tTraining Loss: 0.000218\n",
      "Epoch: 9 \tTraining Loss: 0.000218\n",
      "Epoch: 10 \tTraining Loss: 0.000218\n",
      "Epoch: 11 \tTraining Loss: 0.000218\n",
      "Epoch: 12 \tTraining Loss: 0.000218\n",
      "Epoch: 13 \tTraining Loss: 0.000218\n",
      "Epoch: 14 \tTraining Loss: 0.000218\n",
      "Epoch: 15 \tTraining Loss: 0.000218\n",
      "Epoch: 16 \tTraining Loss: 0.000218\n",
      "Epoch: 17 \tTraining Loss: 0.000218\n",
      "Epoch: 18 \tTraining Loss: 0.000218\n",
      "Epoch: 1 \tTraining Loss: 0.000209\n",
      "Epoch: 2 \tTraining Loss: 0.000209\n",
      "Epoch: 3 \tTraining Loss: 0.000209\n",
      "Epoch: 4 \tTraining Loss: 0.000209\n",
      "Epoch: 5 \tTraining Loss: 0.000209\n",
      "Epoch: 6 \tTraining Loss: 0.000209\n",
      "Epoch: 7 \tTraining Loss: 0.000209\n",
      "Epoch: 8 \tTraining Loss: 0.000209\n",
      "Epoch: 9 \tTraining Loss: 0.000209\n",
      "Epoch: 10 \tTraining Loss: 0.000209\n",
      "Epoch: 11 \tTraining Loss: 0.000209\n",
      "Epoch: 12 \tTraining Loss: 0.000209\n",
      "Epoch: 13 \tTraining Loss: 0.000209\n",
      "Epoch: 14 \tTraining Loss: 0.000209\n",
      "Epoch: 15 \tTraining Loss: 0.000209\n",
      "Epoch: 16 \tTraining Loss: 0.000209\n",
      "Epoch: 17 \tTraining Loss: 0.000209\n",
      "Epoch: 18 \tTraining Loss: 0.000209\n",
      "Epoch: 1 \tTraining Loss: 0.000214\n",
      "Epoch: 2 \tTraining Loss: 0.000214\n",
      "Epoch: 3 \tTraining Loss: 0.000214\n",
      "Epoch: 4 \tTraining Loss: 0.000214\n",
      "Epoch: 5 \tTraining Loss: 0.000214\n",
      "Epoch: 6 \tTraining Loss: 0.000214\n",
      "Epoch: 7 \tTraining Loss: 0.000214\n",
      "Epoch: 8 \tTraining Loss: 0.000214\n",
      "Epoch: 9 \tTraining Loss: 0.000214\n",
      "Epoch: 10 \tTraining Loss: 0.000214\n",
      "Epoch: 11 \tTraining Loss: 0.000214\n",
      "Epoch: 12 \tTraining Loss: 0.000214\n",
      "Epoch: 13 \tTraining Loss: 0.000214\n",
      "Epoch: 14 \tTraining Loss: 0.000214\n",
      "Epoch: 15 \tTraining Loss: 0.000214\n",
      "Epoch: 16 \tTraining Loss: 0.000214\n",
      "Epoch: 17 \tTraining Loss: 0.000214\n",
      "Epoch: 18 \tTraining Loss: 0.000214\n",
      "Epoch: 1 \tTraining Loss: 0.000146\n",
      "Epoch: 2 \tTraining Loss: 0.000146\n",
      "Epoch: 3 \tTraining Loss: 0.000146\n",
      "Epoch: 4 \tTraining Loss: 0.000146\n",
      "Epoch: 5 \tTraining Loss: 0.000146\n",
      "Epoch: 6 \tTraining Loss: 0.000146\n",
      "Epoch: 7 \tTraining Loss: 0.000146\n",
      "Epoch: 8 \tTraining Loss: 0.000146\n",
      "Epoch: 9 \tTraining Loss: 0.000146\n",
      "Epoch: 10 \tTraining Loss: 0.000146\n",
      "Epoch: 11 \tTraining Loss: 0.000146\n",
      "Epoch: 12 \tTraining Loss: 0.000146\n",
      "Epoch: 13 \tTraining Loss: 0.000146\n",
      "Epoch: 14 \tTraining Loss: 0.000146\n",
      "Epoch: 15 \tTraining Loss: 0.000146\n",
      "Epoch: 16 \tTraining Loss: 0.000146\n",
      "Epoch: 17 \tTraining Loss: 0.000146\n",
      "Epoch: 18 \tTraining Loss: 0.000146\n",
      "Epoch: 1 \tTraining Loss: 0.000143\n",
      "Epoch: 2 \tTraining Loss: 0.000143\n",
      "Epoch: 3 \tTraining Loss: 0.000143\n",
      "Epoch: 4 \tTraining Loss: 0.000143\n",
      "Epoch: 5 \tTraining Loss: 0.000143\n",
      "Epoch: 6 \tTraining Loss: 0.000143\n",
      "Epoch: 7 \tTraining Loss: 0.000143\n",
      "Epoch: 8 \tTraining Loss: 0.000143\n",
      "Epoch: 9 \tTraining Loss: 0.000143\n",
      "Epoch: 10 \tTraining Loss: 0.000143\n",
      "Epoch: 11 \tTraining Loss: 0.000143\n",
      "Epoch: 12 \tTraining Loss: 0.000143\n",
      "Epoch: 13 \tTraining Loss: 0.000143\n",
      "Epoch: 14 \tTraining Loss: 0.000143\n",
      "Epoch: 15 \tTraining Loss: 0.000143\n",
      "Epoch: 16 \tTraining Loss: 0.000143\n",
      "Epoch: 17 \tTraining Loss: 0.000143\n",
      "Epoch: 18 \tTraining Loss: 0.000143\n",
      "Epoch: 1 \tTraining Loss: 0.000191\n",
      "Epoch: 2 \tTraining Loss: 0.000191\n",
      "Epoch: 3 \tTraining Loss: 0.000191\n",
      "Epoch: 4 \tTraining Loss: 0.000191\n",
      "Epoch: 5 \tTraining Loss: 0.000191\n",
      "Epoch: 6 \tTraining Loss: 0.000191\n",
      "Epoch: 7 \tTraining Loss: 0.000191\n",
      "Epoch: 8 \tTraining Loss: 0.000191\n",
      "Epoch: 9 \tTraining Loss: 0.000191\n",
      "Epoch: 10 \tTraining Loss: 0.000191\n",
      "Epoch: 11 \tTraining Loss: 0.000191\n",
      "Epoch: 12 \tTraining Loss: 0.000191\n",
      "Epoch: 13 \tTraining Loss: 0.000191\n",
      "Epoch: 14 \tTraining Loss: 0.000191\n",
      "Epoch: 15 \tTraining Loss: 0.000191\n",
      "Epoch: 16 \tTraining Loss: 0.000191\n",
      "Epoch: 17 \tTraining Loss: 0.000191\n",
      "Epoch: 18 \tTraining Loss: 0.000191\n",
      "Epoch: 1 \tTraining Loss: 0.000244\n",
      "Epoch: 2 \tTraining Loss: 0.000244\n",
      "Epoch: 3 \tTraining Loss: 0.000244\n",
      "Epoch: 4 \tTraining Loss: 0.000244\n",
      "Epoch: 5 \tTraining Loss: 0.000244\n",
      "Epoch: 6 \tTraining Loss: 0.000244\n",
      "Epoch: 7 \tTraining Loss: 0.000244\n",
      "Epoch: 8 \tTraining Loss: 0.000244\n",
      "Epoch: 9 \tTraining Loss: 0.000244\n",
      "Epoch: 10 \tTraining Loss: 0.000244\n",
      "Epoch: 11 \tTraining Loss: 0.000244\n",
      "Epoch: 12 \tTraining Loss: 0.000244\n",
      "Epoch: 13 \tTraining Loss: 0.000244\n",
      "Epoch: 14 \tTraining Loss: 0.000244\n",
      "Epoch: 15 \tTraining Loss: 0.000244\n",
      "Epoch: 16 \tTraining Loss: 0.000244\n",
      "Epoch: 17 \tTraining Loss: 0.000244\n",
      "Epoch: 18 \tTraining Loss: 0.000244\n",
      "Epoch: 1 \tTraining Loss: 0.000095\n",
      "Epoch: 2 \tTraining Loss: 0.000095\n",
      "Epoch: 3 \tTraining Loss: 0.000095\n",
      "Epoch: 4 \tTraining Loss: 0.000095\n",
      "Epoch: 5 \tTraining Loss: 0.000095\n",
      "Epoch: 6 \tTraining Loss: 0.000095\n",
      "Epoch: 7 \tTraining Loss: 0.000095\n",
      "Epoch: 8 \tTraining Loss: 0.000095\n",
      "Epoch: 9 \tTraining Loss: 0.000095\n",
      "Epoch: 10 \tTraining Loss: 0.000095\n",
      "Epoch: 11 \tTraining Loss: 0.000095\n",
      "Epoch: 12 \tTraining Loss: 0.000095\n",
      "Epoch: 13 \tTraining Loss: 0.000095\n",
      "Epoch: 14 \tTraining Loss: 0.000095\n",
      "Epoch: 15 \tTraining Loss: 0.000095\n",
      "Epoch: 16 \tTraining Loss: 0.000095\n",
      "Epoch: 17 \tTraining Loss: 0.000095\n",
      "Epoch: 18 \tTraining Loss: 0.000095\n",
      "Epoch: 1 \tTraining Loss: 0.000219\n",
      "Epoch: 2 \tTraining Loss: 0.000219\n",
      "Epoch: 3 \tTraining Loss: 0.000219\n",
      "Epoch: 4 \tTraining Loss: 0.000219\n",
      "Epoch: 5 \tTraining Loss: 0.000219\n",
      "Epoch: 6 \tTraining Loss: 0.000219\n",
      "Epoch: 7 \tTraining Loss: 0.000219\n",
      "Epoch: 8 \tTraining Loss: 0.000219\n",
      "Epoch: 9 \tTraining Loss: 0.000219\n",
      "Epoch: 10 \tTraining Loss: 0.000219\n",
      "Epoch: 11 \tTraining Loss: 0.000219\n",
      "Epoch: 12 \tTraining Loss: 0.000219\n",
      "Epoch: 13 \tTraining Loss: 0.000219\n",
      "Epoch: 14 \tTraining Loss: 0.000219\n",
      "Epoch: 15 \tTraining Loss: 0.000219\n",
      "Epoch: 16 \tTraining Loss: 0.000219\n",
      "Epoch: 17 \tTraining Loss: 0.000219\n",
      "Epoch: 18 \tTraining Loss: 0.000219\n",
      "Epoch: 1 \tTraining Loss: 0.000277\n",
      "Epoch: 2 \tTraining Loss: 0.000277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \tTraining Loss: 0.000277\n",
      "Epoch: 4 \tTraining Loss: 0.000277\n",
      "Epoch: 5 \tTraining Loss: 0.000277\n",
      "Epoch: 6 \tTraining Loss: 0.000277\n",
      "Epoch: 7 \tTraining Loss: 0.000277\n",
      "Epoch: 8 \tTraining Loss: 0.000277\n",
      "Epoch: 9 \tTraining Loss: 0.000277\n",
      "Epoch: 10 \tTraining Loss: 0.000277\n",
      "Epoch: 11 \tTraining Loss: 0.000277\n",
      "Epoch: 12 \tTraining Loss: 0.000277\n",
      "Epoch: 13 \tTraining Loss: 0.000277\n",
      "Epoch: 14 \tTraining Loss: 0.000277\n",
      "Epoch: 15 \tTraining Loss: 0.000277\n",
      "Epoch: 16 \tTraining Loss: 0.000277\n",
      "Epoch: 17 \tTraining Loss: 0.000277\n",
      "Epoch: 18 \tTraining Loss: 0.000277\n",
      "Epoch: 1 \tTraining Loss: 0.000172\n",
      "Epoch: 2 \tTraining Loss: 0.000172\n",
      "Epoch: 3 \tTraining Loss: 0.000172\n",
      "Epoch: 4 \tTraining Loss: 0.000172\n",
      "Epoch: 5 \tTraining Loss: 0.000172\n",
      "Epoch: 6 \tTraining Loss: 0.000172\n",
      "Epoch: 7 \tTraining Loss: 0.000172\n",
      "Epoch: 8 \tTraining Loss: 0.000172\n",
      "Epoch: 9 \tTraining Loss: 0.000172\n",
      "Epoch: 10 \tTraining Loss: 0.000172\n",
      "Epoch: 11 \tTraining Loss: 0.000172\n",
      "Epoch: 12 \tTraining Loss: 0.000172\n",
      "Epoch: 13 \tTraining Loss: 0.000172\n",
      "Epoch: 14 \tTraining Loss: 0.000172\n",
      "Epoch: 15 \tTraining Loss: 0.000172\n",
      "Epoch: 16 \tTraining Loss: 0.000172\n",
      "Epoch: 17 \tTraining Loss: 0.000172\n",
      "Epoch: 18 \tTraining Loss: 0.000172\n",
      "Epoch: 1 \tTraining Loss: 0.000144\n",
      "Epoch: 2 \tTraining Loss: 0.000144\n",
      "Epoch: 3 \tTraining Loss: 0.000144\n",
      "Epoch: 4 \tTraining Loss: 0.000144\n",
      "Epoch: 5 \tTraining Loss: 0.000144\n",
      "Epoch: 6 \tTraining Loss: 0.000144\n",
      "Epoch: 7 \tTraining Loss: 0.000144\n",
      "Epoch: 8 \tTraining Loss: 0.000144\n",
      "Epoch: 9 \tTraining Loss: 0.000144\n",
      "Epoch: 10 \tTraining Loss: 0.000144\n",
      "Epoch: 11 \tTraining Loss: 0.000144\n",
      "Epoch: 12 \tTraining Loss: 0.000144\n",
      "Epoch: 13 \tTraining Loss: 0.000144\n",
      "Epoch: 14 \tTraining Loss: 0.000144\n",
      "Epoch: 15 \tTraining Loss: 0.000144\n",
      "Epoch: 16 \tTraining Loss: 0.000144\n",
      "Epoch: 17 \tTraining Loss: 0.000144\n",
      "Epoch: 18 \tTraining Loss: 0.000144\n",
      "Epoch: 1 \tTraining Loss: 0.000159\n",
      "Epoch: 2 \tTraining Loss: 0.000159\n",
      "Epoch: 3 \tTraining Loss: 0.000159\n",
      "Epoch: 4 \tTraining Loss: 0.000159\n",
      "Epoch: 5 \tTraining Loss: 0.000159\n",
      "Epoch: 6 \tTraining Loss: 0.000159\n",
      "Epoch: 7 \tTraining Loss: 0.000159\n",
      "Epoch: 8 \tTraining Loss: 0.000159\n",
      "Epoch: 9 \tTraining Loss: 0.000159\n",
      "Epoch: 10 \tTraining Loss: 0.000159\n",
      "Epoch: 11 \tTraining Loss: 0.000159\n",
      "Epoch: 12 \tTraining Loss: 0.000159\n",
      "Epoch: 13 \tTraining Loss: 0.000159\n",
      "Epoch: 14 \tTraining Loss: 0.000159\n",
      "Epoch: 15 \tTraining Loss: 0.000159\n",
      "Epoch: 16 \tTraining Loss: 0.000159\n",
      "Epoch: 17 \tTraining Loss: 0.000159\n",
      "Epoch: 18 \tTraining Loss: 0.000159\n",
      "Epoch: 1 \tTraining Loss: 0.000191\n",
      "Epoch: 2 \tTraining Loss: 0.000191\n",
      "Epoch: 3 \tTraining Loss: 0.000191\n",
      "Epoch: 4 \tTraining Loss: 0.000191\n",
      "Epoch: 5 \tTraining Loss: 0.000191\n",
      "Epoch: 6 \tTraining Loss: 0.000191\n",
      "Epoch: 7 \tTraining Loss: 0.000191\n",
      "Epoch: 8 \tTraining Loss: 0.000191\n",
      "Epoch: 9 \tTraining Loss: 0.000191\n",
      "Epoch: 10 \tTraining Loss: 0.000191\n",
      "Epoch: 11 \tTraining Loss: 0.000191\n",
      "Epoch: 12 \tTraining Loss: 0.000191\n",
      "Epoch: 13 \tTraining Loss: 0.000191\n",
      "Epoch: 14 \tTraining Loss: 0.000191\n",
      "Epoch: 15 \tTraining Loss: 0.000191\n",
      "Epoch: 16 \tTraining Loss: 0.000191\n",
      "Epoch: 17 \tTraining Loss: 0.000191\n",
      "Epoch: 18 \tTraining Loss: 0.000191\n",
      "Epoch: 1 \tTraining Loss: 0.000228\n",
      "Epoch: 2 \tTraining Loss: 0.000228\n",
      "Epoch: 3 \tTraining Loss: 0.000228\n",
      "Epoch: 4 \tTraining Loss: 0.000228\n",
      "Epoch: 5 \tTraining Loss: 0.000228\n",
      "Epoch: 6 \tTraining Loss: 0.000228\n",
      "Epoch: 7 \tTraining Loss: 0.000228\n",
      "Epoch: 8 \tTraining Loss: 0.000228\n",
      "Epoch: 9 \tTraining Loss: 0.000228\n",
      "Epoch: 10 \tTraining Loss: 0.000228\n",
      "Epoch: 11 \tTraining Loss: 0.000228\n",
      "Epoch: 12 \tTraining Loss: 0.000228\n",
      "Epoch: 13 \tTraining Loss: 0.000228\n",
      "Epoch: 14 \tTraining Loss: 0.000228\n",
      "Epoch: 15 \tTraining Loss: 0.000228\n",
      "Epoch: 16 \tTraining Loss: 0.000228\n",
      "Epoch: 17 \tTraining Loss: 0.000228\n",
      "Epoch: 18 \tTraining Loss: 0.000228\n",
      "Epoch: 1 \tTraining Loss: 0.000157\n",
      "Epoch: 2 \tTraining Loss: 0.000157\n",
      "Epoch: 3 \tTraining Loss: 0.000157\n",
      "Epoch: 4 \tTraining Loss: 0.000157\n",
      "Epoch: 5 \tTraining Loss: 0.000157\n",
      "Epoch: 6 \tTraining Loss: 0.000157\n",
      "Epoch: 7 \tTraining Loss: 0.000157\n",
      "Epoch: 8 \tTraining Loss: 0.000157\n",
      "Epoch: 9 \tTraining Loss: 0.000157\n",
      "Epoch: 10 \tTraining Loss: 0.000157\n",
      "Epoch: 11 \tTraining Loss: 0.000157\n",
      "Epoch: 12 \tTraining Loss: 0.000157\n",
      "Epoch: 13 \tTraining Loss: 0.000157\n",
      "Epoch: 14 \tTraining Loss: 0.000157\n",
      "Epoch: 15 \tTraining Loss: 0.000157\n",
      "Epoch: 16 \tTraining Loss: 0.000157\n",
      "Epoch: 17 \tTraining Loss: 0.000157\n",
      "Epoch: 18 \tTraining Loss: 0.000157\n",
      "Epoch: 1 \tTraining Loss: 0.000088\n",
      "Epoch: 2 \tTraining Loss: 0.000088\n",
      "Epoch: 3 \tTraining Loss: 0.000088\n",
      "Epoch: 4 \tTraining Loss: 0.000088\n",
      "Epoch: 5 \tTraining Loss: 0.000088\n",
      "Epoch: 6 \tTraining Loss: 0.000088\n",
      "Epoch: 7 \tTraining Loss: 0.000088\n",
      "Epoch: 8 \tTraining Loss: 0.000088\n",
      "Epoch: 9 \tTraining Loss: 0.000088\n",
      "Epoch: 10 \tTraining Loss: 0.000088\n",
      "Epoch: 11 \tTraining Loss: 0.000088\n",
      "Epoch: 12 \tTraining Loss: 0.000088\n",
      "Epoch: 13 \tTraining Loss: 0.000088\n",
      "Epoch: 14 \tTraining Loss: 0.000088\n",
      "Epoch: 15 \tTraining Loss: 0.000088\n",
      "Epoch: 16 \tTraining Loss: 0.000088\n",
      "Epoch: 17 \tTraining Loss: 0.000088\n",
      "Epoch: 18 \tTraining Loss: 0.000088\n",
      "Epoch: 1 \tTraining Loss: 0.000159\n",
      "Epoch: 2 \tTraining Loss: 0.000159\n",
      "Epoch: 3 \tTraining Loss: 0.000159\n",
      "Epoch: 4 \tTraining Loss: 0.000159\n",
      "Epoch: 5 \tTraining Loss: 0.000159\n",
      "Epoch: 6 \tTraining Loss: 0.000159\n",
      "Epoch: 7 \tTraining Loss: 0.000159\n",
      "Epoch: 8 \tTraining Loss: 0.000159\n",
      "Epoch: 9 \tTraining Loss: 0.000159\n",
      "Epoch: 10 \tTraining Loss: 0.000159\n",
      "Epoch: 11 \tTraining Loss: 0.000159\n",
      "Epoch: 12 \tTraining Loss: 0.000159\n",
      "Epoch: 13 \tTraining Loss: 0.000159\n",
      "Epoch: 14 \tTraining Loss: 0.000159\n",
      "Epoch: 15 \tTraining Loss: 0.000159\n",
      "Epoch: 16 \tTraining Loss: 0.000159\n",
      "Epoch: 17 \tTraining Loss: 0.000159\n",
      "Epoch: 18 \tTraining Loss: 0.000159\n",
      "Epoch: 1 \tTraining Loss: 0.000245\n",
      "Epoch: 2 \tTraining Loss: 0.000245\n",
      "Epoch: 3 \tTraining Loss: 0.000245\n",
      "Epoch: 4 \tTraining Loss: 0.000245\n",
      "Epoch: 5 \tTraining Loss: 0.000245\n",
      "Epoch: 6 \tTraining Loss: 0.000245\n",
      "Epoch: 7 \tTraining Loss: 0.000245\n",
      "Epoch: 8 \tTraining Loss: 0.000245\n",
      "Epoch: 9 \tTraining Loss: 0.000245\n",
      "Epoch: 10 \tTraining Loss: 0.000245\n",
      "Epoch: 11 \tTraining Loss: 0.000245\n",
      "Epoch: 12 \tTraining Loss: 0.000245\n",
      "Epoch: 13 \tTraining Loss: 0.000245\n",
      "Epoch: 14 \tTraining Loss: 0.000245\n",
      "Epoch: 15 \tTraining Loss: 0.000245\n",
      "Epoch: 16 \tTraining Loss: 0.000245\n",
      "Epoch: 17 \tTraining Loss: 0.000245\n",
      "Epoch: 18 \tTraining Loss: 0.000245\n",
      "Epoch: 1 \tTraining Loss: 0.000512\n",
      "Epoch: 2 \tTraining Loss: 0.000512\n",
      "Epoch: 3 \tTraining Loss: 0.000512\n",
      "Epoch: 4 \tTraining Loss: 0.000512\n",
      "Epoch: 5 \tTraining Loss: 0.000512\n",
      "Epoch: 6 \tTraining Loss: 0.000512\n",
      "Epoch: 7 \tTraining Loss: 0.000512\n",
      "Epoch: 8 \tTraining Loss: 0.000512\n",
      "Epoch: 9 \tTraining Loss: 0.000512\n",
      "Epoch: 10 \tTraining Loss: 0.000512\n",
      "Epoch: 11 \tTraining Loss: 0.000512\n",
      "Epoch: 12 \tTraining Loss: 0.000512\n",
      "Epoch: 13 \tTraining Loss: 0.000512\n",
      "Epoch: 14 \tTraining Loss: 0.000512\n",
      "Epoch: 15 \tTraining Loss: 0.000512\n",
      "Epoch: 16 \tTraining Loss: 0.000512\n",
      "Epoch: 17 \tTraining Loss: 0.000512\n",
      "Epoch: 18 \tTraining Loss: 0.000512\n",
      "Epoch: 1 \tTraining Loss: 0.000084\n",
      "Epoch: 2 \tTraining Loss: 0.000084\n",
      "Epoch: 3 \tTraining Loss: 0.000084\n",
      "Epoch: 4 \tTraining Loss: 0.000084\n",
      "Epoch: 5 \tTraining Loss: 0.000084\n",
      "Epoch: 6 \tTraining Loss: 0.000084\n",
      "Epoch: 7 \tTraining Loss: 0.000084\n",
      "Epoch: 8 \tTraining Loss: 0.000084\n",
      "Epoch: 9 \tTraining Loss: 0.000084\n",
      "Epoch: 10 \tTraining Loss: 0.000084\n",
      "Epoch: 11 \tTraining Loss: 0.000084\n",
      "Epoch: 12 \tTraining Loss: 0.000084\n",
      "Epoch: 13 \tTraining Loss: 0.000084\n",
      "Epoch: 14 \tTraining Loss: 0.000084\n",
      "Epoch: 15 \tTraining Loss: 0.000084\n",
      "Epoch: 16 \tTraining Loss: 0.000084\n",
      "Epoch: 17 \tTraining Loss: 0.000084\n",
      "Epoch: 18 \tTraining Loss: 0.000084\n",
      "Epoch: 1 \tTraining Loss: 0.000388\n",
      "Epoch: 2 \tTraining Loss: 0.000388\n",
      "Epoch: 3 \tTraining Loss: 0.000388\n",
      "Epoch: 4 \tTraining Loss: 0.000388\n",
      "Epoch: 5 \tTraining Loss: 0.000388\n",
      "Epoch: 6 \tTraining Loss: 0.000388\n",
      "Epoch: 7 \tTraining Loss: 0.000388\n",
      "Epoch: 8 \tTraining Loss: 0.000388\n",
      "Epoch: 9 \tTraining Loss: 0.000388\n",
      "Epoch: 10 \tTraining Loss: 0.000388\n",
      "Epoch: 11 \tTraining Loss: 0.000388\n",
      "Epoch: 12 \tTraining Loss: 0.000388\n",
      "Epoch: 13 \tTraining Loss: 0.000388\n",
      "Epoch: 14 \tTraining Loss: 0.000388\n",
      "Epoch: 15 \tTraining Loss: 0.000388\n",
      "Epoch: 16 \tTraining Loss: 0.000388\n",
      "Epoch: 17 \tTraining Loss: 0.000388\n",
      "Epoch: 18 \tTraining Loss: 0.000388\n",
      "Epoch: 1 \tTraining Loss: 0.000307\n",
      "Epoch: 2 \tTraining Loss: 0.000307\n",
      "Epoch: 3 \tTraining Loss: 0.000307\n",
      "Epoch: 4 \tTraining Loss: 0.000307\n",
      "Epoch: 5 \tTraining Loss: 0.000307\n",
      "Epoch: 6 \tTraining Loss: 0.000307\n",
      "Epoch: 7 \tTraining Loss: 0.000307\n",
      "Epoch: 8 \tTraining Loss: 0.000307\n",
      "Epoch: 9 \tTraining Loss: 0.000307\n",
      "Epoch: 10 \tTraining Loss: 0.000307\n",
      "Epoch: 11 \tTraining Loss: 0.000307\n",
      "Epoch: 12 \tTraining Loss: 0.000307\n",
      "Epoch: 13 \tTraining Loss: 0.000307\n",
      "Epoch: 14 \tTraining Loss: 0.000307\n",
      "Epoch: 15 \tTraining Loss: 0.000307\n",
      "Epoch: 16 \tTraining Loss: 0.000307\n",
      "Epoch: 17 \tTraining Loss: 0.000307\n",
      "Epoch: 18 \tTraining Loss: 0.000307\n",
      "Epoch: 1 \tTraining Loss: 0.000151\n",
      "Epoch: 2 \tTraining Loss: 0.000151\n",
      "Epoch: 3 \tTraining Loss: 0.000151\n",
      "Epoch: 4 \tTraining Loss: 0.000151\n",
      "Epoch: 5 \tTraining Loss: 0.000151\n",
      "Epoch: 6 \tTraining Loss: 0.000151\n",
      "Epoch: 7 \tTraining Loss: 0.000151\n",
      "Epoch: 8 \tTraining Loss: 0.000151\n",
      "Epoch: 9 \tTraining Loss: 0.000151\n",
      "Epoch: 10 \tTraining Loss: 0.000151\n",
      "Epoch: 11 \tTraining Loss: 0.000151\n",
      "Epoch: 12 \tTraining Loss: 0.000151\n",
      "Epoch: 13 \tTraining Loss: 0.000151\n",
      "Epoch: 14 \tTraining Loss: 0.000151\n",
      "Epoch: 15 \tTraining Loss: 0.000151\n",
      "Epoch: 16 \tTraining Loss: 0.000151\n",
      "Epoch: 17 \tTraining Loss: 0.000151\n",
      "Epoch: 18 \tTraining Loss: 0.000151\n",
      "Epoch: 1 \tTraining Loss: 0.000305\n",
      "Epoch: 2 \tTraining Loss: 0.000305\n",
      "Epoch: 3 \tTraining Loss: 0.000305\n",
      "Epoch: 4 \tTraining Loss: 0.000305\n",
      "Epoch: 5 \tTraining Loss: 0.000305\n",
      "Epoch: 6 \tTraining Loss: 0.000305\n",
      "Epoch: 7 \tTraining Loss: 0.000305\n",
      "Epoch: 8 \tTraining Loss: 0.000305\n",
      "Epoch: 9 \tTraining Loss: 0.000305\n",
      "Epoch: 10 \tTraining Loss: 0.000305\n",
      "Epoch: 11 \tTraining Loss: 0.000305\n",
      "Epoch: 12 \tTraining Loss: 0.000305\n",
      "Epoch: 13 \tTraining Loss: 0.000305\n",
      "Epoch: 14 \tTraining Loss: 0.000305\n",
      "Epoch: 15 \tTraining Loss: 0.000305\n",
      "Epoch: 16 \tTraining Loss: 0.000305\n",
      "Epoch: 17 \tTraining Loss: 0.000305\n",
      "Epoch: 18 \tTraining Loss: 0.000305\n",
      "Epoch: 1 \tTraining Loss: 0.000207\n",
      "Epoch: 2 \tTraining Loss: 0.000207\n",
      "Epoch: 3 \tTraining Loss: 0.000207\n",
      "Epoch: 4 \tTraining Loss: 0.000207\n",
      "Epoch: 5 \tTraining Loss: 0.000207\n",
      "Epoch: 6 \tTraining Loss: 0.000207\n",
      "Epoch: 7 \tTraining Loss: 0.000207\n",
      "Epoch: 8 \tTraining Loss: 0.000207\n",
      "Epoch: 9 \tTraining Loss: 0.000207\n",
      "Epoch: 10 \tTraining Loss: 0.000207\n",
      "Epoch: 11 \tTraining Loss: 0.000207\n",
      "Epoch: 12 \tTraining Loss: 0.000207\n",
      "Epoch: 13 \tTraining Loss: 0.000207\n",
      "Epoch: 14 \tTraining Loss: 0.000207\n",
      "Epoch: 15 \tTraining Loss: 0.000207\n",
      "Epoch: 16 \tTraining Loss: 0.000207\n",
      "Epoch: 17 \tTraining Loss: 0.000207\n",
      "Epoch: 18 \tTraining Loss: 0.000207\n",
      "Epoch: 1 \tTraining Loss: 0.000117\n",
      "Epoch: 2 \tTraining Loss: 0.000117\n",
      "Epoch: 3 \tTraining Loss: 0.000117\n",
      "Epoch: 4 \tTraining Loss: 0.000117\n",
      "Epoch: 5 \tTraining Loss: 0.000117\n",
      "Epoch: 6 \tTraining Loss: 0.000117\n",
      "Epoch: 7 \tTraining Loss: 0.000117\n",
      "Epoch: 8 \tTraining Loss: 0.000117\n",
      "Epoch: 9 \tTraining Loss: 0.000117\n",
      "Epoch: 10 \tTraining Loss: 0.000117\n",
      "Epoch: 11 \tTraining Loss: 0.000117\n",
      "Epoch: 12 \tTraining Loss: 0.000117\n",
      "Epoch: 13 \tTraining Loss: 0.000117\n",
      "Epoch: 14 \tTraining Loss: 0.000117\n",
      "Epoch: 15 \tTraining Loss: 0.000117\n",
      "Epoch: 16 \tTraining Loss: 0.000117\n",
      "Epoch: 17 \tTraining Loss: 0.000117\n",
      "Epoch: 18 \tTraining Loss: 0.000117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000183\n",
      "Epoch: 2 \tTraining Loss: 0.000183\n",
      "Epoch: 3 \tTraining Loss: 0.000183\n",
      "Epoch: 4 \tTraining Loss: 0.000183\n",
      "Epoch: 5 \tTraining Loss: 0.000183\n",
      "Epoch: 6 \tTraining Loss: 0.000183\n",
      "Epoch: 7 \tTraining Loss: 0.000183\n",
      "Epoch: 8 \tTraining Loss: 0.000183\n",
      "Epoch: 9 \tTraining Loss: 0.000183\n",
      "Epoch: 10 \tTraining Loss: 0.000183\n",
      "Epoch: 11 \tTraining Loss: 0.000183\n",
      "Epoch: 12 \tTraining Loss: 0.000183\n",
      "Epoch: 13 \tTraining Loss: 0.000183\n",
      "Epoch: 14 \tTraining Loss: 0.000183\n",
      "Epoch: 15 \tTraining Loss: 0.000183\n",
      "Epoch: 16 \tTraining Loss: 0.000183\n",
      "Epoch: 17 \tTraining Loss: 0.000183\n",
      "Epoch: 18 \tTraining Loss: 0.000183\n",
      "Epoch: 1 \tTraining Loss: 0.000158\n",
      "Epoch: 2 \tTraining Loss: 0.000158\n",
      "Epoch: 3 \tTraining Loss: 0.000158\n",
      "Epoch: 4 \tTraining Loss: 0.000158\n",
      "Epoch: 5 \tTraining Loss: 0.000158\n",
      "Epoch: 6 \tTraining Loss: 0.000158\n",
      "Epoch: 7 \tTraining Loss: 0.000158\n",
      "Epoch: 8 \tTraining Loss: 0.000158\n",
      "Epoch: 9 \tTraining Loss: 0.000158\n",
      "Epoch: 10 \tTraining Loss: 0.000158\n",
      "Epoch: 11 \tTraining Loss: 0.000158\n",
      "Epoch: 12 \tTraining Loss: 0.000158\n",
      "Epoch: 13 \tTraining Loss: 0.000158\n",
      "Epoch: 14 \tTraining Loss: 0.000158\n",
      "Epoch: 15 \tTraining Loss: 0.000158\n",
      "Epoch: 16 \tTraining Loss: 0.000158\n",
      "Epoch: 17 \tTraining Loss: 0.000158\n",
      "Epoch: 18 \tTraining Loss: 0.000158\n",
      "Epoch: 1 \tTraining Loss: 0.000261\n",
      "Epoch: 2 \tTraining Loss: 0.000261\n",
      "Epoch: 3 \tTraining Loss: 0.000261\n",
      "Epoch: 4 \tTraining Loss: 0.000261\n",
      "Epoch: 5 \tTraining Loss: 0.000261\n",
      "Epoch: 6 \tTraining Loss: 0.000261\n",
      "Epoch: 7 \tTraining Loss: 0.000261\n",
      "Epoch: 8 \tTraining Loss: 0.000261\n",
      "Epoch: 9 \tTraining Loss: 0.000261\n",
      "Epoch: 10 \tTraining Loss: 0.000261\n",
      "Epoch: 11 \tTraining Loss: 0.000261\n",
      "Epoch: 12 \tTraining Loss: 0.000261\n",
      "Epoch: 13 \tTraining Loss: 0.000261\n",
      "Epoch: 14 \tTraining Loss: 0.000261\n",
      "Epoch: 15 \tTraining Loss: 0.000261\n",
      "Epoch: 16 \tTraining Loss: 0.000261\n",
      "Epoch: 17 \tTraining Loss: 0.000261\n",
      "Epoch: 18 \tTraining Loss: 0.000261\n",
      "Epoch: 1 \tTraining Loss: 0.000217\n",
      "Epoch: 2 \tTraining Loss: 0.000217\n",
      "Epoch: 3 \tTraining Loss: 0.000217\n",
      "Epoch: 4 \tTraining Loss: 0.000217\n",
      "Epoch: 5 \tTraining Loss: 0.000217\n",
      "Epoch: 6 \tTraining Loss: 0.000217\n",
      "Epoch: 7 \tTraining Loss: 0.000217\n",
      "Epoch: 8 \tTraining Loss: 0.000217\n",
      "Epoch: 9 \tTraining Loss: 0.000217\n",
      "Epoch: 10 \tTraining Loss: 0.000217\n",
      "Epoch: 11 \tTraining Loss: 0.000217\n",
      "Epoch: 12 \tTraining Loss: 0.000217\n",
      "Epoch: 13 \tTraining Loss: 0.000217\n",
      "Epoch: 14 \tTraining Loss: 0.000217\n",
      "Epoch: 15 \tTraining Loss: 0.000217\n",
      "Epoch: 16 \tTraining Loss: 0.000217\n",
      "Epoch: 17 \tTraining Loss: 0.000217\n",
      "Epoch: 18 \tTraining Loss: 0.000217\n",
      "Epoch: 1 \tTraining Loss: 0.000077\n",
      "Epoch: 2 \tTraining Loss: 0.000077\n",
      "Epoch: 3 \tTraining Loss: 0.000077\n",
      "Epoch: 4 \tTraining Loss: 0.000077\n",
      "Epoch: 5 \tTraining Loss: 0.000077\n",
      "Epoch: 6 \tTraining Loss: 0.000077\n",
      "Epoch: 7 \tTraining Loss: 0.000077\n",
      "Epoch: 8 \tTraining Loss: 0.000077\n",
      "Epoch: 9 \tTraining Loss: 0.000077\n",
      "Epoch: 10 \tTraining Loss: 0.000077\n",
      "Epoch: 11 \tTraining Loss: 0.000077\n",
      "Epoch: 12 \tTraining Loss: 0.000077\n",
      "Epoch: 13 \tTraining Loss: 0.000077\n",
      "Epoch: 14 \tTraining Loss: 0.000077\n",
      "Epoch: 15 \tTraining Loss: 0.000077\n",
      "Epoch: 16 \tTraining Loss: 0.000077\n",
      "Epoch: 17 \tTraining Loss: 0.000077\n",
      "Epoch: 18 \tTraining Loss: 0.000077\n",
      "Epoch: 1 \tTraining Loss: 0.000113\n",
      "Epoch: 2 \tTraining Loss: 0.000113\n",
      "Epoch: 3 \tTraining Loss: 0.000113\n",
      "Epoch: 4 \tTraining Loss: 0.000113\n",
      "Epoch: 5 \tTraining Loss: 0.000113\n",
      "Epoch: 6 \tTraining Loss: 0.000113\n",
      "Epoch: 7 \tTraining Loss: 0.000113\n",
      "Epoch: 8 \tTraining Loss: 0.000113\n",
      "Epoch: 9 \tTraining Loss: 0.000113\n",
      "Epoch: 10 \tTraining Loss: 0.000113\n",
      "Epoch: 11 \tTraining Loss: 0.000113\n",
      "Epoch: 12 \tTraining Loss: 0.000113\n",
      "Epoch: 13 \tTraining Loss: 0.000113\n",
      "Epoch: 14 \tTraining Loss: 0.000113\n",
      "Epoch: 15 \tTraining Loss: 0.000113\n",
      "Epoch: 16 \tTraining Loss: 0.000113\n",
      "Epoch: 17 \tTraining Loss: 0.000113\n",
      "Epoch: 18 \tTraining Loss: 0.000113\n",
      "Epoch: 1 \tTraining Loss: 0.000129\n",
      "Epoch: 2 \tTraining Loss: 0.000129\n",
      "Epoch: 3 \tTraining Loss: 0.000129\n",
      "Epoch: 4 \tTraining Loss: 0.000129\n",
      "Epoch: 5 \tTraining Loss: 0.000129\n",
      "Epoch: 6 \tTraining Loss: 0.000129\n",
      "Epoch: 7 \tTraining Loss: 0.000129\n",
      "Epoch: 8 \tTraining Loss: 0.000129\n",
      "Epoch: 9 \tTraining Loss: 0.000129\n",
      "Epoch: 10 \tTraining Loss: 0.000129\n",
      "Epoch: 11 \tTraining Loss: 0.000129\n",
      "Epoch: 12 \tTraining Loss: 0.000129\n",
      "Epoch: 13 \tTraining Loss: 0.000129\n",
      "Epoch: 14 \tTraining Loss: 0.000129\n",
      "Epoch: 15 \tTraining Loss: 0.000129\n",
      "Epoch: 16 \tTraining Loss: 0.000129\n",
      "Epoch: 17 \tTraining Loss: 0.000129\n",
      "Epoch: 18 \tTraining Loss: 0.000129\n",
      "Epoch: 1 \tTraining Loss: 0.000236\n",
      "Epoch: 2 \tTraining Loss: 0.000236\n",
      "Epoch: 3 \tTraining Loss: 0.000236\n",
      "Epoch: 4 \tTraining Loss: 0.000236\n",
      "Epoch: 5 \tTraining Loss: 0.000236\n",
      "Epoch: 6 \tTraining Loss: 0.000236\n",
      "Epoch: 7 \tTraining Loss: 0.000236\n",
      "Epoch: 8 \tTraining Loss: 0.000236\n",
      "Epoch: 9 \tTraining Loss: 0.000236\n",
      "Epoch: 10 \tTraining Loss: 0.000236\n",
      "Epoch: 11 \tTraining Loss: 0.000236\n",
      "Epoch: 12 \tTraining Loss: 0.000236\n",
      "Epoch: 13 \tTraining Loss: 0.000236\n",
      "Epoch: 14 \tTraining Loss: 0.000236\n",
      "Epoch: 15 \tTraining Loss: 0.000236\n",
      "Epoch: 16 \tTraining Loss: 0.000236\n",
      "Epoch: 17 \tTraining Loss: 0.000236\n",
      "Epoch: 18 \tTraining Loss: 0.000236\n",
      "Epoch: 1 \tTraining Loss: 0.000318\n",
      "Epoch: 2 \tTraining Loss: 0.000318\n",
      "Epoch: 3 \tTraining Loss: 0.000318\n",
      "Epoch: 4 \tTraining Loss: 0.000318\n",
      "Epoch: 5 \tTraining Loss: 0.000318\n",
      "Epoch: 6 \tTraining Loss: 0.000318\n",
      "Epoch: 7 \tTraining Loss: 0.000318\n",
      "Epoch: 8 \tTraining Loss: 0.000318\n",
      "Epoch: 9 \tTraining Loss: 0.000318\n",
      "Epoch: 10 \tTraining Loss: 0.000318\n",
      "Epoch: 11 \tTraining Loss: 0.000318\n",
      "Epoch: 12 \tTraining Loss: 0.000318\n",
      "Epoch: 13 \tTraining Loss: 0.000318\n",
      "Epoch: 14 \tTraining Loss: 0.000318\n",
      "Epoch: 15 \tTraining Loss: 0.000318\n",
      "Epoch: 16 \tTraining Loss: 0.000318\n",
      "Epoch: 17 \tTraining Loss: 0.000318\n",
      "Epoch: 18 \tTraining Loss: 0.000318\n",
      "Epoch: 1 \tTraining Loss: 0.000127\n",
      "Epoch: 2 \tTraining Loss: 0.000127\n",
      "Epoch: 3 \tTraining Loss: 0.000127\n",
      "Epoch: 4 \tTraining Loss: 0.000127\n",
      "Epoch: 5 \tTraining Loss: 0.000127\n",
      "Epoch: 6 \tTraining Loss: 0.000127\n",
      "Epoch: 7 \tTraining Loss: 0.000127\n",
      "Epoch: 8 \tTraining Loss: 0.000127\n",
      "Epoch: 9 \tTraining Loss: 0.000127\n",
      "Epoch: 10 \tTraining Loss: 0.000127\n",
      "Epoch: 11 \tTraining Loss: 0.000127\n",
      "Epoch: 12 \tTraining Loss: 0.000127\n",
      "Epoch: 13 \tTraining Loss: 0.000127\n",
      "Epoch: 14 \tTraining Loss: 0.000127\n",
      "Epoch: 15 \tTraining Loss: 0.000127\n",
      "Epoch: 16 \tTraining Loss: 0.000127\n",
      "Epoch: 17 \tTraining Loss: 0.000127\n",
      "Epoch: 18 \tTraining Loss: 0.000127\n",
      "Epoch: 1 \tTraining Loss: 0.000183\n",
      "Epoch: 2 \tTraining Loss: 0.000183\n",
      "Epoch: 3 \tTraining Loss: 0.000183\n",
      "Epoch: 4 \tTraining Loss: 0.000183\n",
      "Epoch: 5 \tTraining Loss: 0.000183\n",
      "Epoch: 6 \tTraining Loss: 0.000183\n",
      "Epoch: 7 \tTraining Loss: 0.000183\n",
      "Epoch: 8 \tTraining Loss: 0.000183\n",
      "Epoch: 9 \tTraining Loss: 0.000183\n",
      "Epoch: 10 \tTraining Loss: 0.000183\n",
      "Epoch: 11 \tTraining Loss: 0.000183\n",
      "Epoch: 12 \tTraining Loss: 0.000183\n",
      "Epoch: 13 \tTraining Loss: 0.000183\n",
      "Epoch: 14 \tTraining Loss: 0.000183\n",
      "Epoch: 15 \tTraining Loss: 0.000183\n",
      "Epoch: 16 \tTraining Loss: 0.000183\n",
      "Epoch: 17 \tTraining Loss: 0.000183\n",
      "Epoch: 18 \tTraining Loss: 0.000183\n",
      "Epoch: 1 \tTraining Loss: 0.000227\n",
      "Epoch: 2 \tTraining Loss: 0.000227\n",
      "Epoch: 3 \tTraining Loss: 0.000227\n",
      "Epoch: 4 \tTraining Loss: 0.000227\n",
      "Epoch: 5 \tTraining Loss: 0.000227\n",
      "Epoch: 6 \tTraining Loss: 0.000227\n",
      "Epoch: 7 \tTraining Loss: 0.000227\n",
      "Epoch: 8 \tTraining Loss: 0.000227\n",
      "Epoch: 9 \tTraining Loss: 0.000227\n",
      "Epoch: 10 \tTraining Loss: 0.000227\n",
      "Epoch: 11 \tTraining Loss: 0.000227\n",
      "Epoch: 12 \tTraining Loss: 0.000227\n",
      "Epoch: 13 \tTraining Loss: 0.000227\n",
      "Epoch: 14 \tTraining Loss: 0.000227\n",
      "Epoch: 15 \tTraining Loss: 0.000227\n",
      "Epoch: 16 \tTraining Loss: 0.000227\n",
      "Epoch: 17 \tTraining Loss: 0.000227\n",
      "Epoch: 18 \tTraining Loss: 0.000227\n",
      "Epoch: 1 \tTraining Loss: 0.000286\n",
      "Epoch: 2 \tTraining Loss: 0.000286\n",
      "Epoch: 3 \tTraining Loss: 0.000286\n",
      "Epoch: 4 \tTraining Loss: 0.000286\n",
      "Epoch: 5 \tTraining Loss: 0.000286\n",
      "Epoch: 6 \tTraining Loss: 0.000286\n",
      "Epoch: 7 \tTraining Loss: 0.000286\n",
      "Epoch: 8 \tTraining Loss: 0.000286\n",
      "Epoch: 9 \tTraining Loss: 0.000286\n",
      "Epoch: 10 \tTraining Loss: 0.000286\n",
      "Epoch: 11 \tTraining Loss: 0.000286\n",
      "Epoch: 12 \tTraining Loss: 0.000286\n",
      "Epoch: 13 \tTraining Loss: 0.000286\n",
      "Epoch: 14 \tTraining Loss: 0.000286\n",
      "Epoch: 15 \tTraining Loss: 0.000286\n",
      "Epoch: 16 \tTraining Loss: 0.000286\n",
      "Epoch: 17 \tTraining Loss: 0.000286\n",
      "Epoch: 18 \tTraining Loss: 0.000286\n",
      "Epoch: 1 \tTraining Loss: 0.000152\n",
      "Epoch: 2 \tTraining Loss: 0.000152\n",
      "Epoch: 3 \tTraining Loss: 0.000152\n",
      "Epoch: 4 \tTraining Loss: 0.000152\n",
      "Epoch: 5 \tTraining Loss: 0.000152\n",
      "Epoch: 6 \tTraining Loss: 0.000152\n",
      "Epoch: 7 \tTraining Loss: 0.000152\n",
      "Epoch: 8 \tTraining Loss: 0.000152\n",
      "Epoch: 9 \tTraining Loss: 0.000152\n",
      "Epoch: 10 \tTraining Loss: 0.000152\n",
      "Epoch: 11 \tTraining Loss: 0.000152\n",
      "Epoch: 12 \tTraining Loss: 0.000152\n",
      "Epoch: 13 \tTraining Loss: 0.000152\n",
      "Epoch: 14 \tTraining Loss: 0.000152\n",
      "Epoch: 15 \tTraining Loss: 0.000152\n",
      "Epoch: 16 \tTraining Loss: 0.000152\n",
      "Epoch: 17 \tTraining Loss: 0.000152\n",
      "Epoch: 18 \tTraining Loss: 0.000152\n",
      "Epoch: 1 \tTraining Loss: 0.000350\n",
      "Epoch: 2 \tTraining Loss: 0.000350\n",
      "Epoch: 3 \tTraining Loss: 0.000350\n",
      "Epoch: 4 \tTraining Loss: 0.000350\n",
      "Epoch: 5 \tTraining Loss: 0.000350\n",
      "Epoch: 6 \tTraining Loss: 0.000350\n",
      "Epoch: 7 \tTraining Loss: 0.000350\n",
      "Epoch: 8 \tTraining Loss: 0.000350\n",
      "Epoch: 9 \tTraining Loss: 0.000350\n",
      "Epoch: 10 \tTraining Loss: 0.000350\n",
      "Epoch: 11 \tTraining Loss: 0.000350\n",
      "Epoch: 12 \tTraining Loss: 0.000350\n",
      "Epoch: 13 \tTraining Loss: 0.000350\n",
      "Epoch: 14 \tTraining Loss: 0.000350\n",
      "Epoch: 15 \tTraining Loss: 0.000350\n",
      "Epoch: 16 \tTraining Loss: 0.000350\n",
      "Epoch: 17 \tTraining Loss: 0.000350\n",
      "Epoch: 18 \tTraining Loss: 0.000350\n",
      "Epoch: 1 \tTraining Loss: 0.000178\n",
      "Epoch: 2 \tTraining Loss: 0.000178\n",
      "Epoch: 3 \tTraining Loss: 0.000178\n",
      "Epoch: 4 \tTraining Loss: 0.000178\n",
      "Epoch: 5 \tTraining Loss: 0.000178\n",
      "Epoch: 6 \tTraining Loss: 0.000178\n",
      "Epoch: 7 \tTraining Loss: 0.000178\n",
      "Epoch: 8 \tTraining Loss: 0.000178\n",
      "Epoch: 9 \tTraining Loss: 0.000178\n",
      "Epoch: 10 \tTraining Loss: 0.000178\n",
      "Epoch: 11 \tTraining Loss: 0.000178\n",
      "Epoch: 12 \tTraining Loss: 0.000178\n",
      "Epoch: 13 \tTraining Loss: 0.000178\n",
      "Epoch: 14 \tTraining Loss: 0.000178\n",
      "Epoch: 15 \tTraining Loss: 0.000178\n",
      "Epoch: 16 \tTraining Loss: 0.000178\n",
      "Epoch: 17 \tTraining Loss: 0.000178\n",
      "Epoch: 18 \tTraining Loss: 0.000178\n",
      "Epoch: 1 \tTraining Loss: 0.000320\n",
      "Epoch: 2 \tTraining Loss: 0.000320\n",
      "Epoch: 3 \tTraining Loss: 0.000320\n",
      "Epoch: 4 \tTraining Loss: 0.000320\n",
      "Epoch: 5 \tTraining Loss: 0.000320\n",
      "Epoch: 6 \tTraining Loss: 0.000320\n",
      "Epoch: 7 \tTraining Loss: 0.000320\n",
      "Epoch: 8 \tTraining Loss: 0.000320\n",
      "Epoch: 9 \tTraining Loss: 0.000320\n",
      "Epoch: 10 \tTraining Loss: 0.000320\n",
      "Epoch: 11 \tTraining Loss: 0.000320\n",
      "Epoch: 12 \tTraining Loss: 0.000320\n",
      "Epoch: 13 \tTraining Loss: 0.000320\n",
      "Epoch: 14 \tTraining Loss: 0.000320\n",
      "Epoch: 15 \tTraining Loss: 0.000320\n",
      "Epoch: 16 \tTraining Loss: 0.000320\n",
      "Epoch: 17 \tTraining Loss: 0.000320\n",
      "Epoch: 18 \tTraining Loss: 0.000320\n",
      "Epoch: 1 \tTraining Loss: 0.000160\n",
      "Epoch: 2 \tTraining Loss: 0.000160\n",
      "Epoch: 3 \tTraining Loss: 0.000160\n",
      "Epoch: 4 \tTraining Loss: 0.000160\n",
      "Epoch: 5 \tTraining Loss: 0.000160\n",
      "Epoch: 6 \tTraining Loss: 0.000160\n",
      "Epoch: 7 \tTraining Loss: 0.000160\n",
      "Epoch: 8 \tTraining Loss: 0.000160\n",
      "Epoch: 9 \tTraining Loss: 0.000160\n",
      "Epoch: 10 \tTraining Loss: 0.000160\n",
      "Epoch: 11 \tTraining Loss: 0.000160\n",
      "Epoch: 12 \tTraining Loss: 0.000160\n",
      "Epoch: 13 \tTraining Loss: 0.000160\n",
      "Epoch: 14 \tTraining Loss: 0.000160\n",
      "Epoch: 15 \tTraining Loss: 0.000160\n",
      "Epoch: 16 \tTraining Loss: 0.000160\n",
      "Epoch: 17 \tTraining Loss: 0.000160\n",
      "Epoch: 18 \tTraining Loss: 0.000160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000213\n",
      "Epoch: 2 \tTraining Loss: 0.000213\n",
      "Epoch: 3 \tTraining Loss: 0.000213\n",
      "Epoch: 4 \tTraining Loss: 0.000213\n",
      "Epoch: 5 \tTraining Loss: 0.000213\n",
      "Epoch: 6 \tTraining Loss: 0.000213\n",
      "Epoch: 7 \tTraining Loss: 0.000213\n",
      "Epoch: 8 \tTraining Loss: 0.000213\n",
      "Epoch: 9 \tTraining Loss: 0.000213\n",
      "Epoch: 10 \tTraining Loss: 0.000213\n",
      "Epoch: 11 \tTraining Loss: 0.000213\n",
      "Epoch: 12 \tTraining Loss: 0.000213\n",
      "Epoch: 13 \tTraining Loss: 0.000213\n",
      "Epoch: 14 \tTraining Loss: 0.000213\n",
      "Epoch: 15 \tTraining Loss: 0.000213\n",
      "Epoch: 16 \tTraining Loss: 0.000213\n",
      "Epoch: 17 \tTraining Loss: 0.000213\n",
      "Epoch: 18 \tTraining Loss: 0.000213\n",
      "Epoch: 1 \tTraining Loss: 0.000187\n",
      "Epoch: 2 \tTraining Loss: 0.000187\n",
      "Epoch: 3 \tTraining Loss: 0.000187\n",
      "Epoch: 4 \tTraining Loss: 0.000187\n",
      "Epoch: 5 \tTraining Loss: 0.000187\n",
      "Epoch: 6 \tTraining Loss: 0.000187\n",
      "Epoch: 7 \tTraining Loss: 0.000187\n",
      "Epoch: 8 \tTraining Loss: 0.000187\n",
      "Epoch: 9 \tTraining Loss: 0.000187\n",
      "Epoch: 10 \tTraining Loss: 0.000187\n",
      "Epoch: 11 \tTraining Loss: 0.000187\n",
      "Epoch: 12 \tTraining Loss: 0.000187\n",
      "Epoch: 13 \tTraining Loss: 0.000187\n",
      "Epoch: 14 \tTraining Loss: 0.000187\n",
      "Epoch: 15 \tTraining Loss: 0.000187\n",
      "Epoch: 16 \tTraining Loss: 0.000187\n",
      "Epoch: 17 \tTraining Loss: 0.000187\n",
      "Epoch: 18 \tTraining Loss: 0.000187\n",
      "Epoch: 1 \tTraining Loss: 0.000259\n",
      "Epoch: 2 \tTraining Loss: 0.000259\n",
      "Epoch: 3 \tTraining Loss: 0.000259\n",
      "Epoch: 4 \tTraining Loss: 0.000259\n",
      "Epoch: 5 \tTraining Loss: 0.000259\n",
      "Epoch: 6 \tTraining Loss: 0.000259\n",
      "Epoch: 7 \tTraining Loss: 0.000259\n",
      "Epoch: 8 \tTraining Loss: 0.000259\n",
      "Epoch: 9 \tTraining Loss: 0.000259\n",
      "Epoch: 10 \tTraining Loss: 0.000259\n",
      "Epoch: 11 \tTraining Loss: 0.000259\n",
      "Epoch: 12 \tTraining Loss: 0.000259\n",
      "Epoch: 13 \tTraining Loss: 0.000259\n",
      "Epoch: 14 \tTraining Loss: 0.000259\n",
      "Epoch: 15 \tTraining Loss: 0.000259\n",
      "Epoch: 16 \tTraining Loss: 0.000259\n",
      "Epoch: 17 \tTraining Loss: 0.000259\n",
      "Epoch: 18 \tTraining Loss: 0.000259\n",
      "Epoch: 1 \tTraining Loss: 0.000213\n",
      "Epoch: 2 \tTraining Loss: 0.000213\n",
      "Epoch: 3 \tTraining Loss: 0.000213\n",
      "Epoch: 4 \tTraining Loss: 0.000213\n",
      "Epoch: 5 \tTraining Loss: 0.000213\n",
      "Epoch: 6 \tTraining Loss: 0.000213\n",
      "Epoch: 7 \tTraining Loss: 0.000213\n",
      "Epoch: 8 \tTraining Loss: 0.000213\n",
      "Epoch: 9 \tTraining Loss: 0.000213\n",
      "Epoch: 10 \tTraining Loss: 0.000213\n",
      "Epoch: 11 \tTraining Loss: 0.000213\n",
      "Epoch: 12 \tTraining Loss: 0.000213\n",
      "Epoch: 13 \tTraining Loss: 0.000213\n",
      "Epoch: 14 \tTraining Loss: 0.000213\n",
      "Epoch: 15 \tTraining Loss: 0.000213\n",
      "Epoch: 16 \tTraining Loss: 0.000213\n",
      "Epoch: 17 \tTraining Loss: 0.000213\n",
      "Epoch: 18 \tTraining Loss: 0.000213\n",
      "Epoch: 1 \tTraining Loss: 0.000256\n",
      "Epoch: 2 \tTraining Loss: 0.000256\n",
      "Epoch: 3 \tTraining Loss: 0.000256\n",
      "Epoch: 4 \tTraining Loss: 0.000256\n",
      "Epoch: 5 \tTraining Loss: 0.000256\n",
      "Epoch: 6 \tTraining Loss: 0.000256\n",
      "Epoch: 7 \tTraining Loss: 0.000256\n",
      "Epoch: 8 \tTraining Loss: 0.000256\n",
      "Epoch: 9 \tTraining Loss: 0.000256\n",
      "Epoch: 10 \tTraining Loss: 0.000256\n",
      "Epoch: 11 \tTraining Loss: 0.000256\n",
      "Epoch: 12 \tTraining Loss: 0.000256\n",
      "Epoch: 13 \tTraining Loss: 0.000256\n",
      "Epoch: 14 \tTraining Loss: 0.000256\n",
      "Epoch: 15 \tTraining Loss: 0.000256\n",
      "Epoch: 16 \tTraining Loss: 0.000256\n",
      "Epoch: 17 \tTraining Loss: 0.000256\n",
      "Epoch: 18 \tTraining Loss: 0.000256\n",
      "Epoch: 1 \tTraining Loss: 0.000224\n",
      "Epoch: 2 \tTraining Loss: 0.000224\n",
      "Epoch: 3 \tTraining Loss: 0.000224\n",
      "Epoch: 4 \tTraining Loss: 0.000224\n",
      "Epoch: 5 \tTraining Loss: 0.000224\n",
      "Epoch: 6 \tTraining Loss: 0.000224\n",
      "Epoch: 7 \tTraining Loss: 0.000224\n",
      "Epoch: 8 \tTraining Loss: 0.000224\n",
      "Epoch: 9 \tTraining Loss: 0.000224\n",
      "Epoch: 10 \tTraining Loss: 0.000224\n",
      "Epoch: 11 \tTraining Loss: 0.000224\n",
      "Epoch: 12 \tTraining Loss: 0.000224\n",
      "Epoch: 13 \tTraining Loss: 0.000224\n",
      "Epoch: 14 \tTraining Loss: 0.000224\n",
      "Epoch: 15 \tTraining Loss: 0.000224\n",
      "Epoch: 16 \tTraining Loss: 0.000224\n",
      "Epoch: 17 \tTraining Loss: 0.000224\n",
      "Epoch: 18 \tTraining Loss: 0.000224\n",
      "Epoch: 1 \tTraining Loss: 0.000102\n",
      "Epoch: 2 \tTraining Loss: 0.000102\n",
      "Epoch: 3 \tTraining Loss: 0.000102\n",
      "Epoch: 4 \tTraining Loss: 0.000102\n",
      "Epoch: 5 \tTraining Loss: 0.000102\n",
      "Epoch: 6 \tTraining Loss: 0.000102\n",
      "Epoch: 7 \tTraining Loss: 0.000102\n",
      "Epoch: 8 \tTraining Loss: 0.000102\n",
      "Epoch: 9 \tTraining Loss: 0.000102\n",
      "Epoch: 10 \tTraining Loss: 0.000102\n",
      "Epoch: 11 \tTraining Loss: 0.000102\n",
      "Epoch: 12 \tTraining Loss: 0.000102\n",
      "Epoch: 13 \tTraining Loss: 0.000102\n",
      "Epoch: 14 \tTraining Loss: 0.000102\n",
      "Epoch: 15 \tTraining Loss: 0.000102\n",
      "Epoch: 16 \tTraining Loss: 0.000102\n",
      "Epoch: 17 \tTraining Loss: 0.000102\n",
      "Epoch: 18 \tTraining Loss: 0.000102\n",
      "Epoch: 1 \tTraining Loss: 0.000116\n",
      "Epoch: 2 \tTraining Loss: 0.000116\n",
      "Epoch: 3 \tTraining Loss: 0.000116\n",
      "Epoch: 4 \tTraining Loss: 0.000116\n",
      "Epoch: 5 \tTraining Loss: 0.000116\n",
      "Epoch: 6 \tTraining Loss: 0.000116\n",
      "Epoch: 7 \tTraining Loss: 0.000116\n",
      "Epoch: 8 \tTraining Loss: 0.000116\n",
      "Epoch: 9 \tTraining Loss: 0.000116\n",
      "Epoch: 10 \tTraining Loss: 0.000116\n",
      "Epoch: 11 \tTraining Loss: 0.000116\n",
      "Epoch: 12 \tTraining Loss: 0.000116\n",
      "Epoch: 13 \tTraining Loss: 0.000116\n",
      "Epoch: 14 \tTraining Loss: 0.000116\n",
      "Epoch: 15 \tTraining Loss: 0.000116\n",
      "Epoch: 16 \tTraining Loss: 0.000116\n",
      "Epoch: 17 \tTraining Loss: 0.000116\n",
      "Epoch: 18 \tTraining Loss: 0.000116\n",
      "Epoch: 1 \tTraining Loss: 0.000197\n",
      "Epoch: 2 \tTraining Loss: 0.000197\n",
      "Epoch: 3 \tTraining Loss: 0.000197\n",
      "Epoch: 4 \tTraining Loss: 0.000197\n",
      "Epoch: 5 \tTraining Loss: 0.000197\n",
      "Epoch: 6 \tTraining Loss: 0.000197\n",
      "Epoch: 7 \tTraining Loss: 0.000197\n",
      "Epoch: 8 \tTraining Loss: 0.000197\n",
      "Epoch: 9 \tTraining Loss: 0.000197\n",
      "Epoch: 10 \tTraining Loss: 0.000197\n",
      "Epoch: 11 \tTraining Loss: 0.000197\n",
      "Epoch: 12 \tTraining Loss: 0.000197\n",
      "Epoch: 13 \tTraining Loss: 0.000197\n",
      "Epoch: 14 \tTraining Loss: 0.000197\n",
      "Epoch: 15 \tTraining Loss: 0.000197\n",
      "Epoch: 16 \tTraining Loss: 0.000197\n",
      "Epoch: 17 \tTraining Loss: 0.000197\n",
      "Epoch: 18 \tTraining Loss: 0.000197\n",
      "Epoch: 1 \tTraining Loss: 0.000319\n",
      "Epoch: 2 \tTraining Loss: 0.000319\n",
      "Epoch: 3 \tTraining Loss: 0.000319\n",
      "Epoch: 4 \tTraining Loss: 0.000319\n",
      "Epoch: 5 \tTraining Loss: 0.000319\n",
      "Epoch: 6 \tTraining Loss: 0.000319\n",
      "Epoch: 7 \tTraining Loss: 0.000319\n",
      "Epoch: 8 \tTraining Loss: 0.000319\n",
      "Epoch: 9 \tTraining Loss: 0.000319\n",
      "Epoch: 10 \tTraining Loss: 0.000319\n",
      "Epoch: 11 \tTraining Loss: 0.000319\n",
      "Epoch: 12 \tTraining Loss: 0.000319\n",
      "Epoch: 13 \tTraining Loss: 0.000319\n",
      "Epoch: 14 \tTraining Loss: 0.000319\n",
      "Epoch: 15 \tTraining Loss: 0.000319\n",
      "Epoch: 16 \tTraining Loss: 0.000319\n",
      "Epoch: 17 \tTraining Loss: 0.000319\n",
      "Epoch: 18 \tTraining Loss: 0.000319\n",
      "Epoch: 1 \tTraining Loss: 0.000263\n",
      "Epoch: 2 \tTraining Loss: 0.000263\n",
      "Epoch: 3 \tTraining Loss: 0.000263\n",
      "Epoch: 4 \tTraining Loss: 0.000263\n",
      "Epoch: 5 \tTraining Loss: 0.000263\n",
      "Epoch: 6 \tTraining Loss: 0.000263\n",
      "Epoch: 7 \tTraining Loss: 0.000263\n",
      "Epoch: 8 \tTraining Loss: 0.000263\n",
      "Epoch: 9 \tTraining Loss: 0.000263\n",
      "Epoch: 10 \tTraining Loss: 0.000263\n",
      "Epoch: 11 \tTraining Loss: 0.000263\n",
      "Epoch: 12 \tTraining Loss: 0.000263\n",
      "Epoch: 13 \tTraining Loss: 0.000263\n",
      "Epoch: 14 \tTraining Loss: 0.000263\n",
      "Epoch: 15 \tTraining Loss: 0.000263\n",
      "Epoch: 16 \tTraining Loss: 0.000263\n",
      "Epoch: 17 \tTraining Loss: 0.000263\n",
      "Epoch: 18 \tTraining Loss: 0.000263\n",
      "Epoch: 1 \tTraining Loss: 0.000406\n",
      "Epoch: 2 \tTraining Loss: 0.000406\n",
      "Epoch: 3 \tTraining Loss: 0.000406\n",
      "Epoch: 4 \tTraining Loss: 0.000406\n",
      "Epoch: 5 \tTraining Loss: 0.000406\n",
      "Epoch: 6 \tTraining Loss: 0.000406\n",
      "Epoch: 7 \tTraining Loss: 0.000406\n",
      "Epoch: 8 \tTraining Loss: 0.000406\n",
      "Epoch: 9 \tTraining Loss: 0.000406\n",
      "Epoch: 10 \tTraining Loss: 0.000406\n",
      "Epoch: 11 \tTraining Loss: 0.000406\n",
      "Epoch: 12 \tTraining Loss: 0.000406\n",
      "Epoch: 13 \tTraining Loss: 0.000406\n",
      "Epoch: 14 \tTraining Loss: 0.000406\n",
      "Epoch: 15 \tTraining Loss: 0.000406\n",
      "Epoch: 16 \tTraining Loss: 0.000406\n",
      "Epoch: 17 \tTraining Loss: 0.000406\n",
      "Epoch: 18 \tTraining Loss: 0.000406\n",
      "Epoch: 1 \tTraining Loss: 0.000076\n",
      "Epoch: 2 \tTraining Loss: 0.000076\n",
      "Epoch: 3 \tTraining Loss: 0.000076\n",
      "Epoch: 4 \tTraining Loss: 0.000076\n",
      "Epoch: 5 \tTraining Loss: 0.000076\n",
      "Epoch: 6 \tTraining Loss: 0.000076\n",
      "Epoch: 7 \tTraining Loss: 0.000076\n",
      "Epoch: 8 \tTraining Loss: 0.000076\n",
      "Epoch: 9 \tTraining Loss: 0.000076\n",
      "Epoch: 10 \tTraining Loss: 0.000076\n",
      "Epoch: 11 \tTraining Loss: 0.000076\n",
      "Epoch: 12 \tTraining Loss: 0.000076\n",
      "Epoch: 13 \tTraining Loss: 0.000076\n",
      "Epoch: 14 \tTraining Loss: 0.000076\n",
      "Epoch: 15 \tTraining Loss: 0.000076\n",
      "Epoch: 16 \tTraining Loss: 0.000076\n",
      "Epoch: 17 \tTraining Loss: 0.000076\n",
      "Epoch: 18 \tTraining Loss: 0.000076\n",
      "Epoch: 1 \tTraining Loss: 0.000287\n",
      "Epoch: 2 \tTraining Loss: 0.000287\n",
      "Epoch: 3 \tTraining Loss: 0.000287\n",
      "Epoch: 4 \tTraining Loss: 0.000287\n",
      "Epoch: 5 \tTraining Loss: 0.000287\n",
      "Epoch: 6 \tTraining Loss: 0.000287\n",
      "Epoch: 7 \tTraining Loss: 0.000287\n",
      "Epoch: 8 \tTraining Loss: 0.000287\n",
      "Epoch: 9 \tTraining Loss: 0.000287\n",
      "Epoch: 10 \tTraining Loss: 0.000287\n",
      "Epoch: 11 \tTraining Loss: 0.000287\n",
      "Epoch: 12 \tTraining Loss: 0.000287\n",
      "Epoch: 13 \tTraining Loss: 0.000287\n",
      "Epoch: 14 \tTraining Loss: 0.000287\n",
      "Epoch: 15 \tTraining Loss: 0.000287\n",
      "Epoch: 16 \tTraining Loss: 0.000287\n",
      "Epoch: 17 \tTraining Loss: 0.000287\n",
      "Epoch: 18 \tTraining Loss: 0.000287\n",
      "Epoch: 1 \tTraining Loss: 0.000131\n",
      "Epoch: 2 \tTraining Loss: 0.000131\n",
      "Epoch: 3 \tTraining Loss: 0.000131\n",
      "Epoch: 4 \tTraining Loss: 0.000131\n",
      "Epoch: 5 \tTraining Loss: 0.000131\n",
      "Epoch: 6 \tTraining Loss: 0.000131\n",
      "Epoch: 7 \tTraining Loss: 0.000131\n",
      "Epoch: 8 \tTraining Loss: 0.000131\n",
      "Epoch: 9 \tTraining Loss: 0.000131\n",
      "Epoch: 10 \tTraining Loss: 0.000131\n",
      "Epoch: 11 \tTraining Loss: 0.000131\n",
      "Epoch: 12 \tTraining Loss: 0.000131\n",
      "Epoch: 13 \tTraining Loss: 0.000131\n",
      "Epoch: 14 \tTraining Loss: 0.000131\n",
      "Epoch: 15 \tTraining Loss: 0.000131\n",
      "Epoch: 16 \tTraining Loss: 0.000131\n",
      "Epoch: 17 \tTraining Loss: 0.000131\n",
      "Epoch: 18 \tTraining Loss: 0.000131\n",
      "Epoch: 1 \tTraining Loss: 0.000182\n",
      "Epoch: 2 \tTraining Loss: 0.000182\n",
      "Epoch: 3 \tTraining Loss: 0.000182\n",
      "Epoch: 4 \tTraining Loss: 0.000182\n",
      "Epoch: 5 \tTraining Loss: 0.000182\n",
      "Epoch: 6 \tTraining Loss: 0.000182\n",
      "Epoch: 7 \tTraining Loss: 0.000182\n",
      "Epoch: 8 \tTraining Loss: 0.000182\n",
      "Epoch: 9 \tTraining Loss: 0.000182\n",
      "Epoch: 10 \tTraining Loss: 0.000182\n",
      "Epoch: 11 \tTraining Loss: 0.000182\n",
      "Epoch: 12 \tTraining Loss: 0.000182\n",
      "Epoch: 13 \tTraining Loss: 0.000182\n",
      "Epoch: 14 \tTraining Loss: 0.000182\n",
      "Epoch: 15 \tTraining Loss: 0.000182\n",
      "Epoch: 16 \tTraining Loss: 0.000182\n",
      "Epoch: 17 \tTraining Loss: 0.000182\n",
      "Epoch: 18 \tTraining Loss: 0.000182\n",
      "Epoch: 1 \tTraining Loss: 0.000192\n",
      "Epoch: 2 \tTraining Loss: 0.000192\n",
      "Epoch: 3 \tTraining Loss: 0.000192\n",
      "Epoch: 4 \tTraining Loss: 0.000192\n",
      "Epoch: 5 \tTraining Loss: 0.000192\n",
      "Epoch: 6 \tTraining Loss: 0.000192\n",
      "Epoch: 7 \tTraining Loss: 0.000192\n",
      "Epoch: 8 \tTraining Loss: 0.000192\n",
      "Epoch: 9 \tTraining Loss: 0.000192\n",
      "Epoch: 10 \tTraining Loss: 0.000192\n",
      "Epoch: 11 \tTraining Loss: 0.000192\n",
      "Epoch: 12 \tTraining Loss: 0.000192\n",
      "Epoch: 13 \tTraining Loss: 0.000192\n",
      "Epoch: 14 \tTraining Loss: 0.000192\n",
      "Epoch: 15 \tTraining Loss: 0.000192\n",
      "Epoch: 16 \tTraining Loss: 0.000192\n",
      "Epoch: 17 \tTraining Loss: 0.000192\n",
      "Epoch: 18 \tTraining Loss: 0.000192\n",
      "Epoch: 1 \tTraining Loss: 0.000201\n",
      "Epoch: 2 \tTraining Loss: 0.000201\n",
      "Epoch: 3 \tTraining Loss: 0.000201\n",
      "Epoch: 4 \tTraining Loss: 0.000201\n",
      "Epoch: 5 \tTraining Loss: 0.000201\n",
      "Epoch: 6 \tTraining Loss: 0.000201\n",
      "Epoch: 7 \tTraining Loss: 0.000201\n",
      "Epoch: 8 \tTraining Loss: 0.000201\n",
      "Epoch: 9 \tTraining Loss: 0.000201\n",
      "Epoch: 10 \tTraining Loss: 0.000201\n",
      "Epoch: 11 \tTraining Loss: 0.000201\n",
      "Epoch: 12 \tTraining Loss: 0.000201\n",
      "Epoch: 13 \tTraining Loss: 0.000201\n",
      "Epoch: 14 \tTraining Loss: 0.000201\n",
      "Epoch: 15 \tTraining Loss: 0.000201\n",
      "Epoch: 16 \tTraining Loss: 0.000201\n",
      "Epoch: 17 \tTraining Loss: 0.000201\n",
      "Epoch: 18 \tTraining Loss: 0.000201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000152\n",
      "Epoch: 2 \tTraining Loss: 0.000152\n",
      "Epoch: 3 \tTraining Loss: 0.000152\n",
      "Epoch: 4 \tTraining Loss: 0.000152\n",
      "Epoch: 5 \tTraining Loss: 0.000152\n",
      "Epoch: 6 \tTraining Loss: 0.000152\n",
      "Epoch: 7 \tTraining Loss: 0.000152\n",
      "Epoch: 8 \tTraining Loss: 0.000152\n",
      "Epoch: 9 \tTraining Loss: 0.000152\n",
      "Epoch: 10 \tTraining Loss: 0.000152\n",
      "Epoch: 11 \tTraining Loss: 0.000152\n",
      "Epoch: 12 \tTraining Loss: 0.000152\n",
      "Epoch: 13 \tTraining Loss: 0.000152\n",
      "Epoch: 14 \tTraining Loss: 0.000152\n",
      "Epoch: 15 \tTraining Loss: 0.000152\n",
      "Epoch: 16 \tTraining Loss: 0.000152\n",
      "Epoch: 17 \tTraining Loss: 0.000152\n",
      "Epoch: 18 \tTraining Loss: 0.000152\n",
      "Epoch: 1 \tTraining Loss: 0.000240\n",
      "Epoch: 2 \tTraining Loss: 0.000240\n",
      "Epoch: 3 \tTraining Loss: 0.000240\n",
      "Epoch: 4 \tTraining Loss: 0.000240\n",
      "Epoch: 5 \tTraining Loss: 0.000240\n",
      "Epoch: 6 \tTraining Loss: 0.000240\n",
      "Epoch: 7 \tTraining Loss: 0.000240\n",
      "Epoch: 8 \tTraining Loss: 0.000240\n",
      "Epoch: 9 \tTraining Loss: 0.000240\n",
      "Epoch: 10 \tTraining Loss: 0.000240\n",
      "Epoch: 11 \tTraining Loss: 0.000240\n",
      "Epoch: 12 \tTraining Loss: 0.000240\n",
      "Epoch: 13 \tTraining Loss: 0.000240\n",
      "Epoch: 14 \tTraining Loss: 0.000240\n",
      "Epoch: 15 \tTraining Loss: 0.000240\n",
      "Epoch: 16 \tTraining Loss: 0.000240\n",
      "Epoch: 17 \tTraining Loss: 0.000240\n",
      "Epoch: 18 \tTraining Loss: 0.000240\n",
      "Epoch: 1 \tTraining Loss: 0.000220\n",
      "Epoch: 2 \tTraining Loss: 0.000220\n",
      "Epoch: 3 \tTraining Loss: 0.000220\n",
      "Epoch: 4 \tTraining Loss: 0.000220\n",
      "Epoch: 5 \tTraining Loss: 0.000220\n",
      "Epoch: 6 \tTraining Loss: 0.000220\n",
      "Epoch: 7 \tTraining Loss: 0.000220\n",
      "Epoch: 8 \tTraining Loss: 0.000220\n",
      "Epoch: 9 \tTraining Loss: 0.000220\n",
      "Epoch: 10 \tTraining Loss: 0.000220\n",
      "Epoch: 11 \tTraining Loss: 0.000220\n",
      "Epoch: 12 \tTraining Loss: 0.000220\n",
      "Epoch: 13 \tTraining Loss: 0.000220\n",
      "Epoch: 14 \tTraining Loss: 0.000220\n",
      "Epoch: 15 \tTraining Loss: 0.000220\n",
      "Epoch: 16 \tTraining Loss: 0.000220\n",
      "Epoch: 17 \tTraining Loss: 0.000220\n",
      "Epoch: 18 \tTraining Loss: 0.000220\n",
      "Epoch: 1 \tTraining Loss: 0.000213\n",
      "Epoch: 2 \tTraining Loss: 0.000213\n",
      "Epoch: 3 \tTraining Loss: 0.000213\n",
      "Epoch: 4 \tTraining Loss: 0.000213\n",
      "Epoch: 5 \tTraining Loss: 0.000213\n",
      "Epoch: 6 \tTraining Loss: 0.000213\n",
      "Epoch: 7 \tTraining Loss: 0.000213\n",
      "Epoch: 8 \tTraining Loss: 0.000213\n",
      "Epoch: 9 \tTraining Loss: 0.000213\n",
      "Epoch: 10 \tTraining Loss: 0.000213\n",
      "Epoch: 11 \tTraining Loss: 0.000213\n",
      "Epoch: 12 \tTraining Loss: 0.000213\n",
      "Epoch: 13 \tTraining Loss: 0.000213\n",
      "Epoch: 14 \tTraining Loss: 0.000213\n",
      "Epoch: 15 \tTraining Loss: 0.000213\n",
      "Epoch: 16 \tTraining Loss: 0.000213\n",
      "Epoch: 17 \tTraining Loss: 0.000213\n",
      "Epoch: 18 \tTraining Loss: 0.000213\n",
      "Epoch: 1 \tTraining Loss: 0.000093\n",
      "Epoch: 2 \tTraining Loss: 0.000093\n",
      "Epoch: 3 \tTraining Loss: 0.000093\n",
      "Epoch: 4 \tTraining Loss: 0.000093\n",
      "Epoch: 5 \tTraining Loss: 0.000093\n",
      "Epoch: 6 \tTraining Loss: 0.000093\n",
      "Epoch: 7 \tTraining Loss: 0.000093\n",
      "Epoch: 8 \tTraining Loss: 0.000093\n",
      "Epoch: 9 \tTraining Loss: 0.000093\n",
      "Epoch: 10 \tTraining Loss: 0.000093\n",
      "Epoch: 11 \tTraining Loss: 0.000093\n",
      "Epoch: 12 \tTraining Loss: 0.000093\n",
      "Epoch: 13 \tTraining Loss: 0.000093\n",
      "Epoch: 14 \tTraining Loss: 0.000093\n",
      "Epoch: 15 \tTraining Loss: 0.000093\n",
      "Epoch: 16 \tTraining Loss: 0.000093\n",
      "Epoch: 17 \tTraining Loss: 0.000093\n",
      "Epoch: 18 \tTraining Loss: 0.000093\n",
      "Epoch: 1 \tTraining Loss: 0.000155\n",
      "Epoch: 2 \tTraining Loss: 0.000155\n",
      "Epoch: 3 \tTraining Loss: 0.000155\n",
      "Epoch: 4 \tTraining Loss: 0.000155\n",
      "Epoch: 5 \tTraining Loss: 0.000155\n",
      "Epoch: 6 \tTraining Loss: 0.000155\n",
      "Epoch: 7 \tTraining Loss: 0.000155\n",
      "Epoch: 8 \tTraining Loss: 0.000155\n",
      "Epoch: 9 \tTraining Loss: 0.000155\n",
      "Epoch: 10 \tTraining Loss: 0.000155\n",
      "Epoch: 11 \tTraining Loss: 0.000155\n",
      "Epoch: 12 \tTraining Loss: 0.000155\n",
      "Epoch: 13 \tTraining Loss: 0.000155\n",
      "Epoch: 14 \tTraining Loss: 0.000155\n",
      "Epoch: 15 \tTraining Loss: 0.000155\n",
      "Epoch: 16 \tTraining Loss: 0.000155\n",
      "Epoch: 17 \tTraining Loss: 0.000155\n",
      "Epoch: 18 \tTraining Loss: 0.000155\n",
      "Epoch: 1 \tTraining Loss: 0.000364\n",
      "Epoch: 2 \tTraining Loss: 0.000364\n",
      "Epoch: 3 \tTraining Loss: 0.000364\n",
      "Epoch: 4 \tTraining Loss: 0.000364\n",
      "Epoch: 5 \tTraining Loss: 0.000364\n",
      "Epoch: 6 \tTraining Loss: 0.000364\n",
      "Epoch: 7 \tTraining Loss: 0.000364\n",
      "Epoch: 8 \tTraining Loss: 0.000364\n",
      "Epoch: 9 \tTraining Loss: 0.000364\n",
      "Epoch: 10 \tTraining Loss: 0.000364\n",
      "Epoch: 11 \tTraining Loss: 0.000364\n",
      "Epoch: 12 \tTraining Loss: 0.000364\n",
      "Epoch: 13 \tTraining Loss: 0.000364\n",
      "Epoch: 14 \tTraining Loss: 0.000364\n",
      "Epoch: 15 \tTraining Loss: 0.000364\n",
      "Epoch: 16 \tTraining Loss: 0.000364\n",
      "Epoch: 17 \tTraining Loss: 0.000364\n",
      "Epoch: 18 \tTraining Loss: 0.000364\n",
      "Epoch: 1 \tTraining Loss: 0.000208\n",
      "Epoch: 2 \tTraining Loss: 0.000208\n",
      "Epoch: 3 \tTraining Loss: 0.000208\n",
      "Epoch: 4 \tTraining Loss: 0.000208\n",
      "Epoch: 5 \tTraining Loss: 0.000208\n",
      "Epoch: 6 \tTraining Loss: 0.000208\n",
      "Epoch: 7 \tTraining Loss: 0.000208\n",
      "Epoch: 8 \tTraining Loss: 0.000208\n",
      "Epoch: 9 \tTraining Loss: 0.000208\n",
      "Epoch: 10 \tTraining Loss: 0.000208\n",
      "Epoch: 11 \tTraining Loss: 0.000208\n",
      "Epoch: 12 \tTraining Loss: 0.000208\n",
      "Epoch: 13 \tTraining Loss: 0.000208\n",
      "Epoch: 14 \tTraining Loss: 0.000208\n",
      "Epoch: 15 \tTraining Loss: 0.000208\n",
      "Epoch: 16 \tTraining Loss: 0.000208\n",
      "Epoch: 17 \tTraining Loss: 0.000208\n",
      "Epoch: 18 \tTraining Loss: 0.000208\n",
      "Epoch: 1 \tTraining Loss: 0.000189\n",
      "Epoch: 2 \tTraining Loss: 0.000189\n",
      "Epoch: 3 \tTraining Loss: 0.000189\n",
      "Epoch: 4 \tTraining Loss: 0.000189\n",
      "Epoch: 5 \tTraining Loss: 0.000189\n",
      "Epoch: 6 \tTraining Loss: 0.000189\n",
      "Epoch: 7 \tTraining Loss: 0.000189\n",
      "Epoch: 8 \tTraining Loss: 0.000189\n",
      "Epoch: 9 \tTraining Loss: 0.000189\n",
      "Epoch: 10 \tTraining Loss: 0.000189\n",
      "Epoch: 11 \tTraining Loss: 0.000189\n",
      "Epoch: 12 \tTraining Loss: 0.000189\n",
      "Epoch: 13 \tTraining Loss: 0.000189\n",
      "Epoch: 14 \tTraining Loss: 0.000189\n",
      "Epoch: 15 \tTraining Loss: 0.000189\n",
      "Epoch: 16 \tTraining Loss: 0.000189\n",
      "Epoch: 17 \tTraining Loss: 0.000189\n",
      "Epoch: 18 \tTraining Loss: 0.000189\n",
      "Epoch: 1 \tTraining Loss: 0.000324\n",
      "Epoch: 2 \tTraining Loss: 0.000324\n",
      "Epoch: 3 \tTraining Loss: 0.000324\n",
      "Epoch: 4 \tTraining Loss: 0.000324\n",
      "Epoch: 5 \tTraining Loss: 0.000324\n",
      "Epoch: 6 \tTraining Loss: 0.000324\n",
      "Epoch: 7 \tTraining Loss: 0.000324\n",
      "Epoch: 8 \tTraining Loss: 0.000324\n",
      "Epoch: 9 \tTraining Loss: 0.000324\n",
      "Epoch: 10 \tTraining Loss: 0.000324\n",
      "Epoch: 11 \tTraining Loss: 0.000324\n",
      "Epoch: 12 \tTraining Loss: 0.000324\n",
      "Epoch: 13 \tTraining Loss: 0.000324\n",
      "Epoch: 14 \tTraining Loss: 0.000324\n",
      "Epoch: 15 \tTraining Loss: 0.000324\n",
      "Epoch: 16 \tTraining Loss: 0.000324\n",
      "Epoch: 17 \tTraining Loss: 0.000324\n",
      "Epoch: 18 \tTraining Loss: 0.000324\n",
      "Epoch: 1 \tTraining Loss: 0.000378\n",
      "Epoch: 2 \tTraining Loss: 0.000378\n",
      "Epoch: 3 \tTraining Loss: 0.000378\n",
      "Epoch: 4 \tTraining Loss: 0.000378\n",
      "Epoch: 5 \tTraining Loss: 0.000378\n",
      "Epoch: 6 \tTraining Loss: 0.000378\n",
      "Epoch: 7 \tTraining Loss: 0.000378\n",
      "Epoch: 8 \tTraining Loss: 0.000378\n",
      "Epoch: 9 \tTraining Loss: 0.000378\n",
      "Epoch: 10 \tTraining Loss: 0.000378\n",
      "Epoch: 11 \tTraining Loss: 0.000378\n",
      "Epoch: 12 \tTraining Loss: 0.000378\n",
      "Epoch: 13 \tTraining Loss: 0.000378\n",
      "Epoch: 14 \tTraining Loss: 0.000378\n",
      "Epoch: 15 \tTraining Loss: 0.000378\n",
      "Epoch: 16 \tTraining Loss: 0.000378\n",
      "Epoch: 17 \tTraining Loss: 0.000378\n",
      "Epoch: 18 \tTraining Loss: 0.000378\n",
      "Epoch: 1 \tTraining Loss: 0.000126\n",
      "Epoch: 2 \tTraining Loss: 0.000126\n",
      "Epoch: 3 \tTraining Loss: 0.000126\n",
      "Epoch: 4 \tTraining Loss: 0.000126\n",
      "Epoch: 5 \tTraining Loss: 0.000126\n",
      "Epoch: 6 \tTraining Loss: 0.000126\n",
      "Epoch: 7 \tTraining Loss: 0.000126\n",
      "Epoch: 8 \tTraining Loss: 0.000126\n",
      "Epoch: 9 \tTraining Loss: 0.000126\n",
      "Epoch: 10 \tTraining Loss: 0.000126\n",
      "Epoch: 11 \tTraining Loss: 0.000126\n",
      "Epoch: 12 \tTraining Loss: 0.000126\n",
      "Epoch: 13 \tTraining Loss: 0.000126\n",
      "Epoch: 14 \tTraining Loss: 0.000126\n",
      "Epoch: 15 \tTraining Loss: 0.000126\n",
      "Epoch: 16 \tTraining Loss: 0.000126\n",
      "Epoch: 17 \tTraining Loss: 0.000126\n",
      "Epoch: 18 \tTraining Loss: 0.000126\n",
      "Epoch: 1 \tTraining Loss: 0.000049\n",
      "Epoch: 2 \tTraining Loss: 0.000049\n",
      "Epoch: 3 \tTraining Loss: 0.000049\n",
      "Epoch: 4 \tTraining Loss: 0.000049\n",
      "Epoch: 5 \tTraining Loss: 0.000049\n",
      "Epoch: 6 \tTraining Loss: 0.000049\n",
      "Epoch: 7 \tTraining Loss: 0.000049\n",
      "Epoch: 8 \tTraining Loss: 0.000049\n",
      "Epoch: 9 \tTraining Loss: 0.000049\n",
      "Epoch: 10 \tTraining Loss: 0.000049\n",
      "Epoch: 11 \tTraining Loss: 0.000049\n",
      "Epoch: 12 \tTraining Loss: 0.000049\n",
      "Epoch: 13 \tTraining Loss: 0.000049\n",
      "Epoch: 14 \tTraining Loss: 0.000049\n",
      "Epoch: 15 \tTraining Loss: 0.000049\n",
      "Epoch: 16 \tTraining Loss: 0.000049\n",
      "Epoch: 17 \tTraining Loss: 0.000049\n",
      "Epoch: 18 \tTraining Loss: 0.000049\n",
      "Epoch: 1 \tTraining Loss: 0.000467\n",
      "Epoch: 2 \tTraining Loss: 0.000467\n",
      "Epoch: 3 \tTraining Loss: 0.000467\n",
      "Epoch: 4 \tTraining Loss: 0.000467\n",
      "Epoch: 5 \tTraining Loss: 0.000467\n",
      "Epoch: 6 \tTraining Loss: 0.000467\n",
      "Epoch: 7 \tTraining Loss: 0.000467\n",
      "Epoch: 8 \tTraining Loss: 0.000467\n",
      "Epoch: 9 \tTraining Loss: 0.000467\n",
      "Epoch: 10 \tTraining Loss: 0.000467\n",
      "Epoch: 11 \tTraining Loss: 0.000467\n",
      "Epoch: 12 \tTraining Loss: 0.000467\n",
      "Epoch: 13 \tTraining Loss: 0.000467\n",
      "Epoch: 14 \tTraining Loss: 0.000467\n",
      "Epoch: 15 \tTraining Loss: 0.000467\n",
      "Epoch: 16 \tTraining Loss: 0.000467\n",
      "Epoch: 17 \tTraining Loss: 0.000467\n",
      "Epoch: 18 \tTraining Loss: 0.000467\n",
      "Epoch: 1 \tTraining Loss: 0.000368\n",
      "Epoch: 2 \tTraining Loss: 0.000368\n",
      "Epoch: 3 \tTraining Loss: 0.000368\n",
      "Epoch: 4 \tTraining Loss: 0.000368\n",
      "Epoch: 5 \tTraining Loss: 0.000368\n",
      "Epoch: 6 \tTraining Loss: 0.000368\n",
      "Epoch: 7 \tTraining Loss: 0.000368\n",
      "Epoch: 8 \tTraining Loss: 0.000368\n",
      "Epoch: 9 \tTraining Loss: 0.000368\n",
      "Epoch: 10 \tTraining Loss: 0.000368\n",
      "Epoch: 11 \tTraining Loss: 0.000368\n",
      "Epoch: 12 \tTraining Loss: 0.000368\n",
      "Epoch: 13 \tTraining Loss: 0.000368\n",
      "Epoch: 14 \tTraining Loss: 0.000368\n",
      "Epoch: 15 \tTraining Loss: 0.000368\n",
      "Epoch: 16 \tTraining Loss: 0.000368\n",
      "Epoch: 17 \tTraining Loss: 0.000368\n",
      "Epoch: 18 \tTraining Loss: 0.000368\n",
      "Epoch: 1 \tTraining Loss: 0.000149\n",
      "Epoch: 2 \tTraining Loss: 0.000149\n",
      "Epoch: 3 \tTraining Loss: 0.000149\n",
      "Epoch: 4 \tTraining Loss: 0.000149\n",
      "Epoch: 5 \tTraining Loss: 0.000149\n",
      "Epoch: 6 \tTraining Loss: 0.000149\n",
      "Epoch: 7 \tTraining Loss: 0.000149\n",
      "Epoch: 8 \tTraining Loss: 0.000149\n",
      "Epoch: 9 \tTraining Loss: 0.000149\n",
      "Epoch: 10 \tTraining Loss: 0.000149\n",
      "Epoch: 11 \tTraining Loss: 0.000149\n",
      "Epoch: 12 \tTraining Loss: 0.000149\n",
      "Epoch: 13 \tTraining Loss: 0.000149\n",
      "Epoch: 14 \tTraining Loss: 0.000149\n",
      "Epoch: 15 \tTraining Loss: 0.000149\n",
      "Epoch: 16 \tTraining Loss: 0.000149\n",
      "Epoch: 17 \tTraining Loss: 0.000149\n",
      "Epoch: 18 \tTraining Loss: 0.000149\n",
      "Epoch: 1 \tTraining Loss: 0.000158\n",
      "Epoch: 2 \tTraining Loss: 0.000158\n",
      "Epoch: 3 \tTraining Loss: 0.000158\n",
      "Epoch: 4 \tTraining Loss: 0.000158\n",
      "Epoch: 5 \tTraining Loss: 0.000158\n",
      "Epoch: 6 \tTraining Loss: 0.000158\n",
      "Epoch: 7 \tTraining Loss: 0.000158\n",
      "Epoch: 8 \tTraining Loss: 0.000158\n",
      "Epoch: 9 \tTraining Loss: 0.000158\n",
      "Epoch: 10 \tTraining Loss: 0.000158\n",
      "Epoch: 11 \tTraining Loss: 0.000158\n",
      "Epoch: 12 \tTraining Loss: 0.000158\n",
      "Epoch: 13 \tTraining Loss: 0.000158\n",
      "Epoch: 14 \tTraining Loss: 0.000158\n",
      "Epoch: 15 \tTraining Loss: 0.000158\n",
      "Epoch: 16 \tTraining Loss: 0.000158\n",
      "Epoch: 17 \tTraining Loss: 0.000158\n",
      "Epoch: 18 \tTraining Loss: 0.000158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000162\n",
      "Epoch: 2 \tTraining Loss: 0.000162\n",
      "Epoch: 3 \tTraining Loss: 0.000162\n",
      "Epoch: 4 \tTraining Loss: 0.000162\n",
      "Epoch: 5 \tTraining Loss: 0.000162\n",
      "Epoch: 6 \tTraining Loss: 0.000162\n",
      "Epoch: 7 \tTraining Loss: 0.000162\n",
      "Epoch: 8 \tTraining Loss: 0.000162\n",
      "Epoch: 9 \tTraining Loss: 0.000162\n",
      "Epoch: 10 \tTraining Loss: 0.000162\n",
      "Epoch: 11 \tTraining Loss: 0.000162\n",
      "Epoch: 12 \tTraining Loss: 0.000162\n",
      "Epoch: 13 \tTraining Loss: 0.000162\n",
      "Epoch: 14 \tTraining Loss: 0.000162\n",
      "Epoch: 15 \tTraining Loss: 0.000162\n",
      "Epoch: 16 \tTraining Loss: 0.000162\n",
      "Epoch: 17 \tTraining Loss: 0.000162\n",
      "Epoch: 18 \tTraining Loss: 0.000162\n",
      "Epoch: 1 \tTraining Loss: 0.000227\n",
      "Epoch: 2 \tTraining Loss: 0.000227\n",
      "Epoch: 3 \tTraining Loss: 0.000227\n",
      "Epoch: 4 \tTraining Loss: 0.000227\n",
      "Epoch: 5 \tTraining Loss: 0.000227\n",
      "Epoch: 6 \tTraining Loss: 0.000227\n",
      "Epoch: 7 \tTraining Loss: 0.000227\n",
      "Epoch: 8 \tTraining Loss: 0.000227\n",
      "Epoch: 9 \tTraining Loss: 0.000227\n",
      "Epoch: 10 \tTraining Loss: 0.000227\n",
      "Epoch: 11 \tTraining Loss: 0.000227\n",
      "Epoch: 12 \tTraining Loss: 0.000227\n",
      "Epoch: 13 \tTraining Loss: 0.000227\n",
      "Epoch: 14 \tTraining Loss: 0.000227\n",
      "Epoch: 15 \tTraining Loss: 0.000227\n",
      "Epoch: 16 \tTraining Loss: 0.000227\n",
      "Epoch: 17 \tTraining Loss: 0.000227\n",
      "Epoch: 18 \tTraining Loss: 0.000227\n",
      "Epoch: 1 \tTraining Loss: 0.000229\n",
      "Epoch: 2 \tTraining Loss: 0.000229\n",
      "Epoch: 3 \tTraining Loss: 0.000229\n",
      "Epoch: 4 \tTraining Loss: 0.000229\n",
      "Epoch: 5 \tTraining Loss: 0.000229\n",
      "Epoch: 6 \tTraining Loss: 0.000229\n",
      "Epoch: 7 \tTraining Loss: 0.000229\n",
      "Epoch: 8 \tTraining Loss: 0.000229\n",
      "Epoch: 9 \tTraining Loss: 0.000229\n",
      "Epoch: 10 \tTraining Loss: 0.000229\n",
      "Epoch: 11 \tTraining Loss: 0.000229\n",
      "Epoch: 12 \tTraining Loss: 0.000229\n",
      "Epoch: 13 \tTraining Loss: 0.000229\n",
      "Epoch: 14 \tTraining Loss: 0.000229\n",
      "Epoch: 15 \tTraining Loss: 0.000229\n",
      "Epoch: 16 \tTraining Loss: 0.000229\n",
      "Epoch: 17 \tTraining Loss: 0.000229\n",
      "Epoch: 18 \tTraining Loss: 0.000229\n",
      "Epoch: 1 \tTraining Loss: 0.000281\n",
      "Epoch: 2 \tTraining Loss: 0.000281\n",
      "Epoch: 3 \tTraining Loss: 0.000281\n",
      "Epoch: 4 \tTraining Loss: 0.000281\n",
      "Epoch: 5 \tTraining Loss: 0.000281\n",
      "Epoch: 6 \tTraining Loss: 0.000281\n",
      "Epoch: 7 \tTraining Loss: 0.000281\n",
      "Epoch: 8 \tTraining Loss: 0.000281\n",
      "Epoch: 9 \tTraining Loss: 0.000281\n",
      "Epoch: 10 \tTraining Loss: 0.000281\n",
      "Epoch: 11 \tTraining Loss: 0.000281\n",
      "Epoch: 12 \tTraining Loss: 0.000281\n",
      "Epoch: 13 \tTraining Loss: 0.000281\n",
      "Epoch: 14 \tTraining Loss: 0.000281\n",
      "Epoch: 15 \tTraining Loss: 0.000281\n",
      "Epoch: 16 \tTraining Loss: 0.000281\n",
      "Epoch: 17 \tTraining Loss: 0.000281\n",
      "Epoch: 18 \tTraining Loss: 0.000281\n",
      "Epoch: 1 \tTraining Loss: 0.000218\n",
      "Epoch: 2 \tTraining Loss: 0.000218\n",
      "Epoch: 3 \tTraining Loss: 0.000218\n",
      "Epoch: 4 \tTraining Loss: 0.000218\n",
      "Epoch: 5 \tTraining Loss: 0.000218\n",
      "Epoch: 6 \tTraining Loss: 0.000218\n",
      "Epoch: 7 \tTraining Loss: 0.000218\n",
      "Epoch: 8 \tTraining Loss: 0.000218\n",
      "Epoch: 9 \tTraining Loss: 0.000218\n",
      "Epoch: 10 \tTraining Loss: 0.000218\n",
      "Epoch: 11 \tTraining Loss: 0.000218\n",
      "Epoch: 12 \tTraining Loss: 0.000218\n",
      "Epoch: 13 \tTraining Loss: 0.000218\n",
      "Epoch: 14 \tTraining Loss: 0.000218\n",
      "Epoch: 15 \tTraining Loss: 0.000218\n",
      "Epoch: 16 \tTraining Loss: 0.000218\n",
      "Epoch: 17 \tTraining Loss: 0.000218\n",
      "Epoch: 18 \tTraining Loss: 0.000218\n",
      "Epoch: 1 \tTraining Loss: 0.000226\n",
      "Epoch: 2 \tTraining Loss: 0.000226\n",
      "Epoch: 3 \tTraining Loss: 0.000226\n",
      "Epoch: 4 \tTraining Loss: 0.000226\n",
      "Epoch: 5 \tTraining Loss: 0.000226\n",
      "Epoch: 6 \tTraining Loss: 0.000226\n",
      "Epoch: 7 \tTraining Loss: 0.000226\n",
      "Epoch: 8 \tTraining Loss: 0.000226\n",
      "Epoch: 9 \tTraining Loss: 0.000226\n",
      "Epoch: 10 \tTraining Loss: 0.000226\n",
      "Epoch: 11 \tTraining Loss: 0.000226\n",
      "Epoch: 12 \tTraining Loss: 0.000226\n",
      "Epoch: 13 \tTraining Loss: 0.000226\n",
      "Epoch: 14 \tTraining Loss: 0.000226\n",
      "Epoch: 15 \tTraining Loss: 0.000226\n",
      "Epoch: 16 \tTraining Loss: 0.000226\n",
      "Epoch: 17 \tTraining Loss: 0.000226\n",
      "Epoch: 18 \tTraining Loss: 0.000226\n",
      "Epoch: 1 \tTraining Loss: 0.000148\n",
      "Epoch: 2 \tTraining Loss: 0.000148\n",
      "Epoch: 3 \tTraining Loss: 0.000148\n",
      "Epoch: 4 \tTraining Loss: 0.000148\n",
      "Epoch: 5 \tTraining Loss: 0.000148\n",
      "Epoch: 6 \tTraining Loss: 0.000148\n",
      "Epoch: 7 \tTraining Loss: 0.000148\n",
      "Epoch: 8 \tTraining Loss: 0.000148\n",
      "Epoch: 9 \tTraining Loss: 0.000148\n",
      "Epoch: 10 \tTraining Loss: 0.000148\n",
      "Epoch: 11 \tTraining Loss: 0.000148\n",
      "Epoch: 12 \tTraining Loss: 0.000148\n",
      "Epoch: 13 \tTraining Loss: 0.000148\n",
      "Epoch: 14 \tTraining Loss: 0.000148\n",
      "Epoch: 15 \tTraining Loss: 0.000148\n",
      "Epoch: 16 \tTraining Loss: 0.000148\n",
      "Epoch: 17 \tTraining Loss: 0.000148\n",
      "Epoch: 18 \tTraining Loss: 0.000148\n",
      "Epoch: 1 \tTraining Loss: 0.000474\n",
      "Epoch: 2 \tTraining Loss: 0.000474\n",
      "Epoch: 3 \tTraining Loss: 0.000474\n",
      "Epoch: 4 \tTraining Loss: 0.000474\n",
      "Epoch: 5 \tTraining Loss: 0.000474\n",
      "Epoch: 6 \tTraining Loss: 0.000474\n",
      "Epoch: 7 \tTraining Loss: 0.000474\n",
      "Epoch: 8 \tTraining Loss: 0.000474\n",
      "Epoch: 9 \tTraining Loss: 0.000474\n",
      "Epoch: 10 \tTraining Loss: 0.000474\n",
      "Epoch: 11 \tTraining Loss: 0.000474\n",
      "Epoch: 12 \tTraining Loss: 0.000474\n",
      "Epoch: 13 \tTraining Loss: 0.000474\n",
      "Epoch: 14 \tTraining Loss: 0.000474\n",
      "Epoch: 15 \tTraining Loss: 0.000474\n",
      "Epoch: 16 \tTraining Loss: 0.000474\n",
      "Epoch: 17 \tTraining Loss: 0.000474\n",
      "Epoch: 18 \tTraining Loss: 0.000474\n",
      "Epoch: 1 \tTraining Loss: 0.000240\n",
      "Epoch: 2 \tTraining Loss: 0.000240\n",
      "Epoch: 3 \tTraining Loss: 0.000240\n",
      "Epoch: 4 \tTraining Loss: 0.000240\n",
      "Epoch: 5 \tTraining Loss: 0.000240\n",
      "Epoch: 6 \tTraining Loss: 0.000240\n",
      "Epoch: 7 \tTraining Loss: 0.000240\n",
      "Epoch: 8 \tTraining Loss: 0.000240\n",
      "Epoch: 9 \tTraining Loss: 0.000240\n",
      "Epoch: 10 \tTraining Loss: 0.000240\n",
      "Epoch: 11 \tTraining Loss: 0.000240\n",
      "Epoch: 12 \tTraining Loss: 0.000240\n",
      "Epoch: 13 \tTraining Loss: 0.000240\n",
      "Epoch: 14 \tTraining Loss: 0.000240\n",
      "Epoch: 15 \tTraining Loss: 0.000240\n",
      "Epoch: 16 \tTraining Loss: 0.000240\n",
      "Epoch: 17 \tTraining Loss: 0.000240\n",
      "Epoch: 18 \tTraining Loss: 0.000240\n",
      "Epoch: 1 \tTraining Loss: 0.000144\n",
      "Epoch: 2 \tTraining Loss: 0.000144\n",
      "Epoch: 3 \tTraining Loss: 0.000144\n",
      "Epoch: 4 \tTraining Loss: 0.000144\n",
      "Epoch: 5 \tTraining Loss: 0.000144\n",
      "Epoch: 6 \tTraining Loss: 0.000144\n",
      "Epoch: 7 \tTraining Loss: 0.000144\n",
      "Epoch: 8 \tTraining Loss: 0.000144\n",
      "Epoch: 9 \tTraining Loss: 0.000144\n",
      "Epoch: 10 \tTraining Loss: 0.000144\n",
      "Epoch: 11 \tTraining Loss: 0.000144\n",
      "Epoch: 12 \tTraining Loss: 0.000144\n",
      "Epoch: 13 \tTraining Loss: 0.000144\n",
      "Epoch: 14 \tTraining Loss: 0.000144\n",
      "Epoch: 15 \tTraining Loss: 0.000144\n",
      "Epoch: 16 \tTraining Loss: 0.000144\n",
      "Epoch: 17 \tTraining Loss: 0.000144\n",
      "Epoch: 18 \tTraining Loss: 0.000144\n",
      "Epoch: 1 \tTraining Loss: 0.000068\n",
      "Epoch: 2 \tTraining Loss: 0.000068\n",
      "Epoch: 3 \tTraining Loss: 0.000068\n",
      "Epoch: 4 \tTraining Loss: 0.000068\n",
      "Epoch: 5 \tTraining Loss: 0.000068\n",
      "Epoch: 6 \tTraining Loss: 0.000068\n",
      "Epoch: 7 \tTraining Loss: 0.000068\n",
      "Epoch: 8 \tTraining Loss: 0.000068\n",
      "Epoch: 9 \tTraining Loss: 0.000068\n",
      "Epoch: 10 \tTraining Loss: 0.000068\n",
      "Epoch: 11 \tTraining Loss: 0.000068\n",
      "Epoch: 12 \tTraining Loss: 0.000068\n",
      "Epoch: 13 \tTraining Loss: 0.000068\n",
      "Epoch: 14 \tTraining Loss: 0.000068\n",
      "Epoch: 15 \tTraining Loss: 0.000068\n",
      "Epoch: 16 \tTraining Loss: 0.000068\n",
      "Epoch: 17 \tTraining Loss: 0.000068\n",
      "Epoch: 18 \tTraining Loss: 0.000068\n",
      "Epoch: 1 \tTraining Loss: 0.000235\n",
      "Epoch: 2 \tTraining Loss: 0.000235\n",
      "Epoch: 3 \tTraining Loss: 0.000235\n",
      "Epoch: 4 \tTraining Loss: 0.000235\n",
      "Epoch: 5 \tTraining Loss: 0.000235\n",
      "Epoch: 6 \tTraining Loss: 0.000235\n",
      "Epoch: 7 \tTraining Loss: 0.000235\n",
      "Epoch: 8 \tTraining Loss: 0.000235\n",
      "Epoch: 9 \tTraining Loss: 0.000235\n",
      "Epoch: 10 \tTraining Loss: 0.000235\n",
      "Epoch: 11 \tTraining Loss: 0.000235\n",
      "Epoch: 12 \tTraining Loss: 0.000235\n",
      "Epoch: 13 \tTraining Loss: 0.000235\n",
      "Epoch: 14 \tTraining Loss: 0.000235\n",
      "Epoch: 15 \tTraining Loss: 0.000235\n",
      "Epoch: 16 \tTraining Loss: 0.000235\n",
      "Epoch: 17 \tTraining Loss: 0.000235\n",
      "Epoch: 18 \tTraining Loss: 0.000235\n",
      "Epoch: 1 \tTraining Loss: 0.000141\n",
      "Epoch: 2 \tTraining Loss: 0.000141\n",
      "Epoch: 3 \tTraining Loss: 0.000141\n",
      "Epoch: 4 \tTraining Loss: 0.000141\n",
      "Epoch: 5 \tTraining Loss: 0.000141\n",
      "Epoch: 6 \tTraining Loss: 0.000141\n",
      "Epoch: 7 \tTraining Loss: 0.000141\n",
      "Epoch: 8 \tTraining Loss: 0.000141\n",
      "Epoch: 9 \tTraining Loss: 0.000141\n",
      "Epoch: 10 \tTraining Loss: 0.000141\n",
      "Epoch: 11 \tTraining Loss: 0.000141\n",
      "Epoch: 12 \tTraining Loss: 0.000141\n",
      "Epoch: 13 \tTraining Loss: 0.000141\n",
      "Epoch: 14 \tTraining Loss: 0.000141\n",
      "Epoch: 15 \tTraining Loss: 0.000141\n",
      "Epoch: 16 \tTraining Loss: 0.000141\n",
      "Epoch: 17 \tTraining Loss: 0.000141\n",
      "Epoch: 18 \tTraining Loss: 0.000141\n",
      "Epoch: 1 \tTraining Loss: 0.000253\n",
      "Epoch: 2 \tTraining Loss: 0.000253\n",
      "Epoch: 3 \tTraining Loss: 0.000253\n",
      "Epoch: 4 \tTraining Loss: 0.000253\n",
      "Epoch: 5 \tTraining Loss: 0.000253\n",
      "Epoch: 6 \tTraining Loss: 0.000253\n",
      "Epoch: 7 \tTraining Loss: 0.000253\n",
      "Epoch: 8 \tTraining Loss: 0.000253\n",
      "Epoch: 9 \tTraining Loss: 0.000253\n",
      "Epoch: 10 \tTraining Loss: 0.000253\n",
      "Epoch: 11 \tTraining Loss: 0.000253\n",
      "Epoch: 12 \tTraining Loss: 0.000253\n",
      "Epoch: 13 \tTraining Loss: 0.000253\n",
      "Epoch: 14 \tTraining Loss: 0.000253\n",
      "Epoch: 15 \tTraining Loss: 0.000253\n",
      "Epoch: 16 \tTraining Loss: 0.000253\n",
      "Epoch: 17 \tTraining Loss: 0.000253\n",
      "Epoch: 18 \tTraining Loss: 0.000253\n",
      "Epoch: 1 \tTraining Loss: 0.000231\n",
      "Epoch: 2 \tTraining Loss: 0.000231\n",
      "Epoch: 3 \tTraining Loss: 0.000231\n",
      "Epoch: 4 \tTraining Loss: 0.000231\n",
      "Epoch: 5 \tTraining Loss: 0.000231\n",
      "Epoch: 6 \tTraining Loss: 0.000231\n",
      "Epoch: 7 \tTraining Loss: 0.000231\n",
      "Epoch: 8 \tTraining Loss: 0.000231\n",
      "Epoch: 9 \tTraining Loss: 0.000231\n",
      "Epoch: 10 \tTraining Loss: 0.000231\n",
      "Epoch: 11 \tTraining Loss: 0.000231\n",
      "Epoch: 12 \tTraining Loss: 0.000231\n",
      "Epoch: 13 \tTraining Loss: 0.000231\n",
      "Epoch: 14 \tTraining Loss: 0.000231\n",
      "Epoch: 15 \tTraining Loss: 0.000231\n",
      "Epoch: 16 \tTraining Loss: 0.000231\n",
      "Epoch: 17 \tTraining Loss: 0.000231\n",
      "Epoch: 18 \tTraining Loss: 0.000231\n",
      "Epoch: 1 \tTraining Loss: 0.000296\n",
      "Epoch: 2 \tTraining Loss: 0.000296\n",
      "Epoch: 3 \tTraining Loss: 0.000296\n",
      "Epoch: 4 \tTraining Loss: 0.000296\n",
      "Epoch: 5 \tTraining Loss: 0.000296\n",
      "Epoch: 6 \tTraining Loss: 0.000296\n",
      "Epoch: 7 \tTraining Loss: 0.000296\n",
      "Epoch: 8 \tTraining Loss: 0.000296\n",
      "Epoch: 9 \tTraining Loss: 0.000296\n",
      "Epoch: 10 \tTraining Loss: 0.000296\n",
      "Epoch: 11 \tTraining Loss: 0.000296\n",
      "Epoch: 12 \tTraining Loss: 0.000296\n",
      "Epoch: 13 \tTraining Loss: 0.000296\n",
      "Epoch: 14 \tTraining Loss: 0.000296\n",
      "Epoch: 15 \tTraining Loss: 0.000296\n",
      "Epoch: 16 \tTraining Loss: 0.000296\n",
      "Epoch: 17 \tTraining Loss: 0.000296\n",
      "Epoch: 18 \tTraining Loss: 0.000296\n",
      "Epoch: 1 \tTraining Loss: 0.000225\n",
      "Epoch: 2 \tTraining Loss: 0.000225\n",
      "Epoch: 3 \tTraining Loss: 0.000225\n",
      "Epoch: 4 \tTraining Loss: 0.000225\n",
      "Epoch: 5 \tTraining Loss: 0.000225\n",
      "Epoch: 6 \tTraining Loss: 0.000225\n",
      "Epoch: 7 \tTraining Loss: 0.000225\n",
      "Epoch: 8 \tTraining Loss: 0.000225\n",
      "Epoch: 9 \tTraining Loss: 0.000225\n",
      "Epoch: 10 \tTraining Loss: 0.000225\n",
      "Epoch: 11 \tTraining Loss: 0.000225\n",
      "Epoch: 12 \tTraining Loss: 0.000225\n",
      "Epoch: 13 \tTraining Loss: 0.000225\n",
      "Epoch: 14 \tTraining Loss: 0.000225\n",
      "Epoch: 15 \tTraining Loss: 0.000225\n",
      "Epoch: 16 \tTraining Loss: 0.000225\n",
      "Epoch: 17 \tTraining Loss: 0.000225\n",
      "Epoch: 18 \tTraining Loss: 0.000225\n",
      "Epoch: 1 \tTraining Loss: 0.000140\n",
      "Epoch: 2 \tTraining Loss: 0.000140\n",
      "Epoch: 3 \tTraining Loss: 0.000140\n",
      "Epoch: 4 \tTraining Loss: 0.000140\n",
      "Epoch: 5 \tTraining Loss: 0.000140\n",
      "Epoch: 6 \tTraining Loss: 0.000140\n",
      "Epoch: 7 \tTraining Loss: 0.000140\n",
      "Epoch: 8 \tTraining Loss: 0.000140\n",
      "Epoch: 9 \tTraining Loss: 0.000140\n",
      "Epoch: 10 \tTraining Loss: 0.000140\n",
      "Epoch: 11 \tTraining Loss: 0.000140\n",
      "Epoch: 12 \tTraining Loss: 0.000140\n",
      "Epoch: 13 \tTraining Loss: 0.000140\n",
      "Epoch: 14 \tTraining Loss: 0.000140\n",
      "Epoch: 15 \tTraining Loss: 0.000140\n",
      "Epoch: 16 \tTraining Loss: 0.000140\n",
      "Epoch: 17 \tTraining Loss: 0.000140\n",
      "Epoch: 18 \tTraining Loss: 0.000140\n",
      "Epoch: 1 \tTraining Loss: 0.000198\n",
      "Epoch: 2 \tTraining Loss: 0.000198\n",
      "Epoch: 3 \tTraining Loss: 0.000198\n",
      "Epoch: 4 \tTraining Loss: 0.000198\n",
      "Epoch: 5 \tTraining Loss: 0.000198\n",
      "Epoch: 6 \tTraining Loss: 0.000198\n",
      "Epoch: 7 \tTraining Loss: 0.000198\n",
      "Epoch: 8 \tTraining Loss: 0.000198\n",
      "Epoch: 9 \tTraining Loss: 0.000198\n",
      "Epoch: 10 \tTraining Loss: 0.000198\n",
      "Epoch: 11 \tTraining Loss: 0.000198\n",
      "Epoch: 12 \tTraining Loss: 0.000198\n",
      "Epoch: 13 \tTraining Loss: 0.000198\n",
      "Epoch: 14 \tTraining Loss: 0.000198\n",
      "Epoch: 15 \tTraining Loss: 0.000198\n",
      "Epoch: 16 \tTraining Loss: 0.000198\n",
      "Epoch: 17 \tTraining Loss: 0.000198\n",
      "Epoch: 18 \tTraining Loss: 0.000198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000126\n",
      "Epoch: 2 \tTraining Loss: 0.000126\n",
      "Epoch: 3 \tTraining Loss: 0.000126\n",
      "Epoch: 4 \tTraining Loss: 0.000126\n",
      "Epoch: 5 \tTraining Loss: 0.000126\n",
      "Epoch: 6 \tTraining Loss: 0.000126\n",
      "Epoch: 7 \tTraining Loss: 0.000126\n",
      "Epoch: 8 \tTraining Loss: 0.000126\n",
      "Epoch: 9 \tTraining Loss: 0.000126\n",
      "Epoch: 10 \tTraining Loss: 0.000126\n",
      "Epoch: 11 \tTraining Loss: 0.000126\n",
      "Epoch: 12 \tTraining Loss: 0.000126\n",
      "Epoch: 13 \tTraining Loss: 0.000126\n",
      "Epoch: 14 \tTraining Loss: 0.000126\n",
      "Epoch: 15 \tTraining Loss: 0.000126\n",
      "Epoch: 16 \tTraining Loss: 0.000126\n",
      "Epoch: 17 \tTraining Loss: 0.000126\n",
      "Epoch: 18 \tTraining Loss: 0.000126\n",
      "Epoch: 1 \tTraining Loss: 0.000220\n",
      "Epoch: 2 \tTraining Loss: 0.000220\n",
      "Epoch: 3 \tTraining Loss: 0.000220\n",
      "Epoch: 4 \tTraining Loss: 0.000220\n",
      "Epoch: 5 \tTraining Loss: 0.000220\n",
      "Epoch: 6 \tTraining Loss: 0.000220\n",
      "Epoch: 7 \tTraining Loss: 0.000220\n",
      "Epoch: 8 \tTraining Loss: 0.000220\n",
      "Epoch: 9 \tTraining Loss: 0.000220\n",
      "Epoch: 10 \tTraining Loss: 0.000220\n",
      "Epoch: 11 \tTraining Loss: 0.000220\n",
      "Epoch: 12 \tTraining Loss: 0.000220\n",
      "Epoch: 13 \tTraining Loss: 0.000220\n",
      "Epoch: 14 \tTraining Loss: 0.000220\n",
      "Epoch: 15 \tTraining Loss: 0.000220\n",
      "Epoch: 16 \tTraining Loss: 0.000220\n",
      "Epoch: 17 \tTraining Loss: 0.000220\n",
      "Epoch: 18 \tTraining Loss: 0.000220\n",
      "Epoch: 1 \tTraining Loss: 0.000265\n",
      "Epoch: 2 \tTraining Loss: 0.000265\n",
      "Epoch: 3 \tTraining Loss: 0.000265\n",
      "Epoch: 4 \tTraining Loss: 0.000265\n",
      "Epoch: 5 \tTraining Loss: 0.000265\n",
      "Epoch: 6 \tTraining Loss: 0.000265\n",
      "Epoch: 7 \tTraining Loss: 0.000265\n",
      "Epoch: 8 \tTraining Loss: 0.000265\n",
      "Epoch: 9 \tTraining Loss: 0.000265\n",
      "Epoch: 10 \tTraining Loss: 0.000265\n",
      "Epoch: 11 \tTraining Loss: 0.000265\n",
      "Epoch: 12 \tTraining Loss: 0.000265\n",
      "Epoch: 13 \tTraining Loss: 0.000265\n",
      "Epoch: 14 \tTraining Loss: 0.000265\n",
      "Epoch: 15 \tTraining Loss: 0.000265\n",
      "Epoch: 16 \tTraining Loss: 0.000265\n",
      "Epoch: 17 \tTraining Loss: 0.000265\n",
      "Epoch: 18 \tTraining Loss: 0.000265\n",
      "Epoch: 1 \tTraining Loss: 0.000151\n",
      "Epoch: 2 \tTraining Loss: 0.000151\n",
      "Epoch: 3 \tTraining Loss: 0.000151\n",
      "Epoch: 4 \tTraining Loss: 0.000151\n",
      "Epoch: 5 \tTraining Loss: 0.000151\n",
      "Epoch: 6 \tTraining Loss: 0.000151\n",
      "Epoch: 7 \tTraining Loss: 0.000151\n",
      "Epoch: 8 \tTraining Loss: 0.000151\n",
      "Epoch: 9 \tTraining Loss: 0.000151\n",
      "Epoch: 10 \tTraining Loss: 0.000151\n",
      "Epoch: 11 \tTraining Loss: 0.000151\n",
      "Epoch: 12 \tTraining Loss: 0.000151\n",
      "Epoch: 13 \tTraining Loss: 0.000151\n",
      "Epoch: 14 \tTraining Loss: 0.000151\n",
      "Epoch: 15 \tTraining Loss: 0.000151\n",
      "Epoch: 16 \tTraining Loss: 0.000151\n",
      "Epoch: 17 \tTraining Loss: 0.000151\n",
      "Epoch: 18 \tTraining Loss: 0.000151\n",
      "Epoch: 1 \tTraining Loss: 0.000222\n",
      "Epoch: 2 \tTraining Loss: 0.000222\n",
      "Epoch: 3 \tTraining Loss: 0.000222\n",
      "Epoch: 4 \tTraining Loss: 0.000222\n",
      "Epoch: 5 \tTraining Loss: 0.000222\n",
      "Epoch: 6 \tTraining Loss: 0.000222\n",
      "Epoch: 7 \tTraining Loss: 0.000222\n",
      "Epoch: 8 \tTraining Loss: 0.000222\n",
      "Epoch: 9 \tTraining Loss: 0.000222\n",
      "Epoch: 10 \tTraining Loss: 0.000222\n",
      "Epoch: 11 \tTraining Loss: 0.000222\n",
      "Epoch: 12 \tTraining Loss: 0.000222\n",
      "Epoch: 13 \tTraining Loss: 0.000222\n",
      "Epoch: 14 \tTraining Loss: 0.000222\n",
      "Epoch: 15 \tTraining Loss: 0.000222\n",
      "Epoch: 16 \tTraining Loss: 0.000222\n",
      "Epoch: 17 \tTraining Loss: 0.000222\n",
      "Epoch: 18 \tTraining Loss: 0.000222\n",
      "Epoch: 1 \tTraining Loss: 0.000354\n",
      "Epoch: 2 \tTraining Loss: 0.000354\n",
      "Epoch: 3 \tTraining Loss: 0.000354\n",
      "Epoch: 4 \tTraining Loss: 0.000354\n",
      "Epoch: 5 \tTraining Loss: 0.000354\n",
      "Epoch: 6 \tTraining Loss: 0.000354\n",
      "Epoch: 7 \tTraining Loss: 0.000354\n",
      "Epoch: 8 \tTraining Loss: 0.000354\n",
      "Epoch: 9 \tTraining Loss: 0.000354\n",
      "Epoch: 10 \tTraining Loss: 0.000354\n",
      "Epoch: 11 \tTraining Loss: 0.000354\n",
      "Epoch: 12 \tTraining Loss: 0.000354\n",
      "Epoch: 13 \tTraining Loss: 0.000354\n",
      "Epoch: 14 \tTraining Loss: 0.000354\n",
      "Epoch: 15 \tTraining Loss: 0.000354\n",
      "Epoch: 16 \tTraining Loss: 0.000354\n",
      "Epoch: 17 \tTraining Loss: 0.000354\n",
      "Epoch: 18 \tTraining Loss: 0.000354\n",
      "Epoch: 1 \tTraining Loss: 0.000290\n",
      "Epoch: 2 \tTraining Loss: 0.000290\n",
      "Epoch: 3 \tTraining Loss: 0.000290\n",
      "Epoch: 4 \tTraining Loss: 0.000290\n",
      "Epoch: 5 \tTraining Loss: 0.000290\n",
      "Epoch: 6 \tTraining Loss: 0.000290\n",
      "Epoch: 7 \tTraining Loss: 0.000290\n",
      "Epoch: 8 \tTraining Loss: 0.000290\n",
      "Epoch: 9 \tTraining Loss: 0.000290\n",
      "Epoch: 10 \tTraining Loss: 0.000290\n",
      "Epoch: 11 \tTraining Loss: 0.000290\n",
      "Epoch: 12 \tTraining Loss: 0.000290\n",
      "Epoch: 13 \tTraining Loss: 0.000290\n",
      "Epoch: 14 \tTraining Loss: 0.000290\n",
      "Epoch: 15 \tTraining Loss: 0.000290\n",
      "Epoch: 16 \tTraining Loss: 0.000290\n",
      "Epoch: 17 \tTraining Loss: 0.000290\n",
      "Epoch: 18 \tTraining Loss: 0.000290\n",
      "Epoch: 1 \tTraining Loss: 0.000141\n",
      "Epoch: 2 \tTraining Loss: 0.000141\n",
      "Epoch: 3 \tTraining Loss: 0.000141\n",
      "Epoch: 4 \tTraining Loss: 0.000141\n",
      "Epoch: 5 \tTraining Loss: 0.000141\n",
      "Epoch: 6 \tTraining Loss: 0.000141\n",
      "Epoch: 7 \tTraining Loss: 0.000141\n",
      "Epoch: 8 \tTraining Loss: 0.000141\n",
      "Epoch: 9 \tTraining Loss: 0.000141\n",
      "Epoch: 10 \tTraining Loss: 0.000141\n",
      "Epoch: 11 \tTraining Loss: 0.000141\n",
      "Epoch: 12 \tTraining Loss: 0.000141\n",
      "Epoch: 13 \tTraining Loss: 0.000141\n",
      "Epoch: 14 \tTraining Loss: 0.000141\n",
      "Epoch: 15 \tTraining Loss: 0.000141\n",
      "Epoch: 16 \tTraining Loss: 0.000141\n",
      "Epoch: 17 \tTraining Loss: 0.000141\n",
      "Epoch: 18 \tTraining Loss: 0.000141\n",
      "Epoch: 1 \tTraining Loss: 0.000203\n",
      "Epoch: 2 \tTraining Loss: 0.000203\n",
      "Epoch: 3 \tTraining Loss: 0.000203\n",
      "Epoch: 4 \tTraining Loss: 0.000203\n",
      "Epoch: 5 \tTraining Loss: 0.000203\n",
      "Epoch: 6 \tTraining Loss: 0.000203\n",
      "Epoch: 7 \tTraining Loss: 0.000203\n",
      "Epoch: 8 \tTraining Loss: 0.000203\n",
      "Epoch: 9 \tTraining Loss: 0.000203\n",
      "Epoch: 10 \tTraining Loss: 0.000203\n",
      "Epoch: 11 \tTraining Loss: 0.000203\n",
      "Epoch: 12 \tTraining Loss: 0.000203\n",
      "Epoch: 13 \tTraining Loss: 0.000203\n",
      "Epoch: 14 \tTraining Loss: 0.000203\n",
      "Epoch: 15 \tTraining Loss: 0.000203\n",
      "Epoch: 16 \tTraining Loss: 0.000203\n",
      "Epoch: 17 \tTraining Loss: 0.000203\n",
      "Epoch: 18 \tTraining Loss: 0.000203\n",
      "Epoch: 1 \tTraining Loss: 0.000126\n",
      "Epoch: 2 \tTraining Loss: 0.000126\n",
      "Epoch: 3 \tTraining Loss: 0.000126\n",
      "Epoch: 4 \tTraining Loss: 0.000126\n",
      "Epoch: 5 \tTraining Loss: 0.000126\n",
      "Epoch: 6 \tTraining Loss: 0.000126\n",
      "Epoch: 7 \tTraining Loss: 0.000126\n",
      "Epoch: 8 \tTraining Loss: 0.000126\n",
      "Epoch: 9 \tTraining Loss: 0.000126\n",
      "Epoch: 10 \tTraining Loss: 0.000126\n",
      "Epoch: 11 \tTraining Loss: 0.000126\n",
      "Epoch: 12 \tTraining Loss: 0.000126\n",
      "Epoch: 13 \tTraining Loss: 0.000126\n",
      "Epoch: 14 \tTraining Loss: 0.000126\n",
      "Epoch: 15 \tTraining Loss: 0.000126\n",
      "Epoch: 16 \tTraining Loss: 0.000126\n",
      "Epoch: 17 \tTraining Loss: 0.000126\n",
      "Epoch: 18 \tTraining Loss: 0.000126\n",
      "Epoch: 1 \tTraining Loss: 0.000220\n",
      "Epoch: 2 \tTraining Loss: 0.000220\n",
      "Epoch: 3 \tTraining Loss: 0.000220\n",
      "Epoch: 4 \tTraining Loss: 0.000220\n",
      "Epoch: 5 \tTraining Loss: 0.000220\n",
      "Epoch: 6 \tTraining Loss: 0.000220\n",
      "Epoch: 7 \tTraining Loss: 0.000220\n",
      "Epoch: 8 \tTraining Loss: 0.000220\n",
      "Epoch: 9 \tTraining Loss: 0.000220\n",
      "Epoch: 10 \tTraining Loss: 0.000220\n",
      "Epoch: 11 \tTraining Loss: 0.000220\n",
      "Epoch: 12 \tTraining Loss: 0.000220\n",
      "Epoch: 13 \tTraining Loss: 0.000220\n",
      "Epoch: 14 \tTraining Loss: 0.000220\n",
      "Epoch: 15 \tTraining Loss: 0.000220\n",
      "Epoch: 16 \tTraining Loss: 0.000220\n",
      "Epoch: 17 \tTraining Loss: 0.000220\n",
      "Epoch: 18 \tTraining Loss: 0.000220\n",
      "Epoch: 1 \tTraining Loss: 0.000264\n",
      "Epoch: 2 \tTraining Loss: 0.000264\n",
      "Epoch: 3 \tTraining Loss: 0.000264\n",
      "Epoch: 4 \tTraining Loss: 0.000264\n",
      "Epoch: 5 \tTraining Loss: 0.000264\n",
      "Epoch: 6 \tTraining Loss: 0.000264\n",
      "Epoch: 7 \tTraining Loss: 0.000264\n",
      "Epoch: 8 \tTraining Loss: 0.000264\n",
      "Epoch: 9 \tTraining Loss: 0.000264\n",
      "Epoch: 10 \tTraining Loss: 0.000264\n",
      "Epoch: 11 \tTraining Loss: 0.000264\n",
      "Epoch: 12 \tTraining Loss: 0.000264\n",
      "Epoch: 13 \tTraining Loss: 0.000264\n",
      "Epoch: 14 \tTraining Loss: 0.000264\n",
      "Epoch: 15 \tTraining Loss: 0.000264\n",
      "Epoch: 16 \tTraining Loss: 0.000264\n",
      "Epoch: 17 \tTraining Loss: 0.000264\n",
      "Epoch: 18 \tTraining Loss: 0.000264\n",
      "Epoch: 1 \tTraining Loss: 0.000309\n",
      "Epoch: 2 \tTraining Loss: 0.000309\n",
      "Epoch: 3 \tTraining Loss: 0.000309\n",
      "Epoch: 4 \tTraining Loss: 0.000309\n",
      "Epoch: 5 \tTraining Loss: 0.000309\n",
      "Epoch: 6 \tTraining Loss: 0.000309\n",
      "Epoch: 7 \tTraining Loss: 0.000309\n",
      "Epoch: 8 \tTraining Loss: 0.000309\n",
      "Epoch: 9 \tTraining Loss: 0.000309\n",
      "Epoch: 10 \tTraining Loss: 0.000309\n",
      "Epoch: 11 \tTraining Loss: 0.000309\n",
      "Epoch: 12 \tTraining Loss: 0.000309\n",
      "Epoch: 13 \tTraining Loss: 0.000309\n",
      "Epoch: 14 \tTraining Loss: 0.000309\n",
      "Epoch: 15 \tTraining Loss: 0.000309\n",
      "Epoch: 16 \tTraining Loss: 0.000309\n",
      "Epoch: 17 \tTraining Loss: 0.000309\n",
      "Epoch: 18 \tTraining Loss: 0.000309\n",
      "Epoch: 1 \tTraining Loss: 0.000158\n",
      "Epoch: 2 \tTraining Loss: 0.000158\n",
      "Epoch: 3 \tTraining Loss: 0.000158\n",
      "Epoch: 4 \tTraining Loss: 0.000158\n",
      "Epoch: 5 \tTraining Loss: 0.000158\n",
      "Epoch: 6 \tTraining Loss: 0.000158\n",
      "Epoch: 7 \tTraining Loss: 0.000158\n",
      "Epoch: 8 \tTraining Loss: 0.000158\n",
      "Epoch: 9 \tTraining Loss: 0.000158\n",
      "Epoch: 10 \tTraining Loss: 0.000158\n",
      "Epoch: 11 \tTraining Loss: 0.000158\n",
      "Epoch: 12 \tTraining Loss: 0.000158\n",
      "Epoch: 13 \tTraining Loss: 0.000158\n",
      "Epoch: 14 \tTraining Loss: 0.000158\n",
      "Epoch: 15 \tTraining Loss: 0.000158\n",
      "Epoch: 16 \tTraining Loss: 0.000158\n",
      "Epoch: 17 \tTraining Loss: 0.000158\n",
      "Epoch: 18 \tTraining Loss: 0.000158\n",
      "Epoch: 1 \tTraining Loss: 0.000069\n",
      "Epoch: 2 \tTraining Loss: 0.000069\n",
      "Epoch: 3 \tTraining Loss: 0.000069\n",
      "Epoch: 4 \tTraining Loss: 0.000069\n",
      "Epoch: 5 \tTraining Loss: 0.000069\n",
      "Epoch: 6 \tTraining Loss: 0.000069\n",
      "Epoch: 7 \tTraining Loss: 0.000069\n",
      "Epoch: 8 \tTraining Loss: 0.000069\n",
      "Epoch: 9 \tTraining Loss: 0.000069\n",
      "Epoch: 10 \tTraining Loss: 0.000069\n",
      "Epoch: 11 \tTraining Loss: 0.000069\n",
      "Epoch: 12 \tTraining Loss: 0.000069\n",
      "Epoch: 13 \tTraining Loss: 0.000069\n",
      "Epoch: 14 \tTraining Loss: 0.000069\n",
      "Epoch: 15 \tTraining Loss: 0.000069\n",
      "Epoch: 16 \tTraining Loss: 0.000069\n",
      "Epoch: 17 \tTraining Loss: 0.000069\n",
      "Epoch: 18 \tTraining Loss: 0.000069\n",
      "Epoch: 1 \tTraining Loss: 0.000141\n",
      "Epoch: 2 \tTraining Loss: 0.000141\n",
      "Epoch: 3 \tTraining Loss: 0.000141\n",
      "Epoch: 4 \tTraining Loss: 0.000141\n",
      "Epoch: 5 \tTraining Loss: 0.000141\n",
      "Epoch: 6 \tTraining Loss: 0.000141\n",
      "Epoch: 7 \tTraining Loss: 0.000141\n",
      "Epoch: 8 \tTraining Loss: 0.000141\n",
      "Epoch: 9 \tTraining Loss: 0.000141\n",
      "Epoch: 10 \tTraining Loss: 0.000141\n",
      "Epoch: 11 \tTraining Loss: 0.000141\n",
      "Epoch: 12 \tTraining Loss: 0.000141\n",
      "Epoch: 13 \tTraining Loss: 0.000141\n",
      "Epoch: 14 \tTraining Loss: 0.000141\n",
      "Epoch: 15 \tTraining Loss: 0.000141\n",
      "Epoch: 16 \tTraining Loss: 0.000141\n",
      "Epoch: 17 \tTraining Loss: 0.000141\n",
      "Epoch: 18 \tTraining Loss: 0.000141\n",
      "Epoch: 1 \tTraining Loss: 0.000194\n",
      "Epoch: 2 \tTraining Loss: 0.000194\n",
      "Epoch: 3 \tTraining Loss: 0.000194\n",
      "Epoch: 4 \tTraining Loss: 0.000194\n",
      "Epoch: 5 \tTraining Loss: 0.000194\n",
      "Epoch: 6 \tTraining Loss: 0.000194\n",
      "Epoch: 7 \tTraining Loss: 0.000194\n",
      "Epoch: 8 \tTraining Loss: 0.000194\n",
      "Epoch: 9 \tTraining Loss: 0.000194\n",
      "Epoch: 10 \tTraining Loss: 0.000194\n",
      "Epoch: 11 \tTraining Loss: 0.000194\n",
      "Epoch: 12 \tTraining Loss: 0.000194\n",
      "Epoch: 13 \tTraining Loss: 0.000194\n",
      "Epoch: 14 \tTraining Loss: 0.000194\n",
      "Epoch: 15 \tTraining Loss: 0.000194\n",
      "Epoch: 16 \tTraining Loss: 0.000194\n",
      "Epoch: 17 \tTraining Loss: 0.000194\n",
      "Epoch: 18 \tTraining Loss: 0.000194\n",
      "Epoch: 1 \tTraining Loss: 0.000097\n",
      "Epoch: 2 \tTraining Loss: 0.000097\n",
      "Epoch: 3 \tTraining Loss: 0.000097\n",
      "Epoch: 4 \tTraining Loss: 0.000097\n",
      "Epoch: 5 \tTraining Loss: 0.000097\n",
      "Epoch: 6 \tTraining Loss: 0.000097\n",
      "Epoch: 7 \tTraining Loss: 0.000097\n",
      "Epoch: 8 \tTraining Loss: 0.000097\n",
      "Epoch: 9 \tTraining Loss: 0.000097\n",
      "Epoch: 10 \tTraining Loss: 0.000097\n",
      "Epoch: 11 \tTraining Loss: 0.000097\n",
      "Epoch: 12 \tTraining Loss: 0.000097\n",
      "Epoch: 13 \tTraining Loss: 0.000097\n",
      "Epoch: 14 \tTraining Loss: 0.000097\n",
      "Epoch: 15 \tTraining Loss: 0.000097\n",
      "Epoch: 16 \tTraining Loss: 0.000097\n",
      "Epoch: 17 \tTraining Loss: 0.000097\n",
      "Epoch: 18 \tTraining Loss: 0.000097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000218\n",
      "Epoch: 2 \tTraining Loss: 0.000218\n",
      "Epoch: 3 \tTraining Loss: 0.000218\n",
      "Epoch: 4 \tTraining Loss: 0.000218\n",
      "Epoch: 5 \tTraining Loss: 0.000218\n",
      "Epoch: 6 \tTraining Loss: 0.000218\n",
      "Epoch: 7 \tTraining Loss: 0.000218\n",
      "Epoch: 8 \tTraining Loss: 0.000218\n",
      "Epoch: 9 \tTraining Loss: 0.000218\n",
      "Epoch: 10 \tTraining Loss: 0.000218\n",
      "Epoch: 11 \tTraining Loss: 0.000218\n",
      "Epoch: 12 \tTraining Loss: 0.000218\n",
      "Epoch: 13 \tTraining Loss: 0.000218\n",
      "Epoch: 14 \tTraining Loss: 0.000218\n",
      "Epoch: 15 \tTraining Loss: 0.000218\n",
      "Epoch: 16 \tTraining Loss: 0.000218\n",
      "Epoch: 17 \tTraining Loss: 0.000218\n",
      "Epoch: 18 \tTraining Loss: 0.000218\n",
      "Epoch: 1 \tTraining Loss: 0.000188\n",
      "Epoch: 2 \tTraining Loss: 0.000188\n",
      "Epoch: 3 \tTraining Loss: 0.000188\n",
      "Epoch: 4 \tTraining Loss: 0.000188\n",
      "Epoch: 5 \tTraining Loss: 0.000188\n",
      "Epoch: 6 \tTraining Loss: 0.000188\n",
      "Epoch: 7 \tTraining Loss: 0.000188\n",
      "Epoch: 8 \tTraining Loss: 0.000188\n",
      "Epoch: 9 \tTraining Loss: 0.000188\n",
      "Epoch: 10 \tTraining Loss: 0.000188\n",
      "Epoch: 11 \tTraining Loss: 0.000188\n",
      "Epoch: 12 \tTraining Loss: 0.000188\n",
      "Epoch: 13 \tTraining Loss: 0.000188\n",
      "Epoch: 14 \tTraining Loss: 0.000188\n",
      "Epoch: 15 \tTraining Loss: 0.000188\n",
      "Epoch: 16 \tTraining Loss: 0.000188\n",
      "Epoch: 17 \tTraining Loss: 0.000188\n",
      "Epoch: 18 \tTraining Loss: 0.000188\n",
      "Epoch: 1 \tTraining Loss: 0.000157\n",
      "Epoch: 2 \tTraining Loss: 0.000157\n",
      "Epoch: 3 \tTraining Loss: 0.000157\n",
      "Epoch: 4 \tTraining Loss: 0.000157\n",
      "Epoch: 5 \tTraining Loss: 0.000157\n",
      "Epoch: 6 \tTraining Loss: 0.000157\n",
      "Epoch: 7 \tTraining Loss: 0.000157\n",
      "Epoch: 8 \tTraining Loss: 0.000157\n",
      "Epoch: 9 \tTraining Loss: 0.000157\n",
      "Epoch: 10 \tTraining Loss: 0.000157\n",
      "Epoch: 11 \tTraining Loss: 0.000157\n",
      "Epoch: 12 \tTraining Loss: 0.000157\n",
      "Epoch: 13 \tTraining Loss: 0.000157\n",
      "Epoch: 14 \tTraining Loss: 0.000157\n",
      "Epoch: 15 \tTraining Loss: 0.000157\n",
      "Epoch: 16 \tTraining Loss: 0.000157\n",
      "Epoch: 17 \tTraining Loss: 0.000157\n",
      "Epoch: 18 \tTraining Loss: 0.000157\n",
      "Epoch: 1 \tTraining Loss: 0.000226\n",
      "Epoch: 2 \tTraining Loss: 0.000226\n",
      "Epoch: 3 \tTraining Loss: 0.000226\n",
      "Epoch: 4 \tTraining Loss: 0.000226\n",
      "Epoch: 5 \tTraining Loss: 0.000226\n",
      "Epoch: 6 \tTraining Loss: 0.000226\n",
      "Epoch: 7 \tTraining Loss: 0.000226\n",
      "Epoch: 8 \tTraining Loss: 0.000226\n",
      "Epoch: 9 \tTraining Loss: 0.000226\n",
      "Epoch: 10 \tTraining Loss: 0.000226\n",
      "Epoch: 11 \tTraining Loss: 0.000226\n",
      "Epoch: 12 \tTraining Loss: 0.000226\n",
      "Epoch: 13 \tTraining Loss: 0.000226\n",
      "Epoch: 14 \tTraining Loss: 0.000226\n",
      "Epoch: 15 \tTraining Loss: 0.000226\n",
      "Epoch: 16 \tTraining Loss: 0.000226\n",
      "Epoch: 17 \tTraining Loss: 0.000226\n",
      "Epoch: 18 \tTraining Loss: 0.000226\n",
      "Epoch: 1 \tTraining Loss: 0.000127\n",
      "Epoch: 2 \tTraining Loss: 0.000127\n",
      "Epoch: 3 \tTraining Loss: 0.000127\n",
      "Epoch: 4 \tTraining Loss: 0.000127\n",
      "Epoch: 5 \tTraining Loss: 0.000127\n",
      "Epoch: 6 \tTraining Loss: 0.000127\n",
      "Epoch: 7 \tTraining Loss: 0.000127\n",
      "Epoch: 8 \tTraining Loss: 0.000127\n",
      "Epoch: 9 \tTraining Loss: 0.000127\n",
      "Epoch: 10 \tTraining Loss: 0.000127\n",
      "Epoch: 11 \tTraining Loss: 0.000127\n",
      "Epoch: 12 \tTraining Loss: 0.000127\n",
      "Epoch: 13 \tTraining Loss: 0.000127\n",
      "Epoch: 14 \tTraining Loss: 0.000127\n",
      "Epoch: 15 \tTraining Loss: 0.000127\n",
      "Epoch: 16 \tTraining Loss: 0.000127\n",
      "Epoch: 17 \tTraining Loss: 0.000127\n",
      "Epoch: 18 \tTraining Loss: 0.000127\n",
      "Epoch: 1 \tTraining Loss: 0.000199\n",
      "Epoch: 2 \tTraining Loss: 0.000199\n",
      "Epoch: 3 \tTraining Loss: 0.000199\n",
      "Epoch: 4 \tTraining Loss: 0.000199\n",
      "Epoch: 5 \tTraining Loss: 0.000199\n",
      "Epoch: 6 \tTraining Loss: 0.000199\n",
      "Epoch: 7 \tTraining Loss: 0.000199\n",
      "Epoch: 8 \tTraining Loss: 0.000199\n",
      "Epoch: 9 \tTraining Loss: 0.000199\n",
      "Epoch: 10 \tTraining Loss: 0.000199\n",
      "Epoch: 11 \tTraining Loss: 0.000199\n",
      "Epoch: 12 \tTraining Loss: 0.000199\n",
      "Epoch: 13 \tTraining Loss: 0.000199\n",
      "Epoch: 14 \tTraining Loss: 0.000199\n",
      "Epoch: 15 \tTraining Loss: 0.000199\n",
      "Epoch: 16 \tTraining Loss: 0.000199\n",
      "Epoch: 17 \tTraining Loss: 0.000199\n",
      "Epoch: 18 \tTraining Loss: 0.000199\n",
      "Epoch: 1 \tTraining Loss: 0.000309\n",
      "Epoch: 2 \tTraining Loss: 0.000309\n",
      "Epoch: 3 \tTraining Loss: 0.000309\n",
      "Epoch: 4 \tTraining Loss: 0.000309\n",
      "Epoch: 5 \tTraining Loss: 0.000309\n",
      "Epoch: 6 \tTraining Loss: 0.000309\n",
      "Epoch: 7 \tTraining Loss: 0.000309\n",
      "Epoch: 8 \tTraining Loss: 0.000309\n",
      "Epoch: 9 \tTraining Loss: 0.000309\n",
      "Epoch: 10 \tTraining Loss: 0.000309\n",
      "Epoch: 11 \tTraining Loss: 0.000309\n",
      "Epoch: 12 \tTraining Loss: 0.000309\n",
      "Epoch: 13 \tTraining Loss: 0.000309\n",
      "Epoch: 14 \tTraining Loss: 0.000309\n",
      "Epoch: 15 \tTraining Loss: 0.000309\n",
      "Epoch: 16 \tTraining Loss: 0.000309\n",
      "Epoch: 17 \tTraining Loss: 0.000309\n",
      "Epoch: 18 \tTraining Loss: 0.000309\n",
      "Epoch: 1 \tTraining Loss: 0.000218\n",
      "Epoch: 2 \tTraining Loss: 0.000218\n",
      "Epoch: 3 \tTraining Loss: 0.000218\n",
      "Epoch: 4 \tTraining Loss: 0.000218\n",
      "Epoch: 5 \tTraining Loss: 0.000218\n",
      "Epoch: 6 \tTraining Loss: 0.000218\n",
      "Epoch: 7 \tTraining Loss: 0.000218\n",
      "Epoch: 8 \tTraining Loss: 0.000218\n",
      "Epoch: 9 \tTraining Loss: 0.000218\n",
      "Epoch: 10 \tTraining Loss: 0.000218\n",
      "Epoch: 11 \tTraining Loss: 0.000218\n",
      "Epoch: 12 \tTraining Loss: 0.000218\n",
      "Epoch: 13 \tTraining Loss: 0.000218\n",
      "Epoch: 14 \tTraining Loss: 0.000218\n",
      "Epoch: 15 \tTraining Loss: 0.000218\n",
      "Epoch: 16 \tTraining Loss: 0.000218\n",
      "Epoch: 17 \tTraining Loss: 0.000218\n",
      "Epoch: 18 \tTraining Loss: 0.000218\n",
      "Epoch: 1 \tTraining Loss: 0.000388\n",
      "Epoch: 2 \tTraining Loss: 0.000388\n",
      "Epoch: 3 \tTraining Loss: 0.000388\n",
      "Epoch: 4 \tTraining Loss: 0.000388\n",
      "Epoch: 5 \tTraining Loss: 0.000388\n",
      "Epoch: 6 \tTraining Loss: 0.000388\n",
      "Epoch: 7 \tTraining Loss: 0.000388\n",
      "Epoch: 8 \tTraining Loss: 0.000388\n",
      "Epoch: 9 \tTraining Loss: 0.000388\n",
      "Epoch: 10 \tTraining Loss: 0.000388\n",
      "Epoch: 11 \tTraining Loss: 0.000388\n",
      "Epoch: 12 \tTraining Loss: 0.000388\n",
      "Epoch: 13 \tTraining Loss: 0.000388\n",
      "Epoch: 14 \tTraining Loss: 0.000388\n",
      "Epoch: 15 \tTraining Loss: 0.000388\n",
      "Epoch: 16 \tTraining Loss: 0.000388\n",
      "Epoch: 17 \tTraining Loss: 0.000388\n",
      "Epoch: 18 \tTraining Loss: 0.000388\n",
      "Epoch: 1 \tTraining Loss: 0.000205\n",
      "Epoch: 2 \tTraining Loss: 0.000205\n",
      "Epoch: 3 \tTraining Loss: 0.000205\n",
      "Epoch: 4 \tTraining Loss: 0.000205\n",
      "Epoch: 5 \tTraining Loss: 0.000205\n",
      "Epoch: 6 \tTraining Loss: 0.000205\n",
      "Epoch: 7 \tTraining Loss: 0.000205\n",
      "Epoch: 8 \tTraining Loss: 0.000205\n",
      "Epoch: 9 \tTraining Loss: 0.000205\n",
      "Epoch: 10 \tTraining Loss: 0.000205\n",
      "Epoch: 11 \tTraining Loss: 0.000205\n",
      "Epoch: 12 \tTraining Loss: 0.000205\n",
      "Epoch: 13 \tTraining Loss: 0.000205\n",
      "Epoch: 14 \tTraining Loss: 0.000205\n",
      "Epoch: 15 \tTraining Loss: 0.000205\n",
      "Epoch: 16 \tTraining Loss: 0.000205\n",
      "Epoch: 17 \tTraining Loss: 0.000205\n",
      "Epoch: 18 \tTraining Loss: 0.000205\n",
      "Epoch: 1 \tTraining Loss: 0.000137\n",
      "Epoch: 2 \tTraining Loss: 0.000137\n",
      "Epoch: 3 \tTraining Loss: 0.000137\n",
      "Epoch: 4 \tTraining Loss: 0.000137\n",
      "Epoch: 5 \tTraining Loss: 0.000137\n",
      "Epoch: 6 \tTraining Loss: 0.000137\n",
      "Epoch: 7 \tTraining Loss: 0.000137\n",
      "Epoch: 8 \tTraining Loss: 0.000137\n",
      "Epoch: 9 \tTraining Loss: 0.000137\n",
      "Epoch: 10 \tTraining Loss: 0.000137\n",
      "Epoch: 11 \tTraining Loss: 0.000137\n",
      "Epoch: 12 \tTraining Loss: 0.000137\n",
      "Epoch: 13 \tTraining Loss: 0.000137\n",
      "Epoch: 14 \tTraining Loss: 0.000137\n",
      "Epoch: 15 \tTraining Loss: 0.000137\n",
      "Epoch: 16 \tTraining Loss: 0.000137\n",
      "Epoch: 17 \tTraining Loss: 0.000137\n",
      "Epoch: 18 \tTraining Loss: 0.000137\n",
      "Epoch: 1 \tTraining Loss: 0.000258\n",
      "Epoch: 2 \tTraining Loss: 0.000258\n",
      "Epoch: 3 \tTraining Loss: 0.000258\n",
      "Epoch: 4 \tTraining Loss: 0.000258\n",
      "Epoch: 5 \tTraining Loss: 0.000258\n",
      "Epoch: 6 \tTraining Loss: 0.000258\n",
      "Epoch: 7 \tTraining Loss: 0.000258\n",
      "Epoch: 8 \tTraining Loss: 0.000258\n",
      "Epoch: 9 \tTraining Loss: 0.000258\n",
      "Epoch: 10 \tTraining Loss: 0.000258\n",
      "Epoch: 11 \tTraining Loss: 0.000258\n",
      "Epoch: 12 \tTraining Loss: 0.000258\n",
      "Epoch: 13 \tTraining Loss: 0.000258\n",
      "Epoch: 14 \tTraining Loss: 0.000258\n",
      "Epoch: 15 \tTraining Loss: 0.000258\n",
      "Epoch: 16 \tTraining Loss: 0.000258\n",
      "Epoch: 17 \tTraining Loss: 0.000258\n",
      "Epoch: 18 \tTraining Loss: 0.000258\n",
      "Epoch: 1 \tTraining Loss: 0.000439\n",
      "Epoch: 2 \tTraining Loss: 0.000439\n",
      "Epoch: 3 \tTraining Loss: 0.000439\n",
      "Epoch: 4 \tTraining Loss: 0.000439\n",
      "Epoch: 5 \tTraining Loss: 0.000439\n",
      "Epoch: 6 \tTraining Loss: 0.000439\n",
      "Epoch: 7 \tTraining Loss: 0.000439\n",
      "Epoch: 8 \tTraining Loss: 0.000439\n",
      "Epoch: 9 \tTraining Loss: 0.000439\n",
      "Epoch: 10 \tTraining Loss: 0.000439\n",
      "Epoch: 11 \tTraining Loss: 0.000439\n",
      "Epoch: 12 \tTraining Loss: 0.000439\n",
      "Epoch: 13 \tTraining Loss: 0.000439\n",
      "Epoch: 14 \tTraining Loss: 0.000439\n",
      "Epoch: 15 \tTraining Loss: 0.000439\n",
      "Epoch: 16 \tTraining Loss: 0.000439\n",
      "Epoch: 17 \tTraining Loss: 0.000439\n",
      "Epoch: 18 \tTraining Loss: 0.000439\n",
      "Epoch: 1 \tTraining Loss: 0.000177\n",
      "Epoch: 2 \tTraining Loss: 0.000177\n",
      "Epoch: 3 \tTraining Loss: 0.000177\n",
      "Epoch: 4 \tTraining Loss: 0.000177\n",
      "Epoch: 5 \tTraining Loss: 0.000177\n",
      "Epoch: 6 \tTraining Loss: 0.000177\n",
      "Epoch: 7 \tTraining Loss: 0.000177\n",
      "Epoch: 8 \tTraining Loss: 0.000177\n",
      "Epoch: 9 \tTraining Loss: 0.000177\n",
      "Epoch: 10 \tTraining Loss: 0.000177\n",
      "Epoch: 11 \tTraining Loss: 0.000177\n",
      "Epoch: 12 \tTraining Loss: 0.000177\n",
      "Epoch: 13 \tTraining Loss: 0.000177\n",
      "Epoch: 14 \tTraining Loss: 0.000177\n",
      "Epoch: 15 \tTraining Loss: 0.000177\n",
      "Epoch: 16 \tTraining Loss: 0.000177\n",
      "Epoch: 17 \tTraining Loss: 0.000177\n",
      "Epoch: 18 \tTraining Loss: 0.000177\n",
      "Epoch: 1 \tTraining Loss: 0.000215\n",
      "Epoch: 2 \tTraining Loss: 0.000215\n",
      "Epoch: 3 \tTraining Loss: 0.000215\n",
      "Epoch: 4 \tTraining Loss: 0.000215\n",
      "Epoch: 5 \tTraining Loss: 0.000215\n",
      "Epoch: 6 \tTraining Loss: 0.000215\n",
      "Epoch: 7 \tTraining Loss: 0.000215\n",
      "Epoch: 8 \tTraining Loss: 0.000215\n",
      "Epoch: 9 \tTraining Loss: 0.000215\n",
      "Epoch: 10 \tTraining Loss: 0.000215\n",
      "Epoch: 11 \tTraining Loss: 0.000215\n",
      "Epoch: 12 \tTraining Loss: 0.000215\n",
      "Epoch: 13 \tTraining Loss: 0.000215\n",
      "Epoch: 14 \tTraining Loss: 0.000215\n",
      "Epoch: 15 \tTraining Loss: 0.000215\n",
      "Epoch: 16 \tTraining Loss: 0.000215\n",
      "Epoch: 17 \tTraining Loss: 0.000215\n",
      "Epoch: 18 \tTraining Loss: 0.000215\n",
      "Epoch: 1 \tTraining Loss: 0.000176\n",
      "Epoch: 2 \tTraining Loss: 0.000176\n",
      "Epoch: 3 \tTraining Loss: 0.000176\n",
      "Epoch: 4 \tTraining Loss: 0.000176\n",
      "Epoch: 5 \tTraining Loss: 0.000176\n",
      "Epoch: 6 \tTraining Loss: 0.000176\n",
      "Epoch: 7 \tTraining Loss: 0.000176\n",
      "Epoch: 8 \tTraining Loss: 0.000176\n",
      "Epoch: 9 \tTraining Loss: 0.000176\n",
      "Epoch: 10 \tTraining Loss: 0.000176\n",
      "Epoch: 11 \tTraining Loss: 0.000176\n",
      "Epoch: 12 \tTraining Loss: 0.000176\n",
      "Epoch: 13 \tTraining Loss: 0.000176\n",
      "Epoch: 14 \tTraining Loss: 0.000176\n",
      "Epoch: 15 \tTraining Loss: 0.000176\n",
      "Epoch: 16 \tTraining Loss: 0.000176\n",
      "Epoch: 17 \tTraining Loss: 0.000176\n",
      "Epoch: 18 \tTraining Loss: 0.000176\n",
      "Epoch: 1 \tTraining Loss: 0.000234\n",
      "Epoch: 2 \tTraining Loss: 0.000234\n",
      "Epoch: 3 \tTraining Loss: 0.000234\n",
      "Epoch: 4 \tTraining Loss: 0.000234\n",
      "Epoch: 5 \tTraining Loss: 0.000234\n",
      "Epoch: 6 \tTraining Loss: 0.000234\n",
      "Epoch: 7 \tTraining Loss: 0.000234\n",
      "Epoch: 8 \tTraining Loss: 0.000234\n",
      "Epoch: 9 \tTraining Loss: 0.000234\n",
      "Epoch: 10 \tTraining Loss: 0.000234\n",
      "Epoch: 11 \tTraining Loss: 0.000234\n",
      "Epoch: 12 \tTraining Loss: 0.000234\n",
      "Epoch: 13 \tTraining Loss: 0.000234\n",
      "Epoch: 14 \tTraining Loss: 0.000234\n",
      "Epoch: 15 \tTraining Loss: 0.000234\n",
      "Epoch: 16 \tTraining Loss: 0.000234\n",
      "Epoch: 17 \tTraining Loss: 0.000234\n",
      "Epoch: 18 \tTraining Loss: 0.000234\n",
      "Epoch: 1 \tTraining Loss: 0.000339\n",
      "Epoch: 2 \tTraining Loss: 0.000339\n",
      "Epoch: 3 \tTraining Loss: 0.000339\n",
      "Epoch: 4 \tTraining Loss: 0.000339\n",
      "Epoch: 5 \tTraining Loss: 0.000339\n",
      "Epoch: 6 \tTraining Loss: 0.000339\n",
      "Epoch: 7 \tTraining Loss: 0.000339\n",
      "Epoch: 8 \tTraining Loss: 0.000339\n",
      "Epoch: 9 \tTraining Loss: 0.000339\n",
      "Epoch: 10 \tTraining Loss: 0.000339\n",
      "Epoch: 11 \tTraining Loss: 0.000339\n",
      "Epoch: 12 \tTraining Loss: 0.000339\n",
      "Epoch: 13 \tTraining Loss: 0.000339\n",
      "Epoch: 14 \tTraining Loss: 0.000339\n",
      "Epoch: 15 \tTraining Loss: 0.000339\n",
      "Epoch: 16 \tTraining Loss: 0.000339\n",
      "Epoch: 17 \tTraining Loss: 0.000339\n",
      "Epoch: 18 \tTraining Loss: 0.000339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000236\n",
      "Epoch: 2 \tTraining Loss: 0.000236\n",
      "Epoch: 3 \tTraining Loss: 0.000236\n",
      "Epoch: 4 \tTraining Loss: 0.000236\n",
      "Epoch: 5 \tTraining Loss: 0.000236\n",
      "Epoch: 6 \tTraining Loss: 0.000236\n",
      "Epoch: 7 \tTraining Loss: 0.000236\n",
      "Epoch: 8 \tTraining Loss: 0.000236\n",
      "Epoch: 9 \tTraining Loss: 0.000236\n",
      "Epoch: 10 \tTraining Loss: 0.000236\n",
      "Epoch: 11 \tTraining Loss: 0.000236\n",
      "Epoch: 12 \tTraining Loss: 0.000236\n",
      "Epoch: 13 \tTraining Loss: 0.000236\n",
      "Epoch: 14 \tTraining Loss: 0.000236\n",
      "Epoch: 15 \tTraining Loss: 0.000236\n",
      "Epoch: 16 \tTraining Loss: 0.000236\n",
      "Epoch: 17 \tTraining Loss: 0.000236\n",
      "Epoch: 18 \tTraining Loss: 0.000236\n",
      "Epoch: 1 \tTraining Loss: 0.000493\n",
      "Epoch: 2 \tTraining Loss: 0.000493\n",
      "Epoch: 3 \tTraining Loss: 0.000493\n",
      "Epoch: 4 \tTraining Loss: 0.000493\n",
      "Epoch: 5 \tTraining Loss: 0.000493\n",
      "Epoch: 6 \tTraining Loss: 0.000493\n",
      "Epoch: 7 \tTraining Loss: 0.000493\n",
      "Epoch: 8 \tTraining Loss: 0.000493\n",
      "Epoch: 9 \tTraining Loss: 0.000493\n",
      "Epoch: 10 \tTraining Loss: 0.000493\n",
      "Epoch: 11 \tTraining Loss: 0.000493\n",
      "Epoch: 12 \tTraining Loss: 0.000493\n",
      "Epoch: 13 \tTraining Loss: 0.000493\n",
      "Epoch: 14 \tTraining Loss: 0.000493\n",
      "Epoch: 15 \tTraining Loss: 0.000493\n",
      "Epoch: 16 \tTraining Loss: 0.000493\n",
      "Epoch: 17 \tTraining Loss: 0.000493\n",
      "Epoch: 18 \tTraining Loss: 0.000493\n",
      "Epoch: 1 \tTraining Loss: 0.000328\n",
      "Epoch: 2 \tTraining Loss: 0.000328\n",
      "Epoch: 3 \tTraining Loss: 0.000328\n",
      "Epoch: 4 \tTraining Loss: 0.000328\n",
      "Epoch: 5 \tTraining Loss: 0.000328\n",
      "Epoch: 6 \tTraining Loss: 0.000328\n",
      "Epoch: 7 \tTraining Loss: 0.000328\n",
      "Epoch: 8 \tTraining Loss: 0.000328\n",
      "Epoch: 9 \tTraining Loss: 0.000328\n",
      "Epoch: 10 \tTraining Loss: 0.000328\n",
      "Epoch: 11 \tTraining Loss: 0.000328\n",
      "Epoch: 12 \tTraining Loss: 0.000328\n",
      "Epoch: 13 \tTraining Loss: 0.000328\n",
      "Epoch: 14 \tTraining Loss: 0.000328\n",
      "Epoch: 15 \tTraining Loss: 0.000328\n",
      "Epoch: 16 \tTraining Loss: 0.000328\n",
      "Epoch: 17 \tTraining Loss: 0.000328\n",
      "Epoch: 18 \tTraining Loss: 0.000328\n",
      "Epoch: 1 \tTraining Loss: 0.000368\n",
      "Epoch: 2 \tTraining Loss: 0.000368\n",
      "Epoch: 3 \tTraining Loss: 0.000368\n",
      "Epoch: 4 \tTraining Loss: 0.000368\n",
      "Epoch: 5 \tTraining Loss: 0.000368\n",
      "Epoch: 6 \tTraining Loss: 0.000368\n",
      "Epoch: 7 \tTraining Loss: 0.000368\n",
      "Epoch: 8 \tTraining Loss: 0.000368\n",
      "Epoch: 9 \tTraining Loss: 0.000368\n",
      "Epoch: 10 \tTraining Loss: 0.000368\n",
      "Epoch: 11 \tTraining Loss: 0.000368\n",
      "Epoch: 12 \tTraining Loss: 0.000368\n",
      "Epoch: 13 \tTraining Loss: 0.000368\n",
      "Epoch: 14 \tTraining Loss: 0.000368\n",
      "Epoch: 15 \tTraining Loss: 0.000368\n",
      "Epoch: 16 \tTraining Loss: 0.000368\n",
      "Epoch: 17 \tTraining Loss: 0.000368\n",
      "Epoch: 18 \tTraining Loss: 0.000368\n",
      "Epoch: 1 \tTraining Loss: 0.000182\n",
      "Epoch: 2 \tTraining Loss: 0.000182\n",
      "Epoch: 3 \tTraining Loss: 0.000182\n",
      "Epoch: 4 \tTraining Loss: 0.000182\n",
      "Epoch: 5 \tTraining Loss: 0.000182\n",
      "Epoch: 6 \tTraining Loss: 0.000182\n",
      "Epoch: 7 \tTraining Loss: 0.000182\n",
      "Epoch: 8 \tTraining Loss: 0.000182\n",
      "Epoch: 9 \tTraining Loss: 0.000182\n",
      "Epoch: 10 \tTraining Loss: 0.000182\n",
      "Epoch: 11 \tTraining Loss: 0.000182\n",
      "Epoch: 12 \tTraining Loss: 0.000182\n",
      "Epoch: 13 \tTraining Loss: 0.000182\n",
      "Epoch: 14 \tTraining Loss: 0.000182\n",
      "Epoch: 15 \tTraining Loss: 0.000182\n",
      "Epoch: 16 \tTraining Loss: 0.000182\n",
      "Epoch: 17 \tTraining Loss: 0.000182\n",
      "Epoch: 18 \tTraining Loss: 0.000182\n",
      "Epoch: 1 \tTraining Loss: 0.000293\n",
      "Epoch: 2 \tTraining Loss: 0.000293\n",
      "Epoch: 3 \tTraining Loss: 0.000293\n",
      "Epoch: 4 \tTraining Loss: 0.000293\n",
      "Epoch: 5 \tTraining Loss: 0.000293\n",
      "Epoch: 6 \tTraining Loss: 0.000293\n",
      "Epoch: 7 \tTraining Loss: 0.000293\n",
      "Epoch: 8 \tTraining Loss: 0.000293\n",
      "Epoch: 9 \tTraining Loss: 0.000293\n",
      "Epoch: 10 \tTraining Loss: 0.000293\n",
      "Epoch: 11 \tTraining Loss: 0.000293\n",
      "Epoch: 12 \tTraining Loss: 0.000293\n",
      "Epoch: 13 \tTraining Loss: 0.000293\n",
      "Epoch: 14 \tTraining Loss: 0.000293\n",
      "Epoch: 15 \tTraining Loss: 0.000293\n",
      "Epoch: 16 \tTraining Loss: 0.000293\n",
      "Epoch: 17 \tTraining Loss: 0.000293\n",
      "Epoch: 18 \tTraining Loss: 0.000293\n",
      "Epoch: 1 \tTraining Loss: 0.000208\n",
      "Epoch: 2 \tTraining Loss: 0.000208\n",
      "Epoch: 3 \tTraining Loss: 0.000208\n",
      "Epoch: 4 \tTraining Loss: 0.000208\n",
      "Epoch: 5 \tTraining Loss: 0.000208\n",
      "Epoch: 6 \tTraining Loss: 0.000208\n",
      "Epoch: 7 \tTraining Loss: 0.000208\n",
      "Epoch: 8 \tTraining Loss: 0.000208\n",
      "Epoch: 9 \tTraining Loss: 0.000208\n",
      "Epoch: 10 \tTraining Loss: 0.000208\n",
      "Epoch: 11 \tTraining Loss: 0.000208\n",
      "Epoch: 12 \tTraining Loss: 0.000208\n",
      "Epoch: 13 \tTraining Loss: 0.000208\n",
      "Epoch: 14 \tTraining Loss: 0.000208\n",
      "Epoch: 15 \tTraining Loss: 0.000208\n",
      "Epoch: 16 \tTraining Loss: 0.000208\n",
      "Epoch: 17 \tTraining Loss: 0.000208\n",
      "Epoch: 18 \tTraining Loss: 0.000208\n",
      "Epoch: 1 \tTraining Loss: 0.000339\n",
      "Epoch: 2 \tTraining Loss: 0.000339\n",
      "Epoch: 3 \tTraining Loss: 0.000339\n",
      "Epoch: 4 \tTraining Loss: 0.000339\n",
      "Epoch: 5 \tTraining Loss: 0.000339\n",
      "Epoch: 6 \tTraining Loss: 0.000339\n",
      "Epoch: 7 \tTraining Loss: 0.000339\n",
      "Epoch: 8 \tTraining Loss: 0.000339\n",
      "Epoch: 9 \tTraining Loss: 0.000339\n",
      "Epoch: 10 \tTraining Loss: 0.000339\n",
      "Epoch: 11 \tTraining Loss: 0.000339\n",
      "Epoch: 12 \tTraining Loss: 0.000339\n",
      "Epoch: 13 \tTraining Loss: 0.000339\n",
      "Epoch: 14 \tTraining Loss: 0.000339\n",
      "Epoch: 15 \tTraining Loss: 0.000339\n",
      "Epoch: 16 \tTraining Loss: 0.000339\n",
      "Epoch: 17 \tTraining Loss: 0.000339\n",
      "Epoch: 18 \tTraining Loss: 0.000339\n",
      "Epoch: 1 \tTraining Loss: 0.000180\n",
      "Epoch: 2 \tTraining Loss: 0.000180\n",
      "Epoch: 3 \tTraining Loss: 0.000180\n",
      "Epoch: 4 \tTraining Loss: 0.000180\n",
      "Epoch: 5 \tTraining Loss: 0.000180\n",
      "Epoch: 6 \tTraining Loss: 0.000180\n",
      "Epoch: 7 \tTraining Loss: 0.000180\n",
      "Epoch: 8 \tTraining Loss: 0.000180\n",
      "Epoch: 9 \tTraining Loss: 0.000180\n",
      "Epoch: 10 \tTraining Loss: 0.000180\n",
      "Epoch: 11 \tTraining Loss: 0.000180\n",
      "Epoch: 12 \tTraining Loss: 0.000180\n",
      "Epoch: 13 \tTraining Loss: 0.000180\n",
      "Epoch: 14 \tTraining Loss: 0.000180\n",
      "Epoch: 15 \tTraining Loss: 0.000180\n",
      "Epoch: 16 \tTraining Loss: 0.000180\n",
      "Epoch: 17 \tTraining Loss: 0.000180\n",
      "Epoch: 18 \tTraining Loss: 0.000180\n",
      "Epoch: 1 \tTraining Loss: 0.000159\n",
      "Epoch: 2 \tTraining Loss: 0.000159\n",
      "Epoch: 3 \tTraining Loss: 0.000159\n",
      "Epoch: 4 \tTraining Loss: 0.000159\n",
      "Epoch: 5 \tTraining Loss: 0.000159\n",
      "Epoch: 6 \tTraining Loss: 0.000159\n",
      "Epoch: 7 \tTraining Loss: 0.000159\n",
      "Epoch: 8 \tTraining Loss: 0.000159\n",
      "Epoch: 9 \tTraining Loss: 0.000159\n",
      "Epoch: 10 \tTraining Loss: 0.000159\n",
      "Epoch: 11 \tTraining Loss: 0.000159\n",
      "Epoch: 12 \tTraining Loss: 0.000159\n",
      "Epoch: 13 \tTraining Loss: 0.000159\n",
      "Epoch: 14 \tTraining Loss: 0.000159\n",
      "Epoch: 15 \tTraining Loss: 0.000159\n",
      "Epoch: 16 \tTraining Loss: 0.000159\n",
      "Epoch: 17 \tTraining Loss: 0.000159\n",
      "Epoch: 18 \tTraining Loss: 0.000159\n",
      "Epoch: 1 \tTraining Loss: 0.000201\n",
      "Epoch: 2 \tTraining Loss: 0.000201\n",
      "Epoch: 3 \tTraining Loss: 0.000201\n",
      "Epoch: 4 \tTraining Loss: 0.000201\n",
      "Epoch: 5 \tTraining Loss: 0.000201\n",
      "Epoch: 6 \tTraining Loss: 0.000201\n",
      "Epoch: 7 \tTraining Loss: 0.000201\n",
      "Epoch: 8 \tTraining Loss: 0.000201\n",
      "Epoch: 9 \tTraining Loss: 0.000201\n",
      "Epoch: 10 \tTraining Loss: 0.000201\n",
      "Epoch: 11 \tTraining Loss: 0.000201\n",
      "Epoch: 12 \tTraining Loss: 0.000201\n",
      "Epoch: 13 \tTraining Loss: 0.000201\n",
      "Epoch: 14 \tTraining Loss: 0.000201\n",
      "Epoch: 15 \tTraining Loss: 0.000201\n",
      "Epoch: 16 \tTraining Loss: 0.000201\n",
      "Epoch: 17 \tTraining Loss: 0.000201\n",
      "Epoch: 18 \tTraining Loss: 0.000201\n",
      "Epoch: 1 \tTraining Loss: 0.000202\n",
      "Epoch: 2 \tTraining Loss: 0.000202\n",
      "Epoch: 3 \tTraining Loss: 0.000202\n",
      "Epoch: 4 \tTraining Loss: 0.000202\n",
      "Epoch: 5 \tTraining Loss: 0.000202\n",
      "Epoch: 6 \tTraining Loss: 0.000202\n",
      "Epoch: 7 \tTraining Loss: 0.000202\n",
      "Epoch: 8 \tTraining Loss: 0.000202\n",
      "Epoch: 9 \tTraining Loss: 0.000202\n",
      "Epoch: 10 \tTraining Loss: 0.000202\n",
      "Epoch: 11 \tTraining Loss: 0.000202\n",
      "Epoch: 12 \tTraining Loss: 0.000202\n",
      "Epoch: 13 \tTraining Loss: 0.000202\n",
      "Epoch: 14 \tTraining Loss: 0.000202\n",
      "Epoch: 15 \tTraining Loss: 0.000202\n",
      "Epoch: 16 \tTraining Loss: 0.000202\n",
      "Epoch: 17 \tTraining Loss: 0.000202\n",
      "Epoch: 18 \tTraining Loss: 0.000202\n",
      "Epoch: 1 \tTraining Loss: 0.000120\n",
      "Epoch: 2 \tTraining Loss: 0.000120\n",
      "Epoch: 3 \tTraining Loss: 0.000120\n",
      "Epoch: 4 \tTraining Loss: 0.000120\n",
      "Epoch: 5 \tTraining Loss: 0.000120\n",
      "Epoch: 6 \tTraining Loss: 0.000120\n",
      "Epoch: 7 \tTraining Loss: 0.000120\n",
      "Epoch: 8 \tTraining Loss: 0.000120\n",
      "Epoch: 9 \tTraining Loss: 0.000120\n",
      "Epoch: 10 \tTraining Loss: 0.000120\n",
      "Epoch: 11 \tTraining Loss: 0.000120\n",
      "Epoch: 12 \tTraining Loss: 0.000120\n",
      "Epoch: 13 \tTraining Loss: 0.000120\n",
      "Epoch: 14 \tTraining Loss: 0.000120\n",
      "Epoch: 15 \tTraining Loss: 0.000120\n",
      "Epoch: 16 \tTraining Loss: 0.000120\n",
      "Epoch: 17 \tTraining Loss: 0.000120\n",
      "Epoch: 18 \tTraining Loss: 0.000120\n",
      "Epoch: 1 \tTraining Loss: 0.000329\n",
      "Epoch: 2 \tTraining Loss: 0.000329\n",
      "Epoch: 3 \tTraining Loss: 0.000329\n",
      "Epoch: 4 \tTraining Loss: 0.000329\n",
      "Epoch: 5 \tTraining Loss: 0.000329\n",
      "Epoch: 6 \tTraining Loss: 0.000329\n",
      "Epoch: 7 \tTraining Loss: 0.000329\n",
      "Epoch: 8 \tTraining Loss: 0.000329\n",
      "Epoch: 9 \tTraining Loss: 0.000329\n",
      "Epoch: 10 \tTraining Loss: 0.000329\n",
      "Epoch: 11 \tTraining Loss: 0.000329\n",
      "Epoch: 12 \tTraining Loss: 0.000329\n",
      "Epoch: 13 \tTraining Loss: 0.000329\n",
      "Epoch: 14 \tTraining Loss: 0.000329\n",
      "Epoch: 15 \tTraining Loss: 0.000329\n",
      "Epoch: 16 \tTraining Loss: 0.000329\n",
      "Epoch: 17 \tTraining Loss: 0.000329\n",
      "Epoch: 18 \tTraining Loss: 0.000329\n",
      "Epoch: 1 \tTraining Loss: 0.000337\n",
      "Epoch: 2 \tTraining Loss: 0.000337\n",
      "Epoch: 3 \tTraining Loss: 0.000337\n",
      "Epoch: 4 \tTraining Loss: 0.000337\n",
      "Epoch: 5 \tTraining Loss: 0.000337\n",
      "Epoch: 6 \tTraining Loss: 0.000337\n",
      "Epoch: 7 \tTraining Loss: 0.000337\n",
      "Epoch: 8 \tTraining Loss: 0.000337\n",
      "Epoch: 9 \tTraining Loss: 0.000337\n",
      "Epoch: 10 \tTraining Loss: 0.000337\n",
      "Epoch: 11 \tTraining Loss: 0.000337\n",
      "Epoch: 12 \tTraining Loss: 0.000337\n",
      "Epoch: 13 \tTraining Loss: 0.000337\n",
      "Epoch: 14 \tTraining Loss: 0.000337\n",
      "Epoch: 15 \tTraining Loss: 0.000337\n",
      "Epoch: 16 \tTraining Loss: 0.000337\n",
      "Epoch: 17 \tTraining Loss: 0.000337\n",
      "Epoch: 18 \tTraining Loss: 0.000337\n",
      "Epoch: 1 \tTraining Loss: 0.000111\n",
      "Epoch: 2 \tTraining Loss: 0.000111\n",
      "Epoch: 3 \tTraining Loss: 0.000111\n",
      "Epoch: 4 \tTraining Loss: 0.000111\n",
      "Epoch: 5 \tTraining Loss: 0.000111\n",
      "Epoch: 6 \tTraining Loss: 0.000111\n",
      "Epoch: 7 \tTraining Loss: 0.000111\n",
      "Epoch: 8 \tTraining Loss: 0.000111\n",
      "Epoch: 9 \tTraining Loss: 0.000111\n",
      "Epoch: 10 \tTraining Loss: 0.000111\n",
      "Epoch: 11 \tTraining Loss: 0.000111\n",
      "Epoch: 12 \tTraining Loss: 0.000111\n",
      "Epoch: 13 \tTraining Loss: 0.000111\n",
      "Epoch: 14 \tTraining Loss: 0.000111\n",
      "Epoch: 15 \tTraining Loss: 0.000111\n",
      "Epoch: 16 \tTraining Loss: 0.000111\n",
      "Epoch: 17 \tTraining Loss: 0.000111\n",
      "Epoch: 18 \tTraining Loss: 0.000111\n",
      "Epoch: 1 \tTraining Loss: 0.000335\n",
      "Epoch: 2 \tTraining Loss: 0.000335\n",
      "Epoch: 3 \tTraining Loss: 0.000335\n",
      "Epoch: 4 \tTraining Loss: 0.000335\n",
      "Epoch: 5 \tTraining Loss: 0.000335\n",
      "Epoch: 6 \tTraining Loss: 0.000335\n",
      "Epoch: 7 \tTraining Loss: 0.000335\n",
      "Epoch: 8 \tTraining Loss: 0.000335\n",
      "Epoch: 9 \tTraining Loss: 0.000335\n",
      "Epoch: 10 \tTraining Loss: 0.000335\n",
      "Epoch: 11 \tTraining Loss: 0.000335\n",
      "Epoch: 12 \tTraining Loss: 0.000335\n",
      "Epoch: 13 \tTraining Loss: 0.000335\n",
      "Epoch: 14 \tTraining Loss: 0.000335\n",
      "Epoch: 15 \tTraining Loss: 0.000335\n",
      "Epoch: 16 \tTraining Loss: 0.000335\n",
      "Epoch: 17 \tTraining Loss: 0.000335\n",
      "Epoch: 18 \tTraining Loss: 0.000335\n",
      "Epoch: 1 \tTraining Loss: 0.000102\n",
      "Epoch: 2 \tTraining Loss: 0.000102\n",
      "Epoch: 3 \tTraining Loss: 0.000102\n",
      "Epoch: 4 \tTraining Loss: 0.000102\n",
      "Epoch: 5 \tTraining Loss: 0.000102\n",
      "Epoch: 6 \tTraining Loss: 0.000102\n",
      "Epoch: 7 \tTraining Loss: 0.000102\n",
      "Epoch: 8 \tTraining Loss: 0.000102\n",
      "Epoch: 9 \tTraining Loss: 0.000102\n",
      "Epoch: 10 \tTraining Loss: 0.000102\n",
      "Epoch: 11 \tTraining Loss: 0.000102\n",
      "Epoch: 12 \tTraining Loss: 0.000102\n",
      "Epoch: 13 \tTraining Loss: 0.000102\n",
      "Epoch: 14 \tTraining Loss: 0.000102\n",
      "Epoch: 15 \tTraining Loss: 0.000102\n",
      "Epoch: 16 \tTraining Loss: 0.000102\n",
      "Epoch: 17 \tTraining Loss: 0.000102\n",
      "Epoch: 18 \tTraining Loss: 0.000102\n",
      "Epoch: 1 \tTraining Loss: 0.000227\n",
      "Epoch: 2 \tTraining Loss: 0.000227\n",
      "Epoch: 3 \tTraining Loss: 0.000227\n",
      "Epoch: 4 \tTraining Loss: 0.000227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 \tTraining Loss: 0.000227\n",
      "Epoch: 6 \tTraining Loss: 0.000227\n",
      "Epoch: 7 \tTraining Loss: 0.000227\n",
      "Epoch: 8 \tTraining Loss: 0.000227\n",
      "Epoch: 9 \tTraining Loss: 0.000227\n",
      "Epoch: 10 \tTraining Loss: 0.000227\n",
      "Epoch: 11 \tTraining Loss: 0.000227\n",
      "Epoch: 12 \tTraining Loss: 0.000227\n",
      "Epoch: 13 \tTraining Loss: 0.000227\n",
      "Epoch: 14 \tTraining Loss: 0.000227\n",
      "Epoch: 15 \tTraining Loss: 0.000227\n",
      "Epoch: 16 \tTraining Loss: 0.000227\n",
      "Epoch: 17 \tTraining Loss: 0.000227\n",
      "Epoch: 18 \tTraining Loss: 0.000227\n",
      "Epoch: 1 \tTraining Loss: 0.000375\n",
      "Epoch: 2 \tTraining Loss: 0.000375\n",
      "Epoch: 3 \tTraining Loss: 0.000375\n",
      "Epoch: 4 \tTraining Loss: 0.000375\n",
      "Epoch: 5 \tTraining Loss: 0.000375\n",
      "Epoch: 6 \tTraining Loss: 0.000375\n",
      "Epoch: 7 \tTraining Loss: 0.000375\n",
      "Epoch: 8 \tTraining Loss: 0.000375\n",
      "Epoch: 9 \tTraining Loss: 0.000375\n",
      "Epoch: 10 \tTraining Loss: 0.000375\n",
      "Epoch: 11 \tTraining Loss: 0.000375\n",
      "Epoch: 12 \tTraining Loss: 0.000375\n",
      "Epoch: 13 \tTraining Loss: 0.000375\n",
      "Epoch: 14 \tTraining Loss: 0.000375\n",
      "Epoch: 15 \tTraining Loss: 0.000375\n",
      "Epoch: 16 \tTraining Loss: 0.000375\n",
      "Epoch: 17 \tTraining Loss: 0.000375\n",
      "Epoch: 18 \tTraining Loss: 0.000375\n",
      "Epoch: 1 \tTraining Loss: 0.000353\n",
      "Epoch: 2 \tTraining Loss: 0.000353\n",
      "Epoch: 3 \tTraining Loss: 0.000353\n",
      "Epoch: 4 \tTraining Loss: 0.000353\n",
      "Epoch: 5 \tTraining Loss: 0.000353\n",
      "Epoch: 6 \tTraining Loss: 0.000353\n",
      "Epoch: 7 \tTraining Loss: 0.000353\n",
      "Epoch: 8 \tTraining Loss: 0.000353\n",
      "Epoch: 9 \tTraining Loss: 0.000353\n",
      "Epoch: 10 \tTraining Loss: 0.000353\n",
      "Epoch: 11 \tTraining Loss: 0.000353\n",
      "Epoch: 12 \tTraining Loss: 0.000353\n",
      "Epoch: 13 \tTraining Loss: 0.000353\n",
      "Epoch: 14 \tTraining Loss: 0.000353\n",
      "Epoch: 15 \tTraining Loss: 0.000353\n",
      "Epoch: 16 \tTraining Loss: 0.000353\n",
      "Epoch: 17 \tTraining Loss: 0.000353\n",
      "Epoch: 18 \tTraining Loss: 0.000353\n",
      "Epoch: 1 \tTraining Loss: 0.000242\n",
      "Epoch: 2 \tTraining Loss: 0.000242\n",
      "Epoch: 3 \tTraining Loss: 0.000242\n",
      "Epoch: 4 \tTraining Loss: 0.000242\n",
      "Epoch: 5 \tTraining Loss: 0.000242\n",
      "Epoch: 6 \tTraining Loss: 0.000242\n",
      "Epoch: 7 \tTraining Loss: 0.000242\n",
      "Epoch: 8 \tTraining Loss: 0.000242\n",
      "Epoch: 9 \tTraining Loss: 0.000242\n",
      "Epoch: 10 \tTraining Loss: 0.000242\n",
      "Epoch: 11 \tTraining Loss: 0.000242\n",
      "Epoch: 12 \tTraining Loss: 0.000242\n",
      "Epoch: 13 \tTraining Loss: 0.000242\n",
      "Epoch: 14 \tTraining Loss: 0.000242\n",
      "Epoch: 15 \tTraining Loss: 0.000242\n",
      "Epoch: 16 \tTraining Loss: 0.000242\n",
      "Epoch: 17 \tTraining Loss: 0.000242\n",
      "Epoch: 18 \tTraining Loss: 0.000242\n",
      "Epoch: 1 \tTraining Loss: 0.000260\n",
      "Epoch: 2 \tTraining Loss: 0.000260\n",
      "Epoch: 3 \tTraining Loss: 0.000260\n",
      "Epoch: 4 \tTraining Loss: 0.000260\n",
      "Epoch: 5 \tTraining Loss: 0.000260\n",
      "Epoch: 6 \tTraining Loss: 0.000260\n",
      "Epoch: 7 \tTraining Loss: 0.000260\n",
      "Epoch: 8 \tTraining Loss: 0.000260\n",
      "Epoch: 9 \tTraining Loss: 0.000260\n",
      "Epoch: 10 \tTraining Loss: 0.000260\n",
      "Epoch: 11 \tTraining Loss: 0.000260\n",
      "Epoch: 12 \tTraining Loss: 0.000260\n",
      "Epoch: 13 \tTraining Loss: 0.000260\n",
      "Epoch: 14 \tTraining Loss: 0.000260\n",
      "Epoch: 15 \tTraining Loss: 0.000260\n",
      "Epoch: 16 \tTraining Loss: 0.000260\n",
      "Epoch: 17 \tTraining Loss: 0.000260\n",
      "Epoch: 18 \tTraining Loss: 0.000260\n",
      "Epoch: 1 \tTraining Loss: 0.000157\n",
      "Epoch: 2 \tTraining Loss: 0.000157\n",
      "Epoch: 3 \tTraining Loss: 0.000157\n",
      "Epoch: 4 \tTraining Loss: 0.000157\n",
      "Epoch: 5 \tTraining Loss: 0.000157\n",
      "Epoch: 6 \tTraining Loss: 0.000157\n",
      "Epoch: 7 \tTraining Loss: 0.000157\n",
      "Epoch: 8 \tTraining Loss: 0.000157\n",
      "Epoch: 9 \tTraining Loss: 0.000157\n",
      "Epoch: 10 \tTraining Loss: 0.000157\n",
      "Epoch: 11 \tTraining Loss: 0.000157\n",
      "Epoch: 12 \tTraining Loss: 0.000157\n",
      "Epoch: 13 \tTraining Loss: 0.000157\n",
      "Epoch: 14 \tTraining Loss: 0.000157\n",
      "Epoch: 15 \tTraining Loss: 0.000157\n",
      "Epoch: 16 \tTraining Loss: 0.000157\n",
      "Epoch: 17 \tTraining Loss: 0.000157\n",
      "Epoch: 18 \tTraining Loss: 0.000157\n",
      "Epoch: 1 \tTraining Loss: 0.000128\n",
      "Epoch: 2 \tTraining Loss: 0.000128\n",
      "Epoch: 3 \tTraining Loss: 0.000128\n",
      "Epoch: 4 \tTraining Loss: 0.000128\n",
      "Epoch: 5 \tTraining Loss: 0.000128\n",
      "Epoch: 6 \tTraining Loss: 0.000128\n",
      "Epoch: 7 \tTraining Loss: 0.000128\n",
      "Epoch: 8 \tTraining Loss: 0.000128\n",
      "Epoch: 9 \tTraining Loss: 0.000128\n",
      "Epoch: 10 \tTraining Loss: 0.000128\n",
      "Epoch: 11 \tTraining Loss: 0.000128\n",
      "Epoch: 12 \tTraining Loss: 0.000128\n",
      "Epoch: 13 \tTraining Loss: 0.000128\n",
      "Epoch: 14 \tTraining Loss: 0.000128\n",
      "Epoch: 15 \tTraining Loss: 0.000128\n",
      "Epoch: 16 \tTraining Loss: 0.000128\n",
      "Epoch: 17 \tTraining Loss: 0.000128\n",
      "Epoch: 18 \tTraining Loss: 0.000128\n",
      "Epoch: 1 \tTraining Loss: 0.000472\n",
      "Epoch: 2 \tTraining Loss: 0.000472\n",
      "Epoch: 3 \tTraining Loss: 0.000472\n",
      "Epoch: 4 \tTraining Loss: 0.000472\n",
      "Epoch: 5 \tTraining Loss: 0.000472\n",
      "Epoch: 6 \tTraining Loss: 0.000472\n",
      "Epoch: 7 \tTraining Loss: 0.000472\n",
      "Epoch: 8 \tTraining Loss: 0.000472\n",
      "Epoch: 9 \tTraining Loss: 0.000472\n",
      "Epoch: 10 \tTraining Loss: 0.000472\n",
      "Epoch: 11 \tTraining Loss: 0.000472\n",
      "Epoch: 12 \tTraining Loss: 0.000472\n",
      "Epoch: 13 \tTraining Loss: 0.000472\n",
      "Epoch: 14 \tTraining Loss: 0.000472\n",
      "Epoch: 15 \tTraining Loss: 0.000472\n",
      "Epoch: 16 \tTraining Loss: 0.000472\n",
      "Epoch: 17 \tTraining Loss: 0.000472\n",
      "Epoch: 18 \tTraining Loss: 0.000472\n",
      "Epoch: 1 \tTraining Loss: 0.000129\n",
      "Epoch: 2 \tTraining Loss: 0.000129\n",
      "Epoch: 3 \tTraining Loss: 0.000129\n",
      "Epoch: 4 \tTraining Loss: 0.000129\n",
      "Epoch: 5 \tTraining Loss: 0.000129\n",
      "Epoch: 6 \tTraining Loss: 0.000129\n",
      "Epoch: 7 \tTraining Loss: 0.000129\n",
      "Epoch: 8 \tTraining Loss: 0.000129\n",
      "Epoch: 9 \tTraining Loss: 0.000129\n",
      "Epoch: 10 \tTraining Loss: 0.000129\n",
      "Epoch: 11 \tTraining Loss: 0.000129\n",
      "Epoch: 12 \tTraining Loss: 0.000129\n",
      "Epoch: 13 \tTraining Loss: 0.000129\n",
      "Epoch: 14 \tTraining Loss: 0.000129\n",
      "Epoch: 15 \tTraining Loss: 0.000129\n",
      "Epoch: 16 \tTraining Loss: 0.000129\n",
      "Epoch: 17 \tTraining Loss: 0.000129\n",
      "Epoch: 18 \tTraining Loss: 0.000129\n",
      "Epoch: 1 \tTraining Loss: 0.000286\n",
      "Epoch: 2 \tTraining Loss: 0.000286\n",
      "Epoch: 3 \tTraining Loss: 0.000286\n",
      "Epoch: 4 \tTraining Loss: 0.000286\n",
      "Epoch: 5 \tTraining Loss: 0.000286\n",
      "Epoch: 6 \tTraining Loss: 0.000286\n",
      "Epoch: 7 \tTraining Loss: 0.000286\n",
      "Epoch: 8 \tTraining Loss: 0.000286\n",
      "Epoch: 9 \tTraining Loss: 0.000286\n",
      "Epoch: 10 \tTraining Loss: 0.000286\n",
      "Epoch: 11 \tTraining Loss: 0.000286\n",
      "Epoch: 12 \tTraining Loss: 0.000286\n",
      "Epoch: 13 \tTraining Loss: 0.000286\n",
      "Epoch: 14 \tTraining Loss: 0.000286\n",
      "Epoch: 15 \tTraining Loss: 0.000286\n",
      "Epoch: 16 \tTraining Loss: 0.000286\n",
      "Epoch: 17 \tTraining Loss: 0.000286\n",
      "Epoch: 18 \tTraining Loss: 0.000286\n",
      "Epoch: 1 \tTraining Loss: 0.000350\n",
      "Epoch: 2 \tTraining Loss: 0.000350\n",
      "Epoch: 3 \tTraining Loss: 0.000350\n",
      "Epoch: 4 \tTraining Loss: 0.000350\n",
      "Epoch: 5 \tTraining Loss: 0.000350\n",
      "Epoch: 6 \tTraining Loss: 0.000350\n",
      "Epoch: 7 \tTraining Loss: 0.000350\n",
      "Epoch: 8 \tTraining Loss: 0.000350\n",
      "Epoch: 9 \tTraining Loss: 0.000350\n",
      "Epoch: 10 \tTraining Loss: 0.000350\n",
      "Epoch: 11 \tTraining Loss: 0.000350\n",
      "Epoch: 12 \tTraining Loss: 0.000350\n",
      "Epoch: 13 \tTraining Loss: 0.000350\n",
      "Epoch: 14 \tTraining Loss: 0.000350\n",
      "Epoch: 15 \tTraining Loss: 0.000350\n",
      "Epoch: 16 \tTraining Loss: 0.000350\n",
      "Epoch: 17 \tTraining Loss: 0.000350\n",
      "Epoch: 18 \tTraining Loss: 0.000350\n",
      "Epoch: 1 \tTraining Loss: 0.000264\n",
      "Epoch: 2 \tTraining Loss: 0.000264\n",
      "Epoch: 3 \tTraining Loss: 0.000264\n",
      "Epoch: 4 \tTraining Loss: 0.000264\n",
      "Epoch: 5 \tTraining Loss: 0.000264\n",
      "Epoch: 6 \tTraining Loss: 0.000264\n",
      "Epoch: 7 \tTraining Loss: 0.000264\n",
      "Epoch: 8 \tTraining Loss: 0.000264\n",
      "Epoch: 9 \tTraining Loss: 0.000264\n",
      "Epoch: 10 \tTraining Loss: 0.000264\n",
      "Epoch: 11 \tTraining Loss: 0.000264\n",
      "Epoch: 12 \tTraining Loss: 0.000264\n",
      "Epoch: 13 \tTraining Loss: 0.000264\n",
      "Epoch: 14 \tTraining Loss: 0.000264\n",
      "Epoch: 15 \tTraining Loss: 0.000264\n",
      "Epoch: 16 \tTraining Loss: 0.000264\n",
      "Epoch: 17 \tTraining Loss: 0.000264\n",
      "Epoch: 18 \tTraining Loss: 0.000264\n",
      "Epoch: 1 \tTraining Loss: 0.000150\n",
      "Epoch: 2 \tTraining Loss: 0.000150\n",
      "Epoch: 3 \tTraining Loss: 0.000150\n",
      "Epoch: 4 \tTraining Loss: 0.000150\n",
      "Epoch: 5 \tTraining Loss: 0.000150\n",
      "Epoch: 6 \tTraining Loss: 0.000150\n",
      "Epoch: 7 \tTraining Loss: 0.000150\n",
      "Epoch: 8 \tTraining Loss: 0.000150\n",
      "Epoch: 9 \tTraining Loss: 0.000150\n",
      "Epoch: 10 \tTraining Loss: 0.000150\n",
      "Epoch: 11 \tTraining Loss: 0.000150\n",
      "Epoch: 12 \tTraining Loss: 0.000150\n",
      "Epoch: 13 \tTraining Loss: 0.000150\n",
      "Epoch: 14 \tTraining Loss: 0.000150\n",
      "Epoch: 15 \tTraining Loss: 0.000150\n",
      "Epoch: 16 \tTraining Loss: 0.000150\n",
      "Epoch: 17 \tTraining Loss: 0.000150\n",
      "Epoch: 18 \tTraining Loss: 0.000150\n",
      "Epoch: 1 \tTraining Loss: 0.000230\n",
      "Epoch: 2 \tTraining Loss: 0.000230\n",
      "Epoch: 3 \tTraining Loss: 0.000230\n",
      "Epoch: 4 \tTraining Loss: 0.000230\n",
      "Epoch: 5 \tTraining Loss: 0.000230\n",
      "Epoch: 6 \tTraining Loss: 0.000230\n",
      "Epoch: 7 \tTraining Loss: 0.000230\n",
      "Epoch: 8 \tTraining Loss: 0.000230\n",
      "Epoch: 9 \tTraining Loss: 0.000230\n",
      "Epoch: 10 \tTraining Loss: 0.000230\n",
      "Epoch: 11 \tTraining Loss: 0.000230\n",
      "Epoch: 12 \tTraining Loss: 0.000230\n",
      "Epoch: 13 \tTraining Loss: 0.000230\n",
      "Epoch: 14 \tTraining Loss: 0.000230\n",
      "Epoch: 15 \tTraining Loss: 0.000230\n",
      "Epoch: 16 \tTraining Loss: 0.000230\n",
      "Epoch: 17 \tTraining Loss: 0.000230\n",
      "Epoch: 18 \tTraining Loss: 0.000230\n",
      "Epoch: 1 \tTraining Loss: 0.000230\n",
      "Epoch: 2 \tTraining Loss: 0.000230\n",
      "Epoch: 3 \tTraining Loss: 0.000230\n",
      "Epoch: 4 \tTraining Loss: 0.000230\n",
      "Epoch: 5 \tTraining Loss: 0.000230\n",
      "Epoch: 6 \tTraining Loss: 0.000230\n",
      "Epoch: 7 \tTraining Loss: 0.000230\n",
      "Epoch: 8 \tTraining Loss: 0.000230\n",
      "Epoch: 9 \tTraining Loss: 0.000230\n",
      "Epoch: 10 \tTraining Loss: 0.000230\n",
      "Epoch: 11 \tTraining Loss: 0.000230\n",
      "Epoch: 12 \tTraining Loss: 0.000230\n",
      "Epoch: 13 \tTraining Loss: 0.000230\n",
      "Epoch: 14 \tTraining Loss: 0.000230\n",
      "Epoch: 15 \tTraining Loss: 0.000230\n",
      "Epoch: 16 \tTraining Loss: 0.000230\n",
      "Epoch: 17 \tTraining Loss: 0.000230\n",
      "Epoch: 18 \tTraining Loss: 0.000230\n",
      "Epoch: 1 \tTraining Loss: 0.000121\n",
      "Epoch: 2 \tTraining Loss: 0.000121\n",
      "Epoch: 3 \tTraining Loss: 0.000121\n",
      "Epoch: 4 \tTraining Loss: 0.000121\n",
      "Epoch: 5 \tTraining Loss: 0.000121\n",
      "Epoch: 6 \tTraining Loss: 0.000121\n",
      "Epoch: 7 \tTraining Loss: 0.000121\n",
      "Epoch: 8 \tTraining Loss: 0.000121\n",
      "Epoch: 9 \tTraining Loss: 0.000121\n",
      "Epoch: 10 \tTraining Loss: 0.000121\n",
      "Epoch: 11 \tTraining Loss: 0.000121\n",
      "Epoch: 12 \tTraining Loss: 0.000121\n",
      "Epoch: 13 \tTraining Loss: 0.000121\n",
      "Epoch: 14 \tTraining Loss: 0.000121\n",
      "Epoch: 15 \tTraining Loss: 0.000121\n",
      "Epoch: 16 \tTraining Loss: 0.000121\n",
      "Epoch: 17 \tTraining Loss: 0.000121\n",
      "Epoch: 18 \tTraining Loss: 0.000121\n",
      "Epoch: 1 \tTraining Loss: 0.000179\n",
      "Epoch: 2 \tTraining Loss: 0.000179\n",
      "Epoch: 3 \tTraining Loss: 0.000179\n",
      "Epoch: 4 \tTraining Loss: 0.000179\n",
      "Epoch: 5 \tTraining Loss: 0.000179\n",
      "Epoch: 6 \tTraining Loss: 0.000179\n",
      "Epoch: 7 \tTraining Loss: 0.000179\n",
      "Epoch: 8 \tTraining Loss: 0.000179\n",
      "Epoch: 9 \tTraining Loss: 0.000179\n",
      "Epoch: 10 \tTraining Loss: 0.000179\n",
      "Epoch: 11 \tTraining Loss: 0.000179\n",
      "Epoch: 12 \tTraining Loss: 0.000179\n",
      "Epoch: 13 \tTraining Loss: 0.000179\n",
      "Epoch: 14 \tTraining Loss: 0.000179\n",
      "Epoch: 15 \tTraining Loss: 0.000179\n",
      "Epoch: 16 \tTraining Loss: 0.000179\n",
      "Epoch: 17 \tTraining Loss: 0.000179\n",
      "Epoch: 18 \tTraining Loss: 0.000179\n",
      "Epoch: 1 \tTraining Loss: 0.000215\n",
      "Epoch: 2 \tTraining Loss: 0.000215\n",
      "Epoch: 3 \tTraining Loss: 0.000215\n",
      "Epoch: 4 \tTraining Loss: 0.000215\n",
      "Epoch: 5 \tTraining Loss: 0.000215\n",
      "Epoch: 6 \tTraining Loss: 0.000215\n",
      "Epoch: 7 \tTraining Loss: 0.000215\n",
      "Epoch: 8 \tTraining Loss: 0.000215\n",
      "Epoch: 9 \tTraining Loss: 0.000215\n",
      "Epoch: 10 \tTraining Loss: 0.000215\n",
      "Epoch: 11 \tTraining Loss: 0.000215\n",
      "Epoch: 12 \tTraining Loss: 0.000215\n",
      "Epoch: 13 \tTraining Loss: 0.000215\n",
      "Epoch: 14 \tTraining Loss: 0.000215\n",
      "Epoch: 15 \tTraining Loss: 0.000215\n",
      "Epoch: 16 \tTraining Loss: 0.000215\n",
      "Epoch: 17 \tTraining Loss: 0.000215\n",
      "Epoch: 18 \tTraining Loss: 0.000215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000153\n",
      "Epoch: 2 \tTraining Loss: 0.000153\n",
      "Epoch: 3 \tTraining Loss: 0.000153\n",
      "Epoch: 4 \tTraining Loss: 0.000153\n",
      "Epoch: 5 \tTraining Loss: 0.000153\n",
      "Epoch: 6 \tTraining Loss: 0.000153\n",
      "Epoch: 7 \tTraining Loss: 0.000153\n",
      "Epoch: 8 \tTraining Loss: 0.000153\n",
      "Epoch: 9 \tTraining Loss: 0.000153\n",
      "Epoch: 10 \tTraining Loss: 0.000153\n",
      "Epoch: 11 \tTraining Loss: 0.000153\n",
      "Epoch: 12 \tTraining Loss: 0.000153\n",
      "Epoch: 13 \tTraining Loss: 0.000153\n",
      "Epoch: 14 \tTraining Loss: 0.000153\n",
      "Epoch: 15 \tTraining Loss: 0.000153\n",
      "Epoch: 16 \tTraining Loss: 0.000153\n",
      "Epoch: 17 \tTraining Loss: 0.000153\n",
      "Epoch: 18 \tTraining Loss: 0.000153\n",
      "Epoch: 1 \tTraining Loss: 0.000177\n",
      "Epoch: 2 \tTraining Loss: 0.000177\n",
      "Epoch: 3 \tTraining Loss: 0.000177\n",
      "Epoch: 4 \tTraining Loss: 0.000177\n",
      "Epoch: 5 \tTraining Loss: 0.000177\n",
      "Epoch: 6 \tTraining Loss: 0.000177\n",
      "Epoch: 7 \tTraining Loss: 0.000177\n",
      "Epoch: 8 \tTraining Loss: 0.000177\n",
      "Epoch: 9 \tTraining Loss: 0.000177\n",
      "Epoch: 10 \tTraining Loss: 0.000177\n",
      "Epoch: 11 \tTraining Loss: 0.000177\n",
      "Epoch: 12 \tTraining Loss: 0.000177\n",
      "Epoch: 13 \tTraining Loss: 0.000177\n",
      "Epoch: 14 \tTraining Loss: 0.000177\n",
      "Epoch: 15 \tTraining Loss: 0.000177\n",
      "Epoch: 16 \tTraining Loss: 0.000177\n",
      "Epoch: 17 \tTraining Loss: 0.000177\n",
      "Epoch: 18 \tTraining Loss: 0.000177\n",
      "Epoch: 1 \tTraining Loss: 0.000104\n",
      "Epoch: 2 \tTraining Loss: 0.000104\n",
      "Epoch: 3 \tTraining Loss: 0.000104\n",
      "Epoch: 4 \tTraining Loss: 0.000104\n",
      "Epoch: 5 \tTraining Loss: 0.000104\n",
      "Epoch: 6 \tTraining Loss: 0.000104\n",
      "Epoch: 7 \tTraining Loss: 0.000104\n",
      "Epoch: 8 \tTraining Loss: 0.000104\n",
      "Epoch: 9 \tTraining Loss: 0.000104\n",
      "Epoch: 10 \tTraining Loss: 0.000104\n",
      "Epoch: 11 \tTraining Loss: 0.000104\n",
      "Epoch: 12 \tTraining Loss: 0.000104\n",
      "Epoch: 13 \tTraining Loss: 0.000104\n",
      "Epoch: 14 \tTraining Loss: 0.000104\n",
      "Epoch: 15 \tTraining Loss: 0.000104\n",
      "Epoch: 16 \tTraining Loss: 0.000104\n",
      "Epoch: 17 \tTraining Loss: 0.000104\n",
      "Epoch: 18 \tTraining Loss: 0.000104\n",
      "Epoch: 1 \tTraining Loss: 0.000132\n",
      "Epoch: 2 \tTraining Loss: 0.000132\n",
      "Epoch: 3 \tTraining Loss: 0.000132\n",
      "Epoch: 4 \tTraining Loss: 0.000132\n",
      "Epoch: 5 \tTraining Loss: 0.000132\n",
      "Epoch: 6 \tTraining Loss: 0.000132\n",
      "Epoch: 7 \tTraining Loss: 0.000132\n",
      "Epoch: 8 \tTraining Loss: 0.000132\n",
      "Epoch: 9 \tTraining Loss: 0.000132\n",
      "Epoch: 10 \tTraining Loss: 0.000132\n",
      "Epoch: 11 \tTraining Loss: 0.000132\n",
      "Epoch: 12 \tTraining Loss: 0.000132\n",
      "Epoch: 13 \tTraining Loss: 0.000132\n",
      "Epoch: 14 \tTraining Loss: 0.000132\n",
      "Epoch: 15 \tTraining Loss: 0.000132\n",
      "Epoch: 16 \tTraining Loss: 0.000132\n",
      "Epoch: 17 \tTraining Loss: 0.000132\n",
      "Epoch: 18 \tTraining Loss: 0.000132\n",
      "Epoch: 1 \tTraining Loss: 0.000105\n",
      "Epoch: 2 \tTraining Loss: 0.000105\n",
      "Epoch: 3 \tTraining Loss: 0.000105\n",
      "Epoch: 4 \tTraining Loss: 0.000105\n",
      "Epoch: 5 \tTraining Loss: 0.000105\n",
      "Epoch: 6 \tTraining Loss: 0.000105\n",
      "Epoch: 7 \tTraining Loss: 0.000105\n",
      "Epoch: 8 \tTraining Loss: 0.000105\n",
      "Epoch: 9 \tTraining Loss: 0.000105\n",
      "Epoch: 10 \tTraining Loss: 0.000105\n",
      "Epoch: 11 \tTraining Loss: 0.000105\n",
      "Epoch: 12 \tTraining Loss: 0.000105\n",
      "Epoch: 13 \tTraining Loss: 0.000105\n",
      "Epoch: 14 \tTraining Loss: 0.000105\n",
      "Epoch: 15 \tTraining Loss: 0.000105\n",
      "Epoch: 16 \tTraining Loss: 0.000105\n",
      "Epoch: 17 \tTraining Loss: 0.000105\n",
      "Epoch: 18 \tTraining Loss: 0.000105\n",
      "Epoch: 1 \tTraining Loss: 0.000350\n",
      "Epoch: 2 \tTraining Loss: 0.000350\n",
      "Epoch: 3 \tTraining Loss: 0.000350\n",
      "Epoch: 4 \tTraining Loss: 0.000350\n",
      "Epoch: 5 \tTraining Loss: 0.000350\n",
      "Epoch: 6 \tTraining Loss: 0.000350\n",
      "Epoch: 7 \tTraining Loss: 0.000350\n",
      "Epoch: 8 \tTraining Loss: 0.000350\n",
      "Epoch: 9 \tTraining Loss: 0.000350\n",
      "Epoch: 10 \tTraining Loss: 0.000350\n",
      "Epoch: 11 \tTraining Loss: 0.000350\n",
      "Epoch: 12 \tTraining Loss: 0.000350\n",
      "Epoch: 13 \tTraining Loss: 0.000350\n",
      "Epoch: 14 \tTraining Loss: 0.000350\n",
      "Epoch: 15 \tTraining Loss: 0.000350\n",
      "Epoch: 16 \tTraining Loss: 0.000350\n",
      "Epoch: 17 \tTraining Loss: 0.000350\n",
      "Epoch: 18 \tTraining Loss: 0.000350\n",
      "Epoch: 1 \tTraining Loss: 0.000178\n",
      "Epoch: 2 \tTraining Loss: 0.000178\n",
      "Epoch: 3 \tTraining Loss: 0.000178\n",
      "Epoch: 4 \tTraining Loss: 0.000178\n",
      "Epoch: 5 \tTraining Loss: 0.000178\n",
      "Epoch: 6 \tTraining Loss: 0.000178\n",
      "Epoch: 7 \tTraining Loss: 0.000178\n",
      "Epoch: 8 \tTraining Loss: 0.000178\n",
      "Epoch: 9 \tTraining Loss: 0.000178\n",
      "Epoch: 10 \tTraining Loss: 0.000178\n",
      "Epoch: 11 \tTraining Loss: 0.000178\n",
      "Epoch: 12 \tTraining Loss: 0.000178\n",
      "Epoch: 13 \tTraining Loss: 0.000178\n",
      "Epoch: 14 \tTraining Loss: 0.000178\n",
      "Epoch: 15 \tTraining Loss: 0.000178\n",
      "Epoch: 16 \tTraining Loss: 0.000178\n",
      "Epoch: 17 \tTraining Loss: 0.000178\n",
      "Epoch: 18 \tTraining Loss: 0.000178\n",
      "Epoch: 1 \tTraining Loss: 0.000171\n",
      "Epoch: 2 \tTraining Loss: 0.000171\n",
      "Epoch: 3 \tTraining Loss: 0.000171\n",
      "Epoch: 4 \tTraining Loss: 0.000171\n",
      "Epoch: 5 \tTraining Loss: 0.000171\n",
      "Epoch: 6 \tTraining Loss: 0.000171\n",
      "Epoch: 7 \tTraining Loss: 0.000171\n",
      "Epoch: 8 \tTraining Loss: 0.000171\n",
      "Epoch: 9 \tTraining Loss: 0.000171\n",
      "Epoch: 10 \tTraining Loss: 0.000171\n",
      "Epoch: 11 \tTraining Loss: 0.000171\n",
      "Epoch: 12 \tTraining Loss: 0.000171\n",
      "Epoch: 13 \tTraining Loss: 0.000171\n",
      "Epoch: 14 \tTraining Loss: 0.000171\n",
      "Epoch: 15 \tTraining Loss: 0.000171\n",
      "Epoch: 16 \tTraining Loss: 0.000171\n",
      "Epoch: 17 \tTraining Loss: 0.000171\n",
      "Epoch: 18 \tTraining Loss: 0.000171\n",
      "Epoch: 1 \tTraining Loss: 0.000212\n",
      "Epoch: 2 \tTraining Loss: 0.000212\n",
      "Epoch: 3 \tTraining Loss: 0.000212\n",
      "Epoch: 4 \tTraining Loss: 0.000212\n",
      "Epoch: 5 \tTraining Loss: 0.000212\n",
      "Epoch: 6 \tTraining Loss: 0.000212\n",
      "Epoch: 7 \tTraining Loss: 0.000212\n",
      "Epoch: 8 \tTraining Loss: 0.000212\n",
      "Epoch: 9 \tTraining Loss: 0.000212\n",
      "Epoch: 10 \tTraining Loss: 0.000212\n",
      "Epoch: 11 \tTraining Loss: 0.000212\n",
      "Epoch: 12 \tTraining Loss: 0.000212\n",
      "Epoch: 13 \tTraining Loss: 0.000212\n",
      "Epoch: 14 \tTraining Loss: 0.000212\n",
      "Epoch: 15 \tTraining Loss: 0.000212\n",
      "Epoch: 16 \tTraining Loss: 0.000212\n",
      "Epoch: 17 \tTraining Loss: 0.000212\n",
      "Epoch: 18 \tTraining Loss: 0.000212\n",
      "Epoch: 1 \tTraining Loss: 0.000155\n",
      "Epoch: 2 \tTraining Loss: 0.000155\n",
      "Epoch: 3 \tTraining Loss: 0.000155\n",
      "Epoch: 4 \tTraining Loss: 0.000155\n",
      "Epoch: 5 \tTraining Loss: 0.000155\n",
      "Epoch: 6 \tTraining Loss: 0.000155\n",
      "Epoch: 7 \tTraining Loss: 0.000155\n",
      "Epoch: 8 \tTraining Loss: 0.000155\n",
      "Epoch: 9 \tTraining Loss: 0.000155\n",
      "Epoch: 10 \tTraining Loss: 0.000155\n",
      "Epoch: 11 \tTraining Loss: 0.000155\n",
      "Epoch: 12 \tTraining Loss: 0.000155\n",
      "Epoch: 13 \tTraining Loss: 0.000155\n",
      "Epoch: 14 \tTraining Loss: 0.000155\n",
      "Epoch: 15 \tTraining Loss: 0.000155\n",
      "Epoch: 16 \tTraining Loss: 0.000155\n",
      "Epoch: 17 \tTraining Loss: 0.000155\n",
      "Epoch: 18 \tTraining Loss: 0.000155\n",
      "Epoch: 1 \tTraining Loss: 0.000109\n",
      "Epoch: 2 \tTraining Loss: 0.000109\n",
      "Epoch: 3 \tTraining Loss: 0.000109\n",
      "Epoch: 4 \tTraining Loss: 0.000109\n",
      "Epoch: 5 \tTraining Loss: 0.000109\n",
      "Epoch: 6 \tTraining Loss: 0.000109\n",
      "Epoch: 7 \tTraining Loss: 0.000109\n",
      "Epoch: 8 \tTraining Loss: 0.000109\n",
      "Epoch: 9 \tTraining Loss: 0.000109\n",
      "Epoch: 10 \tTraining Loss: 0.000109\n",
      "Epoch: 11 \tTraining Loss: 0.000109\n",
      "Epoch: 12 \tTraining Loss: 0.000109\n",
      "Epoch: 13 \tTraining Loss: 0.000109\n",
      "Epoch: 14 \tTraining Loss: 0.000109\n",
      "Epoch: 15 \tTraining Loss: 0.000109\n",
      "Epoch: 16 \tTraining Loss: 0.000109\n",
      "Epoch: 17 \tTraining Loss: 0.000109\n",
      "Epoch: 18 \tTraining Loss: 0.000109\n",
      "Epoch: 1 \tTraining Loss: 0.000148\n",
      "Epoch: 2 \tTraining Loss: 0.000148\n",
      "Epoch: 3 \tTraining Loss: 0.000148\n",
      "Epoch: 4 \tTraining Loss: 0.000148\n",
      "Epoch: 5 \tTraining Loss: 0.000148\n",
      "Epoch: 6 \tTraining Loss: 0.000148\n",
      "Epoch: 7 \tTraining Loss: 0.000148\n",
      "Epoch: 8 \tTraining Loss: 0.000148\n",
      "Epoch: 9 \tTraining Loss: 0.000148\n",
      "Epoch: 10 \tTraining Loss: 0.000148\n",
      "Epoch: 11 \tTraining Loss: 0.000148\n",
      "Epoch: 12 \tTraining Loss: 0.000148\n",
      "Epoch: 13 \tTraining Loss: 0.000148\n",
      "Epoch: 14 \tTraining Loss: 0.000148\n",
      "Epoch: 15 \tTraining Loss: 0.000148\n",
      "Epoch: 16 \tTraining Loss: 0.000148\n",
      "Epoch: 17 \tTraining Loss: 0.000148\n",
      "Epoch: 18 \tTraining Loss: 0.000148\n",
      "Epoch: 1 \tTraining Loss: 0.000159\n",
      "Epoch: 2 \tTraining Loss: 0.000159\n",
      "Epoch: 3 \tTraining Loss: 0.000159\n",
      "Epoch: 4 \tTraining Loss: 0.000159\n",
      "Epoch: 5 \tTraining Loss: 0.000159\n",
      "Epoch: 6 \tTraining Loss: 0.000159\n",
      "Epoch: 7 \tTraining Loss: 0.000159\n",
      "Epoch: 8 \tTraining Loss: 0.000159\n",
      "Epoch: 9 \tTraining Loss: 0.000159\n",
      "Epoch: 10 \tTraining Loss: 0.000159\n",
      "Epoch: 11 \tTraining Loss: 0.000159\n",
      "Epoch: 12 \tTraining Loss: 0.000159\n",
      "Epoch: 13 \tTraining Loss: 0.000159\n",
      "Epoch: 14 \tTraining Loss: 0.000159\n",
      "Epoch: 15 \tTraining Loss: 0.000159\n",
      "Epoch: 16 \tTraining Loss: 0.000159\n",
      "Epoch: 17 \tTraining Loss: 0.000159\n",
      "Epoch: 18 \tTraining Loss: 0.000159\n",
      "Epoch: 1 \tTraining Loss: 0.000165\n",
      "Epoch: 2 \tTraining Loss: 0.000165\n",
      "Epoch: 3 \tTraining Loss: 0.000165\n",
      "Epoch: 4 \tTraining Loss: 0.000165\n",
      "Epoch: 5 \tTraining Loss: 0.000165\n",
      "Epoch: 6 \tTraining Loss: 0.000165\n",
      "Epoch: 7 \tTraining Loss: 0.000165\n",
      "Epoch: 8 \tTraining Loss: 0.000165\n",
      "Epoch: 9 \tTraining Loss: 0.000165\n",
      "Epoch: 10 \tTraining Loss: 0.000165\n",
      "Epoch: 11 \tTraining Loss: 0.000165\n",
      "Epoch: 12 \tTraining Loss: 0.000165\n",
      "Epoch: 13 \tTraining Loss: 0.000165\n",
      "Epoch: 14 \tTraining Loss: 0.000165\n",
      "Epoch: 15 \tTraining Loss: 0.000165\n",
      "Epoch: 16 \tTraining Loss: 0.000165\n",
      "Epoch: 17 \tTraining Loss: 0.000165\n",
      "Epoch: 18 \tTraining Loss: 0.000165\n",
      "Epoch: 1 \tTraining Loss: 0.000120\n",
      "Epoch: 2 \tTraining Loss: 0.000120\n",
      "Epoch: 3 \tTraining Loss: 0.000120\n",
      "Epoch: 4 \tTraining Loss: 0.000120\n",
      "Epoch: 5 \tTraining Loss: 0.000120\n",
      "Epoch: 6 \tTraining Loss: 0.000120\n",
      "Epoch: 7 \tTraining Loss: 0.000120\n",
      "Epoch: 8 \tTraining Loss: 0.000120\n",
      "Epoch: 9 \tTraining Loss: 0.000120\n",
      "Epoch: 10 \tTraining Loss: 0.000120\n",
      "Epoch: 11 \tTraining Loss: 0.000120\n",
      "Epoch: 12 \tTraining Loss: 0.000120\n",
      "Epoch: 13 \tTraining Loss: 0.000120\n",
      "Epoch: 14 \tTraining Loss: 0.000120\n",
      "Epoch: 15 \tTraining Loss: 0.000120\n",
      "Epoch: 16 \tTraining Loss: 0.000120\n",
      "Epoch: 17 \tTraining Loss: 0.000120\n",
      "Epoch: 18 \tTraining Loss: 0.000120\n",
      "Epoch: 1 \tTraining Loss: 0.000195\n",
      "Epoch: 2 \tTraining Loss: 0.000195\n",
      "Epoch: 3 \tTraining Loss: 0.000195\n",
      "Epoch: 4 \tTraining Loss: 0.000195\n",
      "Epoch: 5 \tTraining Loss: 0.000195\n",
      "Epoch: 6 \tTraining Loss: 0.000195\n",
      "Epoch: 7 \tTraining Loss: 0.000195\n",
      "Epoch: 8 \tTraining Loss: 0.000195\n",
      "Epoch: 9 \tTraining Loss: 0.000195\n",
      "Epoch: 10 \tTraining Loss: 0.000195\n",
      "Epoch: 11 \tTraining Loss: 0.000195\n",
      "Epoch: 12 \tTraining Loss: 0.000195\n",
      "Epoch: 13 \tTraining Loss: 0.000195\n",
      "Epoch: 14 \tTraining Loss: 0.000195\n",
      "Epoch: 15 \tTraining Loss: 0.000195\n",
      "Epoch: 16 \tTraining Loss: 0.000195\n",
      "Epoch: 17 \tTraining Loss: 0.000195\n",
      "Epoch: 18 \tTraining Loss: 0.000195\n",
      "Epoch: 1 \tTraining Loss: 0.000167\n",
      "Epoch: 2 \tTraining Loss: 0.000167\n",
      "Epoch: 3 \tTraining Loss: 0.000167\n",
      "Epoch: 4 \tTraining Loss: 0.000167\n",
      "Epoch: 5 \tTraining Loss: 0.000167\n",
      "Epoch: 6 \tTraining Loss: 0.000167\n",
      "Epoch: 7 \tTraining Loss: 0.000167\n",
      "Epoch: 8 \tTraining Loss: 0.000167\n",
      "Epoch: 9 \tTraining Loss: 0.000167\n",
      "Epoch: 10 \tTraining Loss: 0.000167\n",
      "Epoch: 11 \tTraining Loss: 0.000167\n",
      "Epoch: 12 \tTraining Loss: 0.000167\n",
      "Epoch: 13 \tTraining Loss: 0.000167\n",
      "Epoch: 14 \tTraining Loss: 0.000167\n",
      "Epoch: 15 \tTraining Loss: 0.000167\n",
      "Epoch: 16 \tTraining Loss: 0.000167\n",
      "Epoch: 17 \tTraining Loss: 0.000167\n",
      "Epoch: 18 \tTraining Loss: 0.000167\n",
      "Epoch: 1 \tTraining Loss: 0.000138\n",
      "Epoch: 2 \tTraining Loss: 0.000138\n",
      "Epoch: 3 \tTraining Loss: 0.000138\n",
      "Epoch: 4 \tTraining Loss: 0.000138\n",
      "Epoch: 5 \tTraining Loss: 0.000138\n",
      "Epoch: 6 \tTraining Loss: 0.000138\n",
      "Epoch: 7 \tTraining Loss: 0.000138\n",
      "Epoch: 8 \tTraining Loss: 0.000138\n",
      "Epoch: 9 \tTraining Loss: 0.000138\n",
      "Epoch: 10 \tTraining Loss: 0.000138\n",
      "Epoch: 11 \tTraining Loss: 0.000138\n",
      "Epoch: 12 \tTraining Loss: 0.000138\n",
      "Epoch: 13 \tTraining Loss: 0.000138\n",
      "Epoch: 14 \tTraining Loss: 0.000138\n",
      "Epoch: 15 \tTraining Loss: 0.000138\n",
      "Epoch: 16 \tTraining Loss: 0.000138\n",
      "Epoch: 17 \tTraining Loss: 0.000138\n",
      "Epoch: 18 \tTraining Loss: 0.000138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000176\n",
      "Epoch: 2 \tTraining Loss: 0.000176\n",
      "Epoch: 3 \tTraining Loss: 0.000176\n",
      "Epoch: 4 \tTraining Loss: 0.000176\n",
      "Epoch: 5 \tTraining Loss: 0.000176\n",
      "Epoch: 6 \tTraining Loss: 0.000176\n",
      "Epoch: 7 \tTraining Loss: 0.000176\n",
      "Epoch: 8 \tTraining Loss: 0.000176\n",
      "Epoch: 9 \tTraining Loss: 0.000176\n",
      "Epoch: 10 \tTraining Loss: 0.000176\n",
      "Epoch: 11 \tTraining Loss: 0.000176\n",
      "Epoch: 12 \tTraining Loss: 0.000176\n",
      "Epoch: 13 \tTraining Loss: 0.000176\n",
      "Epoch: 14 \tTraining Loss: 0.000176\n",
      "Epoch: 15 \tTraining Loss: 0.000176\n",
      "Epoch: 16 \tTraining Loss: 0.000176\n",
      "Epoch: 17 \tTraining Loss: 0.000176\n",
      "Epoch: 18 \tTraining Loss: 0.000176\n",
      "Epoch: 1 \tTraining Loss: 0.000245\n",
      "Epoch: 2 \tTraining Loss: 0.000245\n",
      "Epoch: 3 \tTraining Loss: 0.000245\n",
      "Epoch: 4 \tTraining Loss: 0.000245\n",
      "Epoch: 5 \tTraining Loss: 0.000245\n",
      "Epoch: 6 \tTraining Loss: 0.000245\n",
      "Epoch: 7 \tTraining Loss: 0.000245\n",
      "Epoch: 8 \tTraining Loss: 0.000245\n",
      "Epoch: 9 \tTraining Loss: 0.000245\n",
      "Epoch: 10 \tTraining Loss: 0.000245\n",
      "Epoch: 11 \tTraining Loss: 0.000245\n",
      "Epoch: 12 \tTraining Loss: 0.000245\n",
      "Epoch: 13 \tTraining Loss: 0.000245\n",
      "Epoch: 14 \tTraining Loss: 0.000245\n",
      "Epoch: 15 \tTraining Loss: 0.000245\n",
      "Epoch: 16 \tTraining Loss: 0.000245\n",
      "Epoch: 17 \tTraining Loss: 0.000245\n",
      "Epoch: 18 \tTraining Loss: 0.000245\n",
      "Epoch: 1 \tTraining Loss: 0.000397\n",
      "Epoch: 2 \tTraining Loss: 0.000397\n",
      "Epoch: 3 \tTraining Loss: 0.000397\n",
      "Epoch: 4 \tTraining Loss: 0.000397\n",
      "Epoch: 5 \tTraining Loss: 0.000397\n",
      "Epoch: 6 \tTraining Loss: 0.000397\n",
      "Epoch: 7 \tTraining Loss: 0.000397\n",
      "Epoch: 8 \tTraining Loss: 0.000397\n",
      "Epoch: 9 \tTraining Loss: 0.000397\n",
      "Epoch: 10 \tTraining Loss: 0.000397\n",
      "Epoch: 11 \tTraining Loss: 0.000397\n",
      "Epoch: 12 \tTraining Loss: 0.000397\n",
      "Epoch: 13 \tTraining Loss: 0.000397\n",
      "Epoch: 14 \tTraining Loss: 0.000397\n",
      "Epoch: 15 \tTraining Loss: 0.000397\n",
      "Epoch: 16 \tTraining Loss: 0.000397\n",
      "Epoch: 17 \tTraining Loss: 0.000397\n",
      "Epoch: 18 \tTraining Loss: 0.000397\n",
      "Epoch: 1 \tTraining Loss: 0.000398\n",
      "Epoch: 2 \tTraining Loss: 0.000398\n",
      "Epoch: 3 \tTraining Loss: 0.000398\n",
      "Epoch: 4 \tTraining Loss: 0.000398\n",
      "Epoch: 5 \tTraining Loss: 0.000398\n",
      "Epoch: 6 \tTraining Loss: 0.000398\n",
      "Epoch: 7 \tTraining Loss: 0.000398\n",
      "Epoch: 8 \tTraining Loss: 0.000398\n",
      "Epoch: 9 \tTraining Loss: 0.000398\n",
      "Epoch: 10 \tTraining Loss: 0.000398\n",
      "Epoch: 11 \tTraining Loss: 0.000398\n",
      "Epoch: 12 \tTraining Loss: 0.000398\n",
      "Epoch: 13 \tTraining Loss: 0.000398\n",
      "Epoch: 14 \tTraining Loss: 0.000398\n",
      "Epoch: 15 \tTraining Loss: 0.000398\n",
      "Epoch: 16 \tTraining Loss: 0.000398\n",
      "Epoch: 17 \tTraining Loss: 0.000398\n",
      "Epoch: 18 \tTraining Loss: 0.000398\n",
      "Epoch: 1 \tTraining Loss: 0.000207\n",
      "Epoch: 2 \tTraining Loss: 0.000207\n",
      "Epoch: 3 \tTraining Loss: 0.000207\n",
      "Epoch: 4 \tTraining Loss: 0.000207\n",
      "Epoch: 5 \tTraining Loss: 0.000207\n",
      "Epoch: 6 \tTraining Loss: 0.000207\n",
      "Epoch: 7 \tTraining Loss: 0.000207\n",
      "Epoch: 8 \tTraining Loss: 0.000207\n",
      "Epoch: 9 \tTraining Loss: 0.000207\n",
      "Epoch: 10 \tTraining Loss: 0.000207\n",
      "Epoch: 11 \tTraining Loss: 0.000207\n",
      "Epoch: 12 \tTraining Loss: 0.000207\n",
      "Epoch: 13 \tTraining Loss: 0.000207\n",
      "Epoch: 14 \tTraining Loss: 0.000207\n",
      "Epoch: 15 \tTraining Loss: 0.000207\n",
      "Epoch: 16 \tTraining Loss: 0.000207\n",
      "Epoch: 17 \tTraining Loss: 0.000207\n",
      "Epoch: 18 \tTraining Loss: 0.000207\n",
      "Epoch: 1 \tTraining Loss: 0.000120\n",
      "Epoch: 2 \tTraining Loss: 0.000120\n",
      "Epoch: 3 \tTraining Loss: 0.000120\n",
      "Epoch: 4 \tTraining Loss: 0.000120\n",
      "Epoch: 5 \tTraining Loss: 0.000120\n",
      "Epoch: 6 \tTraining Loss: 0.000120\n",
      "Epoch: 7 \tTraining Loss: 0.000120\n",
      "Epoch: 8 \tTraining Loss: 0.000120\n",
      "Epoch: 9 \tTraining Loss: 0.000120\n",
      "Epoch: 10 \tTraining Loss: 0.000120\n",
      "Epoch: 11 \tTraining Loss: 0.000120\n",
      "Epoch: 12 \tTraining Loss: 0.000120\n",
      "Epoch: 13 \tTraining Loss: 0.000120\n",
      "Epoch: 14 \tTraining Loss: 0.000120\n",
      "Epoch: 15 \tTraining Loss: 0.000120\n",
      "Epoch: 16 \tTraining Loss: 0.000120\n",
      "Epoch: 17 \tTraining Loss: 0.000120\n",
      "Epoch: 18 \tTraining Loss: 0.000120\n",
      "Epoch: 1 \tTraining Loss: 0.000095\n",
      "Epoch: 2 \tTraining Loss: 0.000095\n",
      "Epoch: 3 \tTraining Loss: 0.000095\n",
      "Epoch: 4 \tTraining Loss: 0.000095\n",
      "Epoch: 5 \tTraining Loss: 0.000095\n",
      "Epoch: 6 \tTraining Loss: 0.000095\n",
      "Epoch: 7 \tTraining Loss: 0.000095\n",
      "Epoch: 8 \tTraining Loss: 0.000095\n",
      "Epoch: 9 \tTraining Loss: 0.000095\n",
      "Epoch: 10 \tTraining Loss: 0.000095\n",
      "Epoch: 11 \tTraining Loss: 0.000095\n",
      "Epoch: 12 \tTraining Loss: 0.000095\n",
      "Epoch: 13 \tTraining Loss: 0.000095\n",
      "Epoch: 14 \tTraining Loss: 0.000095\n",
      "Epoch: 15 \tTraining Loss: 0.000095\n",
      "Epoch: 16 \tTraining Loss: 0.000095\n",
      "Epoch: 17 \tTraining Loss: 0.000095\n",
      "Epoch: 18 \tTraining Loss: 0.000095\n",
      "Epoch: 1 \tTraining Loss: 0.000140\n",
      "Epoch: 2 \tTraining Loss: 0.000140\n",
      "Epoch: 3 \tTraining Loss: 0.000140\n",
      "Epoch: 4 \tTraining Loss: 0.000140\n",
      "Epoch: 5 \tTraining Loss: 0.000140\n",
      "Epoch: 6 \tTraining Loss: 0.000140\n",
      "Epoch: 7 \tTraining Loss: 0.000140\n",
      "Epoch: 8 \tTraining Loss: 0.000140\n",
      "Epoch: 9 \tTraining Loss: 0.000140\n",
      "Epoch: 10 \tTraining Loss: 0.000140\n",
      "Epoch: 11 \tTraining Loss: 0.000140\n",
      "Epoch: 12 \tTraining Loss: 0.000140\n",
      "Epoch: 13 \tTraining Loss: 0.000140\n",
      "Epoch: 14 \tTraining Loss: 0.000140\n",
      "Epoch: 15 \tTraining Loss: 0.000140\n",
      "Epoch: 16 \tTraining Loss: 0.000140\n",
      "Epoch: 17 \tTraining Loss: 0.000140\n",
      "Epoch: 18 \tTraining Loss: 0.000140\n",
      "Epoch: 1 \tTraining Loss: 0.000230\n",
      "Epoch: 2 \tTraining Loss: 0.000230\n",
      "Epoch: 3 \tTraining Loss: 0.000230\n",
      "Epoch: 4 \tTraining Loss: 0.000230\n",
      "Epoch: 5 \tTraining Loss: 0.000230\n",
      "Epoch: 6 \tTraining Loss: 0.000230\n",
      "Epoch: 7 \tTraining Loss: 0.000230\n",
      "Epoch: 8 \tTraining Loss: 0.000230\n",
      "Epoch: 9 \tTraining Loss: 0.000230\n",
      "Epoch: 10 \tTraining Loss: 0.000230\n",
      "Epoch: 11 \tTraining Loss: 0.000230\n",
      "Epoch: 12 \tTraining Loss: 0.000230\n",
      "Epoch: 13 \tTraining Loss: 0.000230\n",
      "Epoch: 14 \tTraining Loss: 0.000230\n",
      "Epoch: 15 \tTraining Loss: 0.000230\n",
      "Epoch: 16 \tTraining Loss: 0.000230\n",
      "Epoch: 17 \tTraining Loss: 0.000230\n",
      "Epoch: 18 \tTraining Loss: 0.000230\n",
      "Epoch: 1 \tTraining Loss: 0.000220\n",
      "Epoch: 2 \tTraining Loss: 0.000220\n",
      "Epoch: 3 \tTraining Loss: 0.000220\n",
      "Epoch: 4 \tTraining Loss: 0.000220\n",
      "Epoch: 5 \tTraining Loss: 0.000220\n",
      "Epoch: 6 \tTraining Loss: 0.000220\n",
      "Epoch: 7 \tTraining Loss: 0.000220\n",
      "Epoch: 8 \tTraining Loss: 0.000220\n",
      "Epoch: 9 \tTraining Loss: 0.000220\n",
      "Epoch: 10 \tTraining Loss: 0.000220\n",
      "Epoch: 11 \tTraining Loss: 0.000220\n",
      "Epoch: 12 \tTraining Loss: 0.000220\n",
      "Epoch: 13 \tTraining Loss: 0.000220\n",
      "Epoch: 14 \tTraining Loss: 0.000220\n",
      "Epoch: 15 \tTraining Loss: 0.000220\n",
      "Epoch: 16 \tTraining Loss: 0.000220\n",
      "Epoch: 17 \tTraining Loss: 0.000220\n",
      "Epoch: 18 \tTraining Loss: 0.000220\n",
      "Epoch: 1 \tTraining Loss: 0.000192\n",
      "Epoch: 2 \tTraining Loss: 0.000192\n",
      "Epoch: 3 \tTraining Loss: 0.000192\n",
      "Epoch: 4 \tTraining Loss: 0.000192\n",
      "Epoch: 5 \tTraining Loss: 0.000192\n",
      "Epoch: 6 \tTraining Loss: 0.000192\n",
      "Epoch: 7 \tTraining Loss: 0.000192\n",
      "Epoch: 8 \tTraining Loss: 0.000192\n",
      "Epoch: 9 \tTraining Loss: 0.000192\n",
      "Epoch: 10 \tTraining Loss: 0.000192\n",
      "Epoch: 11 \tTraining Loss: 0.000192\n",
      "Epoch: 12 \tTraining Loss: 0.000192\n",
      "Epoch: 13 \tTraining Loss: 0.000192\n",
      "Epoch: 14 \tTraining Loss: 0.000192\n",
      "Epoch: 15 \tTraining Loss: 0.000192\n",
      "Epoch: 16 \tTraining Loss: 0.000192\n",
      "Epoch: 17 \tTraining Loss: 0.000192\n",
      "Epoch: 18 \tTraining Loss: 0.000192\n",
      "Epoch: 1 \tTraining Loss: 0.000125\n",
      "Epoch: 2 \tTraining Loss: 0.000125\n",
      "Epoch: 3 \tTraining Loss: 0.000125\n",
      "Epoch: 4 \tTraining Loss: 0.000125\n",
      "Epoch: 5 \tTraining Loss: 0.000125\n",
      "Epoch: 6 \tTraining Loss: 0.000125\n",
      "Epoch: 7 \tTraining Loss: 0.000125\n",
      "Epoch: 8 \tTraining Loss: 0.000125\n",
      "Epoch: 9 \tTraining Loss: 0.000125\n",
      "Epoch: 10 \tTraining Loss: 0.000125\n",
      "Epoch: 11 \tTraining Loss: 0.000125\n",
      "Epoch: 12 \tTraining Loss: 0.000125\n",
      "Epoch: 13 \tTraining Loss: 0.000125\n",
      "Epoch: 14 \tTraining Loss: 0.000125\n",
      "Epoch: 15 \tTraining Loss: 0.000125\n",
      "Epoch: 16 \tTraining Loss: 0.000125\n",
      "Epoch: 17 \tTraining Loss: 0.000125\n",
      "Epoch: 18 \tTraining Loss: 0.000125\n",
      "Epoch: 1 \tTraining Loss: 0.000303\n",
      "Epoch: 2 \tTraining Loss: 0.000303\n",
      "Epoch: 3 \tTraining Loss: 0.000303\n",
      "Epoch: 4 \tTraining Loss: 0.000303\n",
      "Epoch: 5 \tTraining Loss: 0.000303\n",
      "Epoch: 6 \tTraining Loss: 0.000303\n",
      "Epoch: 7 \tTraining Loss: 0.000303\n",
      "Epoch: 8 \tTraining Loss: 0.000303\n",
      "Epoch: 9 \tTraining Loss: 0.000303\n",
      "Epoch: 10 \tTraining Loss: 0.000303\n",
      "Epoch: 11 \tTraining Loss: 0.000303\n",
      "Epoch: 12 \tTraining Loss: 0.000303\n",
      "Epoch: 13 \tTraining Loss: 0.000303\n",
      "Epoch: 14 \tTraining Loss: 0.000303\n",
      "Epoch: 15 \tTraining Loss: 0.000303\n",
      "Epoch: 16 \tTraining Loss: 0.000303\n",
      "Epoch: 17 \tTraining Loss: 0.000303\n",
      "Epoch: 18 \tTraining Loss: 0.000303\n",
      "Epoch: 1 \tTraining Loss: 0.000206\n",
      "Epoch: 2 \tTraining Loss: 0.000206\n",
      "Epoch: 3 \tTraining Loss: 0.000206\n",
      "Epoch: 4 \tTraining Loss: 0.000206\n",
      "Epoch: 5 \tTraining Loss: 0.000206\n",
      "Epoch: 6 \tTraining Loss: 0.000206\n",
      "Epoch: 7 \tTraining Loss: 0.000206\n",
      "Epoch: 8 \tTraining Loss: 0.000206\n",
      "Epoch: 9 \tTraining Loss: 0.000206\n",
      "Epoch: 10 \tTraining Loss: 0.000206\n",
      "Epoch: 11 \tTraining Loss: 0.000206\n",
      "Epoch: 12 \tTraining Loss: 0.000206\n",
      "Epoch: 13 \tTraining Loss: 0.000206\n",
      "Epoch: 14 \tTraining Loss: 0.000206\n",
      "Epoch: 15 \tTraining Loss: 0.000206\n",
      "Epoch: 16 \tTraining Loss: 0.000206\n",
      "Epoch: 17 \tTraining Loss: 0.000206\n",
      "Epoch: 18 \tTraining Loss: 0.000206\n",
      "Epoch: 1 \tTraining Loss: 0.000229\n",
      "Epoch: 2 \tTraining Loss: 0.000229\n",
      "Epoch: 3 \tTraining Loss: 0.000229\n",
      "Epoch: 4 \tTraining Loss: 0.000229\n",
      "Epoch: 5 \tTraining Loss: 0.000229\n",
      "Epoch: 6 \tTraining Loss: 0.000229\n",
      "Epoch: 7 \tTraining Loss: 0.000229\n",
      "Epoch: 8 \tTraining Loss: 0.000229\n",
      "Epoch: 9 \tTraining Loss: 0.000229\n",
      "Epoch: 10 \tTraining Loss: 0.000229\n",
      "Epoch: 11 \tTraining Loss: 0.000229\n",
      "Epoch: 12 \tTraining Loss: 0.000229\n",
      "Epoch: 13 \tTraining Loss: 0.000229\n",
      "Epoch: 14 \tTraining Loss: 0.000229\n",
      "Epoch: 15 \tTraining Loss: 0.000229\n",
      "Epoch: 16 \tTraining Loss: 0.000229\n",
      "Epoch: 17 \tTraining Loss: 0.000229\n",
      "Epoch: 18 \tTraining Loss: 0.000229\n",
      "Epoch: 1 \tTraining Loss: 0.000197\n",
      "Epoch: 2 \tTraining Loss: 0.000197\n",
      "Epoch: 3 \tTraining Loss: 0.000197\n",
      "Epoch: 4 \tTraining Loss: 0.000197\n",
      "Epoch: 5 \tTraining Loss: 0.000197\n",
      "Epoch: 6 \tTraining Loss: 0.000197\n",
      "Epoch: 7 \tTraining Loss: 0.000197\n",
      "Epoch: 8 \tTraining Loss: 0.000197\n",
      "Epoch: 9 \tTraining Loss: 0.000197\n",
      "Epoch: 10 \tTraining Loss: 0.000197\n",
      "Epoch: 11 \tTraining Loss: 0.000197\n",
      "Epoch: 12 \tTraining Loss: 0.000197\n",
      "Epoch: 13 \tTraining Loss: 0.000197\n",
      "Epoch: 14 \tTraining Loss: 0.000197\n",
      "Epoch: 15 \tTraining Loss: 0.000197\n",
      "Epoch: 16 \tTraining Loss: 0.000197\n",
      "Epoch: 17 \tTraining Loss: 0.000197\n",
      "Epoch: 18 \tTraining Loss: 0.000197\n",
      "Epoch: 1 \tTraining Loss: 0.000217\n",
      "Epoch: 2 \tTraining Loss: 0.000217\n",
      "Epoch: 3 \tTraining Loss: 0.000217\n",
      "Epoch: 4 \tTraining Loss: 0.000217\n",
      "Epoch: 5 \tTraining Loss: 0.000217\n",
      "Epoch: 6 \tTraining Loss: 0.000217\n",
      "Epoch: 7 \tTraining Loss: 0.000217\n",
      "Epoch: 8 \tTraining Loss: 0.000217\n",
      "Epoch: 9 \tTraining Loss: 0.000217\n",
      "Epoch: 10 \tTraining Loss: 0.000217\n",
      "Epoch: 11 \tTraining Loss: 0.000217\n",
      "Epoch: 12 \tTraining Loss: 0.000217\n",
      "Epoch: 13 \tTraining Loss: 0.000217\n",
      "Epoch: 14 \tTraining Loss: 0.000217\n",
      "Epoch: 15 \tTraining Loss: 0.000217\n",
      "Epoch: 16 \tTraining Loss: 0.000217\n",
      "Epoch: 17 \tTraining Loss: 0.000217\n",
      "Epoch: 18 \tTraining Loss: 0.000217\n",
      "Epoch: 1 \tTraining Loss: 0.000256\n",
      "Epoch: 2 \tTraining Loss: 0.000256\n",
      "Epoch: 3 \tTraining Loss: 0.000256\n",
      "Epoch: 4 \tTraining Loss: 0.000256\n",
      "Epoch: 5 \tTraining Loss: 0.000256\n",
      "Epoch: 6 \tTraining Loss: 0.000256\n",
      "Epoch: 7 \tTraining Loss: 0.000256\n",
      "Epoch: 8 \tTraining Loss: 0.000256\n",
      "Epoch: 9 \tTraining Loss: 0.000256\n",
      "Epoch: 10 \tTraining Loss: 0.000256\n",
      "Epoch: 11 \tTraining Loss: 0.000256\n",
      "Epoch: 12 \tTraining Loss: 0.000256\n",
      "Epoch: 13 \tTraining Loss: 0.000256\n",
      "Epoch: 14 \tTraining Loss: 0.000256\n",
      "Epoch: 15 \tTraining Loss: 0.000256\n",
      "Epoch: 16 \tTraining Loss: 0.000256\n",
      "Epoch: 17 \tTraining Loss: 0.000256\n",
      "Epoch: 18 \tTraining Loss: 0.000256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000192\n",
      "Epoch: 2 \tTraining Loss: 0.000192\n",
      "Epoch: 3 \tTraining Loss: 0.000192\n",
      "Epoch: 4 \tTraining Loss: 0.000192\n",
      "Epoch: 5 \tTraining Loss: 0.000192\n",
      "Epoch: 6 \tTraining Loss: 0.000192\n",
      "Epoch: 7 \tTraining Loss: 0.000192\n",
      "Epoch: 8 \tTraining Loss: 0.000192\n",
      "Epoch: 9 \tTraining Loss: 0.000192\n",
      "Epoch: 10 \tTraining Loss: 0.000192\n",
      "Epoch: 11 \tTraining Loss: 0.000192\n",
      "Epoch: 12 \tTraining Loss: 0.000192\n",
      "Epoch: 13 \tTraining Loss: 0.000192\n",
      "Epoch: 14 \tTraining Loss: 0.000192\n",
      "Epoch: 15 \tTraining Loss: 0.000192\n",
      "Epoch: 16 \tTraining Loss: 0.000192\n",
      "Epoch: 17 \tTraining Loss: 0.000192\n",
      "Epoch: 18 \tTraining Loss: 0.000192\n",
      "Epoch: 1 \tTraining Loss: 0.000222\n",
      "Epoch: 2 \tTraining Loss: 0.000222\n",
      "Epoch: 3 \tTraining Loss: 0.000222\n",
      "Epoch: 4 \tTraining Loss: 0.000222\n",
      "Epoch: 5 \tTraining Loss: 0.000222\n",
      "Epoch: 6 \tTraining Loss: 0.000222\n",
      "Epoch: 7 \tTraining Loss: 0.000222\n",
      "Epoch: 8 \tTraining Loss: 0.000222\n",
      "Epoch: 9 \tTraining Loss: 0.000222\n",
      "Epoch: 10 \tTraining Loss: 0.000222\n",
      "Epoch: 11 \tTraining Loss: 0.000222\n",
      "Epoch: 12 \tTraining Loss: 0.000222\n",
      "Epoch: 13 \tTraining Loss: 0.000222\n",
      "Epoch: 14 \tTraining Loss: 0.000222\n",
      "Epoch: 15 \tTraining Loss: 0.000222\n",
      "Epoch: 16 \tTraining Loss: 0.000222\n",
      "Epoch: 17 \tTraining Loss: 0.000222\n",
      "Epoch: 18 \tTraining Loss: 0.000222\n",
      "Epoch: 1 \tTraining Loss: 0.000157\n",
      "Epoch: 2 \tTraining Loss: 0.000157\n",
      "Epoch: 3 \tTraining Loss: 0.000157\n",
      "Epoch: 4 \tTraining Loss: 0.000157\n",
      "Epoch: 5 \tTraining Loss: 0.000157\n",
      "Epoch: 6 \tTraining Loss: 0.000157\n",
      "Epoch: 7 \tTraining Loss: 0.000157\n",
      "Epoch: 8 \tTraining Loss: 0.000157\n",
      "Epoch: 9 \tTraining Loss: 0.000157\n",
      "Epoch: 10 \tTraining Loss: 0.000157\n",
      "Epoch: 11 \tTraining Loss: 0.000157\n",
      "Epoch: 12 \tTraining Loss: 0.000157\n",
      "Epoch: 13 \tTraining Loss: 0.000157\n",
      "Epoch: 14 \tTraining Loss: 0.000157\n",
      "Epoch: 15 \tTraining Loss: 0.000157\n",
      "Epoch: 16 \tTraining Loss: 0.000157\n",
      "Epoch: 17 \tTraining Loss: 0.000157\n",
      "Epoch: 18 \tTraining Loss: 0.000157\n",
      "Epoch: 1 \tTraining Loss: 0.000211\n",
      "Epoch: 2 \tTraining Loss: 0.000211\n",
      "Epoch: 3 \tTraining Loss: 0.000211\n",
      "Epoch: 4 \tTraining Loss: 0.000211\n",
      "Epoch: 5 \tTraining Loss: 0.000211\n",
      "Epoch: 6 \tTraining Loss: 0.000211\n",
      "Epoch: 7 \tTraining Loss: 0.000211\n",
      "Epoch: 8 \tTraining Loss: 0.000211\n",
      "Epoch: 9 \tTraining Loss: 0.000211\n",
      "Epoch: 10 \tTraining Loss: 0.000211\n",
      "Epoch: 11 \tTraining Loss: 0.000211\n",
      "Epoch: 12 \tTraining Loss: 0.000211\n",
      "Epoch: 13 \tTraining Loss: 0.000211\n",
      "Epoch: 14 \tTraining Loss: 0.000211\n",
      "Epoch: 15 \tTraining Loss: 0.000211\n",
      "Epoch: 16 \tTraining Loss: 0.000211\n",
      "Epoch: 17 \tTraining Loss: 0.000211\n",
      "Epoch: 18 \tTraining Loss: 0.000211\n",
      "Epoch: 1 \tTraining Loss: 0.000256\n",
      "Epoch: 2 \tTraining Loss: 0.000256\n",
      "Epoch: 3 \tTraining Loss: 0.000256\n",
      "Epoch: 4 \tTraining Loss: 0.000256\n",
      "Epoch: 5 \tTraining Loss: 0.000256\n",
      "Epoch: 6 \tTraining Loss: 0.000256\n",
      "Epoch: 7 \tTraining Loss: 0.000256\n",
      "Epoch: 8 \tTraining Loss: 0.000256\n",
      "Epoch: 9 \tTraining Loss: 0.000256\n",
      "Epoch: 10 \tTraining Loss: 0.000256\n",
      "Epoch: 11 \tTraining Loss: 0.000256\n",
      "Epoch: 12 \tTraining Loss: 0.000256\n",
      "Epoch: 13 \tTraining Loss: 0.000256\n",
      "Epoch: 14 \tTraining Loss: 0.000256\n",
      "Epoch: 15 \tTraining Loss: 0.000256\n",
      "Epoch: 16 \tTraining Loss: 0.000256\n",
      "Epoch: 17 \tTraining Loss: 0.000256\n",
      "Epoch: 18 \tTraining Loss: 0.000256\n",
      "Epoch: 1 \tTraining Loss: 0.000268\n",
      "Epoch: 2 \tTraining Loss: 0.000268\n",
      "Epoch: 3 \tTraining Loss: 0.000268\n",
      "Epoch: 4 \tTraining Loss: 0.000268\n",
      "Epoch: 5 \tTraining Loss: 0.000268\n",
      "Epoch: 6 \tTraining Loss: 0.000268\n",
      "Epoch: 7 \tTraining Loss: 0.000268\n",
      "Epoch: 8 \tTraining Loss: 0.000268\n",
      "Epoch: 9 \tTraining Loss: 0.000268\n",
      "Epoch: 10 \tTraining Loss: 0.000268\n",
      "Epoch: 11 \tTraining Loss: 0.000268\n",
      "Epoch: 12 \tTraining Loss: 0.000268\n",
      "Epoch: 13 \tTraining Loss: 0.000268\n",
      "Epoch: 14 \tTraining Loss: 0.000268\n",
      "Epoch: 15 \tTraining Loss: 0.000268\n",
      "Epoch: 16 \tTraining Loss: 0.000268\n",
      "Epoch: 17 \tTraining Loss: 0.000268\n",
      "Epoch: 18 \tTraining Loss: 0.000268\n",
      "Epoch: 1 \tTraining Loss: 0.000174\n",
      "Epoch: 2 \tTraining Loss: 0.000174\n",
      "Epoch: 3 \tTraining Loss: 0.000174\n",
      "Epoch: 4 \tTraining Loss: 0.000174\n",
      "Epoch: 5 \tTraining Loss: 0.000174\n",
      "Epoch: 6 \tTraining Loss: 0.000174\n",
      "Epoch: 7 \tTraining Loss: 0.000174\n",
      "Epoch: 8 \tTraining Loss: 0.000174\n",
      "Epoch: 9 \tTraining Loss: 0.000174\n",
      "Epoch: 10 \tTraining Loss: 0.000174\n",
      "Epoch: 11 \tTraining Loss: 0.000174\n",
      "Epoch: 12 \tTraining Loss: 0.000174\n",
      "Epoch: 13 \tTraining Loss: 0.000174\n",
      "Epoch: 14 \tTraining Loss: 0.000174\n",
      "Epoch: 15 \tTraining Loss: 0.000174\n",
      "Epoch: 16 \tTraining Loss: 0.000174\n",
      "Epoch: 17 \tTraining Loss: 0.000174\n",
      "Epoch: 18 \tTraining Loss: 0.000174\n",
      "Epoch: 1 \tTraining Loss: 0.000201\n",
      "Epoch: 2 \tTraining Loss: 0.000201\n",
      "Epoch: 3 \tTraining Loss: 0.000201\n",
      "Epoch: 4 \tTraining Loss: 0.000201\n",
      "Epoch: 5 \tTraining Loss: 0.000201\n",
      "Epoch: 6 \tTraining Loss: 0.000201\n",
      "Epoch: 7 \tTraining Loss: 0.000201\n",
      "Epoch: 8 \tTraining Loss: 0.000201\n",
      "Epoch: 9 \tTraining Loss: 0.000201\n",
      "Epoch: 10 \tTraining Loss: 0.000201\n",
      "Epoch: 11 \tTraining Loss: 0.000201\n",
      "Epoch: 12 \tTraining Loss: 0.000201\n",
      "Epoch: 13 \tTraining Loss: 0.000201\n",
      "Epoch: 14 \tTraining Loss: 0.000201\n",
      "Epoch: 15 \tTraining Loss: 0.000201\n",
      "Epoch: 16 \tTraining Loss: 0.000201\n",
      "Epoch: 17 \tTraining Loss: 0.000201\n",
      "Epoch: 18 \tTraining Loss: 0.000201\n",
      "Epoch: 1 \tTraining Loss: 0.000195\n",
      "Epoch: 2 \tTraining Loss: 0.000195\n",
      "Epoch: 3 \tTraining Loss: 0.000195\n",
      "Epoch: 4 \tTraining Loss: 0.000195\n",
      "Epoch: 5 \tTraining Loss: 0.000195\n",
      "Epoch: 6 \tTraining Loss: 0.000195\n",
      "Epoch: 7 \tTraining Loss: 0.000195\n",
      "Epoch: 8 \tTraining Loss: 0.000195\n",
      "Epoch: 9 \tTraining Loss: 0.000195\n",
      "Epoch: 10 \tTraining Loss: 0.000195\n",
      "Epoch: 11 \tTraining Loss: 0.000195\n",
      "Epoch: 12 \tTraining Loss: 0.000195\n",
      "Epoch: 13 \tTraining Loss: 0.000195\n",
      "Epoch: 14 \tTraining Loss: 0.000195\n",
      "Epoch: 15 \tTraining Loss: 0.000195\n",
      "Epoch: 16 \tTraining Loss: 0.000195\n",
      "Epoch: 17 \tTraining Loss: 0.000195\n",
      "Epoch: 18 \tTraining Loss: 0.000195\n",
      "Epoch: 1 \tTraining Loss: 0.000171\n",
      "Epoch: 2 \tTraining Loss: 0.000171\n",
      "Epoch: 3 \tTraining Loss: 0.000171\n",
      "Epoch: 4 \tTraining Loss: 0.000171\n",
      "Epoch: 5 \tTraining Loss: 0.000171\n",
      "Epoch: 6 \tTraining Loss: 0.000171\n",
      "Epoch: 7 \tTraining Loss: 0.000171\n",
      "Epoch: 8 \tTraining Loss: 0.000171\n",
      "Epoch: 9 \tTraining Loss: 0.000171\n",
      "Epoch: 10 \tTraining Loss: 0.000171\n",
      "Epoch: 11 \tTraining Loss: 0.000171\n",
      "Epoch: 12 \tTraining Loss: 0.000171\n",
      "Epoch: 13 \tTraining Loss: 0.000171\n",
      "Epoch: 14 \tTraining Loss: 0.000171\n",
      "Epoch: 15 \tTraining Loss: 0.000171\n",
      "Epoch: 16 \tTraining Loss: 0.000171\n",
      "Epoch: 17 \tTraining Loss: 0.000171\n",
      "Epoch: 18 \tTraining Loss: 0.000171\n",
      "Epoch: 1 \tTraining Loss: 0.000084\n",
      "Epoch: 2 \tTraining Loss: 0.000084\n",
      "Epoch: 3 \tTraining Loss: 0.000084\n",
      "Epoch: 4 \tTraining Loss: 0.000084\n",
      "Epoch: 5 \tTraining Loss: 0.000084\n",
      "Epoch: 6 \tTraining Loss: 0.000084\n",
      "Epoch: 7 \tTraining Loss: 0.000084\n",
      "Epoch: 8 \tTraining Loss: 0.000084\n",
      "Epoch: 9 \tTraining Loss: 0.000084\n",
      "Epoch: 10 \tTraining Loss: 0.000084\n",
      "Epoch: 11 \tTraining Loss: 0.000084\n",
      "Epoch: 12 \tTraining Loss: 0.000084\n",
      "Epoch: 13 \tTraining Loss: 0.000084\n",
      "Epoch: 14 \tTraining Loss: 0.000084\n",
      "Epoch: 15 \tTraining Loss: 0.000084\n",
      "Epoch: 16 \tTraining Loss: 0.000084\n",
      "Epoch: 17 \tTraining Loss: 0.000084\n",
      "Epoch: 18 \tTraining Loss: 0.000084\n",
      "Epoch: 1 \tTraining Loss: 0.000123\n",
      "Epoch: 2 \tTraining Loss: 0.000123\n",
      "Epoch: 3 \tTraining Loss: 0.000123\n",
      "Epoch: 4 \tTraining Loss: 0.000123\n",
      "Epoch: 5 \tTraining Loss: 0.000123\n",
      "Epoch: 6 \tTraining Loss: 0.000123\n",
      "Epoch: 7 \tTraining Loss: 0.000123\n",
      "Epoch: 8 \tTraining Loss: 0.000123\n",
      "Epoch: 9 \tTraining Loss: 0.000123\n",
      "Epoch: 10 \tTraining Loss: 0.000123\n",
      "Epoch: 11 \tTraining Loss: 0.000123\n",
      "Epoch: 12 \tTraining Loss: 0.000123\n",
      "Epoch: 13 \tTraining Loss: 0.000123\n",
      "Epoch: 14 \tTraining Loss: 0.000123\n",
      "Epoch: 15 \tTraining Loss: 0.000123\n",
      "Epoch: 16 \tTraining Loss: 0.000123\n",
      "Epoch: 17 \tTraining Loss: 0.000123\n",
      "Epoch: 18 \tTraining Loss: 0.000123\n",
      "Epoch: 1 \tTraining Loss: 0.000245\n",
      "Epoch: 2 \tTraining Loss: 0.000245\n",
      "Epoch: 3 \tTraining Loss: 0.000245\n",
      "Epoch: 4 \tTraining Loss: 0.000245\n",
      "Epoch: 5 \tTraining Loss: 0.000245\n",
      "Epoch: 6 \tTraining Loss: 0.000245\n",
      "Epoch: 7 \tTraining Loss: 0.000245\n",
      "Epoch: 8 \tTraining Loss: 0.000245\n",
      "Epoch: 9 \tTraining Loss: 0.000245\n",
      "Epoch: 10 \tTraining Loss: 0.000245\n",
      "Epoch: 11 \tTraining Loss: 0.000245\n",
      "Epoch: 12 \tTraining Loss: 0.000245\n",
      "Epoch: 13 \tTraining Loss: 0.000245\n",
      "Epoch: 14 \tTraining Loss: 0.000245\n",
      "Epoch: 15 \tTraining Loss: 0.000245\n",
      "Epoch: 16 \tTraining Loss: 0.000245\n",
      "Epoch: 17 \tTraining Loss: 0.000245\n",
      "Epoch: 18 \tTraining Loss: 0.000245\n",
      "Epoch: 1 \tTraining Loss: 0.000248\n",
      "Epoch: 2 \tTraining Loss: 0.000248\n",
      "Epoch: 3 \tTraining Loss: 0.000248\n",
      "Epoch: 4 \tTraining Loss: 0.000248\n",
      "Epoch: 5 \tTraining Loss: 0.000248\n",
      "Epoch: 6 \tTraining Loss: 0.000248\n",
      "Epoch: 7 \tTraining Loss: 0.000248\n",
      "Epoch: 8 \tTraining Loss: 0.000248\n",
      "Epoch: 9 \tTraining Loss: 0.000248\n",
      "Epoch: 10 \tTraining Loss: 0.000248\n",
      "Epoch: 11 \tTraining Loss: 0.000248\n",
      "Epoch: 12 \tTraining Loss: 0.000248\n",
      "Epoch: 13 \tTraining Loss: 0.000248\n",
      "Epoch: 14 \tTraining Loss: 0.000248\n",
      "Epoch: 15 \tTraining Loss: 0.000248\n",
      "Epoch: 16 \tTraining Loss: 0.000248\n",
      "Epoch: 17 \tTraining Loss: 0.000248\n",
      "Epoch: 18 \tTraining Loss: 0.000248\n",
      "Epoch: 1 \tTraining Loss: 0.000263\n",
      "Epoch: 2 \tTraining Loss: 0.000263\n",
      "Epoch: 3 \tTraining Loss: 0.000263\n",
      "Epoch: 4 \tTraining Loss: 0.000263\n",
      "Epoch: 5 \tTraining Loss: 0.000263\n",
      "Epoch: 6 \tTraining Loss: 0.000263\n",
      "Epoch: 7 \tTraining Loss: 0.000263\n",
      "Epoch: 8 \tTraining Loss: 0.000263\n",
      "Epoch: 9 \tTraining Loss: 0.000263\n",
      "Epoch: 10 \tTraining Loss: 0.000263\n",
      "Epoch: 11 \tTraining Loss: 0.000263\n",
      "Epoch: 12 \tTraining Loss: 0.000263\n",
      "Epoch: 13 \tTraining Loss: 0.000263\n",
      "Epoch: 14 \tTraining Loss: 0.000263\n",
      "Epoch: 15 \tTraining Loss: 0.000263\n",
      "Epoch: 16 \tTraining Loss: 0.000263\n",
      "Epoch: 17 \tTraining Loss: 0.000263\n",
      "Epoch: 18 \tTraining Loss: 0.000263\n",
      "Epoch: 1 \tTraining Loss: 0.000216\n",
      "Epoch: 2 \tTraining Loss: 0.000216\n",
      "Epoch: 3 \tTraining Loss: 0.000216\n",
      "Epoch: 4 \tTraining Loss: 0.000216\n",
      "Epoch: 5 \tTraining Loss: 0.000216\n",
      "Epoch: 6 \tTraining Loss: 0.000216\n",
      "Epoch: 7 \tTraining Loss: 0.000216\n",
      "Epoch: 8 \tTraining Loss: 0.000216\n",
      "Epoch: 9 \tTraining Loss: 0.000216\n",
      "Epoch: 10 \tTraining Loss: 0.000216\n",
      "Epoch: 11 \tTraining Loss: 0.000216\n",
      "Epoch: 12 \tTraining Loss: 0.000216\n",
      "Epoch: 13 \tTraining Loss: 0.000216\n",
      "Epoch: 14 \tTraining Loss: 0.000216\n",
      "Epoch: 15 \tTraining Loss: 0.000216\n",
      "Epoch: 16 \tTraining Loss: 0.000216\n",
      "Epoch: 17 \tTraining Loss: 0.000216\n",
      "Epoch: 18 \tTraining Loss: 0.000216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000183\n",
      "Epoch: 2 \tTraining Loss: 0.000183\n",
      "Epoch: 3 \tTraining Loss: 0.000183\n",
      "Epoch: 4 \tTraining Loss: 0.000183\n",
      "Epoch: 5 \tTraining Loss: 0.000183\n",
      "Epoch: 6 \tTraining Loss: 0.000183\n",
      "Epoch: 7 \tTraining Loss: 0.000183\n",
      "Epoch: 8 \tTraining Loss: 0.000183\n",
      "Epoch: 9 \tTraining Loss: 0.000183\n",
      "Epoch: 10 \tTraining Loss: 0.000183\n",
      "Epoch: 11 \tTraining Loss: 0.000183\n",
      "Epoch: 12 \tTraining Loss: 0.000183\n",
      "Epoch: 13 \tTraining Loss: 0.000183\n",
      "Epoch: 14 \tTraining Loss: 0.000183\n",
      "Epoch: 15 \tTraining Loss: 0.000183\n",
      "Epoch: 16 \tTraining Loss: 0.000183\n",
      "Epoch: 17 \tTraining Loss: 0.000183\n",
      "Epoch: 18 \tTraining Loss: 0.000183\n",
      "Epoch: 1 \tTraining Loss: 0.000135\n",
      "Epoch: 2 \tTraining Loss: 0.000135\n",
      "Epoch: 3 \tTraining Loss: 0.000135\n",
      "Epoch: 4 \tTraining Loss: 0.000135\n",
      "Epoch: 5 \tTraining Loss: 0.000135\n",
      "Epoch: 6 \tTraining Loss: 0.000135\n",
      "Epoch: 7 \tTraining Loss: 0.000135\n",
      "Epoch: 8 \tTraining Loss: 0.000135\n",
      "Epoch: 9 \tTraining Loss: 0.000135\n",
      "Epoch: 10 \tTraining Loss: 0.000135\n",
      "Epoch: 11 \tTraining Loss: 0.000135\n",
      "Epoch: 12 \tTraining Loss: 0.000135\n",
      "Epoch: 13 \tTraining Loss: 0.000135\n",
      "Epoch: 14 \tTraining Loss: 0.000135\n",
      "Epoch: 15 \tTraining Loss: 0.000135\n",
      "Epoch: 16 \tTraining Loss: 0.000135\n",
      "Epoch: 17 \tTraining Loss: 0.000135\n",
      "Epoch: 18 \tTraining Loss: 0.000135\n",
      "Epoch: 1 \tTraining Loss: 0.000272\n",
      "Epoch: 2 \tTraining Loss: 0.000272\n",
      "Epoch: 3 \tTraining Loss: 0.000272\n",
      "Epoch: 4 \tTraining Loss: 0.000272\n",
      "Epoch: 5 \tTraining Loss: 0.000272\n",
      "Epoch: 6 \tTraining Loss: 0.000272\n",
      "Epoch: 7 \tTraining Loss: 0.000272\n",
      "Epoch: 8 \tTraining Loss: 0.000272\n",
      "Epoch: 9 \tTraining Loss: 0.000272\n",
      "Epoch: 10 \tTraining Loss: 0.000272\n",
      "Epoch: 11 \tTraining Loss: 0.000272\n",
      "Epoch: 12 \tTraining Loss: 0.000272\n",
      "Epoch: 13 \tTraining Loss: 0.000272\n",
      "Epoch: 14 \tTraining Loss: 0.000272\n",
      "Epoch: 15 \tTraining Loss: 0.000272\n",
      "Epoch: 16 \tTraining Loss: 0.000272\n",
      "Epoch: 17 \tTraining Loss: 0.000272\n",
      "Epoch: 18 \tTraining Loss: 0.000272\n",
      "Epoch: 1 \tTraining Loss: 0.000110\n",
      "Epoch: 2 \tTraining Loss: 0.000110\n",
      "Epoch: 3 \tTraining Loss: 0.000110\n",
      "Epoch: 4 \tTraining Loss: 0.000110\n",
      "Epoch: 5 \tTraining Loss: 0.000110\n",
      "Epoch: 6 \tTraining Loss: 0.000110\n",
      "Epoch: 7 \tTraining Loss: 0.000110\n",
      "Epoch: 8 \tTraining Loss: 0.000110\n",
      "Epoch: 9 \tTraining Loss: 0.000110\n",
      "Epoch: 10 \tTraining Loss: 0.000110\n",
      "Epoch: 11 \tTraining Loss: 0.000110\n",
      "Epoch: 12 \tTraining Loss: 0.000110\n",
      "Epoch: 13 \tTraining Loss: 0.000110\n",
      "Epoch: 14 \tTraining Loss: 0.000110\n",
      "Epoch: 15 \tTraining Loss: 0.000110\n",
      "Epoch: 16 \tTraining Loss: 0.000110\n",
      "Epoch: 17 \tTraining Loss: 0.000110\n",
      "Epoch: 18 \tTraining Loss: 0.000110\n",
      "Epoch: 1 \tTraining Loss: 0.000245\n",
      "Epoch: 2 \tTraining Loss: 0.000245\n",
      "Epoch: 3 \tTraining Loss: 0.000245\n",
      "Epoch: 4 \tTraining Loss: 0.000245\n",
      "Epoch: 5 \tTraining Loss: 0.000245\n",
      "Epoch: 6 \tTraining Loss: 0.000245\n",
      "Epoch: 7 \tTraining Loss: 0.000245\n",
      "Epoch: 8 \tTraining Loss: 0.000245\n",
      "Epoch: 9 \tTraining Loss: 0.000245\n",
      "Epoch: 10 \tTraining Loss: 0.000245\n",
      "Epoch: 11 \tTraining Loss: 0.000245\n",
      "Epoch: 12 \tTraining Loss: 0.000245\n",
      "Epoch: 13 \tTraining Loss: 0.000245\n",
      "Epoch: 14 \tTraining Loss: 0.000245\n",
      "Epoch: 15 \tTraining Loss: 0.000245\n",
      "Epoch: 16 \tTraining Loss: 0.000245\n",
      "Epoch: 17 \tTraining Loss: 0.000245\n",
      "Epoch: 18 \tTraining Loss: 0.000245\n",
      "Epoch: 1 \tTraining Loss: 0.000482\n",
      "Epoch: 2 \tTraining Loss: 0.000482\n",
      "Epoch: 3 \tTraining Loss: 0.000482\n",
      "Epoch: 4 \tTraining Loss: 0.000482\n",
      "Epoch: 5 \tTraining Loss: 0.000482\n",
      "Epoch: 6 \tTraining Loss: 0.000482\n",
      "Epoch: 7 \tTraining Loss: 0.000482\n",
      "Epoch: 8 \tTraining Loss: 0.000482\n",
      "Epoch: 9 \tTraining Loss: 0.000482\n",
      "Epoch: 10 \tTraining Loss: 0.000482\n",
      "Epoch: 11 \tTraining Loss: 0.000482\n",
      "Epoch: 12 \tTraining Loss: 0.000482\n",
      "Epoch: 13 \tTraining Loss: 0.000482\n",
      "Epoch: 14 \tTraining Loss: 0.000482\n",
      "Epoch: 15 \tTraining Loss: 0.000482\n",
      "Epoch: 16 \tTraining Loss: 0.000482\n",
      "Epoch: 17 \tTraining Loss: 0.000482\n",
      "Epoch: 18 \tTraining Loss: 0.000482\n",
      "Epoch: 1 \tTraining Loss: 0.000074\n",
      "Epoch: 2 \tTraining Loss: 0.000074\n",
      "Epoch: 3 \tTraining Loss: 0.000074\n",
      "Epoch: 4 \tTraining Loss: 0.000074\n",
      "Epoch: 5 \tTraining Loss: 0.000074\n",
      "Epoch: 6 \tTraining Loss: 0.000074\n",
      "Epoch: 7 \tTraining Loss: 0.000074\n",
      "Epoch: 8 \tTraining Loss: 0.000074\n",
      "Epoch: 9 \tTraining Loss: 0.000074\n",
      "Epoch: 10 \tTraining Loss: 0.000074\n",
      "Epoch: 11 \tTraining Loss: 0.000074\n",
      "Epoch: 12 \tTraining Loss: 0.000074\n",
      "Epoch: 13 \tTraining Loss: 0.000074\n",
      "Epoch: 14 \tTraining Loss: 0.000074\n",
      "Epoch: 15 \tTraining Loss: 0.000074\n",
      "Epoch: 16 \tTraining Loss: 0.000074\n",
      "Epoch: 17 \tTraining Loss: 0.000074\n",
      "Epoch: 18 \tTraining Loss: 0.000074\n",
      "Epoch: 1 \tTraining Loss: 0.000277\n",
      "Epoch: 2 \tTraining Loss: 0.000277\n",
      "Epoch: 3 \tTraining Loss: 0.000277\n",
      "Epoch: 4 \tTraining Loss: 0.000277\n",
      "Epoch: 5 \tTraining Loss: 0.000277\n",
      "Epoch: 6 \tTraining Loss: 0.000277\n",
      "Epoch: 7 \tTraining Loss: 0.000277\n",
      "Epoch: 8 \tTraining Loss: 0.000277\n",
      "Epoch: 9 \tTraining Loss: 0.000277\n",
      "Epoch: 10 \tTraining Loss: 0.000277\n",
      "Epoch: 11 \tTraining Loss: 0.000277\n",
      "Epoch: 12 \tTraining Loss: 0.000277\n",
      "Epoch: 13 \tTraining Loss: 0.000277\n",
      "Epoch: 14 \tTraining Loss: 0.000277\n",
      "Epoch: 15 \tTraining Loss: 0.000277\n",
      "Epoch: 16 \tTraining Loss: 0.000277\n",
      "Epoch: 17 \tTraining Loss: 0.000277\n",
      "Epoch: 18 \tTraining Loss: 0.000277\n",
      "Epoch: 1 \tTraining Loss: 0.000132\n",
      "Epoch: 2 \tTraining Loss: 0.000132\n",
      "Epoch: 3 \tTraining Loss: 0.000132\n",
      "Epoch: 4 \tTraining Loss: 0.000132\n",
      "Epoch: 5 \tTraining Loss: 0.000132\n",
      "Epoch: 6 \tTraining Loss: 0.000132\n",
      "Epoch: 7 \tTraining Loss: 0.000132\n",
      "Epoch: 8 \tTraining Loss: 0.000132\n",
      "Epoch: 9 \tTraining Loss: 0.000132\n",
      "Epoch: 10 \tTraining Loss: 0.000132\n",
      "Epoch: 11 \tTraining Loss: 0.000132\n",
      "Epoch: 12 \tTraining Loss: 0.000132\n",
      "Epoch: 13 \tTraining Loss: 0.000132\n",
      "Epoch: 14 \tTraining Loss: 0.000132\n",
      "Epoch: 15 \tTraining Loss: 0.000132\n",
      "Epoch: 16 \tTraining Loss: 0.000132\n",
      "Epoch: 17 \tTraining Loss: 0.000132\n",
      "Epoch: 18 \tTraining Loss: 0.000132\n",
      "Epoch: 1 \tTraining Loss: 0.000114\n",
      "Epoch: 2 \tTraining Loss: 0.000114\n",
      "Epoch: 3 \tTraining Loss: 0.000114\n",
      "Epoch: 4 \tTraining Loss: 0.000114\n",
      "Epoch: 5 \tTraining Loss: 0.000114\n",
      "Epoch: 6 \tTraining Loss: 0.000114\n",
      "Epoch: 7 \tTraining Loss: 0.000114\n",
      "Epoch: 8 \tTraining Loss: 0.000114\n",
      "Epoch: 9 \tTraining Loss: 0.000114\n",
      "Epoch: 10 \tTraining Loss: 0.000114\n",
      "Epoch: 11 \tTraining Loss: 0.000114\n",
      "Epoch: 12 \tTraining Loss: 0.000114\n",
      "Epoch: 13 \tTraining Loss: 0.000114\n",
      "Epoch: 14 \tTraining Loss: 0.000114\n",
      "Epoch: 15 \tTraining Loss: 0.000114\n",
      "Epoch: 16 \tTraining Loss: 0.000114\n",
      "Epoch: 17 \tTraining Loss: 0.000114\n",
      "Epoch: 18 \tTraining Loss: 0.000114\n",
      "Epoch: 1 \tTraining Loss: 0.000138\n",
      "Epoch: 2 \tTraining Loss: 0.000138\n",
      "Epoch: 3 \tTraining Loss: 0.000138\n",
      "Epoch: 4 \tTraining Loss: 0.000138\n",
      "Epoch: 5 \tTraining Loss: 0.000138\n",
      "Epoch: 6 \tTraining Loss: 0.000138\n",
      "Epoch: 7 \tTraining Loss: 0.000138\n",
      "Epoch: 8 \tTraining Loss: 0.000138\n",
      "Epoch: 9 \tTraining Loss: 0.000138\n",
      "Epoch: 10 \tTraining Loss: 0.000138\n",
      "Epoch: 11 \tTraining Loss: 0.000138\n",
      "Epoch: 12 \tTraining Loss: 0.000138\n",
      "Epoch: 13 \tTraining Loss: 0.000138\n",
      "Epoch: 14 \tTraining Loss: 0.000138\n",
      "Epoch: 15 \tTraining Loss: 0.000138\n",
      "Epoch: 16 \tTraining Loss: 0.000138\n",
      "Epoch: 17 \tTraining Loss: 0.000138\n",
      "Epoch: 18 \tTraining Loss: 0.000138\n",
      "Epoch: 1 \tTraining Loss: 0.000134\n",
      "Epoch: 2 \tTraining Loss: 0.000134\n",
      "Epoch: 3 \tTraining Loss: 0.000134\n",
      "Epoch: 4 \tTraining Loss: 0.000134\n",
      "Epoch: 5 \tTraining Loss: 0.000134\n",
      "Epoch: 6 \tTraining Loss: 0.000134\n",
      "Epoch: 7 \tTraining Loss: 0.000134\n",
      "Epoch: 8 \tTraining Loss: 0.000134\n",
      "Epoch: 9 \tTraining Loss: 0.000134\n",
      "Epoch: 10 \tTraining Loss: 0.000134\n",
      "Epoch: 11 \tTraining Loss: 0.000134\n",
      "Epoch: 12 \tTraining Loss: 0.000134\n",
      "Epoch: 13 \tTraining Loss: 0.000134\n",
      "Epoch: 14 \tTraining Loss: 0.000134\n",
      "Epoch: 15 \tTraining Loss: 0.000134\n",
      "Epoch: 16 \tTraining Loss: 0.000134\n",
      "Epoch: 17 \tTraining Loss: 0.000134\n",
      "Epoch: 18 \tTraining Loss: 0.000134\n",
      "Epoch: 1 \tTraining Loss: 0.000097\n",
      "Epoch: 2 \tTraining Loss: 0.000097\n",
      "Epoch: 3 \tTraining Loss: 0.000097\n",
      "Epoch: 4 \tTraining Loss: 0.000097\n",
      "Epoch: 5 \tTraining Loss: 0.000097\n",
      "Epoch: 6 \tTraining Loss: 0.000097\n",
      "Epoch: 7 \tTraining Loss: 0.000097\n",
      "Epoch: 8 \tTraining Loss: 0.000097\n",
      "Epoch: 9 \tTraining Loss: 0.000097\n",
      "Epoch: 10 \tTraining Loss: 0.000097\n",
      "Epoch: 11 \tTraining Loss: 0.000097\n",
      "Epoch: 12 \tTraining Loss: 0.000097\n",
      "Epoch: 13 \tTraining Loss: 0.000097\n",
      "Epoch: 14 \tTraining Loss: 0.000097\n",
      "Epoch: 15 \tTraining Loss: 0.000097\n",
      "Epoch: 16 \tTraining Loss: 0.000097\n",
      "Epoch: 17 \tTraining Loss: 0.000097\n",
      "Epoch: 18 \tTraining Loss: 0.000097\n",
      "Epoch: 1 \tTraining Loss: 0.000147\n",
      "Epoch: 2 \tTraining Loss: 0.000147\n",
      "Epoch: 3 \tTraining Loss: 0.000147\n",
      "Epoch: 4 \tTraining Loss: 0.000147\n",
      "Epoch: 5 \tTraining Loss: 0.000147\n",
      "Epoch: 6 \tTraining Loss: 0.000147\n",
      "Epoch: 7 \tTraining Loss: 0.000147\n",
      "Epoch: 8 \tTraining Loss: 0.000147\n",
      "Epoch: 9 \tTraining Loss: 0.000147\n",
      "Epoch: 10 \tTraining Loss: 0.000147\n",
      "Epoch: 11 \tTraining Loss: 0.000147\n",
      "Epoch: 12 \tTraining Loss: 0.000147\n",
      "Epoch: 13 \tTraining Loss: 0.000147\n",
      "Epoch: 14 \tTraining Loss: 0.000147\n",
      "Epoch: 15 \tTraining Loss: 0.000147\n",
      "Epoch: 16 \tTraining Loss: 0.000147\n",
      "Epoch: 17 \tTraining Loss: 0.000147\n",
      "Epoch: 18 \tTraining Loss: 0.000147\n",
      "Epoch: 1 \tTraining Loss: 0.000278\n",
      "Epoch: 2 \tTraining Loss: 0.000278\n",
      "Epoch: 3 \tTraining Loss: 0.000278\n",
      "Epoch: 4 \tTraining Loss: 0.000278\n",
      "Epoch: 5 \tTraining Loss: 0.000278\n",
      "Epoch: 6 \tTraining Loss: 0.000278\n",
      "Epoch: 7 \tTraining Loss: 0.000278\n",
      "Epoch: 8 \tTraining Loss: 0.000278\n",
      "Epoch: 9 \tTraining Loss: 0.000278\n",
      "Epoch: 10 \tTraining Loss: 0.000278\n",
      "Epoch: 11 \tTraining Loss: 0.000278\n",
      "Epoch: 12 \tTraining Loss: 0.000278\n",
      "Epoch: 13 \tTraining Loss: 0.000278\n",
      "Epoch: 14 \tTraining Loss: 0.000278\n",
      "Epoch: 15 \tTraining Loss: 0.000278\n",
      "Epoch: 16 \tTraining Loss: 0.000278\n",
      "Epoch: 17 \tTraining Loss: 0.000278\n",
      "Epoch: 18 \tTraining Loss: 0.000278\n",
      "Epoch: 1 \tTraining Loss: 0.000353\n",
      "Epoch: 2 \tTraining Loss: 0.000353\n",
      "Epoch: 3 \tTraining Loss: 0.000353\n",
      "Epoch: 4 \tTraining Loss: 0.000353\n",
      "Epoch: 5 \tTraining Loss: 0.000353\n",
      "Epoch: 6 \tTraining Loss: 0.000353\n",
      "Epoch: 7 \tTraining Loss: 0.000353\n",
      "Epoch: 8 \tTraining Loss: 0.000353\n",
      "Epoch: 9 \tTraining Loss: 0.000353\n",
      "Epoch: 10 \tTraining Loss: 0.000353\n",
      "Epoch: 11 \tTraining Loss: 0.000353\n",
      "Epoch: 12 \tTraining Loss: 0.000353\n",
      "Epoch: 13 \tTraining Loss: 0.000353\n",
      "Epoch: 14 \tTraining Loss: 0.000353\n",
      "Epoch: 15 \tTraining Loss: 0.000353\n",
      "Epoch: 16 \tTraining Loss: 0.000353\n",
      "Epoch: 17 \tTraining Loss: 0.000353\n",
      "Epoch: 18 \tTraining Loss: 0.000353\n",
      "Epoch: 1 \tTraining Loss: 0.000289\n",
      "Epoch: 2 \tTraining Loss: 0.000289\n",
      "Epoch: 3 \tTraining Loss: 0.000289\n",
      "Epoch: 4 \tTraining Loss: 0.000289\n",
      "Epoch: 5 \tTraining Loss: 0.000289\n",
      "Epoch: 6 \tTraining Loss: 0.000289\n",
      "Epoch: 7 \tTraining Loss: 0.000289\n",
      "Epoch: 8 \tTraining Loss: 0.000289\n",
      "Epoch: 9 \tTraining Loss: 0.000289\n",
      "Epoch: 10 \tTraining Loss: 0.000289\n",
      "Epoch: 11 \tTraining Loss: 0.000289\n",
      "Epoch: 12 \tTraining Loss: 0.000289\n",
      "Epoch: 13 \tTraining Loss: 0.000289\n",
      "Epoch: 14 \tTraining Loss: 0.000289\n",
      "Epoch: 15 \tTraining Loss: 0.000289\n",
      "Epoch: 16 \tTraining Loss: 0.000289\n",
      "Epoch: 17 \tTraining Loss: 0.000289\n",
      "Epoch: 18 \tTraining Loss: 0.000289\n",
      "Epoch: 1 \tTraining Loss: 0.000139\n",
      "Epoch: 2 \tTraining Loss: 0.000139\n",
      "Epoch: 3 \tTraining Loss: 0.000139\n",
      "Epoch: 4 \tTraining Loss: 0.000139\n",
      "Epoch: 5 \tTraining Loss: 0.000139\n",
      "Epoch: 6 \tTraining Loss: 0.000139\n",
      "Epoch: 7 \tTraining Loss: 0.000139\n",
      "Epoch: 8 \tTraining Loss: 0.000139\n",
      "Epoch: 9 \tTraining Loss: 0.000139\n",
      "Epoch: 10 \tTraining Loss: 0.000139\n",
      "Epoch: 11 \tTraining Loss: 0.000139\n",
      "Epoch: 12 \tTraining Loss: 0.000139\n",
      "Epoch: 13 \tTraining Loss: 0.000139\n",
      "Epoch: 14 \tTraining Loss: 0.000139\n",
      "Epoch: 15 \tTraining Loss: 0.000139\n",
      "Epoch: 16 \tTraining Loss: 0.000139\n",
      "Epoch: 17 \tTraining Loss: 0.000139\n",
      "Epoch: 18 \tTraining Loss: 0.000139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000095\n",
      "Epoch: 2 \tTraining Loss: 0.000095\n",
      "Epoch: 3 \tTraining Loss: 0.000095\n",
      "Epoch: 4 \tTraining Loss: 0.000095\n",
      "Epoch: 5 \tTraining Loss: 0.000095\n",
      "Epoch: 6 \tTraining Loss: 0.000095\n",
      "Epoch: 7 \tTraining Loss: 0.000095\n",
      "Epoch: 8 \tTraining Loss: 0.000095\n",
      "Epoch: 9 \tTraining Loss: 0.000095\n",
      "Epoch: 10 \tTraining Loss: 0.000095\n",
      "Epoch: 11 \tTraining Loss: 0.000095\n",
      "Epoch: 12 \tTraining Loss: 0.000095\n",
      "Epoch: 13 \tTraining Loss: 0.000095\n",
      "Epoch: 14 \tTraining Loss: 0.000095\n",
      "Epoch: 15 \tTraining Loss: 0.000095\n",
      "Epoch: 16 \tTraining Loss: 0.000095\n",
      "Epoch: 17 \tTraining Loss: 0.000095\n",
      "Epoch: 18 \tTraining Loss: 0.000095\n",
      "Epoch: 1 \tTraining Loss: 0.000138\n",
      "Epoch: 2 \tTraining Loss: 0.000138\n",
      "Epoch: 3 \tTraining Loss: 0.000138\n",
      "Epoch: 4 \tTraining Loss: 0.000138\n",
      "Epoch: 5 \tTraining Loss: 0.000138\n",
      "Epoch: 6 \tTraining Loss: 0.000138\n",
      "Epoch: 7 \tTraining Loss: 0.000138\n",
      "Epoch: 8 \tTraining Loss: 0.000138\n",
      "Epoch: 9 \tTraining Loss: 0.000138\n",
      "Epoch: 10 \tTraining Loss: 0.000138\n",
      "Epoch: 11 \tTraining Loss: 0.000138\n",
      "Epoch: 12 \tTraining Loss: 0.000138\n",
      "Epoch: 13 \tTraining Loss: 0.000138\n",
      "Epoch: 14 \tTraining Loss: 0.000138\n",
      "Epoch: 15 \tTraining Loss: 0.000138\n",
      "Epoch: 16 \tTraining Loss: 0.000138\n",
      "Epoch: 17 \tTraining Loss: 0.000138\n",
      "Epoch: 18 \tTraining Loss: 0.000138\n",
      "Epoch: 1 \tTraining Loss: 0.000173\n",
      "Epoch: 2 \tTraining Loss: 0.000173\n",
      "Epoch: 3 \tTraining Loss: 0.000173\n",
      "Epoch: 4 \tTraining Loss: 0.000173\n",
      "Epoch: 5 \tTraining Loss: 0.000173\n",
      "Epoch: 6 \tTraining Loss: 0.000173\n",
      "Epoch: 7 \tTraining Loss: 0.000173\n",
      "Epoch: 8 \tTraining Loss: 0.000173\n",
      "Epoch: 9 \tTraining Loss: 0.000173\n",
      "Epoch: 10 \tTraining Loss: 0.000173\n",
      "Epoch: 11 \tTraining Loss: 0.000173\n",
      "Epoch: 12 \tTraining Loss: 0.000173\n",
      "Epoch: 13 \tTraining Loss: 0.000173\n",
      "Epoch: 14 \tTraining Loss: 0.000173\n",
      "Epoch: 15 \tTraining Loss: 0.000173\n",
      "Epoch: 16 \tTraining Loss: 0.000173\n",
      "Epoch: 17 \tTraining Loss: 0.000173\n",
      "Epoch: 18 \tTraining Loss: 0.000173\n",
      "Epoch: 1 \tTraining Loss: 0.000473\n",
      "Epoch: 2 \tTraining Loss: 0.000473\n",
      "Epoch: 3 \tTraining Loss: 0.000473\n",
      "Epoch: 4 \tTraining Loss: 0.000473\n",
      "Epoch: 5 \tTraining Loss: 0.000473\n",
      "Epoch: 6 \tTraining Loss: 0.000473\n",
      "Epoch: 7 \tTraining Loss: 0.000473\n",
      "Epoch: 8 \tTraining Loss: 0.000473\n",
      "Epoch: 9 \tTraining Loss: 0.000473\n",
      "Epoch: 10 \tTraining Loss: 0.000473\n",
      "Epoch: 11 \tTraining Loss: 0.000473\n",
      "Epoch: 12 \tTraining Loss: 0.000473\n",
      "Epoch: 13 \tTraining Loss: 0.000473\n",
      "Epoch: 14 \tTraining Loss: 0.000473\n",
      "Epoch: 15 \tTraining Loss: 0.000473\n",
      "Epoch: 16 \tTraining Loss: 0.000473\n",
      "Epoch: 17 \tTraining Loss: 0.000473\n",
      "Epoch: 18 \tTraining Loss: 0.000473\n",
      "Epoch: 1 \tTraining Loss: 0.000186\n",
      "Epoch: 2 \tTraining Loss: 0.000186\n",
      "Epoch: 3 \tTraining Loss: 0.000186\n",
      "Epoch: 4 \tTraining Loss: 0.000186\n",
      "Epoch: 5 \tTraining Loss: 0.000186\n",
      "Epoch: 6 \tTraining Loss: 0.000186\n",
      "Epoch: 7 \tTraining Loss: 0.000186\n",
      "Epoch: 8 \tTraining Loss: 0.000186\n",
      "Epoch: 9 \tTraining Loss: 0.000186\n",
      "Epoch: 10 \tTraining Loss: 0.000186\n",
      "Epoch: 11 \tTraining Loss: 0.000186\n",
      "Epoch: 12 \tTraining Loss: 0.000186\n",
      "Epoch: 13 \tTraining Loss: 0.000186\n",
      "Epoch: 14 \tTraining Loss: 0.000186\n",
      "Epoch: 15 \tTraining Loss: 0.000186\n",
      "Epoch: 16 \tTraining Loss: 0.000186\n",
      "Epoch: 17 \tTraining Loss: 0.000186\n",
      "Epoch: 18 \tTraining Loss: 0.000186\n",
      "Epoch: 1 \tTraining Loss: 0.000153\n",
      "Epoch: 2 \tTraining Loss: 0.000153\n",
      "Epoch: 3 \tTraining Loss: 0.000153\n",
      "Epoch: 4 \tTraining Loss: 0.000153\n",
      "Epoch: 5 \tTraining Loss: 0.000153\n",
      "Epoch: 6 \tTraining Loss: 0.000153\n",
      "Epoch: 7 \tTraining Loss: 0.000153\n",
      "Epoch: 8 \tTraining Loss: 0.000153\n",
      "Epoch: 9 \tTraining Loss: 0.000153\n",
      "Epoch: 10 \tTraining Loss: 0.000153\n",
      "Epoch: 11 \tTraining Loss: 0.000153\n",
      "Epoch: 12 \tTraining Loss: 0.000153\n",
      "Epoch: 13 \tTraining Loss: 0.000153\n",
      "Epoch: 14 \tTraining Loss: 0.000153\n",
      "Epoch: 15 \tTraining Loss: 0.000153\n",
      "Epoch: 16 \tTraining Loss: 0.000153\n",
      "Epoch: 17 \tTraining Loss: 0.000153\n",
      "Epoch: 18 \tTraining Loss: 0.000153\n",
      "Epoch: 1 \tTraining Loss: 0.000184\n",
      "Epoch: 2 \tTraining Loss: 0.000184\n",
      "Epoch: 3 \tTraining Loss: 0.000184\n",
      "Epoch: 4 \tTraining Loss: 0.000184\n",
      "Epoch: 5 \tTraining Loss: 0.000184\n",
      "Epoch: 6 \tTraining Loss: 0.000184\n",
      "Epoch: 7 \tTraining Loss: 0.000184\n",
      "Epoch: 8 \tTraining Loss: 0.000184\n",
      "Epoch: 9 \tTraining Loss: 0.000184\n",
      "Epoch: 10 \tTraining Loss: 0.000184\n",
      "Epoch: 11 \tTraining Loss: 0.000184\n",
      "Epoch: 12 \tTraining Loss: 0.000184\n",
      "Epoch: 13 \tTraining Loss: 0.000184\n",
      "Epoch: 14 \tTraining Loss: 0.000184\n",
      "Epoch: 15 \tTraining Loss: 0.000184\n",
      "Epoch: 16 \tTraining Loss: 0.000184\n",
      "Epoch: 17 \tTraining Loss: 0.000184\n",
      "Epoch: 18 \tTraining Loss: 0.000184\n",
      "Epoch: 1 \tTraining Loss: 0.000121\n",
      "Epoch: 2 \tTraining Loss: 0.000121\n",
      "Epoch: 3 \tTraining Loss: 0.000121\n",
      "Epoch: 4 \tTraining Loss: 0.000121\n",
      "Epoch: 5 \tTraining Loss: 0.000121\n",
      "Epoch: 6 \tTraining Loss: 0.000121\n",
      "Epoch: 7 \tTraining Loss: 0.000121\n",
      "Epoch: 8 \tTraining Loss: 0.000121\n",
      "Epoch: 9 \tTraining Loss: 0.000121\n",
      "Epoch: 10 \tTraining Loss: 0.000121\n",
      "Epoch: 11 \tTraining Loss: 0.000121\n",
      "Epoch: 12 \tTraining Loss: 0.000121\n",
      "Epoch: 13 \tTraining Loss: 0.000121\n",
      "Epoch: 14 \tTraining Loss: 0.000121\n",
      "Epoch: 15 \tTraining Loss: 0.000121\n",
      "Epoch: 16 \tTraining Loss: 0.000121\n",
      "Epoch: 17 \tTraining Loss: 0.000121\n",
      "Epoch: 18 \tTraining Loss: 0.000121\n",
      "Epoch: 1 \tTraining Loss: 0.000209\n",
      "Epoch: 2 \tTraining Loss: 0.000209\n",
      "Epoch: 3 \tTraining Loss: 0.000209\n",
      "Epoch: 4 \tTraining Loss: 0.000209\n",
      "Epoch: 5 \tTraining Loss: 0.000209\n",
      "Epoch: 6 \tTraining Loss: 0.000209\n",
      "Epoch: 7 \tTraining Loss: 0.000209\n",
      "Epoch: 8 \tTraining Loss: 0.000209\n",
      "Epoch: 9 \tTraining Loss: 0.000209\n",
      "Epoch: 10 \tTraining Loss: 0.000209\n",
      "Epoch: 11 \tTraining Loss: 0.000209\n",
      "Epoch: 12 \tTraining Loss: 0.000209\n",
      "Epoch: 13 \tTraining Loss: 0.000209\n",
      "Epoch: 14 \tTraining Loss: 0.000209\n",
      "Epoch: 15 \tTraining Loss: 0.000209\n",
      "Epoch: 16 \tTraining Loss: 0.000209\n",
      "Epoch: 17 \tTraining Loss: 0.000209\n",
      "Epoch: 18 \tTraining Loss: 0.000209\n",
      "Epoch: 1 \tTraining Loss: 0.000080\n",
      "Epoch: 2 \tTraining Loss: 0.000080\n",
      "Epoch: 3 \tTraining Loss: 0.000080\n",
      "Epoch: 4 \tTraining Loss: 0.000080\n",
      "Epoch: 5 \tTraining Loss: 0.000080\n",
      "Epoch: 6 \tTraining Loss: 0.000080\n",
      "Epoch: 7 \tTraining Loss: 0.000080\n",
      "Epoch: 8 \tTraining Loss: 0.000080\n",
      "Epoch: 9 \tTraining Loss: 0.000080\n",
      "Epoch: 10 \tTraining Loss: 0.000080\n",
      "Epoch: 11 \tTraining Loss: 0.000080\n",
      "Epoch: 12 \tTraining Loss: 0.000080\n",
      "Epoch: 13 \tTraining Loss: 0.000080\n",
      "Epoch: 14 \tTraining Loss: 0.000080\n",
      "Epoch: 15 \tTraining Loss: 0.000080\n",
      "Epoch: 16 \tTraining Loss: 0.000080\n",
      "Epoch: 17 \tTraining Loss: 0.000080\n",
      "Epoch: 18 \tTraining Loss: 0.000080\n",
      "Epoch: 1 \tTraining Loss: 0.000121\n",
      "Epoch: 2 \tTraining Loss: 0.000121\n",
      "Epoch: 3 \tTraining Loss: 0.000121\n",
      "Epoch: 4 \tTraining Loss: 0.000121\n",
      "Epoch: 5 \tTraining Loss: 0.000121\n",
      "Epoch: 6 \tTraining Loss: 0.000121\n",
      "Epoch: 7 \tTraining Loss: 0.000121\n",
      "Epoch: 8 \tTraining Loss: 0.000121\n",
      "Epoch: 9 \tTraining Loss: 0.000121\n",
      "Epoch: 10 \tTraining Loss: 0.000121\n",
      "Epoch: 11 \tTraining Loss: 0.000121\n",
      "Epoch: 12 \tTraining Loss: 0.000121\n",
      "Epoch: 13 \tTraining Loss: 0.000121\n",
      "Epoch: 14 \tTraining Loss: 0.000121\n",
      "Epoch: 15 \tTraining Loss: 0.000121\n",
      "Epoch: 16 \tTraining Loss: 0.000121\n",
      "Epoch: 17 \tTraining Loss: 0.000121\n",
      "Epoch: 18 \tTraining Loss: 0.000121\n",
      "Epoch: 1 \tTraining Loss: 0.000122\n",
      "Epoch: 2 \tTraining Loss: 0.000122\n",
      "Epoch: 3 \tTraining Loss: 0.000122\n",
      "Epoch: 4 \tTraining Loss: 0.000122\n",
      "Epoch: 5 \tTraining Loss: 0.000122\n",
      "Epoch: 6 \tTraining Loss: 0.000122\n",
      "Epoch: 7 \tTraining Loss: 0.000122\n",
      "Epoch: 8 \tTraining Loss: 0.000122\n",
      "Epoch: 9 \tTraining Loss: 0.000122\n",
      "Epoch: 10 \tTraining Loss: 0.000122\n",
      "Epoch: 11 \tTraining Loss: 0.000122\n",
      "Epoch: 12 \tTraining Loss: 0.000122\n",
      "Epoch: 13 \tTraining Loss: 0.000122\n",
      "Epoch: 14 \tTraining Loss: 0.000122\n",
      "Epoch: 15 \tTraining Loss: 0.000122\n",
      "Epoch: 16 \tTraining Loss: 0.000122\n",
      "Epoch: 17 \tTraining Loss: 0.000122\n",
      "Epoch: 18 \tTraining Loss: 0.000122\n",
      "Epoch: 1 \tTraining Loss: 0.000168\n",
      "Epoch: 2 \tTraining Loss: 0.000168\n",
      "Epoch: 3 \tTraining Loss: 0.000168\n",
      "Epoch: 4 \tTraining Loss: 0.000168\n",
      "Epoch: 5 \tTraining Loss: 0.000168\n",
      "Epoch: 6 \tTraining Loss: 0.000168\n",
      "Epoch: 7 \tTraining Loss: 0.000168\n",
      "Epoch: 8 \tTraining Loss: 0.000168\n",
      "Epoch: 9 \tTraining Loss: 0.000168\n",
      "Epoch: 10 \tTraining Loss: 0.000168\n",
      "Epoch: 11 \tTraining Loss: 0.000168\n",
      "Epoch: 12 \tTraining Loss: 0.000168\n",
      "Epoch: 13 \tTraining Loss: 0.000168\n",
      "Epoch: 14 \tTraining Loss: 0.000168\n",
      "Epoch: 15 \tTraining Loss: 0.000168\n",
      "Epoch: 16 \tTraining Loss: 0.000168\n",
      "Epoch: 17 \tTraining Loss: 0.000168\n",
      "Epoch: 18 \tTraining Loss: 0.000168\n",
      "Epoch: 1 \tTraining Loss: 0.000097\n",
      "Epoch: 2 \tTraining Loss: 0.000097\n",
      "Epoch: 3 \tTraining Loss: 0.000097\n",
      "Epoch: 4 \tTraining Loss: 0.000097\n",
      "Epoch: 5 \tTraining Loss: 0.000097\n",
      "Epoch: 6 \tTraining Loss: 0.000097\n",
      "Epoch: 7 \tTraining Loss: 0.000097\n",
      "Epoch: 8 \tTraining Loss: 0.000097\n",
      "Epoch: 9 \tTraining Loss: 0.000097\n",
      "Epoch: 10 \tTraining Loss: 0.000097\n",
      "Epoch: 11 \tTraining Loss: 0.000097\n",
      "Epoch: 12 \tTraining Loss: 0.000097\n",
      "Epoch: 13 \tTraining Loss: 0.000097\n",
      "Epoch: 14 \tTraining Loss: 0.000097\n",
      "Epoch: 15 \tTraining Loss: 0.000097\n",
      "Epoch: 16 \tTraining Loss: 0.000097\n",
      "Epoch: 17 \tTraining Loss: 0.000097\n",
      "Epoch: 18 \tTraining Loss: 0.000097\n",
      "Epoch: 1 \tTraining Loss: 0.000297\n",
      "Epoch: 2 \tTraining Loss: 0.000297\n",
      "Epoch: 3 \tTraining Loss: 0.000297\n",
      "Epoch: 4 \tTraining Loss: 0.000297\n",
      "Epoch: 5 \tTraining Loss: 0.000297\n",
      "Epoch: 6 \tTraining Loss: 0.000297\n",
      "Epoch: 7 \tTraining Loss: 0.000297\n",
      "Epoch: 8 \tTraining Loss: 0.000297\n",
      "Epoch: 9 \tTraining Loss: 0.000297\n",
      "Epoch: 10 \tTraining Loss: 0.000297\n",
      "Epoch: 11 \tTraining Loss: 0.000297\n",
      "Epoch: 12 \tTraining Loss: 0.000297\n",
      "Epoch: 13 \tTraining Loss: 0.000297\n",
      "Epoch: 14 \tTraining Loss: 0.000297\n",
      "Epoch: 15 \tTraining Loss: 0.000297\n",
      "Epoch: 16 \tTraining Loss: 0.000297\n",
      "Epoch: 17 \tTraining Loss: 0.000297\n",
      "Epoch: 18 \tTraining Loss: 0.000297\n",
      "Epoch: 1 \tTraining Loss: 0.000177\n",
      "Epoch: 2 \tTraining Loss: 0.000177\n",
      "Epoch: 3 \tTraining Loss: 0.000177\n",
      "Epoch: 4 \tTraining Loss: 0.000177\n",
      "Epoch: 5 \tTraining Loss: 0.000177\n",
      "Epoch: 6 \tTraining Loss: 0.000177\n",
      "Epoch: 7 \tTraining Loss: 0.000177\n",
      "Epoch: 8 \tTraining Loss: 0.000177\n",
      "Epoch: 9 \tTraining Loss: 0.000177\n",
      "Epoch: 10 \tTraining Loss: 0.000177\n",
      "Epoch: 11 \tTraining Loss: 0.000177\n",
      "Epoch: 12 \tTraining Loss: 0.000177\n",
      "Epoch: 13 \tTraining Loss: 0.000177\n",
      "Epoch: 14 \tTraining Loss: 0.000177\n",
      "Epoch: 15 \tTraining Loss: 0.000177\n",
      "Epoch: 16 \tTraining Loss: 0.000177\n",
      "Epoch: 17 \tTraining Loss: 0.000177\n",
      "Epoch: 18 \tTraining Loss: 0.000177\n",
      "Epoch: 1 \tTraining Loss: 0.000124\n",
      "Epoch: 2 \tTraining Loss: 0.000124\n",
      "Epoch: 3 \tTraining Loss: 0.000124\n",
      "Epoch: 4 \tTraining Loss: 0.000124\n",
      "Epoch: 5 \tTraining Loss: 0.000124\n",
      "Epoch: 6 \tTraining Loss: 0.000124\n",
      "Epoch: 7 \tTraining Loss: 0.000124\n",
      "Epoch: 8 \tTraining Loss: 0.000124\n",
      "Epoch: 9 \tTraining Loss: 0.000124\n",
      "Epoch: 10 \tTraining Loss: 0.000124\n",
      "Epoch: 11 \tTraining Loss: 0.000124\n",
      "Epoch: 12 \tTraining Loss: 0.000124\n",
      "Epoch: 13 \tTraining Loss: 0.000124\n",
      "Epoch: 14 \tTraining Loss: 0.000124\n",
      "Epoch: 15 \tTraining Loss: 0.000124\n",
      "Epoch: 16 \tTraining Loss: 0.000124\n",
      "Epoch: 17 \tTraining Loss: 0.000124\n",
      "Epoch: 18 \tTraining Loss: 0.000124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000352\n",
      "Epoch: 2 \tTraining Loss: 0.000352\n",
      "Epoch: 3 \tTraining Loss: 0.000352\n",
      "Epoch: 4 \tTraining Loss: 0.000352\n",
      "Epoch: 5 \tTraining Loss: 0.000352\n",
      "Epoch: 6 \tTraining Loss: 0.000352\n",
      "Epoch: 7 \tTraining Loss: 0.000352\n",
      "Epoch: 8 \tTraining Loss: 0.000352\n",
      "Epoch: 9 \tTraining Loss: 0.000352\n",
      "Epoch: 10 \tTraining Loss: 0.000352\n",
      "Epoch: 11 \tTraining Loss: 0.000352\n",
      "Epoch: 12 \tTraining Loss: 0.000352\n",
      "Epoch: 13 \tTraining Loss: 0.000352\n",
      "Epoch: 14 \tTraining Loss: 0.000352\n",
      "Epoch: 15 \tTraining Loss: 0.000352\n",
      "Epoch: 16 \tTraining Loss: 0.000352\n",
      "Epoch: 17 \tTraining Loss: 0.000352\n",
      "Epoch: 18 \tTraining Loss: 0.000352\n",
      "Epoch: 1 \tTraining Loss: 0.000171\n",
      "Epoch: 2 \tTraining Loss: 0.000171\n",
      "Epoch: 3 \tTraining Loss: 0.000171\n",
      "Epoch: 4 \tTraining Loss: 0.000171\n",
      "Epoch: 5 \tTraining Loss: 0.000171\n",
      "Epoch: 6 \tTraining Loss: 0.000171\n",
      "Epoch: 7 \tTraining Loss: 0.000171\n",
      "Epoch: 8 \tTraining Loss: 0.000171\n",
      "Epoch: 9 \tTraining Loss: 0.000171\n",
      "Epoch: 10 \tTraining Loss: 0.000171\n",
      "Epoch: 11 \tTraining Loss: 0.000171\n",
      "Epoch: 12 \tTraining Loss: 0.000171\n",
      "Epoch: 13 \tTraining Loss: 0.000171\n",
      "Epoch: 14 \tTraining Loss: 0.000171\n",
      "Epoch: 15 \tTraining Loss: 0.000171\n",
      "Epoch: 16 \tTraining Loss: 0.000171\n",
      "Epoch: 17 \tTraining Loss: 0.000171\n",
      "Epoch: 18 \tTraining Loss: 0.000171\n",
      "Epoch: 1 \tTraining Loss: 0.000151\n",
      "Epoch: 2 \tTraining Loss: 0.000151\n",
      "Epoch: 3 \tTraining Loss: 0.000151\n",
      "Epoch: 4 \tTraining Loss: 0.000151\n",
      "Epoch: 5 \tTraining Loss: 0.000151\n",
      "Epoch: 6 \tTraining Loss: 0.000151\n",
      "Epoch: 7 \tTraining Loss: 0.000151\n",
      "Epoch: 8 \tTraining Loss: 0.000151\n",
      "Epoch: 9 \tTraining Loss: 0.000151\n",
      "Epoch: 10 \tTraining Loss: 0.000151\n",
      "Epoch: 11 \tTraining Loss: 0.000151\n",
      "Epoch: 12 \tTraining Loss: 0.000151\n",
      "Epoch: 13 \tTraining Loss: 0.000151\n",
      "Epoch: 14 \tTraining Loss: 0.000151\n",
      "Epoch: 15 \tTraining Loss: 0.000151\n",
      "Epoch: 16 \tTraining Loss: 0.000151\n",
      "Epoch: 17 \tTraining Loss: 0.000151\n",
      "Epoch: 18 \tTraining Loss: 0.000151\n",
      "Epoch: 1 \tTraining Loss: 0.000153\n",
      "Epoch: 2 \tTraining Loss: 0.000153\n",
      "Epoch: 3 \tTraining Loss: 0.000153\n",
      "Epoch: 4 \tTraining Loss: 0.000153\n",
      "Epoch: 5 \tTraining Loss: 0.000153\n",
      "Epoch: 6 \tTraining Loss: 0.000153\n",
      "Epoch: 7 \tTraining Loss: 0.000153\n",
      "Epoch: 8 \tTraining Loss: 0.000153\n",
      "Epoch: 9 \tTraining Loss: 0.000153\n",
      "Epoch: 10 \tTraining Loss: 0.000153\n",
      "Epoch: 11 \tTraining Loss: 0.000153\n",
      "Epoch: 12 \tTraining Loss: 0.000153\n",
      "Epoch: 13 \tTraining Loss: 0.000153\n",
      "Epoch: 14 \tTraining Loss: 0.000153\n",
      "Epoch: 15 \tTraining Loss: 0.000153\n",
      "Epoch: 16 \tTraining Loss: 0.000153\n",
      "Epoch: 17 \tTraining Loss: 0.000153\n",
      "Epoch: 18 \tTraining Loss: 0.000153\n",
      "Epoch: 1 \tTraining Loss: 0.000158\n",
      "Epoch: 2 \tTraining Loss: 0.000158\n",
      "Epoch: 3 \tTraining Loss: 0.000158\n",
      "Epoch: 4 \tTraining Loss: 0.000158\n",
      "Epoch: 5 \tTraining Loss: 0.000158\n",
      "Epoch: 6 \tTraining Loss: 0.000158\n",
      "Epoch: 7 \tTraining Loss: 0.000158\n",
      "Epoch: 8 \tTraining Loss: 0.000158\n",
      "Epoch: 9 \tTraining Loss: 0.000158\n",
      "Epoch: 10 \tTraining Loss: 0.000158\n",
      "Epoch: 11 \tTraining Loss: 0.000158\n",
      "Epoch: 12 \tTraining Loss: 0.000158\n",
      "Epoch: 13 \tTraining Loss: 0.000158\n",
      "Epoch: 14 \tTraining Loss: 0.000158\n",
      "Epoch: 15 \tTraining Loss: 0.000158\n",
      "Epoch: 16 \tTraining Loss: 0.000158\n",
      "Epoch: 17 \tTraining Loss: 0.000158\n",
      "Epoch: 18 \tTraining Loss: 0.000158\n",
      "Epoch: 1 \tTraining Loss: 0.000353\n",
      "Epoch: 2 \tTraining Loss: 0.000353\n",
      "Epoch: 3 \tTraining Loss: 0.000353\n",
      "Epoch: 4 \tTraining Loss: 0.000353\n",
      "Epoch: 5 \tTraining Loss: 0.000353\n",
      "Epoch: 6 \tTraining Loss: 0.000353\n",
      "Epoch: 7 \tTraining Loss: 0.000353\n",
      "Epoch: 8 \tTraining Loss: 0.000353\n",
      "Epoch: 9 \tTraining Loss: 0.000353\n",
      "Epoch: 10 \tTraining Loss: 0.000353\n",
      "Epoch: 11 \tTraining Loss: 0.000353\n",
      "Epoch: 12 \tTraining Loss: 0.000353\n",
      "Epoch: 13 \tTraining Loss: 0.000353\n",
      "Epoch: 14 \tTraining Loss: 0.000353\n",
      "Epoch: 15 \tTraining Loss: 0.000353\n",
      "Epoch: 16 \tTraining Loss: 0.000353\n",
      "Epoch: 17 \tTraining Loss: 0.000353\n",
      "Epoch: 18 \tTraining Loss: 0.000353\n",
      "Epoch: 1 \tTraining Loss: 0.000134\n",
      "Epoch: 2 \tTraining Loss: 0.000134\n",
      "Epoch: 3 \tTraining Loss: 0.000134\n",
      "Epoch: 4 \tTraining Loss: 0.000134\n",
      "Epoch: 5 \tTraining Loss: 0.000134\n",
      "Epoch: 6 \tTraining Loss: 0.000134\n",
      "Epoch: 7 \tTraining Loss: 0.000134\n",
      "Epoch: 8 \tTraining Loss: 0.000134\n",
      "Epoch: 9 \tTraining Loss: 0.000134\n",
      "Epoch: 10 \tTraining Loss: 0.000134\n",
      "Epoch: 11 \tTraining Loss: 0.000134\n",
      "Epoch: 12 \tTraining Loss: 0.000134\n",
      "Epoch: 13 \tTraining Loss: 0.000134\n",
      "Epoch: 14 \tTraining Loss: 0.000134\n",
      "Epoch: 15 \tTraining Loss: 0.000134\n",
      "Epoch: 16 \tTraining Loss: 0.000134\n",
      "Epoch: 17 \tTraining Loss: 0.000134\n",
      "Epoch: 18 \tTraining Loss: 0.000134\n",
      "Epoch: 1 \tTraining Loss: 0.000156\n",
      "Epoch: 2 \tTraining Loss: 0.000156\n",
      "Epoch: 3 \tTraining Loss: 0.000156\n",
      "Epoch: 4 \tTraining Loss: 0.000156\n",
      "Epoch: 5 \tTraining Loss: 0.000156\n",
      "Epoch: 6 \tTraining Loss: 0.000156\n",
      "Epoch: 7 \tTraining Loss: 0.000156\n",
      "Epoch: 8 \tTraining Loss: 0.000156\n",
      "Epoch: 9 \tTraining Loss: 0.000156\n",
      "Epoch: 10 \tTraining Loss: 0.000156\n",
      "Epoch: 11 \tTraining Loss: 0.000156\n",
      "Epoch: 12 \tTraining Loss: 0.000156\n",
      "Epoch: 13 \tTraining Loss: 0.000156\n",
      "Epoch: 14 \tTraining Loss: 0.000156\n",
      "Epoch: 15 \tTraining Loss: 0.000156\n",
      "Epoch: 16 \tTraining Loss: 0.000156\n",
      "Epoch: 17 \tTraining Loss: 0.000156\n",
      "Epoch: 18 \tTraining Loss: 0.000156\n",
      "Epoch: 1 \tTraining Loss: 0.000198\n",
      "Epoch: 2 \tTraining Loss: 0.000198\n",
      "Epoch: 3 \tTraining Loss: 0.000198\n",
      "Epoch: 4 \tTraining Loss: 0.000198\n",
      "Epoch: 5 \tTraining Loss: 0.000198\n",
      "Epoch: 6 \tTraining Loss: 0.000198\n",
      "Epoch: 7 \tTraining Loss: 0.000198\n",
      "Epoch: 8 \tTraining Loss: 0.000198\n",
      "Epoch: 9 \tTraining Loss: 0.000198\n",
      "Epoch: 10 \tTraining Loss: 0.000198\n",
      "Epoch: 11 \tTraining Loss: 0.000198\n",
      "Epoch: 12 \tTraining Loss: 0.000198\n",
      "Epoch: 13 \tTraining Loss: 0.000198\n",
      "Epoch: 14 \tTraining Loss: 0.000198\n",
      "Epoch: 15 \tTraining Loss: 0.000198\n",
      "Epoch: 16 \tTraining Loss: 0.000198\n",
      "Epoch: 17 \tTraining Loss: 0.000198\n",
      "Epoch: 18 \tTraining Loss: 0.000198\n",
      "Epoch: 1 \tTraining Loss: 0.000378\n",
      "Epoch: 2 \tTraining Loss: 0.000378\n",
      "Epoch: 3 \tTraining Loss: 0.000378\n",
      "Epoch: 4 \tTraining Loss: 0.000378\n",
      "Epoch: 5 \tTraining Loss: 0.000378\n",
      "Epoch: 6 \tTraining Loss: 0.000378\n",
      "Epoch: 7 \tTraining Loss: 0.000378\n",
      "Epoch: 8 \tTraining Loss: 0.000378\n",
      "Epoch: 9 \tTraining Loss: 0.000378\n",
      "Epoch: 10 \tTraining Loss: 0.000378\n",
      "Epoch: 11 \tTraining Loss: 0.000378\n",
      "Epoch: 12 \tTraining Loss: 0.000378\n",
      "Epoch: 13 \tTraining Loss: 0.000378\n",
      "Epoch: 14 \tTraining Loss: 0.000378\n",
      "Epoch: 15 \tTraining Loss: 0.000378\n",
      "Epoch: 16 \tTraining Loss: 0.000378\n",
      "Epoch: 17 \tTraining Loss: 0.000378\n",
      "Epoch: 18 \tTraining Loss: 0.000378\n",
      "Epoch: 1 \tTraining Loss: 0.000279\n",
      "Epoch: 2 \tTraining Loss: 0.000279\n",
      "Epoch: 3 \tTraining Loss: 0.000279\n",
      "Epoch: 4 \tTraining Loss: 0.000279\n",
      "Epoch: 5 \tTraining Loss: 0.000279\n",
      "Epoch: 6 \tTraining Loss: 0.000279\n",
      "Epoch: 7 \tTraining Loss: 0.000279\n",
      "Epoch: 8 \tTraining Loss: 0.000279\n",
      "Epoch: 9 \tTraining Loss: 0.000279\n",
      "Epoch: 10 \tTraining Loss: 0.000279\n",
      "Epoch: 11 \tTraining Loss: 0.000279\n",
      "Epoch: 12 \tTraining Loss: 0.000279\n",
      "Epoch: 13 \tTraining Loss: 0.000279\n",
      "Epoch: 14 \tTraining Loss: 0.000279\n",
      "Epoch: 15 \tTraining Loss: 0.000279\n",
      "Epoch: 16 \tTraining Loss: 0.000279\n",
      "Epoch: 17 \tTraining Loss: 0.000279\n",
      "Epoch: 18 \tTraining Loss: 0.000279\n",
      "Epoch: 1 \tTraining Loss: 0.000210\n",
      "Epoch: 2 \tTraining Loss: 0.000210\n",
      "Epoch: 3 \tTraining Loss: 0.000210\n",
      "Epoch: 4 \tTraining Loss: 0.000210\n",
      "Epoch: 5 \tTraining Loss: 0.000210\n",
      "Epoch: 6 \tTraining Loss: 0.000210\n",
      "Epoch: 7 \tTraining Loss: 0.000210\n",
      "Epoch: 8 \tTraining Loss: 0.000210\n",
      "Epoch: 9 \tTraining Loss: 0.000210\n",
      "Epoch: 10 \tTraining Loss: 0.000210\n",
      "Epoch: 11 \tTraining Loss: 0.000210\n",
      "Epoch: 12 \tTraining Loss: 0.000210\n",
      "Epoch: 13 \tTraining Loss: 0.000210\n",
      "Epoch: 14 \tTraining Loss: 0.000210\n",
      "Epoch: 15 \tTraining Loss: 0.000210\n",
      "Epoch: 16 \tTraining Loss: 0.000210\n",
      "Epoch: 17 \tTraining Loss: 0.000210\n",
      "Epoch: 18 \tTraining Loss: 0.000210\n",
      "Epoch: 1 \tTraining Loss: 0.000343\n",
      "Epoch: 2 \tTraining Loss: 0.000343\n",
      "Epoch: 3 \tTraining Loss: 0.000343\n",
      "Epoch: 4 \tTraining Loss: 0.000343\n",
      "Epoch: 5 \tTraining Loss: 0.000343\n",
      "Epoch: 6 \tTraining Loss: 0.000343\n",
      "Epoch: 7 \tTraining Loss: 0.000343\n",
      "Epoch: 8 \tTraining Loss: 0.000343\n",
      "Epoch: 9 \tTraining Loss: 0.000343\n",
      "Epoch: 10 \tTraining Loss: 0.000343\n",
      "Epoch: 11 \tTraining Loss: 0.000343\n",
      "Epoch: 12 \tTraining Loss: 0.000343\n",
      "Epoch: 13 \tTraining Loss: 0.000343\n",
      "Epoch: 14 \tTraining Loss: 0.000343\n",
      "Epoch: 15 \tTraining Loss: 0.000343\n",
      "Epoch: 16 \tTraining Loss: 0.000343\n",
      "Epoch: 17 \tTraining Loss: 0.000343\n",
      "Epoch: 18 \tTraining Loss: 0.000343\n",
      "Epoch: 1 \tTraining Loss: 0.000157\n",
      "Epoch: 2 \tTraining Loss: 0.000157\n",
      "Epoch: 3 \tTraining Loss: 0.000157\n",
      "Epoch: 4 \tTraining Loss: 0.000157\n",
      "Epoch: 5 \tTraining Loss: 0.000157\n",
      "Epoch: 6 \tTraining Loss: 0.000157\n",
      "Epoch: 7 \tTraining Loss: 0.000157\n",
      "Epoch: 8 \tTraining Loss: 0.000157\n",
      "Epoch: 9 \tTraining Loss: 0.000157\n",
      "Epoch: 10 \tTraining Loss: 0.000157\n",
      "Epoch: 11 \tTraining Loss: 0.000157\n",
      "Epoch: 12 \tTraining Loss: 0.000157\n",
      "Epoch: 13 \tTraining Loss: 0.000157\n",
      "Epoch: 14 \tTraining Loss: 0.000157\n",
      "Epoch: 15 \tTraining Loss: 0.000157\n",
      "Epoch: 16 \tTraining Loss: 0.000157\n",
      "Epoch: 17 \tTraining Loss: 0.000157\n",
      "Epoch: 18 \tTraining Loss: 0.000157\n",
      "Epoch: 1 \tTraining Loss: 0.000269\n",
      "Epoch: 2 \tTraining Loss: 0.000269\n",
      "Epoch: 3 \tTraining Loss: 0.000269\n",
      "Epoch: 4 \tTraining Loss: 0.000269\n",
      "Epoch: 5 \tTraining Loss: 0.000269\n",
      "Epoch: 6 \tTraining Loss: 0.000269\n",
      "Epoch: 7 \tTraining Loss: 0.000269\n",
      "Epoch: 8 \tTraining Loss: 0.000269\n",
      "Epoch: 9 \tTraining Loss: 0.000269\n",
      "Epoch: 10 \tTraining Loss: 0.000269\n",
      "Epoch: 11 \tTraining Loss: 0.000269\n",
      "Epoch: 12 \tTraining Loss: 0.000269\n",
      "Epoch: 13 \tTraining Loss: 0.000269\n",
      "Epoch: 14 \tTraining Loss: 0.000269\n",
      "Epoch: 15 \tTraining Loss: 0.000269\n",
      "Epoch: 16 \tTraining Loss: 0.000269\n",
      "Epoch: 17 \tTraining Loss: 0.000269\n",
      "Epoch: 18 \tTraining Loss: 0.000269\n",
      "Epoch: 1 \tTraining Loss: 0.000308\n",
      "Epoch: 2 \tTraining Loss: 0.000308\n",
      "Epoch: 3 \tTraining Loss: 0.000308\n",
      "Epoch: 4 \tTraining Loss: 0.000308\n",
      "Epoch: 5 \tTraining Loss: 0.000308\n",
      "Epoch: 6 \tTraining Loss: 0.000308\n",
      "Epoch: 7 \tTraining Loss: 0.000308\n",
      "Epoch: 8 \tTraining Loss: 0.000308\n",
      "Epoch: 9 \tTraining Loss: 0.000308\n",
      "Epoch: 10 \tTraining Loss: 0.000308\n",
      "Epoch: 11 \tTraining Loss: 0.000308\n",
      "Epoch: 12 \tTraining Loss: 0.000308\n",
      "Epoch: 13 \tTraining Loss: 0.000308\n",
      "Epoch: 14 \tTraining Loss: 0.000308\n",
      "Epoch: 15 \tTraining Loss: 0.000308\n",
      "Epoch: 16 \tTraining Loss: 0.000308\n",
      "Epoch: 17 \tTraining Loss: 0.000308\n",
      "Epoch: 18 \tTraining Loss: 0.000308\n",
      "Epoch: 1 \tTraining Loss: 0.000234\n",
      "Epoch: 2 \tTraining Loss: 0.000234\n",
      "Epoch: 3 \tTraining Loss: 0.000234\n",
      "Epoch: 4 \tTraining Loss: 0.000234\n",
      "Epoch: 5 \tTraining Loss: 0.000234\n",
      "Epoch: 6 \tTraining Loss: 0.000234\n",
      "Epoch: 7 \tTraining Loss: 0.000234\n",
      "Epoch: 8 \tTraining Loss: 0.000234\n",
      "Epoch: 9 \tTraining Loss: 0.000234\n",
      "Epoch: 10 \tTraining Loss: 0.000234\n",
      "Epoch: 11 \tTraining Loss: 0.000234\n",
      "Epoch: 12 \tTraining Loss: 0.000234\n",
      "Epoch: 13 \tTraining Loss: 0.000234\n",
      "Epoch: 14 \tTraining Loss: 0.000234\n",
      "Epoch: 15 \tTraining Loss: 0.000234\n",
      "Epoch: 16 \tTraining Loss: 0.000234\n",
      "Epoch: 17 \tTraining Loss: 0.000234\n",
      "Epoch: 18 \tTraining Loss: 0.000234\n",
      "Epoch: 1 \tTraining Loss: 0.000072\n",
      "Epoch: 2 \tTraining Loss: 0.000072\n",
      "Epoch: 3 \tTraining Loss: 0.000072\n",
      "Epoch: 4 \tTraining Loss: 0.000072\n",
      "Epoch: 5 \tTraining Loss: 0.000072\n",
      "Epoch: 6 \tTraining Loss: 0.000072\n",
      "Epoch: 7 \tTraining Loss: 0.000072\n",
      "Epoch: 8 \tTraining Loss: 0.000072\n",
      "Epoch: 9 \tTraining Loss: 0.000072\n",
      "Epoch: 10 \tTraining Loss: 0.000072\n",
      "Epoch: 11 \tTraining Loss: 0.000072\n",
      "Epoch: 12 \tTraining Loss: 0.000072\n",
      "Epoch: 13 \tTraining Loss: 0.000072\n",
      "Epoch: 14 \tTraining Loss: 0.000072\n",
      "Epoch: 15 \tTraining Loss: 0.000072\n",
      "Epoch: 16 \tTraining Loss: 0.000072\n",
      "Epoch: 17 \tTraining Loss: 0.000072\n",
      "Epoch: 18 \tTraining Loss: 0.000072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000270\n",
      "Epoch: 2 \tTraining Loss: 0.000270\n",
      "Epoch: 3 \tTraining Loss: 0.000270\n",
      "Epoch: 4 \tTraining Loss: 0.000270\n",
      "Epoch: 5 \tTraining Loss: 0.000270\n",
      "Epoch: 6 \tTraining Loss: 0.000270\n",
      "Epoch: 7 \tTraining Loss: 0.000270\n",
      "Epoch: 8 \tTraining Loss: 0.000270\n",
      "Epoch: 9 \tTraining Loss: 0.000270\n",
      "Epoch: 10 \tTraining Loss: 0.000270\n",
      "Epoch: 11 \tTraining Loss: 0.000270\n",
      "Epoch: 12 \tTraining Loss: 0.000270\n",
      "Epoch: 13 \tTraining Loss: 0.000270\n",
      "Epoch: 14 \tTraining Loss: 0.000270\n",
      "Epoch: 15 \tTraining Loss: 0.000270\n",
      "Epoch: 16 \tTraining Loss: 0.000270\n",
      "Epoch: 17 \tTraining Loss: 0.000270\n",
      "Epoch: 18 \tTraining Loss: 0.000270\n",
      "Epoch: 1 \tTraining Loss: 0.000153\n",
      "Epoch: 2 \tTraining Loss: 0.000153\n",
      "Epoch: 3 \tTraining Loss: 0.000153\n",
      "Epoch: 4 \tTraining Loss: 0.000153\n",
      "Epoch: 5 \tTraining Loss: 0.000153\n",
      "Epoch: 6 \tTraining Loss: 0.000153\n",
      "Epoch: 7 \tTraining Loss: 0.000153\n",
      "Epoch: 8 \tTraining Loss: 0.000153\n",
      "Epoch: 9 \tTraining Loss: 0.000153\n",
      "Epoch: 10 \tTraining Loss: 0.000153\n",
      "Epoch: 11 \tTraining Loss: 0.000153\n",
      "Epoch: 12 \tTraining Loss: 0.000153\n",
      "Epoch: 13 \tTraining Loss: 0.000153\n",
      "Epoch: 14 \tTraining Loss: 0.000153\n",
      "Epoch: 15 \tTraining Loss: 0.000153\n",
      "Epoch: 16 \tTraining Loss: 0.000153\n",
      "Epoch: 17 \tTraining Loss: 0.000153\n",
      "Epoch: 18 \tTraining Loss: 0.000153\n",
      "Epoch: 1 \tTraining Loss: 0.000135\n",
      "Epoch: 2 \tTraining Loss: 0.000135\n",
      "Epoch: 3 \tTraining Loss: 0.000135\n",
      "Epoch: 4 \tTraining Loss: 0.000135\n",
      "Epoch: 5 \tTraining Loss: 0.000135\n",
      "Epoch: 6 \tTraining Loss: 0.000135\n",
      "Epoch: 7 \tTraining Loss: 0.000135\n",
      "Epoch: 8 \tTraining Loss: 0.000135\n",
      "Epoch: 9 \tTraining Loss: 0.000135\n",
      "Epoch: 10 \tTraining Loss: 0.000135\n",
      "Epoch: 11 \tTraining Loss: 0.000135\n",
      "Epoch: 12 \tTraining Loss: 0.000135\n",
      "Epoch: 13 \tTraining Loss: 0.000135\n",
      "Epoch: 14 \tTraining Loss: 0.000135\n",
      "Epoch: 15 \tTraining Loss: 0.000135\n",
      "Epoch: 16 \tTraining Loss: 0.000135\n",
      "Epoch: 17 \tTraining Loss: 0.000135\n",
      "Epoch: 18 \tTraining Loss: 0.000135\n",
      "Epoch: 1 \tTraining Loss: 0.000377\n",
      "Epoch: 2 \tTraining Loss: 0.000377\n",
      "Epoch: 3 \tTraining Loss: 0.000377\n",
      "Epoch: 4 \tTraining Loss: 0.000377\n",
      "Epoch: 5 \tTraining Loss: 0.000377\n",
      "Epoch: 6 \tTraining Loss: 0.000377\n",
      "Epoch: 7 \tTraining Loss: 0.000377\n",
      "Epoch: 8 \tTraining Loss: 0.000377\n",
      "Epoch: 9 \tTraining Loss: 0.000377\n",
      "Epoch: 10 \tTraining Loss: 0.000377\n",
      "Epoch: 11 \tTraining Loss: 0.000377\n",
      "Epoch: 12 \tTraining Loss: 0.000377\n",
      "Epoch: 13 \tTraining Loss: 0.000377\n",
      "Epoch: 14 \tTraining Loss: 0.000377\n",
      "Epoch: 15 \tTraining Loss: 0.000377\n",
      "Epoch: 16 \tTraining Loss: 0.000377\n",
      "Epoch: 17 \tTraining Loss: 0.000377\n",
      "Epoch: 18 \tTraining Loss: 0.000377\n",
      "Epoch: 1 \tTraining Loss: 0.000191\n",
      "Epoch: 2 \tTraining Loss: 0.000191\n",
      "Epoch: 3 \tTraining Loss: 0.000191\n",
      "Epoch: 4 \tTraining Loss: 0.000191\n",
      "Epoch: 5 \tTraining Loss: 0.000191\n",
      "Epoch: 6 \tTraining Loss: 0.000191\n",
      "Epoch: 7 \tTraining Loss: 0.000191\n",
      "Epoch: 8 \tTraining Loss: 0.000191\n",
      "Epoch: 9 \tTraining Loss: 0.000191\n",
      "Epoch: 10 \tTraining Loss: 0.000191\n",
      "Epoch: 11 \tTraining Loss: 0.000191\n",
      "Epoch: 12 \tTraining Loss: 0.000191\n",
      "Epoch: 13 \tTraining Loss: 0.000191\n",
      "Epoch: 14 \tTraining Loss: 0.000191\n",
      "Epoch: 15 \tTraining Loss: 0.000191\n",
      "Epoch: 16 \tTraining Loss: 0.000191\n",
      "Epoch: 17 \tTraining Loss: 0.000191\n",
      "Epoch: 18 \tTraining Loss: 0.000191\n",
      "Epoch: 1 \tTraining Loss: 0.000123\n",
      "Epoch: 2 \tTraining Loss: 0.000123\n",
      "Epoch: 3 \tTraining Loss: 0.000123\n",
      "Epoch: 4 \tTraining Loss: 0.000123\n",
      "Epoch: 5 \tTraining Loss: 0.000123\n",
      "Epoch: 6 \tTraining Loss: 0.000123\n",
      "Epoch: 7 \tTraining Loss: 0.000123\n",
      "Epoch: 8 \tTraining Loss: 0.000123\n",
      "Epoch: 9 \tTraining Loss: 0.000123\n",
      "Epoch: 10 \tTraining Loss: 0.000123\n",
      "Epoch: 11 \tTraining Loss: 0.000123\n",
      "Epoch: 12 \tTraining Loss: 0.000123\n",
      "Epoch: 13 \tTraining Loss: 0.000123\n",
      "Epoch: 14 \tTraining Loss: 0.000123\n",
      "Epoch: 15 \tTraining Loss: 0.000123\n",
      "Epoch: 16 \tTraining Loss: 0.000123\n",
      "Epoch: 17 \tTraining Loss: 0.000123\n",
      "Epoch: 18 \tTraining Loss: 0.000123\n",
      "Epoch: 1 \tTraining Loss: 0.000167\n",
      "Epoch: 2 \tTraining Loss: 0.000167\n",
      "Epoch: 3 \tTraining Loss: 0.000167\n",
      "Epoch: 4 \tTraining Loss: 0.000167\n",
      "Epoch: 5 \tTraining Loss: 0.000167\n",
      "Epoch: 6 \tTraining Loss: 0.000167\n",
      "Epoch: 7 \tTraining Loss: 0.000167\n",
      "Epoch: 8 \tTraining Loss: 0.000167\n",
      "Epoch: 9 \tTraining Loss: 0.000167\n",
      "Epoch: 10 \tTraining Loss: 0.000167\n",
      "Epoch: 11 \tTraining Loss: 0.000167\n",
      "Epoch: 12 \tTraining Loss: 0.000167\n",
      "Epoch: 13 \tTraining Loss: 0.000167\n",
      "Epoch: 14 \tTraining Loss: 0.000167\n",
      "Epoch: 15 \tTraining Loss: 0.000167\n",
      "Epoch: 16 \tTraining Loss: 0.000167\n",
      "Epoch: 17 \tTraining Loss: 0.000167\n",
      "Epoch: 18 \tTraining Loss: 0.000167\n",
      "Epoch: 1 \tTraining Loss: 0.000191\n",
      "Epoch: 2 \tTraining Loss: 0.000191\n",
      "Epoch: 3 \tTraining Loss: 0.000191\n",
      "Epoch: 4 \tTraining Loss: 0.000191\n",
      "Epoch: 5 \tTraining Loss: 0.000191\n",
      "Epoch: 6 \tTraining Loss: 0.000191\n",
      "Epoch: 7 \tTraining Loss: 0.000191\n",
      "Epoch: 8 \tTraining Loss: 0.000191\n",
      "Epoch: 9 \tTraining Loss: 0.000191\n",
      "Epoch: 10 \tTraining Loss: 0.000191\n",
      "Epoch: 11 \tTraining Loss: 0.000191\n",
      "Epoch: 12 \tTraining Loss: 0.000191\n",
      "Epoch: 13 \tTraining Loss: 0.000191\n",
      "Epoch: 14 \tTraining Loss: 0.000191\n",
      "Epoch: 15 \tTraining Loss: 0.000191\n",
      "Epoch: 16 \tTraining Loss: 0.000191\n",
      "Epoch: 17 \tTraining Loss: 0.000191\n",
      "Epoch: 18 \tTraining Loss: 0.000191\n",
      "Epoch: 1 \tTraining Loss: 0.000140\n",
      "Epoch: 2 \tTraining Loss: 0.000140\n",
      "Epoch: 3 \tTraining Loss: 0.000140\n",
      "Epoch: 4 \tTraining Loss: 0.000140\n",
      "Epoch: 5 \tTraining Loss: 0.000140\n",
      "Epoch: 6 \tTraining Loss: 0.000140\n",
      "Epoch: 7 \tTraining Loss: 0.000140\n",
      "Epoch: 8 \tTraining Loss: 0.000140\n",
      "Epoch: 9 \tTraining Loss: 0.000140\n",
      "Epoch: 10 \tTraining Loss: 0.000140\n",
      "Epoch: 11 \tTraining Loss: 0.000140\n",
      "Epoch: 12 \tTraining Loss: 0.000140\n",
      "Epoch: 13 \tTraining Loss: 0.000140\n",
      "Epoch: 14 \tTraining Loss: 0.000140\n",
      "Epoch: 15 \tTraining Loss: 0.000140\n",
      "Epoch: 16 \tTraining Loss: 0.000140\n",
      "Epoch: 17 \tTraining Loss: 0.000140\n",
      "Epoch: 18 \tTraining Loss: 0.000140\n",
      "Epoch: 1 \tTraining Loss: 0.000155\n",
      "Epoch: 2 \tTraining Loss: 0.000155\n",
      "Epoch: 3 \tTraining Loss: 0.000155\n",
      "Epoch: 4 \tTraining Loss: 0.000155\n",
      "Epoch: 5 \tTraining Loss: 0.000155\n",
      "Epoch: 6 \tTraining Loss: 0.000155\n",
      "Epoch: 7 \tTraining Loss: 0.000155\n",
      "Epoch: 8 \tTraining Loss: 0.000155\n",
      "Epoch: 9 \tTraining Loss: 0.000155\n",
      "Epoch: 10 \tTraining Loss: 0.000155\n",
      "Epoch: 11 \tTraining Loss: 0.000155\n",
      "Epoch: 12 \tTraining Loss: 0.000155\n",
      "Epoch: 13 \tTraining Loss: 0.000155\n",
      "Epoch: 14 \tTraining Loss: 0.000155\n",
      "Epoch: 15 \tTraining Loss: 0.000155\n",
      "Epoch: 16 \tTraining Loss: 0.000155\n",
      "Epoch: 17 \tTraining Loss: 0.000155\n",
      "Epoch: 18 \tTraining Loss: 0.000155\n",
      "Epoch: 1 \tTraining Loss: 0.000102\n",
      "Epoch: 2 \tTraining Loss: 0.000102\n",
      "Epoch: 3 \tTraining Loss: 0.000102\n",
      "Epoch: 4 \tTraining Loss: 0.000102\n",
      "Epoch: 5 \tTraining Loss: 0.000102\n",
      "Epoch: 6 \tTraining Loss: 0.000102\n",
      "Epoch: 7 \tTraining Loss: 0.000102\n",
      "Epoch: 8 \tTraining Loss: 0.000102\n",
      "Epoch: 9 \tTraining Loss: 0.000102\n",
      "Epoch: 10 \tTraining Loss: 0.000102\n",
      "Epoch: 11 \tTraining Loss: 0.000102\n",
      "Epoch: 12 \tTraining Loss: 0.000102\n",
      "Epoch: 13 \tTraining Loss: 0.000102\n",
      "Epoch: 14 \tTraining Loss: 0.000102\n",
      "Epoch: 15 \tTraining Loss: 0.000102\n",
      "Epoch: 16 \tTraining Loss: 0.000102\n",
      "Epoch: 17 \tTraining Loss: 0.000102\n",
      "Epoch: 18 \tTraining Loss: 0.000102\n",
      "Epoch: 1 \tTraining Loss: 0.000275\n",
      "Epoch: 2 \tTraining Loss: 0.000275\n",
      "Epoch: 3 \tTraining Loss: 0.000275\n",
      "Epoch: 4 \tTraining Loss: 0.000275\n",
      "Epoch: 5 \tTraining Loss: 0.000275\n",
      "Epoch: 6 \tTraining Loss: 0.000275\n",
      "Epoch: 7 \tTraining Loss: 0.000275\n",
      "Epoch: 8 \tTraining Loss: 0.000275\n",
      "Epoch: 9 \tTraining Loss: 0.000275\n",
      "Epoch: 10 \tTraining Loss: 0.000275\n",
      "Epoch: 11 \tTraining Loss: 0.000275\n",
      "Epoch: 12 \tTraining Loss: 0.000275\n",
      "Epoch: 13 \tTraining Loss: 0.000275\n",
      "Epoch: 14 \tTraining Loss: 0.000275\n",
      "Epoch: 15 \tTraining Loss: 0.000275\n",
      "Epoch: 16 \tTraining Loss: 0.000275\n",
      "Epoch: 17 \tTraining Loss: 0.000275\n",
      "Epoch: 18 \tTraining Loss: 0.000275\n",
      "Epoch: 1 \tTraining Loss: 0.000146\n",
      "Epoch: 2 \tTraining Loss: 0.000146\n",
      "Epoch: 3 \tTraining Loss: 0.000146\n",
      "Epoch: 4 \tTraining Loss: 0.000146\n",
      "Epoch: 5 \tTraining Loss: 0.000146\n",
      "Epoch: 6 \tTraining Loss: 0.000146\n",
      "Epoch: 7 \tTraining Loss: 0.000146\n",
      "Epoch: 8 \tTraining Loss: 0.000146\n",
      "Epoch: 9 \tTraining Loss: 0.000146\n",
      "Epoch: 10 \tTraining Loss: 0.000146\n",
      "Epoch: 11 \tTraining Loss: 0.000146\n",
      "Epoch: 12 \tTraining Loss: 0.000146\n",
      "Epoch: 13 \tTraining Loss: 0.000146\n",
      "Epoch: 14 \tTraining Loss: 0.000146\n",
      "Epoch: 15 \tTraining Loss: 0.000146\n",
      "Epoch: 16 \tTraining Loss: 0.000146\n",
      "Epoch: 17 \tTraining Loss: 0.000146\n",
      "Epoch: 18 \tTraining Loss: 0.000146\n",
      "Epoch: 1 \tTraining Loss: 0.000257\n",
      "Epoch: 2 \tTraining Loss: 0.000257\n",
      "Epoch: 3 \tTraining Loss: 0.000257\n",
      "Epoch: 4 \tTraining Loss: 0.000257\n",
      "Epoch: 5 \tTraining Loss: 0.000257\n",
      "Epoch: 6 \tTraining Loss: 0.000257\n",
      "Epoch: 7 \tTraining Loss: 0.000257\n",
      "Epoch: 8 \tTraining Loss: 0.000257\n",
      "Epoch: 9 \tTraining Loss: 0.000257\n",
      "Epoch: 10 \tTraining Loss: 0.000257\n",
      "Epoch: 11 \tTraining Loss: 0.000257\n",
      "Epoch: 12 \tTraining Loss: 0.000257\n",
      "Epoch: 13 \tTraining Loss: 0.000257\n",
      "Epoch: 14 \tTraining Loss: 0.000257\n",
      "Epoch: 15 \tTraining Loss: 0.000257\n",
      "Epoch: 16 \tTraining Loss: 0.000257\n",
      "Epoch: 17 \tTraining Loss: 0.000257\n",
      "Epoch: 18 \tTraining Loss: 0.000257\n",
      "Epoch: 1 \tTraining Loss: 0.000100\n",
      "Epoch: 2 \tTraining Loss: 0.000100\n",
      "Epoch: 3 \tTraining Loss: 0.000100\n",
      "Epoch: 4 \tTraining Loss: 0.000100\n",
      "Epoch: 5 \tTraining Loss: 0.000100\n",
      "Epoch: 6 \tTraining Loss: 0.000100\n",
      "Epoch: 7 \tTraining Loss: 0.000100\n",
      "Epoch: 8 \tTraining Loss: 0.000100\n",
      "Epoch: 9 \tTraining Loss: 0.000100\n",
      "Epoch: 10 \tTraining Loss: 0.000100\n",
      "Epoch: 11 \tTraining Loss: 0.000100\n",
      "Epoch: 12 \tTraining Loss: 0.000100\n",
      "Epoch: 13 \tTraining Loss: 0.000100\n",
      "Epoch: 14 \tTraining Loss: 0.000100\n",
      "Epoch: 15 \tTraining Loss: 0.000100\n",
      "Epoch: 16 \tTraining Loss: 0.000100\n",
      "Epoch: 17 \tTraining Loss: 0.000100\n",
      "Epoch: 18 \tTraining Loss: 0.000100\n",
      "Epoch: 1 \tTraining Loss: 0.000229\n",
      "Epoch: 2 \tTraining Loss: 0.000229\n",
      "Epoch: 3 \tTraining Loss: 0.000229\n",
      "Epoch: 4 \tTraining Loss: 0.000229\n",
      "Epoch: 5 \tTraining Loss: 0.000229\n",
      "Epoch: 6 \tTraining Loss: 0.000229\n",
      "Epoch: 7 \tTraining Loss: 0.000229\n",
      "Epoch: 8 \tTraining Loss: 0.000229\n",
      "Epoch: 9 \tTraining Loss: 0.000229\n",
      "Epoch: 10 \tTraining Loss: 0.000229\n",
      "Epoch: 11 \tTraining Loss: 0.000229\n",
      "Epoch: 12 \tTraining Loss: 0.000229\n",
      "Epoch: 13 \tTraining Loss: 0.000229\n",
      "Epoch: 14 \tTraining Loss: 0.000229\n",
      "Epoch: 15 \tTraining Loss: 0.000229\n",
      "Epoch: 16 \tTraining Loss: 0.000229\n",
      "Epoch: 17 \tTraining Loss: 0.000229\n",
      "Epoch: 18 \tTraining Loss: 0.000229\n",
      "Epoch: 1 \tTraining Loss: 0.000212\n",
      "Epoch: 2 \tTraining Loss: 0.000212\n",
      "Epoch: 3 \tTraining Loss: 0.000212\n",
      "Epoch: 4 \tTraining Loss: 0.000212\n",
      "Epoch: 5 \tTraining Loss: 0.000212\n",
      "Epoch: 6 \tTraining Loss: 0.000212\n",
      "Epoch: 7 \tTraining Loss: 0.000212\n",
      "Epoch: 8 \tTraining Loss: 0.000212\n",
      "Epoch: 9 \tTraining Loss: 0.000212\n",
      "Epoch: 10 \tTraining Loss: 0.000212\n",
      "Epoch: 11 \tTraining Loss: 0.000212\n",
      "Epoch: 12 \tTraining Loss: 0.000212\n",
      "Epoch: 13 \tTraining Loss: 0.000212\n",
      "Epoch: 14 \tTraining Loss: 0.000212\n",
      "Epoch: 15 \tTraining Loss: 0.000212\n",
      "Epoch: 16 \tTraining Loss: 0.000212\n",
      "Epoch: 17 \tTraining Loss: 0.000212\n",
      "Epoch: 18 \tTraining Loss: 0.000212\n",
      "Epoch: 1 \tTraining Loss: 0.000193\n",
      "Epoch: 2 \tTraining Loss: 0.000193\n",
      "Epoch: 3 \tTraining Loss: 0.000193\n",
      "Epoch: 4 \tTraining Loss: 0.000193\n",
      "Epoch: 5 \tTraining Loss: 0.000193\n",
      "Epoch: 6 \tTraining Loss: 0.000193\n",
      "Epoch: 7 \tTraining Loss: 0.000193\n",
      "Epoch: 8 \tTraining Loss: 0.000193\n",
      "Epoch: 9 \tTraining Loss: 0.000193\n",
      "Epoch: 10 \tTraining Loss: 0.000193\n",
      "Epoch: 11 \tTraining Loss: 0.000193\n",
      "Epoch: 12 \tTraining Loss: 0.000193\n",
      "Epoch: 13 \tTraining Loss: 0.000193\n",
      "Epoch: 14 \tTraining Loss: 0.000193\n",
      "Epoch: 15 \tTraining Loss: 0.000193\n",
      "Epoch: 16 \tTraining Loss: 0.000193\n",
      "Epoch: 17 \tTraining Loss: 0.000193\n",
      "Epoch: 18 \tTraining Loss: 0.000193\n",
      "Epoch: 1 \tTraining Loss: 0.000297\n",
      "Epoch: 2 \tTraining Loss: 0.000297\n",
      "Epoch: 3 \tTraining Loss: 0.000297\n",
      "Epoch: 4 \tTraining Loss: 0.000297\n",
      "Epoch: 5 \tTraining Loss: 0.000297\n",
      "Epoch: 6 \tTraining Loss: 0.000297\n",
      "Epoch: 7 \tTraining Loss: 0.000297\n",
      "Epoch: 8 \tTraining Loss: 0.000297\n",
      "Epoch: 9 \tTraining Loss: 0.000297\n",
      "Epoch: 10 \tTraining Loss: 0.000297\n",
      "Epoch: 11 \tTraining Loss: 0.000297\n",
      "Epoch: 12 \tTraining Loss: 0.000297\n",
      "Epoch: 13 \tTraining Loss: 0.000297\n",
      "Epoch: 14 \tTraining Loss: 0.000297\n",
      "Epoch: 15 \tTraining Loss: 0.000297\n",
      "Epoch: 16 \tTraining Loss: 0.000297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 \tTraining Loss: 0.000297\n",
      "Epoch: 18 \tTraining Loss: 0.000297\n",
      "Epoch: 1 \tTraining Loss: 0.000228\n",
      "Epoch: 2 \tTraining Loss: 0.000228\n",
      "Epoch: 3 \tTraining Loss: 0.000228\n",
      "Epoch: 4 \tTraining Loss: 0.000228\n",
      "Epoch: 5 \tTraining Loss: 0.000228\n",
      "Epoch: 6 \tTraining Loss: 0.000228\n",
      "Epoch: 7 \tTraining Loss: 0.000228\n",
      "Epoch: 8 \tTraining Loss: 0.000228\n",
      "Epoch: 9 \tTraining Loss: 0.000228\n",
      "Epoch: 10 \tTraining Loss: 0.000228\n",
      "Epoch: 11 \tTraining Loss: 0.000228\n",
      "Epoch: 12 \tTraining Loss: 0.000228\n",
      "Epoch: 13 \tTraining Loss: 0.000228\n",
      "Epoch: 14 \tTraining Loss: 0.000228\n",
      "Epoch: 15 \tTraining Loss: 0.000228\n",
      "Epoch: 16 \tTraining Loss: 0.000228\n",
      "Epoch: 17 \tTraining Loss: 0.000228\n",
      "Epoch: 18 \tTraining Loss: 0.000228\n",
      "Epoch: 1 \tTraining Loss: 0.000221\n",
      "Epoch: 2 \tTraining Loss: 0.000221\n",
      "Epoch: 3 \tTraining Loss: 0.000221\n",
      "Epoch: 4 \tTraining Loss: 0.000221\n",
      "Epoch: 5 \tTraining Loss: 0.000221\n",
      "Epoch: 6 \tTraining Loss: 0.000221\n",
      "Epoch: 7 \tTraining Loss: 0.000221\n",
      "Epoch: 8 \tTraining Loss: 0.000221\n",
      "Epoch: 9 \tTraining Loss: 0.000221\n",
      "Epoch: 10 \tTraining Loss: 0.000221\n",
      "Epoch: 11 \tTraining Loss: 0.000221\n",
      "Epoch: 12 \tTraining Loss: 0.000221\n",
      "Epoch: 13 \tTraining Loss: 0.000221\n",
      "Epoch: 14 \tTraining Loss: 0.000221\n",
      "Epoch: 15 \tTraining Loss: 0.000221\n",
      "Epoch: 16 \tTraining Loss: 0.000221\n",
      "Epoch: 17 \tTraining Loss: 0.000221\n",
      "Epoch: 18 \tTraining Loss: 0.000221\n",
      "Epoch: 1 \tTraining Loss: 0.000257\n",
      "Epoch: 2 \tTraining Loss: 0.000257\n",
      "Epoch: 3 \tTraining Loss: 0.000257\n",
      "Epoch: 4 \tTraining Loss: 0.000257\n",
      "Epoch: 5 \tTraining Loss: 0.000257\n",
      "Epoch: 6 \tTraining Loss: 0.000257\n",
      "Epoch: 7 \tTraining Loss: 0.000257\n",
      "Epoch: 8 \tTraining Loss: 0.000257\n",
      "Epoch: 9 \tTraining Loss: 0.000257\n",
      "Epoch: 10 \tTraining Loss: 0.000257\n",
      "Epoch: 11 \tTraining Loss: 0.000257\n",
      "Epoch: 12 \tTraining Loss: 0.000257\n",
      "Epoch: 13 \tTraining Loss: 0.000257\n",
      "Epoch: 14 \tTraining Loss: 0.000257\n",
      "Epoch: 15 \tTraining Loss: 0.000257\n",
      "Epoch: 16 \tTraining Loss: 0.000257\n",
      "Epoch: 17 \tTraining Loss: 0.000257\n",
      "Epoch: 18 \tTraining Loss: 0.000257\n",
      "Epoch: 1 \tTraining Loss: 0.000203\n",
      "Epoch: 2 \tTraining Loss: 0.000203\n",
      "Epoch: 3 \tTraining Loss: 0.000203\n",
      "Epoch: 4 \tTraining Loss: 0.000203\n",
      "Epoch: 5 \tTraining Loss: 0.000203\n",
      "Epoch: 6 \tTraining Loss: 0.000203\n",
      "Epoch: 7 \tTraining Loss: 0.000203\n",
      "Epoch: 8 \tTraining Loss: 0.000203\n",
      "Epoch: 9 \tTraining Loss: 0.000203\n",
      "Epoch: 10 \tTraining Loss: 0.000203\n",
      "Epoch: 11 \tTraining Loss: 0.000203\n",
      "Epoch: 12 \tTraining Loss: 0.000203\n",
      "Epoch: 13 \tTraining Loss: 0.000203\n",
      "Epoch: 14 \tTraining Loss: 0.000203\n",
      "Epoch: 15 \tTraining Loss: 0.000203\n",
      "Epoch: 16 \tTraining Loss: 0.000203\n",
      "Epoch: 17 \tTraining Loss: 0.000203\n",
      "Epoch: 18 \tTraining Loss: 0.000203\n",
      "Epoch: 1 \tTraining Loss: 0.000100\n",
      "Epoch: 2 \tTraining Loss: 0.000100\n",
      "Epoch: 3 \tTraining Loss: 0.000100\n",
      "Epoch: 4 \tTraining Loss: 0.000100\n",
      "Epoch: 5 \tTraining Loss: 0.000100\n",
      "Epoch: 6 \tTraining Loss: 0.000100\n",
      "Epoch: 7 \tTraining Loss: 0.000100\n",
      "Epoch: 8 \tTraining Loss: 0.000100\n",
      "Epoch: 9 \tTraining Loss: 0.000100\n",
      "Epoch: 10 \tTraining Loss: 0.000100\n",
      "Epoch: 11 \tTraining Loss: 0.000100\n",
      "Epoch: 12 \tTraining Loss: 0.000100\n",
      "Epoch: 13 \tTraining Loss: 0.000100\n",
      "Epoch: 14 \tTraining Loss: 0.000100\n",
      "Epoch: 15 \tTraining Loss: 0.000100\n",
      "Epoch: 16 \tTraining Loss: 0.000100\n",
      "Epoch: 17 \tTraining Loss: 0.000100\n",
      "Epoch: 18 \tTraining Loss: 0.000100\n",
      "Epoch: 1 \tTraining Loss: 0.000080\n",
      "Epoch: 2 \tTraining Loss: 0.000080\n",
      "Epoch: 3 \tTraining Loss: 0.000080\n",
      "Epoch: 4 \tTraining Loss: 0.000080\n",
      "Epoch: 5 \tTraining Loss: 0.000080\n",
      "Epoch: 6 \tTraining Loss: 0.000080\n",
      "Epoch: 7 \tTraining Loss: 0.000080\n",
      "Epoch: 8 \tTraining Loss: 0.000080\n",
      "Epoch: 9 \tTraining Loss: 0.000080\n",
      "Epoch: 10 \tTraining Loss: 0.000080\n",
      "Epoch: 11 \tTraining Loss: 0.000080\n",
      "Epoch: 12 \tTraining Loss: 0.000080\n",
      "Epoch: 13 \tTraining Loss: 0.000080\n",
      "Epoch: 14 \tTraining Loss: 0.000080\n",
      "Epoch: 15 \tTraining Loss: 0.000080\n",
      "Epoch: 16 \tTraining Loss: 0.000080\n",
      "Epoch: 17 \tTraining Loss: 0.000080\n",
      "Epoch: 18 \tTraining Loss: 0.000080\n",
      "Epoch: 1 \tTraining Loss: 0.000113\n",
      "Epoch: 2 \tTraining Loss: 0.000113\n",
      "Epoch: 3 \tTraining Loss: 0.000113\n",
      "Epoch: 4 \tTraining Loss: 0.000113\n",
      "Epoch: 5 \tTraining Loss: 0.000113\n",
      "Epoch: 6 \tTraining Loss: 0.000113\n",
      "Epoch: 7 \tTraining Loss: 0.000113\n",
      "Epoch: 8 \tTraining Loss: 0.000113\n",
      "Epoch: 9 \tTraining Loss: 0.000113\n",
      "Epoch: 10 \tTraining Loss: 0.000113\n",
      "Epoch: 11 \tTraining Loss: 0.000113\n",
      "Epoch: 12 \tTraining Loss: 0.000113\n",
      "Epoch: 13 \tTraining Loss: 0.000113\n",
      "Epoch: 14 \tTraining Loss: 0.000113\n",
      "Epoch: 15 \tTraining Loss: 0.000113\n",
      "Epoch: 16 \tTraining Loss: 0.000113\n",
      "Epoch: 17 \tTraining Loss: 0.000113\n",
      "Epoch: 18 \tTraining Loss: 0.000113\n",
      "Epoch: 1 \tTraining Loss: 0.000254\n",
      "Epoch: 2 \tTraining Loss: 0.000254\n",
      "Epoch: 3 \tTraining Loss: 0.000254\n",
      "Epoch: 4 \tTraining Loss: 0.000254\n",
      "Epoch: 5 \tTraining Loss: 0.000254\n",
      "Epoch: 6 \tTraining Loss: 0.000254\n",
      "Epoch: 7 \tTraining Loss: 0.000254\n",
      "Epoch: 8 \tTraining Loss: 0.000254\n",
      "Epoch: 9 \tTraining Loss: 0.000254\n",
      "Epoch: 10 \tTraining Loss: 0.000254\n",
      "Epoch: 11 \tTraining Loss: 0.000254\n",
      "Epoch: 12 \tTraining Loss: 0.000254\n",
      "Epoch: 13 \tTraining Loss: 0.000254\n",
      "Epoch: 14 \tTraining Loss: 0.000254\n",
      "Epoch: 15 \tTraining Loss: 0.000254\n",
      "Epoch: 16 \tTraining Loss: 0.000254\n",
      "Epoch: 17 \tTraining Loss: 0.000254\n",
      "Epoch: 18 \tTraining Loss: 0.000254\n",
      "Epoch: 1 \tTraining Loss: 0.000130\n",
      "Epoch: 2 \tTraining Loss: 0.000130\n",
      "Epoch: 3 \tTraining Loss: 0.000130\n",
      "Epoch: 4 \tTraining Loss: 0.000130\n",
      "Epoch: 5 \tTraining Loss: 0.000130\n",
      "Epoch: 6 \tTraining Loss: 0.000130\n",
      "Epoch: 7 \tTraining Loss: 0.000130\n",
      "Epoch: 8 \tTraining Loss: 0.000130\n",
      "Epoch: 9 \tTraining Loss: 0.000130\n",
      "Epoch: 10 \tTraining Loss: 0.000130\n",
      "Epoch: 11 \tTraining Loss: 0.000130\n",
      "Epoch: 12 \tTraining Loss: 0.000130\n",
      "Epoch: 13 \tTraining Loss: 0.000130\n",
      "Epoch: 14 \tTraining Loss: 0.000130\n",
      "Epoch: 15 \tTraining Loss: 0.000130\n",
      "Epoch: 16 \tTraining Loss: 0.000130\n",
      "Epoch: 17 \tTraining Loss: 0.000130\n",
      "Epoch: 18 \tTraining Loss: 0.000130\n",
      "Epoch: 1 \tTraining Loss: 0.000194\n",
      "Epoch: 2 \tTraining Loss: 0.000194\n",
      "Epoch: 3 \tTraining Loss: 0.000194\n",
      "Epoch: 4 \tTraining Loss: 0.000194\n",
      "Epoch: 5 \tTraining Loss: 0.000194\n",
      "Epoch: 6 \tTraining Loss: 0.000194\n",
      "Epoch: 7 \tTraining Loss: 0.000194\n",
      "Epoch: 8 \tTraining Loss: 0.000194\n",
      "Epoch: 9 \tTraining Loss: 0.000194\n",
      "Epoch: 10 \tTraining Loss: 0.000194\n",
      "Epoch: 11 \tTraining Loss: 0.000194\n",
      "Epoch: 12 \tTraining Loss: 0.000194\n",
      "Epoch: 13 \tTraining Loss: 0.000194\n",
      "Epoch: 14 \tTraining Loss: 0.000194\n",
      "Epoch: 15 \tTraining Loss: 0.000194\n",
      "Epoch: 16 \tTraining Loss: 0.000194\n",
      "Epoch: 17 \tTraining Loss: 0.000194\n",
      "Epoch: 18 \tTraining Loss: 0.000194\n",
      "Epoch: 1 \tTraining Loss: 0.000197\n",
      "Epoch: 2 \tTraining Loss: 0.000197\n",
      "Epoch: 3 \tTraining Loss: 0.000197\n",
      "Epoch: 4 \tTraining Loss: 0.000197\n",
      "Epoch: 5 \tTraining Loss: 0.000197\n",
      "Epoch: 6 \tTraining Loss: 0.000197\n",
      "Epoch: 7 \tTraining Loss: 0.000197\n",
      "Epoch: 8 \tTraining Loss: 0.000197\n",
      "Epoch: 9 \tTraining Loss: 0.000197\n",
      "Epoch: 10 \tTraining Loss: 0.000197\n",
      "Epoch: 11 \tTraining Loss: 0.000197\n",
      "Epoch: 12 \tTraining Loss: 0.000197\n",
      "Epoch: 13 \tTraining Loss: 0.000197\n",
      "Epoch: 14 \tTraining Loss: 0.000197\n",
      "Epoch: 15 \tTraining Loss: 0.000197\n",
      "Epoch: 16 \tTraining Loss: 0.000197\n",
      "Epoch: 17 \tTraining Loss: 0.000197\n",
      "Epoch: 18 \tTraining Loss: 0.000197\n",
      "Epoch: 1 \tTraining Loss: 0.000182\n",
      "Epoch: 2 \tTraining Loss: 0.000182\n",
      "Epoch: 3 \tTraining Loss: 0.000182\n",
      "Epoch: 4 \tTraining Loss: 0.000182\n",
      "Epoch: 5 \tTraining Loss: 0.000182\n",
      "Epoch: 6 \tTraining Loss: 0.000182\n",
      "Epoch: 7 \tTraining Loss: 0.000182\n",
      "Epoch: 8 \tTraining Loss: 0.000182\n",
      "Epoch: 9 \tTraining Loss: 0.000182\n",
      "Epoch: 10 \tTraining Loss: 0.000182\n",
      "Epoch: 11 \tTraining Loss: 0.000182\n",
      "Epoch: 12 \tTraining Loss: 0.000182\n",
      "Epoch: 13 \tTraining Loss: 0.000182\n",
      "Epoch: 14 \tTraining Loss: 0.000182\n",
      "Epoch: 15 \tTraining Loss: 0.000182\n",
      "Epoch: 16 \tTraining Loss: 0.000182\n",
      "Epoch: 17 \tTraining Loss: 0.000182\n",
      "Epoch: 18 \tTraining Loss: 0.000182\n",
      "Epoch: 1 \tTraining Loss: 0.000303\n",
      "Epoch: 2 \tTraining Loss: 0.000303\n",
      "Epoch: 3 \tTraining Loss: 0.000303\n",
      "Epoch: 4 \tTraining Loss: 0.000303\n",
      "Epoch: 5 \tTraining Loss: 0.000303\n",
      "Epoch: 6 \tTraining Loss: 0.000303\n",
      "Epoch: 7 \tTraining Loss: 0.000303\n",
      "Epoch: 8 \tTraining Loss: 0.000303\n",
      "Epoch: 9 \tTraining Loss: 0.000303\n",
      "Epoch: 10 \tTraining Loss: 0.000303\n",
      "Epoch: 11 \tTraining Loss: 0.000303\n",
      "Epoch: 12 \tTraining Loss: 0.000303\n",
      "Epoch: 13 \tTraining Loss: 0.000303\n",
      "Epoch: 14 \tTraining Loss: 0.000303\n",
      "Epoch: 15 \tTraining Loss: 0.000303\n",
      "Epoch: 16 \tTraining Loss: 0.000303\n",
      "Epoch: 17 \tTraining Loss: 0.000303\n",
      "Epoch: 18 \tTraining Loss: 0.000303\n",
      "Epoch: 1 \tTraining Loss: 0.000125\n",
      "Epoch: 2 \tTraining Loss: 0.000125\n",
      "Epoch: 3 \tTraining Loss: 0.000125\n",
      "Epoch: 4 \tTraining Loss: 0.000125\n",
      "Epoch: 5 \tTraining Loss: 0.000125\n",
      "Epoch: 6 \tTraining Loss: 0.000125\n",
      "Epoch: 7 \tTraining Loss: 0.000125\n",
      "Epoch: 8 \tTraining Loss: 0.000125\n",
      "Epoch: 9 \tTraining Loss: 0.000125\n",
      "Epoch: 10 \tTraining Loss: 0.000125\n",
      "Epoch: 11 \tTraining Loss: 0.000125\n",
      "Epoch: 12 \tTraining Loss: 0.000125\n",
      "Epoch: 13 \tTraining Loss: 0.000125\n",
      "Epoch: 14 \tTraining Loss: 0.000125\n",
      "Epoch: 15 \tTraining Loss: 0.000125\n",
      "Epoch: 16 \tTraining Loss: 0.000125\n",
      "Epoch: 17 \tTraining Loss: 0.000125\n",
      "Epoch: 18 \tTraining Loss: 0.000125\n",
      "Epoch: 1 \tTraining Loss: 0.000101\n",
      "Epoch: 2 \tTraining Loss: 0.000101\n",
      "Epoch: 3 \tTraining Loss: 0.000101\n",
      "Epoch: 4 \tTraining Loss: 0.000101\n",
      "Epoch: 5 \tTraining Loss: 0.000101\n",
      "Epoch: 6 \tTraining Loss: 0.000101\n",
      "Epoch: 7 \tTraining Loss: 0.000101\n",
      "Epoch: 8 \tTraining Loss: 0.000101\n",
      "Epoch: 9 \tTraining Loss: 0.000101\n",
      "Epoch: 10 \tTraining Loss: 0.000101\n",
      "Epoch: 11 \tTraining Loss: 0.000101\n",
      "Epoch: 12 \tTraining Loss: 0.000101\n",
      "Epoch: 13 \tTraining Loss: 0.000101\n",
      "Epoch: 14 \tTraining Loss: 0.000101\n",
      "Epoch: 15 \tTraining Loss: 0.000101\n",
      "Epoch: 16 \tTraining Loss: 0.000101\n",
      "Epoch: 17 \tTraining Loss: 0.000101\n",
      "Epoch: 18 \tTraining Loss: 0.000101\n",
      "Epoch: 1 \tTraining Loss: 0.000127\n",
      "Epoch: 2 \tTraining Loss: 0.000127\n",
      "Epoch: 3 \tTraining Loss: 0.000127\n",
      "Epoch: 4 \tTraining Loss: 0.000127\n",
      "Epoch: 5 \tTraining Loss: 0.000127\n",
      "Epoch: 6 \tTraining Loss: 0.000127\n",
      "Epoch: 7 \tTraining Loss: 0.000127\n",
      "Epoch: 8 \tTraining Loss: 0.000127\n",
      "Epoch: 9 \tTraining Loss: 0.000127\n",
      "Epoch: 10 \tTraining Loss: 0.000127\n",
      "Epoch: 11 \tTraining Loss: 0.000127\n",
      "Epoch: 12 \tTraining Loss: 0.000127\n",
      "Epoch: 13 \tTraining Loss: 0.000127\n",
      "Epoch: 14 \tTraining Loss: 0.000127\n",
      "Epoch: 15 \tTraining Loss: 0.000127\n",
      "Epoch: 16 \tTraining Loss: 0.000127\n",
      "Epoch: 17 \tTraining Loss: 0.000127\n",
      "Epoch: 18 \tTraining Loss: 0.000127\n",
      "Epoch: 1 \tTraining Loss: 0.000361\n",
      "Epoch: 2 \tTraining Loss: 0.000361\n",
      "Epoch: 3 \tTraining Loss: 0.000361\n",
      "Epoch: 4 \tTraining Loss: 0.000361\n",
      "Epoch: 5 \tTraining Loss: 0.000361\n",
      "Epoch: 6 \tTraining Loss: 0.000361\n",
      "Epoch: 7 \tTraining Loss: 0.000361\n",
      "Epoch: 8 \tTraining Loss: 0.000361\n",
      "Epoch: 9 \tTraining Loss: 0.000361\n",
      "Epoch: 10 \tTraining Loss: 0.000361\n",
      "Epoch: 11 \tTraining Loss: 0.000361\n",
      "Epoch: 12 \tTraining Loss: 0.000361\n",
      "Epoch: 13 \tTraining Loss: 0.000361\n",
      "Epoch: 14 \tTraining Loss: 0.000361\n",
      "Epoch: 15 \tTraining Loss: 0.000361\n",
      "Epoch: 16 \tTraining Loss: 0.000361\n",
      "Epoch: 17 \tTraining Loss: 0.000361\n",
      "Epoch: 18 \tTraining Loss: 0.000361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000135\n",
      "Epoch: 2 \tTraining Loss: 0.000135\n",
      "Epoch: 3 \tTraining Loss: 0.000135\n",
      "Epoch: 4 \tTraining Loss: 0.000135\n",
      "Epoch: 5 \tTraining Loss: 0.000135\n",
      "Epoch: 6 \tTraining Loss: 0.000135\n",
      "Epoch: 7 \tTraining Loss: 0.000135\n",
      "Epoch: 8 \tTraining Loss: 0.000135\n",
      "Epoch: 9 \tTraining Loss: 0.000135\n",
      "Epoch: 10 \tTraining Loss: 0.000135\n",
      "Epoch: 11 \tTraining Loss: 0.000135\n",
      "Epoch: 12 \tTraining Loss: 0.000135\n",
      "Epoch: 13 \tTraining Loss: 0.000135\n",
      "Epoch: 14 \tTraining Loss: 0.000135\n",
      "Epoch: 15 \tTraining Loss: 0.000135\n",
      "Epoch: 16 \tTraining Loss: 0.000135\n",
      "Epoch: 17 \tTraining Loss: 0.000135\n",
      "Epoch: 18 \tTraining Loss: 0.000135\n",
      "Epoch: 1 \tTraining Loss: 0.000324\n",
      "Epoch: 2 \tTraining Loss: 0.000324\n",
      "Epoch: 3 \tTraining Loss: 0.000324\n",
      "Epoch: 4 \tTraining Loss: 0.000324\n",
      "Epoch: 5 \tTraining Loss: 0.000324\n",
      "Epoch: 6 \tTraining Loss: 0.000324\n",
      "Epoch: 7 \tTraining Loss: 0.000324\n",
      "Epoch: 8 \tTraining Loss: 0.000324\n",
      "Epoch: 9 \tTraining Loss: 0.000324\n",
      "Epoch: 10 \tTraining Loss: 0.000324\n",
      "Epoch: 11 \tTraining Loss: 0.000324\n",
      "Epoch: 12 \tTraining Loss: 0.000324\n",
      "Epoch: 13 \tTraining Loss: 0.000324\n",
      "Epoch: 14 \tTraining Loss: 0.000324\n",
      "Epoch: 15 \tTraining Loss: 0.000324\n",
      "Epoch: 16 \tTraining Loss: 0.000324\n",
      "Epoch: 17 \tTraining Loss: 0.000324\n",
      "Epoch: 18 \tTraining Loss: 0.000324\n",
      "Epoch: 1 \tTraining Loss: 0.000145\n",
      "Epoch: 2 \tTraining Loss: 0.000145\n",
      "Epoch: 3 \tTraining Loss: 0.000145\n",
      "Epoch: 4 \tTraining Loss: 0.000145\n",
      "Epoch: 5 \tTraining Loss: 0.000145\n",
      "Epoch: 6 \tTraining Loss: 0.000145\n",
      "Epoch: 7 \tTraining Loss: 0.000145\n",
      "Epoch: 8 \tTraining Loss: 0.000145\n",
      "Epoch: 9 \tTraining Loss: 0.000145\n",
      "Epoch: 10 \tTraining Loss: 0.000145\n",
      "Epoch: 11 \tTraining Loss: 0.000145\n",
      "Epoch: 12 \tTraining Loss: 0.000145\n",
      "Epoch: 13 \tTraining Loss: 0.000145\n",
      "Epoch: 14 \tTraining Loss: 0.000145\n",
      "Epoch: 15 \tTraining Loss: 0.000145\n",
      "Epoch: 16 \tTraining Loss: 0.000145\n",
      "Epoch: 17 \tTraining Loss: 0.000145\n",
      "Epoch: 18 \tTraining Loss: 0.000145\n",
      "Epoch: 1 \tTraining Loss: 0.000097\n",
      "Epoch: 2 \tTraining Loss: 0.000097\n",
      "Epoch: 3 \tTraining Loss: 0.000097\n",
      "Epoch: 4 \tTraining Loss: 0.000097\n",
      "Epoch: 5 \tTraining Loss: 0.000097\n",
      "Epoch: 6 \tTraining Loss: 0.000097\n",
      "Epoch: 7 \tTraining Loss: 0.000097\n",
      "Epoch: 8 \tTraining Loss: 0.000097\n",
      "Epoch: 9 \tTraining Loss: 0.000097\n",
      "Epoch: 10 \tTraining Loss: 0.000097\n",
      "Epoch: 11 \tTraining Loss: 0.000097\n",
      "Epoch: 12 \tTraining Loss: 0.000097\n",
      "Epoch: 13 \tTraining Loss: 0.000097\n",
      "Epoch: 14 \tTraining Loss: 0.000097\n",
      "Epoch: 15 \tTraining Loss: 0.000097\n",
      "Epoch: 16 \tTraining Loss: 0.000097\n",
      "Epoch: 17 \tTraining Loss: 0.000097\n",
      "Epoch: 18 \tTraining Loss: 0.000097\n",
      "Epoch: 1 \tTraining Loss: 0.000109\n",
      "Epoch: 2 \tTraining Loss: 0.000109\n",
      "Epoch: 3 \tTraining Loss: 0.000109\n",
      "Epoch: 4 \tTraining Loss: 0.000109\n",
      "Epoch: 5 \tTraining Loss: 0.000109\n",
      "Epoch: 6 \tTraining Loss: 0.000109\n",
      "Epoch: 7 \tTraining Loss: 0.000109\n",
      "Epoch: 8 \tTraining Loss: 0.000109\n",
      "Epoch: 9 \tTraining Loss: 0.000109\n",
      "Epoch: 10 \tTraining Loss: 0.000109\n",
      "Epoch: 11 \tTraining Loss: 0.000109\n",
      "Epoch: 12 \tTraining Loss: 0.000109\n",
      "Epoch: 13 \tTraining Loss: 0.000109\n",
      "Epoch: 14 \tTraining Loss: 0.000109\n",
      "Epoch: 15 \tTraining Loss: 0.000109\n",
      "Epoch: 16 \tTraining Loss: 0.000109\n",
      "Epoch: 17 \tTraining Loss: 0.000109\n",
      "Epoch: 18 \tTraining Loss: 0.000109\n",
      "Epoch: 1 \tTraining Loss: 0.000215\n",
      "Epoch: 2 \tTraining Loss: 0.000215\n",
      "Epoch: 3 \tTraining Loss: 0.000215\n",
      "Epoch: 4 \tTraining Loss: 0.000215\n",
      "Epoch: 5 \tTraining Loss: 0.000215\n",
      "Epoch: 6 \tTraining Loss: 0.000215\n",
      "Epoch: 7 \tTraining Loss: 0.000215\n",
      "Epoch: 8 \tTraining Loss: 0.000215\n",
      "Epoch: 9 \tTraining Loss: 0.000215\n",
      "Epoch: 10 \tTraining Loss: 0.000215\n",
      "Epoch: 11 \tTraining Loss: 0.000215\n",
      "Epoch: 12 \tTraining Loss: 0.000215\n",
      "Epoch: 13 \tTraining Loss: 0.000215\n",
      "Epoch: 14 \tTraining Loss: 0.000215\n",
      "Epoch: 15 \tTraining Loss: 0.000215\n",
      "Epoch: 16 \tTraining Loss: 0.000215\n",
      "Epoch: 17 \tTraining Loss: 0.000215\n",
      "Epoch: 18 \tTraining Loss: 0.000215\n",
      "Epoch: 1 \tTraining Loss: 0.000312\n",
      "Epoch: 2 \tTraining Loss: 0.000312\n",
      "Epoch: 3 \tTraining Loss: 0.000312\n",
      "Epoch: 4 \tTraining Loss: 0.000312\n",
      "Epoch: 5 \tTraining Loss: 0.000312\n",
      "Epoch: 6 \tTraining Loss: 0.000312\n",
      "Epoch: 7 \tTraining Loss: 0.000312\n",
      "Epoch: 8 \tTraining Loss: 0.000312\n",
      "Epoch: 9 \tTraining Loss: 0.000312\n",
      "Epoch: 10 \tTraining Loss: 0.000312\n",
      "Epoch: 11 \tTraining Loss: 0.000312\n",
      "Epoch: 12 \tTraining Loss: 0.000312\n",
      "Epoch: 13 \tTraining Loss: 0.000312\n",
      "Epoch: 14 \tTraining Loss: 0.000312\n",
      "Epoch: 15 \tTraining Loss: 0.000312\n",
      "Epoch: 16 \tTraining Loss: 0.000312\n",
      "Epoch: 17 \tTraining Loss: 0.000312\n",
      "Epoch: 18 \tTraining Loss: 0.000312\n",
      "Epoch: 1 \tTraining Loss: 0.000165\n",
      "Epoch: 2 \tTraining Loss: 0.000165\n",
      "Epoch: 3 \tTraining Loss: 0.000165\n",
      "Epoch: 4 \tTraining Loss: 0.000165\n",
      "Epoch: 5 \tTraining Loss: 0.000165\n",
      "Epoch: 6 \tTraining Loss: 0.000165\n",
      "Epoch: 7 \tTraining Loss: 0.000165\n",
      "Epoch: 8 \tTraining Loss: 0.000165\n",
      "Epoch: 9 \tTraining Loss: 0.000165\n",
      "Epoch: 10 \tTraining Loss: 0.000165\n",
      "Epoch: 11 \tTraining Loss: 0.000165\n",
      "Epoch: 12 \tTraining Loss: 0.000165\n",
      "Epoch: 13 \tTraining Loss: 0.000165\n",
      "Epoch: 14 \tTraining Loss: 0.000165\n",
      "Epoch: 15 \tTraining Loss: 0.000165\n",
      "Epoch: 16 \tTraining Loss: 0.000165\n",
      "Epoch: 17 \tTraining Loss: 0.000165\n",
      "Epoch: 18 \tTraining Loss: 0.000165\n",
      "Epoch: 1 \tTraining Loss: 0.000229\n",
      "Epoch: 2 \tTraining Loss: 0.000229\n",
      "Epoch: 3 \tTraining Loss: 0.000229\n",
      "Epoch: 4 \tTraining Loss: 0.000229\n",
      "Epoch: 5 \tTraining Loss: 0.000229\n",
      "Epoch: 6 \tTraining Loss: 0.000229\n",
      "Epoch: 7 \tTraining Loss: 0.000229\n",
      "Epoch: 8 \tTraining Loss: 0.000229\n",
      "Epoch: 9 \tTraining Loss: 0.000229\n",
      "Epoch: 10 \tTraining Loss: 0.000229\n",
      "Epoch: 11 \tTraining Loss: 0.000229\n",
      "Epoch: 12 \tTraining Loss: 0.000229\n",
      "Epoch: 13 \tTraining Loss: 0.000229\n",
      "Epoch: 14 \tTraining Loss: 0.000229\n",
      "Epoch: 15 \tTraining Loss: 0.000229\n",
      "Epoch: 16 \tTraining Loss: 0.000229\n",
      "Epoch: 17 \tTraining Loss: 0.000229\n",
      "Epoch: 18 \tTraining Loss: 0.000229\n",
      "Epoch: 1 \tTraining Loss: 0.000208\n",
      "Epoch: 2 \tTraining Loss: 0.000208\n",
      "Epoch: 3 \tTraining Loss: 0.000208\n",
      "Epoch: 4 \tTraining Loss: 0.000208\n",
      "Epoch: 5 \tTraining Loss: 0.000208\n",
      "Epoch: 6 \tTraining Loss: 0.000208\n",
      "Epoch: 7 \tTraining Loss: 0.000208\n",
      "Epoch: 8 \tTraining Loss: 0.000208\n",
      "Epoch: 9 \tTraining Loss: 0.000208\n",
      "Epoch: 10 \tTraining Loss: 0.000208\n",
      "Epoch: 11 \tTraining Loss: 0.000208\n",
      "Epoch: 12 \tTraining Loss: 0.000208\n",
      "Epoch: 13 \tTraining Loss: 0.000208\n",
      "Epoch: 14 \tTraining Loss: 0.000208\n",
      "Epoch: 15 \tTraining Loss: 0.000208\n",
      "Epoch: 16 \tTraining Loss: 0.000208\n",
      "Epoch: 17 \tTraining Loss: 0.000208\n",
      "Epoch: 18 \tTraining Loss: 0.000208\n",
      "Epoch: 1 \tTraining Loss: 0.000119\n",
      "Epoch: 2 \tTraining Loss: 0.000119\n",
      "Epoch: 3 \tTraining Loss: 0.000119\n",
      "Epoch: 4 \tTraining Loss: 0.000119\n",
      "Epoch: 5 \tTraining Loss: 0.000119\n",
      "Epoch: 6 \tTraining Loss: 0.000119\n",
      "Epoch: 7 \tTraining Loss: 0.000119\n",
      "Epoch: 8 \tTraining Loss: 0.000119\n",
      "Epoch: 9 \tTraining Loss: 0.000119\n",
      "Epoch: 10 \tTraining Loss: 0.000119\n",
      "Epoch: 11 \tTraining Loss: 0.000119\n",
      "Epoch: 12 \tTraining Loss: 0.000119\n",
      "Epoch: 13 \tTraining Loss: 0.000119\n",
      "Epoch: 14 \tTraining Loss: 0.000119\n",
      "Epoch: 15 \tTraining Loss: 0.000119\n",
      "Epoch: 16 \tTraining Loss: 0.000119\n",
      "Epoch: 17 \tTraining Loss: 0.000119\n",
      "Epoch: 18 \tTraining Loss: 0.000119\n",
      "Epoch: 1 \tTraining Loss: 0.000124\n",
      "Epoch: 2 \tTraining Loss: 0.000124\n",
      "Epoch: 3 \tTraining Loss: 0.000124\n",
      "Epoch: 4 \tTraining Loss: 0.000124\n",
      "Epoch: 5 \tTraining Loss: 0.000124\n",
      "Epoch: 6 \tTraining Loss: 0.000124\n",
      "Epoch: 7 \tTraining Loss: 0.000124\n",
      "Epoch: 8 \tTraining Loss: 0.000124\n",
      "Epoch: 9 \tTraining Loss: 0.000124\n",
      "Epoch: 10 \tTraining Loss: 0.000124\n",
      "Epoch: 11 \tTraining Loss: 0.000124\n",
      "Epoch: 12 \tTraining Loss: 0.000124\n",
      "Epoch: 13 \tTraining Loss: 0.000124\n",
      "Epoch: 14 \tTraining Loss: 0.000124\n",
      "Epoch: 15 \tTraining Loss: 0.000124\n",
      "Epoch: 16 \tTraining Loss: 0.000124\n",
      "Epoch: 17 \tTraining Loss: 0.000124\n",
      "Epoch: 18 \tTraining Loss: 0.000124\n",
      "Epoch: 1 \tTraining Loss: 0.000132\n",
      "Epoch: 2 \tTraining Loss: 0.000132\n",
      "Epoch: 3 \tTraining Loss: 0.000132\n",
      "Epoch: 4 \tTraining Loss: 0.000132\n",
      "Epoch: 5 \tTraining Loss: 0.000132\n",
      "Epoch: 6 \tTraining Loss: 0.000132\n",
      "Epoch: 7 \tTraining Loss: 0.000132\n",
      "Epoch: 8 \tTraining Loss: 0.000132\n",
      "Epoch: 9 \tTraining Loss: 0.000132\n",
      "Epoch: 10 \tTraining Loss: 0.000132\n",
      "Epoch: 11 \tTraining Loss: 0.000132\n",
      "Epoch: 12 \tTraining Loss: 0.000132\n",
      "Epoch: 13 \tTraining Loss: 0.000132\n",
      "Epoch: 14 \tTraining Loss: 0.000132\n",
      "Epoch: 15 \tTraining Loss: 0.000132\n",
      "Epoch: 16 \tTraining Loss: 0.000132\n",
      "Epoch: 17 \tTraining Loss: 0.000132\n",
      "Epoch: 18 \tTraining Loss: 0.000132\n",
      "Epoch: 1 \tTraining Loss: 0.000251\n",
      "Epoch: 2 \tTraining Loss: 0.000251\n",
      "Epoch: 3 \tTraining Loss: 0.000251\n",
      "Epoch: 4 \tTraining Loss: 0.000251\n",
      "Epoch: 5 \tTraining Loss: 0.000251\n",
      "Epoch: 6 \tTraining Loss: 0.000251\n",
      "Epoch: 7 \tTraining Loss: 0.000251\n",
      "Epoch: 8 \tTraining Loss: 0.000251\n",
      "Epoch: 9 \tTraining Loss: 0.000251\n",
      "Epoch: 10 \tTraining Loss: 0.000251\n",
      "Epoch: 11 \tTraining Loss: 0.000251\n",
      "Epoch: 12 \tTraining Loss: 0.000251\n",
      "Epoch: 13 \tTraining Loss: 0.000251\n",
      "Epoch: 14 \tTraining Loss: 0.000251\n",
      "Epoch: 15 \tTraining Loss: 0.000251\n",
      "Epoch: 16 \tTraining Loss: 0.000251\n",
      "Epoch: 17 \tTraining Loss: 0.000251\n",
      "Epoch: 18 \tTraining Loss: 0.000251\n",
      "Epoch: 1 \tTraining Loss: 0.000192\n",
      "Epoch: 2 \tTraining Loss: 0.000192\n",
      "Epoch: 3 \tTraining Loss: 0.000192\n",
      "Epoch: 4 \tTraining Loss: 0.000192\n",
      "Epoch: 5 \tTraining Loss: 0.000192\n",
      "Epoch: 6 \tTraining Loss: 0.000192\n",
      "Epoch: 7 \tTraining Loss: 0.000192\n",
      "Epoch: 8 \tTraining Loss: 0.000192\n",
      "Epoch: 9 \tTraining Loss: 0.000192\n",
      "Epoch: 10 \tTraining Loss: 0.000192\n",
      "Epoch: 11 \tTraining Loss: 0.000192\n",
      "Epoch: 12 \tTraining Loss: 0.000192\n",
      "Epoch: 13 \tTraining Loss: 0.000192\n",
      "Epoch: 14 \tTraining Loss: 0.000192\n",
      "Epoch: 15 \tTraining Loss: 0.000192\n",
      "Epoch: 16 \tTraining Loss: 0.000192\n",
      "Epoch: 17 \tTraining Loss: 0.000192\n",
      "Epoch: 18 \tTraining Loss: 0.000192\n",
      "Epoch: 1 \tTraining Loss: 0.000147\n",
      "Epoch: 2 \tTraining Loss: 0.000147\n",
      "Epoch: 3 \tTraining Loss: 0.000147\n",
      "Epoch: 4 \tTraining Loss: 0.000147\n",
      "Epoch: 5 \tTraining Loss: 0.000147\n",
      "Epoch: 6 \tTraining Loss: 0.000147\n",
      "Epoch: 7 \tTraining Loss: 0.000147\n",
      "Epoch: 8 \tTraining Loss: 0.000147\n",
      "Epoch: 9 \tTraining Loss: 0.000147\n",
      "Epoch: 10 \tTraining Loss: 0.000147\n",
      "Epoch: 11 \tTraining Loss: 0.000147\n",
      "Epoch: 12 \tTraining Loss: 0.000147\n",
      "Epoch: 13 \tTraining Loss: 0.000147\n",
      "Epoch: 14 \tTraining Loss: 0.000147\n",
      "Epoch: 15 \tTraining Loss: 0.000147\n",
      "Epoch: 16 \tTraining Loss: 0.000147\n",
      "Epoch: 17 \tTraining Loss: 0.000147\n",
      "Epoch: 18 \tTraining Loss: 0.000147\n",
      "Epoch: 1 \tTraining Loss: 0.000076\n",
      "Epoch: 2 \tTraining Loss: 0.000076\n",
      "Epoch: 3 \tTraining Loss: 0.000076\n",
      "Epoch: 4 \tTraining Loss: 0.000076\n",
      "Epoch: 5 \tTraining Loss: 0.000076\n",
      "Epoch: 6 \tTraining Loss: 0.000076\n",
      "Epoch: 7 \tTraining Loss: 0.000076\n",
      "Epoch: 8 \tTraining Loss: 0.000076\n",
      "Epoch: 9 \tTraining Loss: 0.000076\n",
      "Epoch: 10 \tTraining Loss: 0.000076\n",
      "Epoch: 11 \tTraining Loss: 0.000076\n",
      "Epoch: 12 \tTraining Loss: 0.000076\n",
      "Epoch: 13 \tTraining Loss: 0.000076\n",
      "Epoch: 14 \tTraining Loss: 0.000076\n",
      "Epoch: 15 \tTraining Loss: 0.000076\n",
      "Epoch: 16 \tTraining Loss: 0.000076\n",
      "Epoch: 17 \tTraining Loss: 0.000076\n",
      "Epoch: 18 \tTraining Loss: 0.000076\n",
      "Epoch: 1 \tTraining Loss: 0.000175\n",
      "Epoch: 2 \tTraining Loss: 0.000175\n",
      "Epoch: 3 \tTraining Loss: 0.000175\n",
      "Epoch: 4 \tTraining Loss: 0.000175\n",
      "Epoch: 5 \tTraining Loss: 0.000175\n",
      "Epoch: 6 \tTraining Loss: 0.000175\n",
      "Epoch: 7 \tTraining Loss: 0.000175\n",
      "Epoch: 8 \tTraining Loss: 0.000175\n",
      "Epoch: 9 \tTraining Loss: 0.000175\n",
      "Epoch: 10 \tTraining Loss: 0.000175\n",
      "Epoch: 11 \tTraining Loss: 0.000175\n",
      "Epoch: 12 \tTraining Loss: 0.000175\n",
      "Epoch: 13 \tTraining Loss: 0.000175\n",
      "Epoch: 14 \tTraining Loss: 0.000175\n",
      "Epoch: 15 \tTraining Loss: 0.000175\n",
      "Epoch: 16 \tTraining Loss: 0.000175\n",
      "Epoch: 17 \tTraining Loss: 0.000175\n",
      "Epoch: 18 \tTraining Loss: 0.000175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000115\n",
      "Epoch: 2 \tTraining Loss: 0.000115\n",
      "Epoch: 3 \tTraining Loss: 0.000115\n",
      "Epoch: 4 \tTraining Loss: 0.000115\n",
      "Epoch: 5 \tTraining Loss: 0.000115\n",
      "Epoch: 6 \tTraining Loss: 0.000115\n",
      "Epoch: 7 \tTraining Loss: 0.000115\n",
      "Epoch: 8 \tTraining Loss: 0.000115\n",
      "Epoch: 9 \tTraining Loss: 0.000115\n",
      "Epoch: 10 \tTraining Loss: 0.000115\n",
      "Epoch: 11 \tTraining Loss: 0.000115\n",
      "Epoch: 12 \tTraining Loss: 0.000115\n",
      "Epoch: 13 \tTraining Loss: 0.000115\n",
      "Epoch: 14 \tTraining Loss: 0.000115\n",
      "Epoch: 15 \tTraining Loss: 0.000115\n",
      "Epoch: 16 \tTraining Loss: 0.000115\n",
      "Epoch: 17 \tTraining Loss: 0.000115\n",
      "Epoch: 18 \tTraining Loss: 0.000115\n",
      "Epoch: 1 \tTraining Loss: 0.000334\n",
      "Epoch: 2 \tTraining Loss: 0.000334\n",
      "Epoch: 3 \tTraining Loss: 0.000334\n",
      "Epoch: 4 \tTraining Loss: 0.000334\n",
      "Epoch: 5 \tTraining Loss: 0.000334\n",
      "Epoch: 6 \tTraining Loss: 0.000334\n",
      "Epoch: 7 \tTraining Loss: 0.000334\n",
      "Epoch: 8 \tTraining Loss: 0.000334\n",
      "Epoch: 9 \tTraining Loss: 0.000334\n",
      "Epoch: 10 \tTraining Loss: 0.000334\n",
      "Epoch: 11 \tTraining Loss: 0.000334\n",
      "Epoch: 12 \tTraining Loss: 0.000334\n",
      "Epoch: 13 \tTraining Loss: 0.000334\n",
      "Epoch: 14 \tTraining Loss: 0.000334\n",
      "Epoch: 15 \tTraining Loss: 0.000334\n",
      "Epoch: 16 \tTraining Loss: 0.000334\n",
      "Epoch: 17 \tTraining Loss: 0.000334\n",
      "Epoch: 18 \tTraining Loss: 0.000334\n",
      "Epoch: 1 \tTraining Loss: 0.000127\n",
      "Epoch: 2 \tTraining Loss: 0.000127\n",
      "Epoch: 3 \tTraining Loss: 0.000127\n",
      "Epoch: 4 \tTraining Loss: 0.000127\n",
      "Epoch: 5 \tTraining Loss: 0.000127\n",
      "Epoch: 6 \tTraining Loss: 0.000127\n",
      "Epoch: 7 \tTraining Loss: 0.000127\n",
      "Epoch: 8 \tTraining Loss: 0.000127\n",
      "Epoch: 9 \tTraining Loss: 0.000127\n",
      "Epoch: 10 \tTraining Loss: 0.000127\n",
      "Epoch: 11 \tTraining Loss: 0.000127\n",
      "Epoch: 12 \tTraining Loss: 0.000127\n",
      "Epoch: 13 \tTraining Loss: 0.000127\n",
      "Epoch: 14 \tTraining Loss: 0.000127\n",
      "Epoch: 15 \tTraining Loss: 0.000127\n",
      "Epoch: 16 \tTraining Loss: 0.000127\n",
      "Epoch: 17 \tTraining Loss: 0.000127\n",
      "Epoch: 18 \tTraining Loss: 0.000127\n",
      "Epoch: 1 \tTraining Loss: 0.000115\n",
      "Epoch: 2 \tTraining Loss: 0.000115\n",
      "Epoch: 3 \tTraining Loss: 0.000115\n",
      "Epoch: 4 \tTraining Loss: 0.000115\n",
      "Epoch: 5 \tTraining Loss: 0.000115\n",
      "Epoch: 6 \tTraining Loss: 0.000115\n",
      "Epoch: 7 \tTraining Loss: 0.000115\n",
      "Epoch: 8 \tTraining Loss: 0.000115\n",
      "Epoch: 9 \tTraining Loss: 0.000115\n",
      "Epoch: 10 \tTraining Loss: 0.000115\n",
      "Epoch: 11 \tTraining Loss: 0.000115\n",
      "Epoch: 12 \tTraining Loss: 0.000115\n",
      "Epoch: 13 \tTraining Loss: 0.000115\n",
      "Epoch: 14 \tTraining Loss: 0.000115\n",
      "Epoch: 15 \tTraining Loss: 0.000115\n",
      "Epoch: 16 \tTraining Loss: 0.000115\n",
      "Epoch: 17 \tTraining Loss: 0.000115\n",
      "Epoch: 18 \tTraining Loss: 0.000115\n",
      "Epoch: 1 \tTraining Loss: 0.000166\n",
      "Epoch: 2 \tTraining Loss: 0.000166\n",
      "Epoch: 3 \tTraining Loss: 0.000166\n",
      "Epoch: 4 \tTraining Loss: 0.000166\n",
      "Epoch: 5 \tTraining Loss: 0.000166\n",
      "Epoch: 6 \tTraining Loss: 0.000166\n",
      "Epoch: 7 \tTraining Loss: 0.000166\n",
      "Epoch: 8 \tTraining Loss: 0.000166\n",
      "Epoch: 9 \tTraining Loss: 0.000166\n",
      "Epoch: 10 \tTraining Loss: 0.000166\n",
      "Epoch: 11 \tTraining Loss: 0.000166\n",
      "Epoch: 12 \tTraining Loss: 0.000166\n",
      "Epoch: 13 \tTraining Loss: 0.000166\n",
      "Epoch: 14 \tTraining Loss: 0.000166\n",
      "Epoch: 15 \tTraining Loss: 0.000166\n",
      "Epoch: 16 \tTraining Loss: 0.000166\n",
      "Epoch: 17 \tTraining Loss: 0.000166\n",
      "Epoch: 18 \tTraining Loss: 0.000166\n",
      "Epoch: 1 \tTraining Loss: 0.000259\n",
      "Epoch: 2 \tTraining Loss: 0.000259\n",
      "Epoch: 3 \tTraining Loss: 0.000259\n",
      "Epoch: 4 \tTraining Loss: 0.000259\n",
      "Epoch: 5 \tTraining Loss: 0.000259\n",
      "Epoch: 6 \tTraining Loss: 0.000259\n",
      "Epoch: 7 \tTraining Loss: 0.000259\n",
      "Epoch: 8 \tTraining Loss: 0.000259\n",
      "Epoch: 9 \tTraining Loss: 0.000259\n",
      "Epoch: 10 \tTraining Loss: 0.000259\n",
      "Epoch: 11 \tTraining Loss: 0.000259\n",
      "Epoch: 12 \tTraining Loss: 0.000259\n",
      "Epoch: 13 \tTraining Loss: 0.000259\n",
      "Epoch: 14 \tTraining Loss: 0.000259\n",
      "Epoch: 15 \tTraining Loss: 0.000259\n",
      "Epoch: 16 \tTraining Loss: 0.000259\n",
      "Epoch: 17 \tTraining Loss: 0.000259\n",
      "Epoch: 18 \tTraining Loss: 0.000259\n",
      "Epoch: 1 \tTraining Loss: 0.000216\n",
      "Epoch: 2 \tTraining Loss: 0.000216\n",
      "Epoch: 3 \tTraining Loss: 0.000216\n",
      "Epoch: 4 \tTraining Loss: 0.000216\n",
      "Epoch: 5 \tTraining Loss: 0.000216\n",
      "Epoch: 6 \tTraining Loss: 0.000216\n",
      "Epoch: 7 \tTraining Loss: 0.000216\n",
      "Epoch: 8 \tTraining Loss: 0.000216\n",
      "Epoch: 9 \tTraining Loss: 0.000216\n",
      "Epoch: 10 \tTraining Loss: 0.000216\n",
      "Epoch: 11 \tTraining Loss: 0.000216\n",
      "Epoch: 12 \tTraining Loss: 0.000216\n",
      "Epoch: 13 \tTraining Loss: 0.000216\n",
      "Epoch: 14 \tTraining Loss: 0.000216\n",
      "Epoch: 15 \tTraining Loss: 0.000216\n",
      "Epoch: 16 \tTraining Loss: 0.000216\n",
      "Epoch: 17 \tTraining Loss: 0.000216\n",
      "Epoch: 18 \tTraining Loss: 0.000216\n",
      "Epoch: 1 \tTraining Loss: 0.000253\n",
      "Epoch: 2 \tTraining Loss: 0.000253\n",
      "Epoch: 3 \tTraining Loss: 0.000253\n",
      "Epoch: 4 \tTraining Loss: 0.000253\n",
      "Epoch: 5 \tTraining Loss: 0.000253\n",
      "Epoch: 6 \tTraining Loss: 0.000253\n",
      "Epoch: 7 \tTraining Loss: 0.000253\n",
      "Epoch: 8 \tTraining Loss: 0.000253\n",
      "Epoch: 9 \tTraining Loss: 0.000253\n",
      "Epoch: 10 \tTraining Loss: 0.000253\n",
      "Epoch: 11 \tTraining Loss: 0.000253\n",
      "Epoch: 12 \tTraining Loss: 0.000253\n",
      "Epoch: 13 \tTraining Loss: 0.000253\n",
      "Epoch: 14 \tTraining Loss: 0.000253\n",
      "Epoch: 15 \tTraining Loss: 0.000253\n",
      "Epoch: 16 \tTraining Loss: 0.000253\n",
      "Epoch: 17 \tTraining Loss: 0.000253\n",
      "Epoch: 18 \tTraining Loss: 0.000253\n",
      "Epoch: 1 \tTraining Loss: 0.000270\n",
      "Epoch: 2 \tTraining Loss: 0.000270\n",
      "Epoch: 3 \tTraining Loss: 0.000270\n",
      "Epoch: 4 \tTraining Loss: 0.000270\n",
      "Epoch: 5 \tTraining Loss: 0.000270\n",
      "Epoch: 6 \tTraining Loss: 0.000270\n",
      "Epoch: 7 \tTraining Loss: 0.000270\n",
      "Epoch: 8 \tTraining Loss: 0.000270\n",
      "Epoch: 9 \tTraining Loss: 0.000270\n",
      "Epoch: 10 \tTraining Loss: 0.000270\n",
      "Epoch: 11 \tTraining Loss: 0.000270\n",
      "Epoch: 12 \tTraining Loss: 0.000270\n",
      "Epoch: 13 \tTraining Loss: 0.000270\n",
      "Epoch: 14 \tTraining Loss: 0.000270\n",
      "Epoch: 15 \tTraining Loss: 0.000270\n",
      "Epoch: 16 \tTraining Loss: 0.000270\n",
      "Epoch: 17 \tTraining Loss: 0.000270\n",
      "Epoch: 18 \tTraining Loss: 0.000270\n",
      "Epoch: 1 \tTraining Loss: 0.000152\n",
      "Epoch: 2 \tTraining Loss: 0.000152\n",
      "Epoch: 3 \tTraining Loss: 0.000152\n",
      "Epoch: 4 \tTraining Loss: 0.000152\n",
      "Epoch: 5 \tTraining Loss: 0.000152\n",
      "Epoch: 6 \tTraining Loss: 0.000152\n",
      "Epoch: 7 \tTraining Loss: 0.000152\n",
      "Epoch: 8 \tTraining Loss: 0.000152\n",
      "Epoch: 9 \tTraining Loss: 0.000152\n",
      "Epoch: 10 \tTraining Loss: 0.000152\n",
      "Epoch: 11 \tTraining Loss: 0.000152\n",
      "Epoch: 12 \tTraining Loss: 0.000152\n",
      "Epoch: 13 \tTraining Loss: 0.000152\n",
      "Epoch: 14 \tTraining Loss: 0.000152\n",
      "Epoch: 15 \tTraining Loss: 0.000152\n",
      "Epoch: 16 \tTraining Loss: 0.000152\n",
      "Epoch: 17 \tTraining Loss: 0.000152\n",
      "Epoch: 18 \tTraining Loss: 0.000152\n",
      "Epoch: 1 \tTraining Loss: 0.000247\n",
      "Epoch: 2 \tTraining Loss: 0.000247\n",
      "Epoch: 3 \tTraining Loss: 0.000247\n",
      "Epoch: 4 \tTraining Loss: 0.000247\n",
      "Epoch: 5 \tTraining Loss: 0.000247\n",
      "Epoch: 6 \tTraining Loss: 0.000247\n",
      "Epoch: 7 \tTraining Loss: 0.000247\n",
      "Epoch: 8 \tTraining Loss: 0.000247\n",
      "Epoch: 9 \tTraining Loss: 0.000247\n",
      "Epoch: 10 \tTraining Loss: 0.000247\n",
      "Epoch: 11 \tTraining Loss: 0.000247\n",
      "Epoch: 12 \tTraining Loss: 0.000247\n",
      "Epoch: 13 \tTraining Loss: 0.000247\n",
      "Epoch: 14 \tTraining Loss: 0.000247\n",
      "Epoch: 15 \tTraining Loss: 0.000247\n",
      "Epoch: 16 \tTraining Loss: 0.000247\n",
      "Epoch: 17 \tTraining Loss: 0.000247\n",
      "Epoch: 18 \tTraining Loss: 0.000247\n",
      "Epoch: 1 \tTraining Loss: 0.000402\n",
      "Epoch: 2 \tTraining Loss: 0.000402\n",
      "Epoch: 3 \tTraining Loss: 0.000402\n",
      "Epoch: 4 \tTraining Loss: 0.000402\n",
      "Epoch: 5 \tTraining Loss: 0.000402\n",
      "Epoch: 6 \tTraining Loss: 0.000402\n",
      "Epoch: 7 \tTraining Loss: 0.000402\n",
      "Epoch: 8 \tTraining Loss: 0.000402\n",
      "Epoch: 9 \tTraining Loss: 0.000402\n",
      "Epoch: 10 \tTraining Loss: 0.000402\n",
      "Epoch: 11 \tTraining Loss: 0.000402\n",
      "Epoch: 12 \tTraining Loss: 0.000402\n",
      "Epoch: 13 \tTraining Loss: 0.000402\n",
      "Epoch: 14 \tTraining Loss: 0.000402\n",
      "Epoch: 15 \tTraining Loss: 0.000402\n",
      "Epoch: 16 \tTraining Loss: 0.000402\n",
      "Epoch: 17 \tTraining Loss: 0.000402\n",
      "Epoch: 18 \tTraining Loss: 0.000402\n",
      "Epoch: 1 \tTraining Loss: 0.000251\n",
      "Epoch: 2 \tTraining Loss: 0.000251\n",
      "Epoch: 3 \tTraining Loss: 0.000251\n",
      "Epoch: 4 \tTraining Loss: 0.000251\n",
      "Epoch: 5 \tTraining Loss: 0.000251\n",
      "Epoch: 6 \tTraining Loss: 0.000251\n",
      "Epoch: 7 \tTraining Loss: 0.000251\n",
      "Epoch: 8 \tTraining Loss: 0.000251\n",
      "Epoch: 9 \tTraining Loss: 0.000251\n",
      "Epoch: 10 \tTraining Loss: 0.000251\n",
      "Epoch: 11 \tTraining Loss: 0.000251\n",
      "Epoch: 12 \tTraining Loss: 0.000251\n",
      "Epoch: 13 \tTraining Loss: 0.000251\n",
      "Epoch: 14 \tTraining Loss: 0.000251\n",
      "Epoch: 15 \tTraining Loss: 0.000251\n",
      "Epoch: 16 \tTraining Loss: 0.000251\n",
      "Epoch: 17 \tTraining Loss: 0.000251\n",
      "Epoch: 18 \tTraining Loss: 0.000251\n",
      "Epoch: 1 \tTraining Loss: 0.000286\n",
      "Epoch: 2 \tTraining Loss: 0.000286\n",
      "Epoch: 3 \tTraining Loss: 0.000286\n",
      "Epoch: 4 \tTraining Loss: 0.000286\n",
      "Epoch: 5 \tTraining Loss: 0.000286\n",
      "Epoch: 6 \tTraining Loss: 0.000286\n",
      "Epoch: 7 \tTraining Loss: 0.000286\n",
      "Epoch: 8 \tTraining Loss: 0.000286\n",
      "Epoch: 9 \tTraining Loss: 0.000286\n",
      "Epoch: 10 \tTraining Loss: 0.000286\n",
      "Epoch: 11 \tTraining Loss: 0.000286\n",
      "Epoch: 12 \tTraining Loss: 0.000286\n",
      "Epoch: 13 \tTraining Loss: 0.000286\n",
      "Epoch: 14 \tTraining Loss: 0.000286\n",
      "Epoch: 15 \tTraining Loss: 0.000286\n",
      "Epoch: 16 \tTraining Loss: 0.000286\n",
      "Epoch: 17 \tTraining Loss: 0.000286\n",
      "Epoch: 18 \tTraining Loss: 0.000286\n",
      "Epoch: 1 \tTraining Loss: 0.000134\n",
      "Epoch: 2 \tTraining Loss: 0.000134\n",
      "Epoch: 3 \tTraining Loss: 0.000134\n",
      "Epoch: 4 \tTraining Loss: 0.000134\n",
      "Epoch: 5 \tTraining Loss: 0.000134\n",
      "Epoch: 6 \tTraining Loss: 0.000134\n",
      "Epoch: 7 \tTraining Loss: 0.000134\n",
      "Epoch: 8 \tTraining Loss: 0.000134\n",
      "Epoch: 9 \tTraining Loss: 0.000134\n",
      "Epoch: 10 \tTraining Loss: 0.000134\n",
      "Epoch: 11 \tTraining Loss: 0.000134\n",
      "Epoch: 12 \tTraining Loss: 0.000134\n",
      "Epoch: 13 \tTraining Loss: 0.000134\n",
      "Epoch: 14 \tTraining Loss: 0.000134\n",
      "Epoch: 15 \tTraining Loss: 0.000134\n",
      "Epoch: 16 \tTraining Loss: 0.000134\n",
      "Epoch: 17 \tTraining Loss: 0.000134\n",
      "Epoch: 18 \tTraining Loss: 0.000134\n",
      "Epoch: 1 \tTraining Loss: 0.000158\n",
      "Epoch: 2 \tTraining Loss: 0.000158\n",
      "Epoch: 3 \tTraining Loss: 0.000158\n",
      "Epoch: 4 \tTraining Loss: 0.000158\n",
      "Epoch: 5 \tTraining Loss: 0.000158\n",
      "Epoch: 6 \tTraining Loss: 0.000158\n",
      "Epoch: 7 \tTraining Loss: 0.000158\n",
      "Epoch: 8 \tTraining Loss: 0.000158\n",
      "Epoch: 9 \tTraining Loss: 0.000158\n",
      "Epoch: 10 \tTraining Loss: 0.000158\n",
      "Epoch: 11 \tTraining Loss: 0.000158\n",
      "Epoch: 12 \tTraining Loss: 0.000158\n",
      "Epoch: 13 \tTraining Loss: 0.000158\n",
      "Epoch: 14 \tTraining Loss: 0.000158\n",
      "Epoch: 15 \tTraining Loss: 0.000158\n",
      "Epoch: 16 \tTraining Loss: 0.000158\n",
      "Epoch: 17 \tTraining Loss: 0.000158\n",
      "Epoch: 18 \tTraining Loss: 0.000158\n",
      "Epoch: 1 \tTraining Loss: 0.000204\n",
      "Epoch: 2 \tTraining Loss: 0.000204\n",
      "Epoch: 3 \tTraining Loss: 0.000204\n",
      "Epoch: 4 \tTraining Loss: 0.000204\n",
      "Epoch: 5 \tTraining Loss: 0.000204\n",
      "Epoch: 6 \tTraining Loss: 0.000204\n",
      "Epoch: 7 \tTraining Loss: 0.000204\n",
      "Epoch: 8 \tTraining Loss: 0.000204\n",
      "Epoch: 9 \tTraining Loss: 0.000204\n",
      "Epoch: 10 \tTraining Loss: 0.000204\n",
      "Epoch: 11 \tTraining Loss: 0.000204\n",
      "Epoch: 12 \tTraining Loss: 0.000204\n",
      "Epoch: 13 \tTraining Loss: 0.000204\n",
      "Epoch: 14 \tTraining Loss: 0.000204\n",
      "Epoch: 15 \tTraining Loss: 0.000204\n",
      "Epoch: 16 \tTraining Loss: 0.000204\n",
      "Epoch: 17 \tTraining Loss: 0.000204\n",
      "Epoch: 18 \tTraining Loss: 0.000204\n",
      "Epoch: 1 \tTraining Loss: 0.000227\n",
      "Epoch: 2 \tTraining Loss: 0.000227\n",
      "Epoch: 3 \tTraining Loss: 0.000227\n",
      "Epoch: 4 \tTraining Loss: 0.000227\n",
      "Epoch: 5 \tTraining Loss: 0.000227\n",
      "Epoch: 6 \tTraining Loss: 0.000227\n",
      "Epoch: 7 \tTraining Loss: 0.000227\n",
      "Epoch: 8 \tTraining Loss: 0.000227\n",
      "Epoch: 9 \tTraining Loss: 0.000227\n",
      "Epoch: 10 \tTraining Loss: 0.000227\n",
      "Epoch: 11 \tTraining Loss: 0.000227\n",
      "Epoch: 12 \tTraining Loss: 0.000227\n",
      "Epoch: 13 \tTraining Loss: 0.000227\n",
      "Epoch: 14 \tTraining Loss: 0.000227\n",
      "Epoch: 15 \tTraining Loss: 0.000227\n",
      "Epoch: 16 \tTraining Loss: 0.000227\n",
      "Epoch: 17 \tTraining Loss: 0.000227\n",
      "Epoch: 18 \tTraining Loss: 0.000227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000226\n",
      "Epoch: 2 \tTraining Loss: 0.000226\n",
      "Epoch: 3 \tTraining Loss: 0.000226\n",
      "Epoch: 4 \tTraining Loss: 0.000226\n",
      "Epoch: 5 \tTraining Loss: 0.000226\n",
      "Epoch: 6 \tTraining Loss: 0.000226\n",
      "Epoch: 7 \tTraining Loss: 0.000226\n",
      "Epoch: 8 \tTraining Loss: 0.000226\n",
      "Epoch: 9 \tTraining Loss: 0.000226\n",
      "Epoch: 10 \tTraining Loss: 0.000226\n",
      "Epoch: 11 \tTraining Loss: 0.000226\n",
      "Epoch: 12 \tTraining Loss: 0.000226\n",
      "Epoch: 13 \tTraining Loss: 0.000226\n",
      "Epoch: 14 \tTraining Loss: 0.000226\n",
      "Epoch: 15 \tTraining Loss: 0.000226\n",
      "Epoch: 16 \tTraining Loss: 0.000226\n",
      "Epoch: 17 \tTraining Loss: 0.000226\n",
      "Epoch: 18 \tTraining Loss: 0.000226\n",
      "Epoch: 1 \tTraining Loss: 0.000166\n",
      "Epoch: 2 \tTraining Loss: 0.000166\n",
      "Epoch: 3 \tTraining Loss: 0.000166\n",
      "Epoch: 4 \tTraining Loss: 0.000166\n",
      "Epoch: 5 \tTraining Loss: 0.000166\n",
      "Epoch: 6 \tTraining Loss: 0.000166\n",
      "Epoch: 7 \tTraining Loss: 0.000166\n",
      "Epoch: 8 \tTraining Loss: 0.000166\n",
      "Epoch: 9 \tTraining Loss: 0.000166\n",
      "Epoch: 10 \tTraining Loss: 0.000166\n",
      "Epoch: 11 \tTraining Loss: 0.000166\n",
      "Epoch: 12 \tTraining Loss: 0.000166\n",
      "Epoch: 13 \tTraining Loss: 0.000166\n",
      "Epoch: 14 \tTraining Loss: 0.000166\n",
      "Epoch: 15 \tTraining Loss: 0.000166\n",
      "Epoch: 16 \tTraining Loss: 0.000166\n",
      "Epoch: 17 \tTraining Loss: 0.000166\n",
      "Epoch: 18 \tTraining Loss: 0.000166\n",
      "Epoch: 1 \tTraining Loss: 0.000184\n",
      "Epoch: 2 \tTraining Loss: 0.000184\n",
      "Epoch: 3 \tTraining Loss: 0.000184\n",
      "Epoch: 4 \tTraining Loss: 0.000184\n",
      "Epoch: 5 \tTraining Loss: 0.000184\n",
      "Epoch: 6 \tTraining Loss: 0.000184\n",
      "Epoch: 7 \tTraining Loss: 0.000184\n",
      "Epoch: 8 \tTraining Loss: 0.000184\n",
      "Epoch: 9 \tTraining Loss: 0.000184\n",
      "Epoch: 10 \tTraining Loss: 0.000184\n",
      "Epoch: 11 \tTraining Loss: 0.000184\n",
      "Epoch: 12 \tTraining Loss: 0.000184\n",
      "Epoch: 13 \tTraining Loss: 0.000184\n",
      "Epoch: 14 \tTraining Loss: 0.000184\n",
      "Epoch: 15 \tTraining Loss: 0.000184\n",
      "Epoch: 16 \tTraining Loss: 0.000184\n",
      "Epoch: 17 \tTraining Loss: 0.000184\n",
      "Epoch: 18 \tTraining Loss: 0.000184\n",
      "Epoch: 1 \tTraining Loss: 0.000376\n",
      "Epoch: 2 \tTraining Loss: 0.000376\n",
      "Epoch: 3 \tTraining Loss: 0.000376\n",
      "Epoch: 4 \tTraining Loss: 0.000376\n",
      "Epoch: 5 \tTraining Loss: 0.000376\n",
      "Epoch: 6 \tTraining Loss: 0.000376\n",
      "Epoch: 7 \tTraining Loss: 0.000376\n",
      "Epoch: 8 \tTraining Loss: 0.000376\n",
      "Epoch: 9 \tTraining Loss: 0.000376\n",
      "Epoch: 10 \tTraining Loss: 0.000376\n",
      "Epoch: 11 \tTraining Loss: 0.000376\n",
      "Epoch: 12 \tTraining Loss: 0.000376\n",
      "Epoch: 13 \tTraining Loss: 0.000376\n",
      "Epoch: 14 \tTraining Loss: 0.000376\n",
      "Epoch: 15 \tTraining Loss: 0.000376\n",
      "Epoch: 16 \tTraining Loss: 0.000376\n",
      "Epoch: 17 \tTraining Loss: 0.000376\n",
      "Epoch: 18 \tTraining Loss: 0.000376\n",
      "Epoch: 1 \tTraining Loss: 0.000091\n",
      "Epoch: 2 \tTraining Loss: 0.000091\n",
      "Epoch: 3 \tTraining Loss: 0.000091\n",
      "Epoch: 4 \tTraining Loss: 0.000091\n",
      "Epoch: 5 \tTraining Loss: 0.000091\n",
      "Epoch: 6 \tTraining Loss: 0.000091\n",
      "Epoch: 7 \tTraining Loss: 0.000091\n",
      "Epoch: 8 \tTraining Loss: 0.000091\n",
      "Epoch: 9 \tTraining Loss: 0.000091\n",
      "Epoch: 10 \tTraining Loss: 0.000091\n",
      "Epoch: 11 \tTraining Loss: 0.000091\n",
      "Epoch: 12 \tTraining Loss: 0.000091\n",
      "Epoch: 13 \tTraining Loss: 0.000091\n",
      "Epoch: 14 \tTraining Loss: 0.000091\n",
      "Epoch: 15 \tTraining Loss: 0.000091\n",
      "Epoch: 16 \tTraining Loss: 0.000091\n",
      "Epoch: 17 \tTraining Loss: 0.000091\n",
      "Epoch: 18 \tTraining Loss: 0.000091\n",
      "Epoch: 1 \tTraining Loss: 0.000236\n",
      "Epoch: 2 \tTraining Loss: 0.000236\n",
      "Epoch: 3 \tTraining Loss: 0.000236\n",
      "Epoch: 4 \tTraining Loss: 0.000236\n",
      "Epoch: 5 \tTraining Loss: 0.000236\n",
      "Epoch: 6 \tTraining Loss: 0.000236\n",
      "Epoch: 7 \tTraining Loss: 0.000236\n",
      "Epoch: 8 \tTraining Loss: 0.000236\n",
      "Epoch: 9 \tTraining Loss: 0.000236\n",
      "Epoch: 10 \tTraining Loss: 0.000236\n",
      "Epoch: 11 \tTraining Loss: 0.000236\n",
      "Epoch: 12 \tTraining Loss: 0.000236\n",
      "Epoch: 13 \tTraining Loss: 0.000236\n",
      "Epoch: 14 \tTraining Loss: 0.000236\n",
      "Epoch: 15 \tTraining Loss: 0.000236\n",
      "Epoch: 16 \tTraining Loss: 0.000236\n",
      "Epoch: 17 \tTraining Loss: 0.000236\n",
      "Epoch: 18 \tTraining Loss: 0.000236\n",
      "Epoch: 1 \tTraining Loss: 0.000145\n",
      "Epoch: 2 \tTraining Loss: 0.000145\n",
      "Epoch: 3 \tTraining Loss: 0.000145\n",
      "Epoch: 4 \tTraining Loss: 0.000145\n",
      "Epoch: 5 \tTraining Loss: 0.000145\n",
      "Epoch: 6 \tTraining Loss: 0.000145\n",
      "Epoch: 7 \tTraining Loss: 0.000145\n",
      "Epoch: 8 \tTraining Loss: 0.000145\n",
      "Epoch: 9 \tTraining Loss: 0.000145\n",
      "Epoch: 10 \tTraining Loss: 0.000145\n",
      "Epoch: 11 \tTraining Loss: 0.000145\n",
      "Epoch: 12 \tTraining Loss: 0.000145\n",
      "Epoch: 13 \tTraining Loss: 0.000145\n",
      "Epoch: 14 \tTraining Loss: 0.000145\n",
      "Epoch: 15 \tTraining Loss: 0.000145\n",
      "Epoch: 16 \tTraining Loss: 0.000145\n",
      "Epoch: 17 \tTraining Loss: 0.000145\n",
      "Epoch: 18 \tTraining Loss: 0.000145\n",
      "Epoch: 1 \tTraining Loss: 0.000145\n",
      "Epoch: 2 \tTraining Loss: 0.000145\n",
      "Epoch: 3 \tTraining Loss: 0.000145\n",
      "Epoch: 4 \tTraining Loss: 0.000145\n",
      "Epoch: 5 \tTraining Loss: 0.000145\n",
      "Epoch: 6 \tTraining Loss: 0.000145\n",
      "Epoch: 7 \tTraining Loss: 0.000145\n",
      "Epoch: 8 \tTraining Loss: 0.000145\n",
      "Epoch: 9 \tTraining Loss: 0.000145\n",
      "Epoch: 10 \tTraining Loss: 0.000145\n",
      "Epoch: 11 \tTraining Loss: 0.000145\n",
      "Epoch: 12 \tTraining Loss: 0.000145\n",
      "Epoch: 13 \tTraining Loss: 0.000145\n",
      "Epoch: 14 \tTraining Loss: 0.000145\n",
      "Epoch: 15 \tTraining Loss: 0.000145\n",
      "Epoch: 16 \tTraining Loss: 0.000145\n",
      "Epoch: 17 \tTraining Loss: 0.000145\n",
      "Epoch: 18 \tTraining Loss: 0.000145\n",
      "Epoch: 1 \tTraining Loss: 0.000210\n",
      "Epoch: 2 \tTraining Loss: 0.000210\n",
      "Epoch: 3 \tTraining Loss: 0.000210\n",
      "Epoch: 4 \tTraining Loss: 0.000210\n",
      "Epoch: 5 \tTraining Loss: 0.000210\n",
      "Epoch: 6 \tTraining Loss: 0.000210\n",
      "Epoch: 7 \tTraining Loss: 0.000210\n",
      "Epoch: 8 \tTraining Loss: 0.000210\n",
      "Epoch: 9 \tTraining Loss: 0.000210\n",
      "Epoch: 10 \tTraining Loss: 0.000210\n",
      "Epoch: 11 \tTraining Loss: 0.000210\n",
      "Epoch: 12 \tTraining Loss: 0.000210\n",
      "Epoch: 13 \tTraining Loss: 0.000210\n",
      "Epoch: 14 \tTraining Loss: 0.000210\n",
      "Epoch: 15 \tTraining Loss: 0.000210\n",
      "Epoch: 16 \tTraining Loss: 0.000210\n",
      "Epoch: 17 \tTraining Loss: 0.000210\n",
      "Epoch: 18 \tTraining Loss: 0.000210\n",
      "Epoch: 1 \tTraining Loss: 0.000140\n",
      "Epoch: 2 \tTraining Loss: 0.000140\n",
      "Epoch: 3 \tTraining Loss: 0.000140\n",
      "Epoch: 4 \tTraining Loss: 0.000140\n",
      "Epoch: 5 \tTraining Loss: 0.000140\n",
      "Epoch: 6 \tTraining Loss: 0.000140\n",
      "Epoch: 7 \tTraining Loss: 0.000140\n",
      "Epoch: 8 \tTraining Loss: 0.000140\n",
      "Epoch: 9 \tTraining Loss: 0.000140\n",
      "Epoch: 10 \tTraining Loss: 0.000140\n",
      "Epoch: 11 \tTraining Loss: 0.000140\n",
      "Epoch: 12 \tTraining Loss: 0.000140\n",
      "Epoch: 13 \tTraining Loss: 0.000140\n",
      "Epoch: 14 \tTraining Loss: 0.000140\n",
      "Epoch: 15 \tTraining Loss: 0.000140\n",
      "Epoch: 16 \tTraining Loss: 0.000140\n",
      "Epoch: 17 \tTraining Loss: 0.000140\n",
      "Epoch: 18 \tTraining Loss: 0.000140\n",
      "Epoch: 1 \tTraining Loss: 0.000185\n",
      "Epoch: 2 \tTraining Loss: 0.000185\n",
      "Epoch: 3 \tTraining Loss: 0.000185\n",
      "Epoch: 4 \tTraining Loss: 0.000185\n",
      "Epoch: 5 \tTraining Loss: 0.000185\n",
      "Epoch: 6 \tTraining Loss: 0.000185\n",
      "Epoch: 7 \tTraining Loss: 0.000185\n",
      "Epoch: 8 \tTraining Loss: 0.000185\n",
      "Epoch: 9 \tTraining Loss: 0.000185\n",
      "Epoch: 10 \tTraining Loss: 0.000185\n",
      "Epoch: 11 \tTraining Loss: 0.000185\n",
      "Epoch: 12 \tTraining Loss: 0.000185\n",
      "Epoch: 13 \tTraining Loss: 0.000185\n",
      "Epoch: 14 \tTraining Loss: 0.000185\n",
      "Epoch: 15 \tTraining Loss: 0.000185\n",
      "Epoch: 16 \tTraining Loss: 0.000185\n",
      "Epoch: 17 \tTraining Loss: 0.000185\n",
      "Epoch: 18 \tTraining Loss: 0.000185\n",
      "Epoch: 1 \tTraining Loss: 0.000076\n",
      "Epoch: 2 \tTraining Loss: 0.000076\n",
      "Epoch: 3 \tTraining Loss: 0.000076\n",
      "Epoch: 4 \tTraining Loss: 0.000076\n",
      "Epoch: 5 \tTraining Loss: 0.000076\n",
      "Epoch: 6 \tTraining Loss: 0.000076\n",
      "Epoch: 7 \tTraining Loss: 0.000076\n",
      "Epoch: 8 \tTraining Loss: 0.000076\n",
      "Epoch: 9 \tTraining Loss: 0.000076\n",
      "Epoch: 10 \tTraining Loss: 0.000076\n",
      "Epoch: 11 \tTraining Loss: 0.000076\n",
      "Epoch: 12 \tTraining Loss: 0.000076\n",
      "Epoch: 13 \tTraining Loss: 0.000076\n",
      "Epoch: 14 \tTraining Loss: 0.000076\n",
      "Epoch: 15 \tTraining Loss: 0.000076\n",
      "Epoch: 16 \tTraining Loss: 0.000076\n",
      "Epoch: 17 \tTraining Loss: 0.000076\n",
      "Epoch: 18 \tTraining Loss: 0.000076\n",
      "Epoch: 1 \tTraining Loss: 0.000139\n",
      "Epoch: 2 \tTraining Loss: 0.000139\n",
      "Epoch: 3 \tTraining Loss: 0.000139\n",
      "Epoch: 4 \tTraining Loss: 0.000139\n",
      "Epoch: 5 \tTraining Loss: 0.000139\n",
      "Epoch: 6 \tTraining Loss: 0.000139\n",
      "Epoch: 7 \tTraining Loss: 0.000139\n",
      "Epoch: 8 \tTraining Loss: 0.000139\n",
      "Epoch: 9 \tTraining Loss: 0.000139\n",
      "Epoch: 10 \tTraining Loss: 0.000139\n",
      "Epoch: 11 \tTraining Loss: 0.000139\n",
      "Epoch: 12 \tTraining Loss: 0.000139\n",
      "Epoch: 13 \tTraining Loss: 0.000139\n",
      "Epoch: 14 \tTraining Loss: 0.000139\n",
      "Epoch: 15 \tTraining Loss: 0.000139\n",
      "Epoch: 16 \tTraining Loss: 0.000139\n",
      "Epoch: 17 \tTraining Loss: 0.000139\n",
      "Epoch: 18 \tTraining Loss: 0.000139\n",
      "Epoch: 1 \tTraining Loss: 0.000117\n",
      "Epoch: 2 \tTraining Loss: 0.000117\n",
      "Epoch: 3 \tTraining Loss: 0.000117\n",
      "Epoch: 4 \tTraining Loss: 0.000117\n",
      "Epoch: 5 \tTraining Loss: 0.000117\n",
      "Epoch: 6 \tTraining Loss: 0.000117\n",
      "Epoch: 7 \tTraining Loss: 0.000117\n",
      "Epoch: 8 \tTraining Loss: 0.000117\n",
      "Epoch: 9 \tTraining Loss: 0.000117\n",
      "Epoch: 10 \tTraining Loss: 0.000117\n",
      "Epoch: 11 \tTraining Loss: 0.000117\n",
      "Epoch: 12 \tTraining Loss: 0.000117\n",
      "Epoch: 13 \tTraining Loss: 0.000117\n",
      "Epoch: 14 \tTraining Loss: 0.000117\n",
      "Epoch: 15 \tTraining Loss: 0.000117\n",
      "Epoch: 16 \tTraining Loss: 0.000117\n",
      "Epoch: 17 \tTraining Loss: 0.000117\n",
      "Epoch: 18 \tTraining Loss: 0.000117\n",
      "Epoch: 1 \tTraining Loss: 0.000204\n",
      "Epoch: 2 \tTraining Loss: 0.000204\n",
      "Epoch: 3 \tTraining Loss: 0.000204\n",
      "Epoch: 4 \tTraining Loss: 0.000204\n",
      "Epoch: 5 \tTraining Loss: 0.000204\n",
      "Epoch: 6 \tTraining Loss: 0.000204\n",
      "Epoch: 7 \tTraining Loss: 0.000204\n",
      "Epoch: 8 \tTraining Loss: 0.000204\n",
      "Epoch: 9 \tTraining Loss: 0.000204\n",
      "Epoch: 10 \tTraining Loss: 0.000204\n",
      "Epoch: 11 \tTraining Loss: 0.000204\n",
      "Epoch: 12 \tTraining Loss: 0.000204\n",
      "Epoch: 13 \tTraining Loss: 0.000204\n",
      "Epoch: 14 \tTraining Loss: 0.000204\n",
      "Epoch: 15 \tTraining Loss: 0.000204\n",
      "Epoch: 16 \tTraining Loss: 0.000204\n",
      "Epoch: 17 \tTraining Loss: 0.000204\n",
      "Epoch: 18 \tTraining Loss: 0.000204\n",
      "Epoch: 1 \tTraining Loss: 0.000289\n",
      "Epoch: 2 \tTraining Loss: 0.000289\n",
      "Epoch: 3 \tTraining Loss: 0.000289\n",
      "Epoch: 4 \tTraining Loss: 0.000289\n",
      "Epoch: 5 \tTraining Loss: 0.000289\n",
      "Epoch: 6 \tTraining Loss: 0.000289\n",
      "Epoch: 7 \tTraining Loss: 0.000289\n",
      "Epoch: 8 \tTraining Loss: 0.000289\n",
      "Epoch: 9 \tTraining Loss: 0.000289\n",
      "Epoch: 10 \tTraining Loss: 0.000289\n",
      "Epoch: 11 \tTraining Loss: 0.000289\n",
      "Epoch: 12 \tTraining Loss: 0.000289\n",
      "Epoch: 13 \tTraining Loss: 0.000289\n",
      "Epoch: 14 \tTraining Loss: 0.000289\n",
      "Epoch: 15 \tTraining Loss: 0.000289\n",
      "Epoch: 16 \tTraining Loss: 0.000289\n",
      "Epoch: 17 \tTraining Loss: 0.000289\n",
      "Epoch: 18 \tTraining Loss: 0.000289\n",
      "Epoch: 1 \tTraining Loss: 0.000113\n",
      "Epoch: 2 \tTraining Loss: 0.000113\n",
      "Epoch: 3 \tTraining Loss: 0.000113\n",
      "Epoch: 4 \tTraining Loss: 0.000113\n",
      "Epoch: 5 \tTraining Loss: 0.000113\n",
      "Epoch: 6 \tTraining Loss: 0.000113\n",
      "Epoch: 7 \tTraining Loss: 0.000113\n",
      "Epoch: 8 \tTraining Loss: 0.000113\n",
      "Epoch: 9 \tTraining Loss: 0.000113\n",
      "Epoch: 10 \tTraining Loss: 0.000113\n",
      "Epoch: 11 \tTraining Loss: 0.000113\n",
      "Epoch: 12 \tTraining Loss: 0.000113\n",
      "Epoch: 13 \tTraining Loss: 0.000113\n",
      "Epoch: 14 \tTraining Loss: 0.000113\n",
      "Epoch: 15 \tTraining Loss: 0.000113\n",
      "Epoch: 16 \tTraining Loss: 0.000113\n",
      "Epoch: 17 \tTraining Loss: 0.000113\n",
      "Epoch: 18 \tTraining Loss: 0.000113\n",
      "Epoch: 1 \tTraining Loss: 0.000192\n",
      "Epoch: 2 \tTraining Loss: 0.000192\n",
      "Epoch: 3 \tTraining Loss: 0.000192\n",
      "Epoch: 4 \tTraining Loss: 0.000192\n",
      "Epoch: 5 \tTraining Loss: 0.000192\n",
      "Epoch: 6 \tTraining Loss: 0.000192\n",
      "Epoch: 7 \tTraining Loss: 0.000192\n",
      "Epoch: 8 \tTraining Loss: 0.000192\n",
      "Epoch: 9 \tTraining Loss: 0.000192\n",
      "Epoch: 10 \tTraining Loss: 0.000192\n",
      "Epoch: 11 \tTraining Loss: 0.000192\n",
      "Epoch: 12 \tTraining Loss: 0.000192\n",
      "Epoch: 13 \tTraining Loss: 0.000192\n",
      "Epoch: 14 \tTraining Loss: 0.000192\n",
      "Epoch: 15 \tTraining Loss: 0.000192\n",
      "Epoch: 16 \tTraining Loss: 0.000192\n",
      "Epoch: 17 \tTraining Loss: 0.000192\n",
      "Epoch: 18 \tTraining Loss: 0.000192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000169\n",
      "Epoch: 2 \tTraining Loss: 0.000169\n",
      "Epoch: 3 \tTraining Loss: 0.000169\n",
      "Epoch: 4 \tTraining Loss: 0.000169\n",
      "Epoch: 5 \tTraining Loss: 0.000169\n",
      "Epoch: 6 \tTraining Loss: 0.000169\n",
      "Epoch: 7 \tTraining Loss: 0.000169\n",
      "Epoch: 8 \tTraining Loss: 0.000169\n",
      "Epoch: 9 \tTraining Loss: 0.000169\n",
      "Epoch: 10 \tTraining Loss: 0.000169\n",
      "Epoch: 11 \tTraining Loss: 0.000169\n",
      "Epoch: 12 \tTraining Loss: 0.000169\n",
      "Epoch: 13 \tTraining Loss: 0.000169\n",
      "Epoch: 14 \tTraining Loss: 0.000169\n",
      "Epoch: 15 \tTraining Loss: 0.000169\n",
      "Epoch: 16 \tTraining Loss: 0.000169\n",
      "Epoch: 17 \tTraining Loss: 0.000169\n",
      "Epoch: 18 \tTraining Loss: 0.000169\n",
      "Epoch: 1 \tTraining Loss: 0.000169\n",
      "Epoch: 2 \tTraining Loss: 0.000169\n",
      "Epoch: 3 \tTraining Loss: 0.000169\n",
      "Epoch: 4 \tTraining Loss: 0.000169\n",
      "Epoch: 5 \tTraining Loss: 0.000169\n",
      "Epoch: 6 \tTraining Loss: 0.000169\n",
      "Epoch: 7 \tTraining Loss: 0.000169\n",
      "Epoch: 8 \tTraining Loss: 0.000169\n",
      "Epoch: 9 \tTraining Loss: 0.000169\n",
      "Epoch: 10 \tTraining Loss: 0.000169\n",
      "Epoch: 11 \tTraining Loss: 0.000169\n",
      "Epoch: 12 \tTraining Loss: 0.000169\n",
      "Epoch: 13 \tTraining Loss: 0.000169\n",
      "Epoch: 14 \tTraining Loss: 0.000169\n",
      "Epoch: 15 \tTraining Loss: 0.000169\n",
      "Epoch: 16 \tTraining Loss: 0.000169\n",
      "Epoch: 17 \tTraining Loss: 0.000169\n",
      "Epoch: 18 \tTraining Loss: 0.000169\n",
      "Epoch: 1 \tTraining Loss: 0.000292\n",
      "Epoch: 2 \tTraining Loss: 0.000292\n",
      "Epoch: 3 \tTraining Loss: 0.000292\n",
      "Epoch: 4 \tTraining Loss: 0.000292\n",
      "Epoch: 5 \tTraining Loss: 0.000292\n",
      "Epoch: 6 \tTraining Loss: 0.000292\n",
      "Epoch: 7 \tTraining Loss: 0.000292\n",
      "Epoch: 8 \tTraining Loss: 0.000292\n",
      "Epoch: 9 \tTraining Loss: 0.000292\n",
      "Epoch: 10 \tTraining Loss: 0.000292\n",
      "Epoch: 11 \tTraining Loss: 0.000292\n",
      "Epoch: 12 \tTraining Loss: 0.000292\n",
      "Epoch: 13 \tTraining Loss: 0.000292\n",
      "Epoch: 14 \tTraining Loss: 0.000292\n",
      "Epoch: 15 \tTraining Loss: 0.000292\n",
      "Epoch: 16 \tTraining Loss: 0.000292\n",
      "Epoch: 17 \tTraining Loss: 0.000292\n",
      "Epoch: 18 \tTraining Loss: 0.000292\n",
      "Epoch: 1 \tTraining Loss: 0.000257\n",
      "Epoch: 2 \tTraining Loss: 0.000257\n",
      "Epoch: 3 \tTraining Loss: 0.000257\n",
      "Epoch: 4 \tTraining Loss: 0.000257\n",
      "Epoch: 5 \tTraining Loss: 0.000257\n",
      "Epoch: 6 \tTraining Loss: 0.000257\n",
      "Epoch: 7 \tTraining Loss: 0.000257\n",
      "Epoch: 8 \tTraining Loss: 0.000257\n",
      "Epoch: 9 \tTraining Loss: 0.000257\n",
      "Epoch: 10 \tTraining Loss: 0.000257\n",
      "Epoch: 11 \tTraining Loss: 0.000257\n",
      "Epoch: 12 \tTraining Loss: 0.000257\n",
      "Epoch: 13 \tTraining Loss: 0.000257\n",
      "Epoch: 14 \tTraining Loss: 0.000257\n",
      "Epoch: 15 \tTraining Loss: 0.000257\n",
      "Epoch: 16 \tTraining Loss: 0.000257\n",
      "Epoch: 17 \tTraining Loss: 0.000257\n",
      "Epoch: 18 \tTraining Loss: 0.000257\n",
      "Epoch: 1 \tTraining Loss: 0.000076\n",
      "Epoch: 2 \tTraining Loss: 0.000076\n",
      "Epoch: 3 \tTraining Loss: 0.000076\n",
      "Epoch: 4 \tTraining Loss: 0.000076\n",
      "Epoch: 5 \tTraining Loss: 0.000076\n",
      "Epoch: 6 \tTraining Loss: 0.000076\n",
      "Epoch: 7 \tTraining Loss: 0.000076\n",
      "Epoch: 8 \tTraining Loss: 0.000076\n",
      "Epoch: 9 \tTraining Loss: 0.000076\n",
      "Epoch: 10 \tTraining Loss: 0.000076\n",
      "Epoch: 11 \tTraining Loss: 0.000076\n",
      "Epoch: 12 \tTraining Loss: 0.000076\n",
      "Epoch: 13 \tTraining Loss: 0.000076\n",
      "Epoch: 14 \tTraining Loss: 0.000076\n",
      "Epoch: 15 \tTraining Loss: 0.000076\n",
      "Epoch: 16 \tTraining Loss: 0.000076\n",
      "Epoch: 17 \tTraining Loss: 0.000076\n",
      "Epoch: 18 \tTraining Loss: 0.000076\n",
      "Epoch: 1 \tTraining Loss: 0.000162\n",
      "Epoch: 2 \tTraining Loss: 0.000162\n",
      "Epoch: 3 \tTraining Loss: 0.000162\n",
      "Epoch: 4 \tTraining Loss: 0.000162\n",
      "Epoch: 5 \tTraining Loss: 0.000162\n",
      "Epoch: 6 \tTraining Loss: 0.000162\n",
      "Epoch: 7 \tTraining Loss: 0.000162\n",
      "Epoch: 8 \tTraining Loss: 0.000162\n",
      "Epoch: 9 \tTraining Loss: 0.000162\n",
      "Epoch: 10 \tTraining Loss: 0.000162\n",
      "Epoch: 11 \tTraining Loss: 0.000162\n",
      "Epoch: 12 \tTraining Loss: 0.000162\n",
      "Epoch: 13 \tTraining Loss: 0.000162\n",
      "Epoch: 14 \tTraining Loss: 0.000162\n",
      "Epoch: 15 \tTraining Loss: 0.000162\n",
      "Epoch: 16 \tTraining Loss: 0.000162\n",
      "Epoch: 17 \tTraining Loss: 0.000162\n",
      "Epoch: 18 \tTraining Loss: 0.000162\n",
      "Epoch: 1 \tTraining Loss: 0.000154\n",
      "Epoch: 2 \tTraining Loss: 0.000154\n",
      "Epoch: 3 \tTraining Loss: 0.000154\n",
      "Epoch: 4 \tTraining Loss: 0.000154\n",
      "Epoch: 5 \tTraining Loss: 0.000154\n",
      "Epoch: 6 \tTraining Loss: 0.000154\n",
      "Epoch: 7 \tTraining Loss: 0.000154\n",
      "Epoch: 8 \tTraining Loss: 0.000154\n",
      "Epoch: 9 \tTraining Loss: 0.000154\n",
      "Epoch: 10 \tTraining Loss: 0.000154\n",
      "Epoch: 11 \tTraining Loss: 0.000154\n",
      "Epoch: 12 \tTraining Loss: 0.000154\n",
      "Epoch: 13 \tTraining Loss: 0.000154\n",
      "Epoch: 14 \tTraining Loss: 0.000154\n",
      "Epoch: 15 \tTraining Loss: 0.000154\n",
      "Epoch: 16 \tTraining Loss: 0.000154\n",
      "Epoch: 17 \tTraining Loss: 0.000154\n",
      "Epoch: 18 \tTraining Loss: 0.000154\n",
      "Epoch: 1 \tTraining Loss: 0.000183\n",
      "Epoch: 2 \tTraining Loss: 0.000183\n",
      "Epoch: 3 \tTraining Loss: 0.000183\n",
      "Epoch: 4 \tTraining Loss: 0.000183\n",
      "Epoch: 5 \tTraining Loss: 0.000183\n",
      "Epoch: 6 \tTraining Loss: 0.000183\n",
      "Epoch: 7 \tTraining Loss: 0.000183\n",
      "Epoch: 8 \tTraining Loss: 0.000183\n",
      "Epoch: 9 \tTraining Loss: 0.000183\n",
      "Epoch: 10 \tTraining Loss: 0.000183\n",
      "Epoch: 11 \tTraining Loss: 0.000183\n",
      "Epoch: 12 \tTraining Loss: 0.000183\n",
      "Epoch: 13 \tTraining Loss: 0.000183\n",
      "Epoch: 14 \tTraining Loss: 0.000183\n",
      "Epoch: 15 \tTraining Loss: 0.000183\n",
      "Epoch: 16 \tTraining Loss: 0.000183\n",
      "Epoch: 17 \tTraining Loss: 0.000183\n",
      "Epoch: 18 \tTraining Loss: 0.000183\n",
      "Epoch: 1 \tTraining Loss: 0.000247\n",
      "Epoch: 2 \tTraining Loss: 0.000247\n",
      "Epoch: 3 \tTraining Loss: 0.000247\n",
      "Epoch: 4 \tTraining Loss: 0.000247\n",
      "Epoch: 5 \tTraining Loss: 0.000247\n",
      "Epoch: 6 \tTraining Loss: 0.000247\n",
      "Epoch: 7 \tTraining Loss: 0.000247\n",
      "Epoch: 8 \tTraining Loss: 0.000247\n",
      "Epoch: 9 \tTraining Loss: 0.000247\n",
      "Epoch: 10 \tTraining Loss: 0.000247\n",
      "Epoch: 11 \tTraining Loss: 0.000247\n",
      "Epoch: 12 \tTraining Loss: 0.000247\n",
      "Epoch: 13 \tTraining Loss: 0.000247\n",
      "Epoch: 14 \tTraining Loss: 0.000247\n",
      "Epoch: 15 \tTraining Loss: 0.000247\n",
      "Epoch: 16 \tTraining Loss: 0.000247\n",
      "Epoch: 17 \tTraining Loss: 0.000247\n",
      "Epoch: 18 \tTraining Loss: 0.000247\n",
      "Epoch: 1 \tTraining Loss: 0.000335\n",
      "Epoch: 2 \tTraining Loss: 0.000335\n",
      "Epoch: 3 \tTraining Loss: 0.000335\n",
      "Epoch: 4 \tTraining Loss: 0.000335\n",
      "Epoch: 5 \tTraining Loss: 0.000335\n",
      "Epoch: 6 \tTraining Loss: 0.000335\n",
      "Epoch: 7 \tTraining Loss: 0.000335\n",
      "Epoch: 8 \tTraining Loss: 0.000335\n",
      "Epoch: 9 \tTraining Loss: 0.000335\n",
      "Epoch: 10 \tTraining Loss: 0.000335\n",
      "Epoch: 11 \tTraining Loss: 0.000335\n",
      "Epoch: 12 \tTraining Loss: 0.000335\n",
      "Epoch: 13 \tTraining Loss: 0.000335\n",
      "Epoch: 14 \tTraining Loss: 0.000335\n",
      "Epoch: 15 \tTraining Loss: 0.000335\n",
      "Epoch: 16 \tTraining Loss: 0.000335\n",
      "Epoch: 17 \tTraining Loss: 0.000335\n",
      "Epoch: 18 \tTraining Loss: 0.000335\n",
      "Epoch: 1 \tTraining Loss: 0.000193\n",
      "Epoch: 2 \tTraining Loss: 0.000193\n",
      "Epoch: 3 \tTraining Loss: 0.000193\n",
      "Epoch: 4 \tTraining Loss: 0.000193\n",
      "Epoch: 5 \tTraining Loss: 0.000193\n",
      "Epoch: 6 \tTraining Loss: 0.000193\n",
      "Epoch: 7 \tTraining Loss: 0.000193\n",
      "Epoch: 8 \tTraining Loss: 0.000193\n",
      "Epoch: 9 \tTraining Loss: 0.000193\n",
      "Epoch: 10 \tTraining Loss: 0.000193\n",
      "Epoch: 11 \tTraining Loss: 0.000193\n",
      "Epoch: 12 \tTraining Loss: 0.000193\n",
      "Epoch: 13 \tTraining Loss: 0.000193\n",
      "Epoch: 14 \tTraining Loss: 0.000193\n",
      "Epoch: 15 \tTraining Loss: 0.000193\n",
      "Epoch: 16 \tTraining Loss: 0.000193\n",
      "Epoch: 17 \tTraining Loss: 0.000193\n",
      "Epoch: 18 \tTraining Loss: 0.000193\n",
      "Epoch: 1 \tTraining Loss: 0.000109\n",
      "Epoch: 2 \tTraining Loss: 0.000109\n",
      "Epoch: 3 \tTraining Loss: 0.000109\n",
      "Epoch: 4 \tTraining Loss: 0.000109\n",
      "Epoch: 5 \tTraining Loss: 0.000109\n",
      "Epoch: 6 \tTraining Loss: 0.000109\n",
      "Epoch: 7 \tTraining Loss: 0.000109\n",
      "Epoch: 8 \tTraining Loss: 0.000109\n",
      "Epoch: 9 \tTraining Loss: 0.000109\n",
      "Epoch: 10 \tTraining Loss: 0.000109\n",
      "Epoch: 11 \tTraining Loss: 0.000109\n",
      "Epoch: 12 \tTraining Loss: 0.000109\n",
      "Epoch: 13 \tTraining Loss: 0.000109\n",
      "Epoch: 14 \tTraining Loss: 0.000109\n",
      "Epoch: 15 \tTraining Loss: 0.000109\n",
      "Epoch: 16 \tTraining Loss: 0.000109\n",
      "Epoch: 17 \tTraining Loss: 0.000109\n",
      "Epoch: 18 \tTraining Loss: 0.000109\n",
      "Epoch: 1 \tTraining Loss: 0.000167\n",
      "Epoch: 2 \tTraining Loss: 0.000167\n",
      "Epoch: 3 \tTraining Loss: 0.000167\n",
      "Epoch: 4 \tTraining Loss: 0.000167\n",
      "Epoch: 5 \tTraining Loss: 0.000167\n",
      "Epoch: 6 \tTraining Loss: 0.000167\n",
      "Epoch: 7 \tTraining Loss: 0.000167\n",
      "Epoch: 8 \tTraining Loss: 0.000167\n",
      "Epoch: 9 \tTraining Loss: 0.000167\n",
      "Epoch: 10 \tTraining Loss: 0.000167\n",
      "Epoch: 11 \tTraining Loss: 0.000167\n",
      "Epoch: 12 \tTraining Loss: 0.000167\n",
      "Epoch: 13 \tTraining Loss: 0.000167\n",
      "Epoch: 14 \tTraining Loss: 0.000167\n",
      "Epoch: 15 \tTraining Loss: 0.000167\n",
      "Epoch: 16 \tTraining Loss: 0.000167\n",
      "Epoch: 17 \tTraining Loss: 0.000167\n",
      "Epoch: 18 \tTraining Loss: 0.000167\n",
      "Epoch: 1 \tTraining Loss: 0.000201\n",
      "Epoch: 2 \tTraining Loss: 0.000201\n",
      "Epoch: 3 \tTraining Loss: 0.000201\n",
      "Epoch: 4 \tTraining Loss: 0.000201\n",
      "Epoch: 5 \tTraining Loss: 0.000201\n",
      "Epoch: 6 \tTraining Loss: 0.000201\n",
      "Epoch: 7 \tTraining Loss: 0.000201\n",
      "Epoch: 8 \tTraining Loss: 0.000201\n",
      "Epoch: 9 \tTraining Loss: 0.000201\n",
      "Epoch: 10 \tTraining Loss: 0.000201\n",
      "Epoch: 11 \tTraining Loss: 0.000201\n",
      "Epoch: 12 \tTraining Loss: 0.000201\n",
      "Epoch: 13 \tTraining Loss: 0.000201\n",
      "Epoch: 14 \tTraining Loss: 0.000201\n",
      "Epoch: 15 \tTraining Loss: 0.000201\n",
      "Epoch: 16 \tTraining Loss: 0.000201\n",
      "Epoch: 17 \tTraining Loss: 0.000201\n",
      "Epoch: 18 \tTraining Loss: 0.000201\n",
      "Epoch: 1 \tTraining Loss: 0.000393\n",
      "Epoch: 2 \tTraining Loss: 0.000393\n",
      "Epoch: 3 \tTraining Loss: 0.000393\n",
      "Epoch: 4 \tTraining Loss: 0.000393\n",
      "Epoch: 5 \tTraining Loss: 0.000393\n",
      "Epoch: 6 \tTraining Loss: 0.000393\n",
      "Epoch: 7 \tTraining Loss: 0.000393\n",
      "Epoch: 8 \tTraining Loss: 0.000393\n",
      "Epoch: 9 \tTraining Loss: 0.000393\n",
      "Epoch: 10 \tTraining Loss: 0.000393\n",
      "Epoch: 11 \tTraining Loss: 0.000393\n",
      "Epoch: 12 \tTraining Loss: 0.000393\n",
      "Epoch: 13 \tTraining Loss: 0.000393\n",
      "Epoch: 14 \tTraining Loss: 0.000393\n",
      "Epoch: 15 \tTraining Loss: 0.000393\n",
      "Epoch: 16 \tTraining Loss: 0.000393\n",
      "Epoch: 17 \tTraining Loss: 0.000393\n",
      "Epoch: 18 \tTraining Loss: 0.000393\n",
      "Epoch: 1 \tTraining Loss: 0.000105\n",
      "Epoch: 2 \tTraining Loss: 0.000105\n",
      "Epoch: 3 \tTraining Loss: 0.000105\n",
      "Epoch: 4 \tTraining Loss: 0.000105\n",
      "Epoch: 5 \tTraining Loss: 0.000105\n",
      "Epoch: 6 \tTraining Loss: 0.000105\n",
      "Epoch: 7 \tTraining Loss: 0.000105\n",
      "Epoch: 8 \tTraining Loss: 0.000105\n",
      "Epoch: 9 \tTraining Loss: 0.000105\n",
      "Epoch: 10 \tTraining Loss: 0.000105\n",
      "Epoch: 11 \tTraining Loss: 0.000105\n",
      "Epoch: 12 \tTraining Loss: 0.000105\n",
      "Epoch: 13 \tTraining Loss: 0.000105\n",
      "Epoch: 14 \tTraining Loss: 0.000105\n",
      "Epoch: 15 \tTraining Loss: 0.000105\n",
      "Epoch: 16 \tTraining Loss: 0.000105\n",
      "Epoch: 17 \tTraining Loss: 0.000105\n",
      "Epoch: 18 \tTraining Loss: 0.000105\n",
      "Epoch: 1 \tTraining Loss: 0.000147\n",
      "Epoch: 2 \tTraining Loss: 0.000147\n",
      "Epoch: 3 \tTraining Loss: 0.000147\n",
      "Epoch: 4 \tTraining Loss: 0.000147\n",
      "Epoch: 5 \tTraining Loss: 0.000147\n",
      "Epoch: 6 \tTraining Loss: 0.000147\n",
      "Epoch: 7 \tTraining Loss: 0.000147\n",
      "Epoch: 8 \tTraining Loss: 0.000147\n",
      "Epoch: 9 \tTraining Loss: 0.000147\n",
      "Epoch: 10 \tTraining Loss: 0.000147\n",
      "Epoch: 11 \tTraining Loss: 0.000147\n",
      "Epoch: 12 \tTraining Loss: 0.000147\n",
      "Epoch: 13 \tTraining Loss: 0.000147\n",
      "Epoch: 14 \tTraining Loss: 0.000147\n",
      "Epoch: 15 \tTraining Loss: 0.000147\n",
      "Epoch: 16 \tTraining Loss: 0.000147\n",
      "Epoch: 17 \tTraining Loss: 0.000147\n",
      "Epoch: 18 \tTraining Loss: 0.000147\n",
      "Epoch: 1 \tTraining Loss: 0.000247\n",
      "Epoch: 2 \tTraining Loss: 0.000247\n",
      "Epoch: 3 \tTraining Loss: 0.000247\n",
      "Epoch: 4 \tTraining Loss: 0.000247\n",
      "Epoch: 5 \tTraining Loss: 0.000247\n",
      "Epoch: 6 \tTraining Loss: 0.000247\n",
      "Epoch: 7 \tTraining Loss: 0.000247\n",
      "Epoch: 8 \tTraining Loss: 0.000247\n",
      "Epoch: 9 \tTraining Loss: 0.000247\n",
      "Epoch: 10 \tTraining Loss: 0.000247\n",
      "Epoch: 11 \tTraining Loss: 0.000247\n",
      "Epoch: 12 \tTraining Loss: 0.000247\n",
      "Epoch: 13 \tTraining Loss: 0.000247\n",
      "Epoch: 14 \tTraining Loss: 0.000247\n",
      "Epoch: 15 \tTraining Loss: 0.000247\n",
      "Epoch: 16 \tTraining Loss: 0.000247\n",
      "Epoch: 17 \tTraining Loss: 0.000247\n",
      "Epoch: 18 \tTraining Loss: 0.000247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000157\n",
      "Epoch: 2 \tTraining Loss: 0.000157\n",
      "Epoch: 3 \tTraining Loss: 0.000157\n",
      "Epoch: 4 \tTraining Loss: 0.000157\n",
      "Epoch: 5 \tTraining Loss: 0.000157\n",
      "Epoch: 6 \tTraining Loss: 0.000157\n",
      "Epoch: 7 \tTraining Loss: 0.000157\n",
      "Epoch: 8 \tTraining Loss: 0.000157\n",
      "Epoch: 9 \tTraining Loss: 0.000157\n",
      "Epoch: 10 \tTraining Loss: 0.000157\n",
      "Epoch: 11 \tTraining Loss: 0.000157\n",
      "Epoch: 12 \tTraining Loss: 0.000157\n",
      "Epoch: 13 \tTraining Loss: 0.000157\n",
      "Epoch: 14 \tTraining Loss: 0.000157\n",
      "Epoch: 15 \tTraining Loss: 0.000157\n",
      "Epoch: 16 \tTraining Loss: 0.000157\n",
      "Epoch: 17 \tTraining Loss: 0.000157\n",
      "Epoch: 18 \tTraining Loss: 0.000157\n",
      "Epoch: 1 \tTraining Loss: 0.000148\n",
      "Epoch: 2 \tTraining Loss: 0.000148\n",
      "Epoch: 3 \tTraining Loss: 0.000148\n",
      "Epoch: 4 \tTraining Loss: 0.000148\n",
      "Epoch: 5 \tTraining Loss: 0.000148\n",
      "Epoch: 6 \tTraining Loss: 0.000148\n",
      "Epoch: 7 \tTraining Loss: 0.000148\n",
      "Epoch: 8 \tTraining Loss: 0.000148\n",
      "Epoch: 9 \tTraining Loss: 0.000148\n",
      "Epoch: 10 \tTraining Loss: 0.000148\n",
      "Epoch: 11 \tTraining Loss: 0.000148\n",
      "Epoch: 12 \tTraining Loss: 0.000148\n",
      "Epoch: 13 \tTraining Loss: 0.000148\n",
      "Epoch: 14 \tTraining Loss: 0.000148\n",
      "Epoch: 15 \tTraining Loss: 0.000148\n",
      "Epoch: 16 \tTraining Loss: 0.000148\n",
      "Epoch: 17 \tTraining Loss: 0.000148\n",
      "Epoch: 18 \tTraining Loss: 0.000148\n",
      "Epoch: 1 \tTraining Loss: 0.000098\n",
      "Epoch: 2 \tTraining Loss: 0.000098\n",
      "Epoch: 3 \tTraining Loss: 0.000098\n",
      "Epoch: 4 \tTraining Loss: 0.000098\n",
      "Epoch: 5 \tTraining Loss: 0.000098\n",
      "Epoch: 6 \tTraining Loss: 0.000098\n",
      "Epoch: 7 \tTraining Loss: 0.000098\n",
      "Epoch: 8 \tTraining Loss: 0.000098\n",
      "Epoch: 9 \tTraining Loss: 0.000098\n",
      "Epoch: 10 \tTraining Loss: 0.000098\n",
      "Epoch: 11 \tTraining Loss: 0.000098\n",
      "Epoch: 12 \tTraining Loss: 0.000098\n",
      "Epoch: 13 \tTraining Loss: 0.000098\n",
      "Epoch: 14 \tTraining Loss: 0.000098\n",
      "Epoch: 15 \tTraining Loss: 0.000098\n",
      "Epoch: 16 \tTraining Loss: 0.000098\n",
      "Epoch: 17 \tTraining Loss: 0.000098\n",
      "Epoch: 18 \tTraining Loss: 0.000098\n",
      "Epoch: 1 \tTraining Loss: 0.000239\n",
      "Epoch: 2 \tTraining Loss: 0.000239\n",
      "Epoch: 3 \tTraining Loss: 0.000239\n",
      "Epoch: 4 \tTraining Loss: 0.000239\n",
      "Epoch: 5 \tTraining Loss: 0.000239\n",
      "Epoch: 6 \tTraining Loss: 0.000239\n",
      "Epoch: 7 \tTraining Loss: 0.000239\n",
      "Epoch: 8 \tTraining Loss: 0.000239\n",
      "Epoch: 9 \tTraining Loss: 0.000239\n",
      "Epoch: 10 \tTraining Loss: 0.000239\n",
      "Epoch: 11 \tTraining Loss: 0.000239\n",
      "Epoch: 12 \tTraining Loss: 0.000239\n",
      "Epoch: 13 \tTraining Loss: 0.000239\n",
      "Epoch: 14 \tTraining Loss: 0.000239\n",
      "Epoch: 15 \tTraining Loss: 0.000239\n",
      "Epoch: 16 \tTraining Loss: 0.000239\n",
      "Epoch: 17 \tTraining Loss: 0.000239\n",
      "Epoch: 18 \tTraining Loss: 0.000239\n",
      "Epoch: 1 \tTraining Loss: 0.000121\n",
      "Epoch: 2 \tTraining Loss: 0.000121\n",
      "Epoch: 3 \tTraining Loss: 0.000121\n",
      "Epoch: 4 \tTraining Loss: 0.000121\n",
      "Epoch: 5 \tTraining Loss: 0.000121\n",
      "Epoch: 6 \tTraining Loss: 0.000121\n",
      "Epoch: 7 \tTraining Loss: 0.000121\n",
      "Epoch: 8 \tTraining Loss: 0.000121\n",
      "Epoch: 9 \tTraining Loss: 0.000121\n",
      "Epoch: 10 \tTraining Loss: 0.000121\n",
      "Epoch: 11 \tTraining Loss: 0.000121\n",
      "Epoch: 12 \tTraining Loss: 0.000121\n",
      "Epoch: 13 \tTraining Loss: 0.000121\n",
      "Epoch: 14 \tTraining Loss: 0.000121\n",
      "Epoch: 15 \tTraining Loss: 0.000121\n",
      "Epoch: 16 \tTraining Loss: 0.000121\n",
      "Epoch: 17 \tTraining Loss: 0.000121\n",
      "Epoch: 18 \tTraining Loss: 0.000121\n",
      "Epoch: 1 \tTraining Loss: 0.000243\n",
      "Epoch: 2 \tTraining Loss: 0.000243\n",
      "Epoch: 3 \tTraining Loss: 0.000243\n",
      "Epoch: 4 \tTraining Loss: 0.000243\n",
      "Epoch: 5 \tTraining Loss: 0.000243\n",
      "Epoch: 6 \tTraining Loss: 0.000243\n",
      "Epoch: 7 \tTraining Loss: 0.000243\n",
      "Epoch: 8 \tTraining Loss: 0.000243\n",
      "Epoch: 9 \tTraining Loss: 0.000243\n",
      "Epoch: 10 \tTraining Loss: 0.000243\n",
      "Epoch: 11 \tTraining Loss: 0.000243\n",
      "Epoch: 12 \tTraining Loss: 0.000243\n",
      "Epoch: 13 \tTraining Loss: 0.000243\n",
      "Epoch: 14 \tTraining Loss: 0.000243\n",
      "Epoch: 15 \tTraining Loss: 0.000243\n",
      "Epoch: 16 \tTraining Loss: 0.000243\n",
      "Epoch: 17 \tTraining Loss: 0.000243\n",
      "Epoch: 18 \tTraining Loss: 0.000243\n",
      "Epoch: 1 \tTraining Loss: 0.000214\n",
      "Epoch: 2 \tTraining Loss: 0.000214\n",
      "Epoch: 3 \tTraining Loss: 0.000214\n",
      "Epoch: 4 \tTraining Loss: 0.000214\n",
      "Epoch: 5 \tTraining Loss: 0.000214\n",
      "Epoch: 6 \tTraining Loss: 0.000214\n",
      "Epoch: 7 \tTraining Loss: 0.000214\n",
      "Epoch: 8 \tTraining Loss: 0.000214\n",
      "Epoch: 9 \tTraining Loss: 0.000214\n",
      "Epoch: 10 \tTraining Loss: 0.000214\n",
      "Epoch: 11 \tTraining Loss: 0.000214\n",
      "Epoch: 12 \tTraining Loss: 0.000214\n",
      "Epoch: 13 \tTraining Loss: 0.000214\n",
      "Epoch: 14 \tTraining Loss: 0.000214\n",
      "Epoch: 15 \tTraining Loss: 0.000214\n",
      "Epoch: 16 \tTraining Loss: 0.000214\n",
      "Epoch: 17 \tTraining Loss: 0.000214\n",
      "Epoch: 18 \tTraining Loss: 0.000214\n",
      "Epoch: 1 \tTraining Loss: 0.000140\n",
      "Epoch: 2 \tTraining Loss: 0.000140\n",
      "Epoch: 3 \tTraining Loss: 0.000140\n",
      "Epoch: 4 \tTraining Loss: 0.000140\n",
      "Epoch: 5 \tTraining Loss: 0.000140\n",
      "Epoch: 6 \tTraining Loss: 0.000140\n",
      "Epoch: 7 \tTraining Loss: 0.000140\n",
      "Epoch: 8 \tTraining Loss: 0.000140\n",
      "Epoch: 9 \tTraining Loss: 0.000140\n",
      "Epoch: 10 \tTraining Loss: 0.000140\n",
      "Epoch: 11 \tTraining Loss: 0.000140\n",
      "Epoch: 12 \tTraining Loss: 0.000140\n",
      "Epoch: 13 \tTraining Loss: 0.000140\n",
      "Epoch: 14 \tTraining Loss: 0.000140\n",
      "Epoch: 15 \tTraining Loss: 0.000140\n",
      "Epoch: 16 \tTraining Loss: 0.000140\n",
      "Epoch: 17 \tTraining Loss: 0.000140\n",
      "Epoch: 18 \tTraining Loss: 0.000140\n",
      "Epoch: 1 \tTraining Loss: 0.000328\n",
      "Epoch: 2 \tTraining Loss: 0.000328\n",
      "Epoch: 3 \tTraining Loss: 0.000328\n",
      "Epoch: 4 \tTraining Loss: 0.000328\n",
      "Epoch: 5 \tTraining Loss: 0.000328\n",
      "Epoch: 6 \tTraining Loss: 0.000328\n",
      "Epoch: 7 \tTraining Loss: 0.000328\n",
      "Epoch: 8 \tTraining Loss: 0.000328\n",
      "Epoch: 9 \tTraining Loss: 0.000328\n",
      "Epoch: 10 \tTraining Loss: 0.000328\n",
      "Epoch: 11 \tTraining Loss: 0.000328\n",
      "Epoch: 12 \tTraining Loss: 0.000328\n",
      "Epoch: 13 \tTraining Loss: 0.000328\n",
      "Epoch: 14 \tTraining Loss: 0.000328\n",
      "Epoch: 15 \tTraining Loss: 0.000328\n",
      "Epoch: 16 \tTraining Loss: 0.000328\n",
      "Epoch: 17 \tTraining Loss: 0.000328\n",
      "Epoch: 18 \tTraining Loss: 0.000328\n",
      "Epoch: 1 \tTraining Loss: 0.000086\n",
      "Epoch: 2 \tTraining Loss: 0.000086\n",
      "Epoch: 3 \tTraining Loss: 0.000086\n",
      "Epoch: 4 \tTraining Loss: 0.000086\n",
      "Epoch: 5 \tTraining Loss: 0.000086\n",
      "Epoch: 6 \tTraining Loss: 0.000086\n",
      "Epoch: 7 \tTraining Loss: 0.000086\n",
      "Epoch: 8 \tTraining Loss: 0.000086\n",
      "Epoch: 9 \tTraining Loss: 0.000086\n",
      "Epoch: 10 \tTraining Loss: 0.000086\n",
      "Epoch: 11 \tTraining Loss: 0.000086\n",
      "Epoch: 12 \tTraining Loss: 0.000086\n",
      "Epoch: 13 \tTraining Loss: 0.000086\n",
      "Epoch: 14 \tTraining Loss: 0.000086\n",
      "Epoch: 15 \tTraining Loss: 0.000086\n",
      "Epoch: 16 \tTraining Loss: 0.000086\n",
      "Epoch: 17 \tTraining Loss: 0.000086\n",
      "Epoch: 18 \tTraining Loss: 0.000086\n",
      "Epoch: 1 \tTraining Loss: 0.000127\n",
      "Epoch: 2 \tTraining Loss: 0.000127\n",
      "Epoch: 3 \tTraining Loss: 0.000127\n",
      "Epoch: 4 \tTraining Loss: 0.000127\n",
      "Epoch: 5 \tTraining Loss: 0.000127\n",
      "Epoch: 6 \tTraining Loss: 0.000127\n",
      "Epoch: 7 \tTraining Loss: 0.000127\n",
      "Epoch: 8 \tTraining Loss: 0.000127\n",
      "Epoch: 9 \tTraining Loss: 0.000127\n",
      "Epoch: 10 \tTraining Loss: 0.000127\n",
      "Epoch: 11 \tTraining Loss: 0.000127\n",
      "Epoch: 12 \tTraining Loss: 0.000127\n",
      "Epoch: 13 \tTraining Loss: 0.000127\n",
      "Epoch: 14 \tTraining Loss: 0.000127\n",
      "Epoch: 15 \tTraining Loss: 0.000127\n",
      "Epoch: 16 \tTraining Loss: 0.000127\n",
      "Epoch: 17 \tTraining Loss: 0.000127\n",
      "Epoch: 18 \tTraining Loss: 0.000127\n",
      "Epoch: 1 \tTraining Loss: 0.000265\n",
      "Epoch: 2 \tTraining Loss: 0.000265\n",
      "Epoch: 3 \tTraining Loss: 0.000265\n",
      "Epoch: 4 \tTraining Loss: 0.000265\n",
      "Epoch: 5 \tTraining Loss: 0.000265\n",
      "Epoch: 6 \tTraining Loss: 0.000265\n",
      "Epoch: 7 \tTraining Loss: 0.000265\n",
      "Epoch: 8 \tTraining Loss: 0.000265\n",
      "Epoch: 9 \tTraining Loss: 0.000265\n",
      "Epoch: 10 \tTraining Loss: 0.000265\n",
      "Epoch: 11 \tTraining Loss: 0.000265\n",
      "Epoch: 12 \tTraining Loss: 0.000265\n",
      "Epoch: 13 \tTraining Loss: 0.000265\n",
      "Epoch: 14 \tTraining Loss: 0.000265\n",
      "Epoch: 15 \tTraining Loss: 0.000265\n",
      "Epoch: 16 \tTraining Loss: 0.000265\n",
      "Epoch: 17 \tTraining Loss: 0.000265\n",
      "Epoch: 18 \tTraining Loss: 0.000265\n",
      "Epoch: 1 \tTraining Loss: 0.000198\n",
      "Epoch: 2 \tTraining Loss: 0.000198\n",
      "Epoch: 3 \tTraining Loss: 0.000198\n",
      "Epoch: 4 \tTraining Loss: 0.000198\n",
      "Epoch: 5 \tTraining Loss: 0.000198\n",
      "Epoch: 6 \tTraining Loss: 0.000198\n",
      "Epoch: 7 \tTraining Loss: 0.000198\n",
      "Epoch: 8 \tTraining Loss: 0.000198\n",
      "Epoch: 9 \tTraining Loss: 0.000198\n",
      "Epoch: 10 \tTraining Loss: 0.000198\n",
      "Epoch: 11 \tTraining Loss: 0.000198\n",
      "Epoch: 12 \tTraining Loss: 0.000198\n",
      "Epoch: 13 \tTraining Loss: 0.000198\n",
      "Epoch: 14 \tTraining Loss: 0.000198\n",
      "Epoch: 15 \tTraining Loss: 0.000198\n",
      "Epoch: 16 \tTraining Loss: 0.000198\n",
      "Epoch: 17 \tTraining Loss: 0.000198\n",
      "Epoch: 18 \tTraining Loss: 0.000198\n",
      "Epoch: 1 \tTraining Loss: 0.000327\n",
      "Epoch: 2 \tTraining Loss: 0.000327\n",
      "Epoch: 3 \tTraining Loss: 0.000327\n",
      "Epoch: 4 \tTraining Loss: 0.000327\n",
      "Epoch: 5 \tTraining Loss: 0.000327\n",
      "Epoch: 6 \tTraining Loss: 0.000327\n",
      "Epoch: 7 \tTraining Loss: 0.000327\n",
      "Epoch: 8 \tTraining Loss: 0.000327\n",
      "Epoch: 9 \tTraining Loss: 0.000327\n",
      "Epoch: 10 \tTraining Loss: 0.000327\n",
      "Epoch: 11 \tTraining Loss: 0.000327\n",
      "Epoch: 12 \tTraining Loss: 0.000327\n",
      "Epoch: 13 \tTraining Loss: 0.000327\n",
      "Epoch: 14 \tTraining Loss: 0.000327\n",
      "Epoch: 15 \tTraining Loss: 0.000327\n",
      "Epoch: 16 \tTraining Loss: 0.000327\n",
      "Epoch: 17 \tTraining Loss: 0.000327\n",
      "Epoch: 18 \tTraining Loss: 0.000327\n",
      "Epoch: 1 \tTraining Loss: 0.000220\n",
      "Epoch: 2 \tTraining Loss: 0.000220\n",
      "Epoch: 3 \tTraining Loss: 0.000220\n",
      "Epoch: 4 \tTraining Loss: 0.000220\n",
      "Epoch: 5 \tTraining Loss: 0.000220\n",
      "Epoch: 6 \tTraining Loss: 0.000220\n",
      "Epoch: 7 \tTraining Loss: 0.000220\n",
      "Epoch: 8 \tTraining Loss: 0.000220\n",
      "Epoch: 9 \tTraining Loss: 0.000220\n",
      "Epoch: 10 \tTraining Loss: 0.000220\n",
      "Epoch: 11 \tTraining Loss: 0.000220\n",
      "Epoch: 12 \tTraining Loss: 0.000220\n",
      "Epoch: 13 \tTraining Loss: 0.000220\n",
      "Epoch: 14 \tTraining Loss: 0.000220\n",
      "Epoch: 15 \tTraining Loss: 0.000220\n",
      "Epoch: 16 \tTraining Loss: 0.000220\n",
      "Epoch: 17 \tTraining Loss: 0.000220\n",
      "Epoch: 18 \tTraining Loss: 0.000220\n",
      "Epoch: 1 \tTraining Loss: 0.000246\n",
      "Epoch: 2 \tTraining Loss: 0.000246\n",
      "Epoch: 3 \tTraining Loss: 0.000246\n",
      "Epoch: 4 \tTraining Loss: 0.000246\n",
      "Epoch: 5 \tTraining Loss: 0.000246\n",
      "Epoch: 6 \tTraining Loss: 0.000246\n",
      "Epoch: 7 \tTraining Loss: 0.000246\n",
      "Epoch: 8 \tTraining Loss: 0.000246\n",
      "Epoch: 9 \tTraining Loss: 0.000246\n",
      "Epoch: 10 \tTraining Loss: 0.000246\n",
      "Epoch: 11 \tTraining Loss: 0.000246\n",
      "Epoch: 12 \tTraining Loss: 0.000246\n",
      "Epoch: 13 \tTraining Loss: 0.000246\n",
      "Epoch: 14 \tTraining Loss: 0.000246\n",
      "Epoch: 15 \tTraining Loss: 0.000246\n",
      "Epoch: 16 \tTraining Loss: 0.000246\n",
      "Epoch: 17 \tTraining Loss: 0.000246\n",
      "Epoch: 18 \tTraining Loss: 0.000246\n",
      "Epoch: 1 \tTraining Loss: 0.000192\n",
      "Epoch: 2 \tTraining Loss: 0.000192\n",
      "Epoch: 3 \tTraining Loss: 0.000192\n",
      "Epoch: 4 \tTraining Loss: 0.000192\n",
      "Epoch: 5 \tTraining Loss: 0.000192\n",
      "Epoch: 6 \tTraining Loss: 0.000192\n",
      "Epoch: 7 \tTraining Loss: 0.000192\n",
      "Epoch: 8 \tTraining Loss: 0.000192\n",
      "Epoch: 9 \tTraining Loss: 0.000192\n",
      "Epoch: 10 \tTraining Loss: 0.000192\n",
      "Epoch: 11 \tTraining Loss: 0.000192\n",
      "Epoch: 12 \tTraining Loss: 0.000192\n",
      "Epoch: 13 \tTraining Loss: 0.000192\n",
      "Epoch: 14 \tTraining Loss: 0.000192\n",
      "Epoch: 15 \tTraining Loss: 0.000192\n",
      "Epoch: 16 \tTraining Loss: 0.000192\n",
      "Epoch: 17 \tTraining Loss: 0.000192\n",
      "Epoch: 18 \tTraining Loss: 0.000192\n",
      "Epoch: 1 \tTraining Loss: 0.000114\n",
      "Epoch: 2 \tTraining Loss: 0.000114\n",
      "Epoch: 3 \tTraining Loss: 0.000114\n",
      "Epoch: 4 \tTraining Loss: 0.000114\n",
      "Epoch: 5 \tTraining Loss: 0.000114\n",
      "Epoch: 6 \tTraining Loss: 0.000114\n",
      "Epoch: 7 \tTraining Loss: 0.000114\n",
      "Epoch: 8 \tTraining Loss: 0.000114\n",
      "Epoch: 9 \tTraining Loss: 0.000114\n",
      "Epoch: 10 \tTraining Loss: 0.000114\n",
      "Epoch: 11 \tTraining Loss: 0.000114\n",
      "Epoch: 12 \tTraining Loss: 0.000114\n",
      "Epoch: 13 \tTraining Loss: 0.000114\n",
      "Epoch: 14 \tTraining Loss: 0.000114\n",
      "Epoch: 15 \tTraining Loss: 0.000114\n",
      "Epoch: 16 \tTraining Loss: 0.000114\n",
      "Epoch: 17 \tTraining Loss: 0.000114\n",
      "Epoch: 18 \tTraining Loss: 0.000114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000175\n",
      "Epoch: 2 \tTraining Loss: 0.000175\n",
      "Epoch: 3 \tTraining Loss: 0.000175\n",
      "Epoch: 4 \tTraining Loss: 0.000175\n",
      "Epoch: 5 \tTraining Loss: 0.000175\n",
      "Epoch: 6 \tTraining Loss: 0.000175\n",
      "Epoch: 7 \tTraining Loss: 0.000175\n",
      "Epoch: 8 \tTraining Loss: 0.000175\n",
      "Epoch: 9 \tTraining Loss: 0.000175\n",
      "Epoch: 10 \tTraining Loss: 0.000175\n",
      "Epoch: 11 \tTraining Loss: 0.000175\n",
      "Epoch: 12 \tTraining Loss: 0.000175\n",
      "Epoch: 13 \tTraining Loss: 0.000175\n",
      "Epoch: 14 \tTraining Loss: 0.000175\n",
      "Epoch: 15 \tTraining Loss: 0.000175\n",
      "Epoch: 16 \tTraining Loss: 0.000175\n",
      "Epoch: 17 \tTraining Loss: 0.000175\n",
      "Epoch: 18 \tTraining Loss: 0.000175\n",
      "Epoch: 1 \tTraining Loss: 0.000225\n",
      "Epoch: 2 \tTraining Loss: 0.000225\n",
      "Epoch: 3 \tTraining Loss: 0.000225\n",
      "Epoch: 4 \tTraining Loss: 0.000225\n",
      "Epoch: 5 \tTraining Loss: 0.000225\n",
      "Epoch: 6 \tTraining Loss: 0.000225\n",
      "Epoch: 7 \tTraining Loss: 0.000225\n",
      "Epoch: 8 \tTraining Loss: 0.000225\n",
      "Epoch: 9 \tTraining Loss: 0.000225\n",
      "Epoch: 10 \tTraining Loss: 0.000225\n",
      "Epoch: 11 \tTraining Loss: 0.000225\n",
      "Epoch: 12 \tTraining Loss: 0.000225\n",
      "Epoch: 13 \tTraining Loss: 0.000225\n",
      "Epoch: 14 \tTraining Loss: 0.000225\n",
      "Epoch: 15 \tTraining Loss: 0.000225\n",
      "Epoch: 16 \tTraining Loss: 0.000225\n",
      "Epoch: 17 \tTraining Loss: 0.000225\n",
      "Epoch: 18 \tTraining Loss: 0.000225\n",
      "Epoch: 1 \tTraining Loss: 0.000224\n",
      "Epoch: 2 \tTraining Loss: 0.000224\n",
      "Epoch: 3 \tTraining Loss: 0.000224\n",
      "Epoch: 4 \tTraining Loss: 0.000224\n",
      "Epoch: 5 \tTraining Loss: 0.000224\n",
      "Epoch: 6 \tTraining Loss: 0.000224\n",
      "Epoch: 7 \tTraining Loss: 0.000224\n",
      "Epoch: 8 \tTraining Loss: 0.000224\n",
      "Epoch: 9 \tTraining Loss: 0.000224\n",
      "Epoch: 10 \tTraining Loss: 0.000224\n",
      "Epoch: 11 \tTraining Loss: 0.000224\n",
      "Epoch: 12 \tTraining Loss: 0.000224\n",
      "Epoch: 13 \tTraining Loss: 0.000224\n",
      "Epoch: 14 \tTraining Loss: 0.000224\n",
      "Epoch: 15 \tTraining Loss: 0.000224\n",
      "Epoch: 16 \tTraining Loss: 0.000224\n",
      "Epoch: 17 \tTraining Loss: 0.000224\n",
      "Epoch: 18 \tTraining Loss: 0.000224\n",
      "Epoch: 1 \tTraining Loss: 0.000222\n",
      "Epoch: 2 \tTraining Loss: 0.000222\n",
      "Epoch: 3 \tTraining Loss: 0.000222\n",
      "Epoch: 4 \tTraining Loss: 0.000222\n",
      "Epoch: 5 \tTraining Loss: 0.000222\n",
      "Epoch: 6 \tTraining Loss: 0.000222\n",
      "Epoch: 7 \tTraining Loss: 0.000222\n",
      "Epoch: 8 \tTraining Loss: 0.000222\n",
      "Epoch: 9 \tTraining Loss: 0.000222\n",
      "Epoch: 10 \tTraining Loss: 0.000222\n",
      "Epoch: 11 \tTraining Loss: 0.000222\n",
      "Epoch: 12 \tTraining Loss: 0.000222\n",
      "Epoch: 13 \tTraining Loss: 0.000222\n",
      "Epoch: 14 \tTraining Loss: 0.000222\n",
      "Epoch: 15 \tTraining Loss: 0.000222\n",
      "Epoch: 16 \tTraining Loss: 0.000222\n",
      "Epoch: 17 \tTraining Loss: 0.000222\n",
      "Epoch: 18 \tTraining Loss: 0.000222\n",
      "Epoch: 1 \tTraining Loss: 0.000222\n",
      "Epoch: 2 \tTraining Loss: 0.000222\n",
      "Epoch: 3 \tTraining Loss: 0.000222\n",
      "Epoch: 4 \tTraining Loss: 0.000222\n",
      "Epoch: 5 \tTraining Loss: 0.000222\n",
      "Epoch: 6 \tTraining Loss: 0.000222\n",
      "Epoch: 7 \tTraining Loss: 0.000222\n",
      "Epoch: 8 \tTraining Loss: 0.000222\n",
      "Epoch: 9 \tTraining Loss: 0.000222\n",
      "Epoch: 10 \tTraining Loss: 0.000222\n",
      "Epoch: 11 \tTraining Loss: 0.000222\n",
      "Epoch: 12 \tTraining Loss: 0.000222\n",
      "Epoch: 13 \tTraining Loss: 0.000222\n",
      "Epoch: 14 \tTraining Loss: 0.000222\n",
      "Epoch: 15 \tTraining Loss: 0.000222\n",
      "Epoch: 16 \tTraining Loss: 0.000222\n",
      "Epoch: 17 \tTraining Loss: 0.000222\n",
      "Epoch: 18 \tTraining Loss: 0.000222\n",
      "Epoch: 1 \tTraining Loss: 0.000335\n",
      "Epoch: 2 \tTraining Loss: 0.000335\n",
      "Epoch: 3 \tTraining Loss: 0.000335\n",
      "Epoch: 4 \tTraining Loss: 0.000335\n",
      "Epoch: 5 \tTraining Loss: 0.000335\n",
      "Epoch: 6 \tTraining Loss: 0.000335\n",
      "Epoch: 7 \tTraining Loss: 0.000335\n",
      "Epoch: 8 \tTraining Loss: 0.000335\n",
      "Epoch: 9 \tTraining Loss: 0.000335\n",
      "Epoch: 10 \tTraining Loss: 0.000335\n",
      "Epoch: 11 \tTraining Loss: 0.000335\n",
      "Epoch: 12 \tTraining Loss: 0.000335\n",
      "Epoch: 13 \tTraining Loss: 0.000335\n",
      "Epoch: 14 \tTraining Loss: 0.000335\n",
      "Epoch: 15 \tTraining Loss: 0.000335\n",
      "Epoch: 16 \tTraining Loss: 0.000335\n",
      "Epoch: 17 \tTraining Loss: 0.000335\n",
      "Epoch: 18 \tTraining Loss: 0.000335\n",
      "Epoch: 1 \tTraining Loss: 0.000303\n",
      "Epoch: 2 \tTraining Loss: 0.000303\n",
      "Epoch: 3 \tTraining Loss: 0.000303\n",
      "Epoch: 4 \tTraining Loss: 0.000303\n",
      "Epoch: 5 \tTraining Loss: 0.000303\n",
      "Epoch: 6 \tTraining Loss: 0.000303\n",
      "Epoch: 7 \tTraining Loss: 0.000303\n",
      "Epoch: 8 \tTraining Loss: 0.000303\n",
      "Epoch: 9 \tTraining Loss: 0.000303\n",
      "Epoch: 10 \tTraining Loss: 0.000303\n",
      "Epoch: 11 \tTraining Loss: 0.000303\n",
      "Epoch: 12 \tTraining Loss: 0.000303\n",
      "Epoch: 13 \tTraining Loss: 0.000303\n",
      "Epoch: 14 \tTraining Loss: 0.000303\n",
      "Epoch: 15 \tTraining Loss: 0.000303\n",
      "Epoch: 16 \tTraining Loss: 0.000303\n",
      "Epoch: 17 \tTraining Loss: 0.000303\n",
      "Epoch: 18 \tTraining Loss: 0.000303\n",
      "Epoch: 1 \tTraining Loss: 0.000172\n",
      "Epoch: 2 \tTraining Loss: 0.000172\n",
      "Epoch: 3 \tTraining Loss: 0.000172\n",
      "Epoch: 4 \tTraining Loss: 0.000172\n",
      "Epoch: 5 \tTraining Loss: 0.000172\n",
      "Epoch: 6 \tTraining Loss: 0.000172\n",
      "Epoch: 7 \tTraining Loss: 0.000172\n",
      "Epoch: 8 \tTraining Loss: 0.000172\n",
      "Epoch: 9 \tTraining Loss: 0.000172\n",
      "Epoch: 10 \tTraining Loss: 0.000172\n",
      "Epoch: 11 \tTraining Loss: 0.000172\n",
      "Epoch: 12 \tTraining Loss: 0.000172\n",
      "Epoch: 13 \tTraining Loss: 0.000172\n",
      "Epoch: 14 \tTraining Loss: 0.000172\n",
      "Epoch: 15 \tTraining Loss: 0.000172\n",
      "Epoch: 16 \tTraining Loss: 0.000172\n",
      "Epoch: 17 \tTraining Loss: 0.000172\n",
      "Epoch: 18 \tTraining Loss: 0.000172\n",
      "Epoch: 1 \tTraining Loss: 0.000240\n",
      "Epoch: 2 \tTraining Loss: 0.000240\n",
      "Epoch: 3 \tTraining Loss: 0.000240\n",
      "Epoch: 4 \tTraining Loss: 0.000240\n",
      "Epoch: 5 \tTraining Loss: 0.000240\n",
      "Epoch: 6 \tTraining Loss: 0.000240\n",
      "Epoch: 7 \tTraining Loss: 0.000240\n",
      "Epoch: 8 \tTraining Loss: 0.000240\n",
      "Epoch: 9 \tTraining Loss: 0.000240\n",
      "Epoch: 10 \tTraining Loss: 0.000240\n",
      "Epoch: 11 \tTraining Loss: 0.000240\n",
      "Epoch: 12 \tTraining Loss: 0.000240\n",
      "Epoch: 13 \tTraining Loss: 0.000240\n",
      "Epoch: 14 \tTraining Loss: 0.000240\n",
      "Epoch: 15 \tTraining Loss: 0.000240\n",
      "Epoch: 16 \tTraining Loss: 0.000240\n",
      "Epoch: 17 \tTraining Loss: 0.000240\n",
      "Epoch: 18 \tTraining Loss: 0.000240\n",
      "Epoch: 1 \tTraining Loss: 0.000197\n",
      "Epoch: 2 \tTraining Loss: 0.000197\n",
      "Epoch: 3 \tTraining Loss: 0.000197\n",
      "Epoch: 4 \tTraining Loss: 0.000197\n",
      "Epoch: 5 \tTraining Loss: 0.000197\n",
      "Epoch: 6 \tTraining Loss: 0.000197\n",
      "Epoch: 7 \tTraining Loss: 0.000197\n",
      "Epoch: 8 \tTraining Loss: 0.000197\n",
      "Epoch: 9 \tTraining Loss: 0.000197\n",
      "Epoch: 10 \tTraining Loss: 0.000197\n",
      "Epoch: 11 \tTraining Loss: 0.000197\n",
      "Epoch: 12 \tTraining Loss: 0.000197\n",
      "Epoch: 13 \tTraining Loss: 0.000197\n",
      "Epoch: 14 \tTraining Loss: 0.000197\n",
      "Epoch: 15 \tTraining Loss: 0.000197\n",
      "Epoch: 16 \tTraining Loss: 0.000197\n",
      "Epoch: 17 \tTraining Loss: 0.000197\n",
      "Epoch: 18 \tTraining Loss: 0.000197\n",
      "Epoch: 1 \tTraining Loss: 0.000111\n",
      "Epoch: 2 \tTraining Loss: 0.000111\n",
      "Epoch: 3 \tTraining Loss: 0.000111\n",
      "Epoch: 4 \tTraining Loss: 0.000111\n",
      "Epoch: 5 \tTraining Loss: 0.000111\n",
      "Epoch: 6 \tTraining Loss: 0.000111\n",
      "Epoch: 7 \tTraining Loss: 0.000111\n",
      "Epoch: 8 \tTraining Loss: 0.000111\n",
      "Epoch: 9 \tTraining Loss: 0.000111\n",
      "Epoch: 10 \tTraining Loss: 0.000111\n",
      "Epoch: 11 \tTraining Loss: 0.000111\n",
      "Epoch: 12 \tTraining Loss: 0.000111\n",
      "Epoch: 13 \tTraining Loss: 0.000111\n",
      "Epoch: 14 \tTraining Loss: 0.000111\n",
      "Epoch: 15 \tTraining Loss: 0.000111\n",
      "Epoch: 16 \tTraining Loss: 0.000111\n",
      "Epoch: 17 \tTraining Loss: 0.000111\n",
      "Epoch: 18 \tTraining Loss: 0.000111\n",
      "Epoch: 1 \tTraining Loss: 0.000113\n",
      "Epoch: 2 \tTraining Loss: 0.000113\n",
      "Epoch: 3 \tTraining Loss: 0.000113\n",
      "Epoch: 4 \tTraining Loss: 0.000113\n",
      "Epoch: 5 \tTraining Loss: 0.000113\n",
      "Epoch: 6 \tTraining Loss: 0.000113\n",
      "Epoch: 7 \tTraining Loss: 0.000113\n",
      "Epoch: 8 \tTraining Loss: 0.000113\n",
      "Epoch: 9 \tTraining Loss: 0.000113\n",
      "Epoch: 10 \tTraining Loss: 0.000113\n",
      "Epoch: 11 \tTraining Loss: 0.000113\n",
      "Epoch: 12 \tTraining Loss: 0.000113\n",
      "Epoch: 13 \tTraining Loss: 0.000113\n",
      "Epoch: 14 \tTraining Loss: 0.000113\n",
      "Epoch: 15 \tTraining Loss: 0.000113\n",
      "Epoch: 16 \tTraining Loss: 0.000113\n",
      "Epoch: 17 \tTraining Loss: 0.000113\n",
      "Epoch: 18 \tTraining Loss: 0.000113\n",
      "Epoch: 1 \tTraining Loss: 0.000192\n",
      "Epoch: 2 \tTraining Loss: 0.000192\n",
      "Epoch: 3 \tTraining Loss: 0.000192\n",
      "Epoch: 4 \tTraining Loss: 0.000192\n",
      "Epoch: 5 \tTraining Loss: 0.000192\n",
      "Epoch: 6 \tTraining Loss: 0.000192\n",
      "Epoch: 7 \tTraining Loss: 0.000192\n",
      "Epoch: 8 \tTraining Loss: 0.000192\n",
      "Epoch: 9 \tTraining Loss: 0.000192\n",
      "Epoch: 10 \tTraining Loss: 0.000192\n",
      "Epoch: 11 \tTraining Loss: 0.000192\n",
      "Epoch: 12 \tTraining Loss: 0.000192\n",
      "Epoch: 13 \tTraining Loss: 0.000192\n",
      "Epoch: 14 \tTraining Loss: 0.000192\n",
      "Epoch: 15 \tTraining Loss: 0.000192\n",
      "Epoch: 16 \tTraining Loss: 0.000192\n",
      "Epoch: 17 \tTraining Loss: 0.000192\n",
      "Epoch: 18 \tTraining Loss: 0.000192\n",
      "Epoch: 1 \tTraining Loss: 0.000148\n",
      "Epoch: 2 \tTraining Loss: 0.000148\n",
      "Epoch: 3 \tTraining Loss: 0.000148\n",
      "Epoch: 4 \tTraining Loss: 0.000148\n",
      "Epoch: 5 \tTraining Loss: 0.000148\n",
      "Epoch: 6 \tTraining Loss: 0.000148\n",
      "Epoch: 7 \tTraining Loss: 0.000148\n",
      "Epoch: 8 \tTraining Loss: 0.000148\n",
      "Epoch: 9 \tTraining Loss: 0.000148\n",
      "Epoch: 10 \tTraining Loss: 0.000148\n",
      "Epoch: 11 \tTraining Loss: 0.000148\n",
      "Epoch: 12 \tTraining Loss: 0.000148\n",
      "Epoch: 13 \tTraining Loss: 0.000148\n",
      "Epoch: 14 \tTraining Loss: 0.000148\n",
      "Epoch: 15 \tTraining Loss: 0.000148\n",
      "Epoch: 16 \tTraining Loss: 0.000148\n",
      "Epoch: 17 \tTraining Loss: 0.000148\n",
      "Epoch: 18 \tTraining Loss: 0.000148\n",
      "Epoch: 1 \tTraining Loss: 0.000210\n",
      "Epoch: 2 \tTraining Loss: 0.000210\n",
      "Epoch: 3 \tTraining Loss: 0.000210\n",
      "Epoch: 4 \tTraining Loss: 0.000210\n",
      "Epoch: 5 \tTraining Loss: 0.000210\n",
      "Epoch: 6 \tTraining Loss: 0.000210\n",
      "Epoch: 7 \tTraining Loss: 0.000210\n",
      "Epoch: 8 \tTraining Loss: 0.000210\n",
      "Epoch: 9 \tTraining Loss: 0.000210\n",
      "Epoch: 10 \tTraining Loss: 0.000210\n",
      "Epoch: 11 \tTraining Loss: 0.000210\n",
      "Epoch: 12 \tTraining Loss: 0.000210\n",
      "Epoch: 13 \tTraining Loss: 0.000210\n",
      "Epoch: 14 \tTraining Loss: 0.000210\n",
      "Epoch: 15 \tTraining Loss: 0.000210\n",
      "Epoch: 16 \tTraining Loss: 0.000210\n",
      "Epoch: 17 \tTraining Loss: 0.000210\n",
      "Epoch: 18 \tTraining Loss: 0.000210\n",
      "Epoch: 1 \tTraining Loss: 0.000310\n",
      "Epoch: 2 \tTraining Loss: 0.000310\n",
      "Epoch: 3 \tTraining Loss: 0.000310\n",
      "Epoch: 4 \tTraining Loss: 0.000310\n",
      "Epoch: 5 \tTraining Loss: 0.000310\n",
      "Epoch: 6 \tTraining Loss: 0.000310\n",
      "Epoch: 7 \tTraining Loss: 0.000310\n",
      "Epoch: 8 \tTraining Loss: 0.000310\n",
      "Epoch: 9 \tTraining Loss: 0.000310\n",
      "Epoch: 10 \tTraining Loss: 0.000310\n",
      "Epoch: 11 \tTraining Loss: 0.000310\n",
      "Epoch: 12 \tTraining Loss: 0.000310\n",
      "Epoch: 13 \tTraining Loss: 0.000310\n",
      "Epoch: 14 \tTraining Loss: 0.000310\n",
      "Epoch: 15 \tTraining Loss: 0.000310\n",
      "Epoch: 16 \tTraining Loss: 0.000310\n",
      "Epoch: 17 \tTraining Loss: 0.000310\n",
      "Epoch: 18 \tTraining Loss: 0.000310\n",
      "Epoch: 1 \tTraining Loss: 0.000061\n",
      "Epoch: 2 \tTraining Loss: 0.000061\n",
      "Epoch: 3 \tTraining Loss: 0.000061\n",
      "Epoch: 4 \tTraining Loss: 0.000061\n",
      "Epoch: 5 \tTraining Loss: 0.000061\n",
      "Epoch: 6 \tTraining Loss: 0.000061\n",
      "Epoch: 7 \tTraining Loss: 0.000061\n",
      "Epoch: 8 \tTraining Loss: 0.000061\n",
      "Epoch: 9 \tTraining Loss: 0.000061\n",
      "Epoch: 10 \tTraining Loss: 0.000061\n",
      "Epoch: 11 \tTraining Loss: 0.000061\n",
      "Epoch: 12 \tTraining Loss: 0.000061\n",
      "Epoch: 13 \tTraining Loss: 0.000061\n",
      "Epoch: 14 \tTraining Loss: 0.000061\n",
      "Epoch: 15 \tTraining Loss: 0.000061\n",
      "Epoch: 16 \tTraining Loss: 0.000061\n",
      "Epoch: 17 \tTraining Loss: 0.000061\n",
      "Epoch: 18 \tTraining Loss: 0.000061\n",
      "Epoch: 1 \tTraining Loss: 0.000149\n",
      "Epoch: 2 \tTraining Loss: 0.000149\n",
      "Epoch: 3 \tTraining Loss: 0.000149\n",
      "Epoch: 4 \tTraining Loss: 0.000149\n",
      "Epoch: 5 \tTraining Loss: 0.000149\n",
      "Epoch: 6 \tTraining Loss: 0.000149\n",
      "Epoch: 7 \tTraining Loss: 0.000149\n",
      "Epoch: 8 \tTraining Loss: 0.000149\n",
      "Epoch: 9 \tTraining Loss: 0.000149\n",
      "Epoch: 10 \tTraining Loss: 0.000149\n",
      "Epoch: 11 \tTraining Loss: 0.000149\n",
      "Epoch: 12 \tTraining Loss: 0.000149\n",
      "Epoch: 13 \tTraining Loss: 0.000149\n",
      "Epoch: 14 \tTraining Loss: 0.000149\n",
      "Epoch: 15 \tTraining Loss: 0.000149\n",
      "Epoch: 16 \tTraining Loss: 0.000149\n",
      "Epoch: 17 \tTraining Loss: 0.000149\n",
      "Epoch: 18 \tTraining Loss: 0.000149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000388\n",
      "Epoch: 2 \tTraining Loss: 0.000388\n",
      "Epoch: 3 \tTraining Loss: 0.000388\n",
      "Epoch: 4 \tTraining Loss: 0.000388\n",
      "Epoch: 5 \tTraining Loss: 0.000388\n",
      "Epoch: 6 \tTraining Loss: 0.000388\n",
      "Epoch: 7 \tTraining Loss: 0.000388\n",
      "Epoch: 8 \tTraining Loss: 0.000388\n",
      "Epoch: 9 \tTraining Loss: 0.000388\n",
      "Epoch: 10 \tTraining Loss: 0.000388\n",
      "Epoch: 11 \tTraining Loss: 0.000388\n",
      "Epoch: 12 \tTraining Loss: 0.000388\n",
      "Epoch: 13 \tTraining Loss: 0.000388\n",
      "Epoch: 14 \tTraining Loss: 0.000388\n",
      "Epoch: 15 \tTraining Loss: 0.000388\n",
      "Epoch: 16 \tTraining Loss: 0.000388\n",
      "Epoch: 17 \tTraining Loss: 0.000388\n",
      "Epoch: 18 \tTraining Loss: 0.000388\n",
      "Epoch: 1 \tTraining Loss: 0.000169\n",
      "Epoch: 2 \tTraining Loss: 0.000169\n",
      "Epoch: 3 \tTraining Loss: 0.000169\n",
      "Epoch: 4 \tTraining Loss: 0.000169\n",
      "Epoch: 5 \tTraining Loss: 0.000169\n",
      "Epoch: 6 \tTraining Loss: 0.000169\n",
      "Epoch: 7 \tTraining Loss: 0.000169\n",
      "Epoch: 8 \tTraining Loss: 0.000169\n",
      "Epoch: 9 \tTraining Loss: 0.000169\n",
      "Epoch: 10 \tTraining Loss: 0.000169\n",
      "Epoch: 11 \tTraining Loss: 0.000169\n",
      "Epoch: 12 \tTraining Loss: 0.000169\n",
      "Epoch: 13 \tTraining Loss: 0.000169\n",
      "Epoch: 14 \tTraining Loss: 0.000169\n",
      "Epoch: 15 \tTraining Loss: 0.000169\n",
      "Epoch: 16 \tTraining Loss: 0.000169\n",
      "Epoch: 17 \tTraining Loss: 0.000169\n",
      "Epoch: 18 \tTraining Loss: 0.000169\n",
      "Epoch: 1 \tTraining Loss: 0.000233\n",
      "Epoch: 2 \tTraining Loss: 0.000233\n",
      "Epoch: 3 \tTraining Loss: 0.000233\n",
      "Epoch: 4 \tTraining Loss: 0.000233\n",
      "Epoch: 5 \tTraining Loss: 0.000233\n",
      "Epoch: 6 \tTraining Loss: 0.000233\n",
      "Epoch: 7 \tTraining Loss: 0.000233\n",
      "Epoch: 8 \tTraining Loss: 0.000233\n",
      "Epoch: 9 \tTraining Loss: 0.000233\n",
      "Epoch: 10 \tTraining Loss: 0.000233\n",
      "Epoch: 11 \tTraining Loss: 0.000233\n",
      "Epoch: 12 \tTraining Loss: 0.000233\n",
      "Epoch: 13 \tTraining Loss: 0.000233\n",
      "Epoch: 14 \tTraining Loss: 0.000233\n",
      "Epoch: 15 \tTraining Loss: 0.000233\n",
      "Epoch: 16 \tTraining Loss: 0.000233\n",
      "Epoch: 17 \tTraining Loss: 0.000233\n",
      "Epoch: 18 \tTraining Loss: 0.000233\n",
      "Epoch: 1 \tTraining Loss: 0.000178\n",
      "Epoch: 2 \tTraining Loss: 0.000178\n",
      "Epoch: 3 \tTraining Loss: 0.000178\n",
      "Epoch: 4 \tTraining Loss: 0.000178\n",
      "Epoch: 5 \tTraining Loss: 0.000178\n",
      "Epoch: 6 \tTraining Loss: 0.000178\n",
      "Epoch: 7 \tTraining Loss: 0.000178\n",
      "Epoch: 8 \tTraining Loss: 0.000178\n",
      "Epoch: 9 \tTraining Loss: 0.000178\n",
      "Epoch: 10 \tTraining Loss: 0.000178\n",
      "Epoch: 11 \tTraining Loss: 0.000178\n",
      "Epoch: 12 \tTraining Loss: 0.000178\n",
      "Epoch: 13 \tTraining Loss: 0.000178\n",
      "Epoch: 14 \tTraining Loss: 0.000178\n",
      "Epoch: 15 \tTraining Loss: 0.000178\n",
      "Epoch: 16 \tTraining Loss: 0.000178\n",
      "Epoch: 17 \tTraining Loss: 0.000178\n",
      "Epoch: 18 \tTraining Loss: 0.000178\n",
      "Epoch: 1 \tTraining Loss: 0.000091\n",
      "Epoch: 2 \tTraining Loss: 0.000091\n",
      "Epoch: 3 \tTraining Loss: 0.000091\n",
      "Epoch: 4 \tTraining Loss: 0.000091\n",
      "Epoch: 5 \tTraining Loss: 0.000091\n",
      "Epoch: 6 \tTraining Loss: 0.000091\n",
      "Epoch: 7 \tTraining Loss: 0.000091\n",
      "Epoch: 8 \tTraining Loss: 0.000091\n",
      "Epoch: 9 \tTraining Loss: 0.000091\n",
      "Epoch: 10 \tTraining Loss: 0.000091\n",
      "Epoch: 11 \tTraining Loss: 0.000091\n",
      "Epoch: 12 \tTraining Loss: 0.000091\n",
      "Epoch: 13 \tTraining Loss: 0.000091\n",
      "Epoch: 14 \tTraining Loss: 0.000091\n",
      "Epoch: 15 \tTraining Loss: 0.000091\n",
      "Epoch: 16 \tTraining Loss: 0.000091\n",
      "Epoch: 17 \tTraining Loss: 0.000091\n",
      "Epoch: 18 \tTraining Loss: 0.000091\n",
      "Epoch: 1 \tTraining Loss: 0.000287\n",
      "Epoch: 2 \tTraining Loss: 0.000287\n",
      "Epoch: 3 \tTraining Loss: 0.000287\n",
      "Epoch: 4 \tTraining Loss: 0.000287\n",
      "Epoch: 5 \tTraining Loss: 0.000287\n",
      "Epoch: 6 \tTraining Loss: 0.000287\n",
      "Epoch: 7 \tTraining Loss: 0.000287\n",
      "Epoch: 8 \tTraining Loss: 0.000287\n",
      "Epoch: 9 \tTraining Loss: 0.000287\n",
      "Epoch: 10 \tTraining Loss: 0.000287\n",
      "Epoch: 11 \tTraining Loss: 0.000287\n",
      "Epoch: 12 \tTraining Loss: 0.000287\n",
      "Epoch: 13 \tTraining Loss: 0.000287\n",
      "Epoch: 14 \tTraining Loss: 0.000287\n",
      "Epoch: 15 \tTraining Loss: 0.000287\n",
      "Epoch: 16 \tTraining Loss: 0.000287\n",
      "Epoch: 17 \tTraining Loss: 0.000287\n",
      "Epoch: 18 \tTraining Loss: 0.000287\n",
      "Epoch: 1 \tTraining Loss: 0.000329\n",
      "Epoch: 2 \tTraining Loss: 0.000329\n",
      "Epoch: 3 \tTraining Loss: 0.000329\n",
      "Epoch: 4 \tTraining Loss: 0.000329\n",
      "Epoch: 5 \tTraining Loss: 0.000329\n",
      "Epoch: 6 \tTraining Loss: 0.000329\n",
      "Epoch: 7 \tTraining Loss: 0.000329\n",
      "Epoch: 8 \tTraining Loss: 0.000329\n",
      "Epoch: 9 \tTraining Loss: 0.000329\n",
      "Epoch: 10 \tTraining Loss: 0.000329\n",
      "Epoch: 11 \tTraining Loss: 0.000329\n",
      "Epoch: 12 \tTraining Loss: 0.000329\n",
      "Epoch: 13 \tTraining Loss: 0.000329\n",
      "Epoch: 14 \tTraining Loss: 0.000329\n",
      "Epoch: 15 \tTraining Loss: 0.000329\n",
      "Epoch: 16 \tTraining Loss: 0.000329\n",
      "Epoch: 17 \tTraining Loss: 0.000329\n",
      "Epoch: 18 \tTraining Loss: 0.000329\n",
      "Epoch: 1 \tTraining Loss: 0.000168\n",
      "Epoch: 2 \tTraining Loss: 0.000168\n",
      "Epoch: 3 \tTraining Loss: 0.000168\n",
      "Epoch: 4 \tTraining Loss: 0.000168\n",
      "Epoch: 5 \tTraining Loss: 0.000168\n",
      "Epoch: 6 \tTraining Loss: 0.000168\n",
      "Epoch: 7 \tTraining Loss: 0.000168\n",
      "Epoch: 8 \tTraining Loss: 0.000168\n",
      "Epoch: 9 \tTraining Loss: 0.000168\n",
      "Epoch: 10 \tTraining Loss: 0.000168\n",
      "Epoch: 11 \tTraining Loss: 0.000168\n",
      "Epoch: 12 \tTraining Loss: 0.000168\n",
      "Epoch: 13 \tTraining Loss: 0.000168\n",
      "Epoch: 14 \tTraining Loss: 0.000168\n",
      "Epoch: 15 \tTraining Loss: 0.000168\n",
      "Epoch: 16 \tTraining Loss: 0.000168\n",
      "Epoch: 17 \tTraining Loss: 0.000168\n",
      "Epoch: 18 \tTraining Loss: 0.000168\n",
      "Epoch: 1 \tTraining Loss: 0.000140\n",
      "Epoch: 2 \tTraining Loss: 0.000140\n",
      "Epoch: 3 \tTraining Loss: 0.000140\n",
      "Epoch: 4 \tTraining Loss: 0.000140\n",
      "Epoch: 5 \tTraining Loss: 0.000140\n",
      "Epoch: 6 \tTraining Loss: 0.000140\n",
      "Epoch: 7 \tTraining Loss: 0.000140\n",
      "Epoch: 8 \tTraining Loss: 0.000140\n",
      "Epoch: 9 \tTraining Loss: 0.000140\n",
      "Epoch: 10 \tTraining Loss: 0.000140\n",
      "Epoch: 11 \tTraining Loss: 0.000140\n",
      "Epoch: 12 \tTraining Loss: 0.000140\n",
      "Epoch: 13 \tTraining Loss: 0.000140\n",
      "Epoch: 14 \tTraining Loss: 0.000140\n",
      "Epoch: 15 \tTraining Loss: 0.000140\n",
      "Epoch: 16 \tTraining Loss: 0.000140\n",
      "Epoch: 17 \tTraining Loss: 0.000140\n",
      "Epoch: 18 \tTraining Loss: 0.000140\n",
      "Epoch: 1 \tTraining Loss: 0.000339\n",
      "Epoch: 2 \tTraining Loss: 0.000339\n",
      "Epoch: 3 \tTraining Loss: 0.000339\n",
      "Epoch: 4 \tTraining Loss: 0.000339\n",
      "Epoch: 5 \tTraining Loss: 0.000339\n",
      "Epoch: 6 \tTraining Loss: 0.000339\n",
      "Epoch: 7 \tTraining Loss: 0.000339\n",
      "Epoch: 8 \tTraining Loss: 0.000339\n",
      "Epoch: 9 \tTraining Loss: 0.000339\n",
      "Epoch: 10 \tTraining Loss: 0.000339\n",
      "Epoch: 11 \tTraining Loss: 0.000339\n",
      "Epoch: 12 \tTraining Loss: 0.000339\n",
      "Epoch: 13 \tTraining Loss: 0.000339\n",
      "Epoch: 14 \tTraining Loss: 0.000339\n",
      "Epoch: 15 \tTraining Loss: 0.000339\n",
      "Epoch: 16 \tTraining Loss: 0.000339\n",
      "Epoch: 17 \tTraining Loss: 0.000339\n",
      "Epoch: 18 \tTraining Loss: 0.000339\n",
      "Epoch: 1 \tTraining Loss: 0.000178\n",
      "Epoch: 2 \tTraining Loss: 0.000178\n",
      "Epoch: 3 \tTraining Loss: 0.000178\n",
      "Epoch: 4 \tTraining Loss: 0.000178\n",
      "Epoch: 5 \tTraining Loss: 0.000178\n",
      "Epoch: 6 \tTraining Loss: 0.000178\n",
      "Epoch: 7 \tTraining Loss: 0.000178\n",
      "Epoch: 8 \tTraining Loss: 0.000178\n",
      "Epoch: 9 \tTraining Loss: 0.000178\n",
      "Epoch: 10 \tTraining Loss: 0.000178\n",
      "Epoch: 11 \tTraining Loss: 0.000178\n",
      "Epoch: 12 \tTraining Loss: 0.000178\n",
      "Epoch: 13 \tTraining Loss: 0.000178\n",
      "Epoch: 14 \tTraining Loss: 0.000178\n",
      "Epoch: 15 \tTraining Loss: 0.000178\n",
      "Epoch: 16 \tTraining Loss: 0.000178\n",
      "Epoch: 17 \tTraining Loss: 0.000178\n",
      "Epoch: 18 \tTraining Loss: 0.000178\n",
      "Epoch: 1 \tTraining Loss: 0.000306\n",
      "Epoch: 2 \tTraining Loss: 0.000306\n",
      "Epoch: 3 \tTraining Loss: 0.000306\n",
      "Epoch: 4 \tTraining Loss: 0.000306\n",
      "Epoch: 5 \tTraining Loss: 0.000306\n",
      "Epoch: 6 \tTraining Loss: 0.000306\n",
      "Epoch: 7 \tTraining Loss: 0.000306\n",
      "Epoch: 8 \tTraining Loss: 0.000306\n",
      "Epoch: 9 \tTraining Loss: 0.000306\n",
      "Epoch: 10 \tTraining Loss: 0.000306\n",
      "Epoch: 11 \tTraining Loss: 0.000306\n",
      "Epoch: 12 \tTraining Loss: 0.000306\n",
      "Epoch: 13 \tTraining Loss: 0.000306\n",
      "Epoch: 14 \tTraining Loss: 0.000306\n",
      "Epoch: 15 \tTraining Loss: 0.000306\n",
      "Epoch: 16 \tTraining Loss: 0.000306\n",
      "Epoch: 17 \tTraining Loss: 0.000306\n",
      "Epoch: 18 \tTraining Loss: 0.000306\n",
      "Epoch: 1 \tTraining Loss: 0.000239\n",
      "Epoch: 2 \tTraining Loss: 0.000239\n",
      "Epoch: 3 \tTraining Loss: 0.000239\n",
      "Epoch: 4 \tTraining Loss: 0.000239\n",
      "Epoch: 5 \tTraining Loss: 0.000239\n",
      "Epoch: 6 \tTraining Loss: 0.000239\n",
      "Epoch: 7 \tTraining Loss: 0.000239\n",
      "Epoch: 8 \tTraining Loss: 0.000239\n",
      "Epoch: 9 \tTraining Loss: 0.000239\n",
      "Epoch: 10 \tTraining Loss: 0.000239\n",
      "Epoch: 11 \tTraining Loss: 0.000239\n",
      "Epoch: 12 \tTraining Loss: 0.000239\n",
      "Epoch: 13 \tTraining Loss: 0.000239\n",
      "Epoch: 14 \tTraining Loss: 0.000239\n",
      "Epoch: 15 \tTraining Loss: 0.000239\n",
      "Epoch: 16 \tTraining Loss: 0.000239\n",
      "Epoch: 17 \tTraining Loss: 0.000239\n",
      "Epoch: 18 \tTraining Loss: 0.000239\n",
      "Epoch: 1 \tTraining Loss: 0.000264\n",
      "Epoch: 2 \tTraining Loss: 0.000264\n",
      "Epoch: 3 \tTraining Loss: 0.000264\n",
      "Epoch: 4 \tTraining Loss: 0.000264\n",
      "Epoch: 5 \tTraining Loss: 0.000264\n",
      "Epoch: 6 \tTraining Loss: 0.000264\n",
      "Epoch: 7 \tTraining Loss: 0.000264\n",
      "Epoch: 8 \tTraining Loss: 0.000264\n",
      "Epoch: 9 \tTraining Loss: 0.000264\n",
      "Epoch: 10 \tTraining Loss: 0.000264\n",
      "Epoch: 11 \tTraining Loss: 0.000264\n",
      "Epoch: 12 \tTraining Loss: 0.000264\n",
      "Epoch: 13 \tTraining Loss: 0.000264\n",
      "Epoch: 14 \tTraining Loss: 0.000264\n",
      "Epoch: 15 \tTraining Loss: 0.000264\n",
      "Epoch: 16 \tTraining Loss: 0.000264\n",
      "Epoch: 17 \tTraining Loss: 0.000264\n",
      "Epoch: 18 \tTraining Loss: 0.000264\n",
      "Epoch: 1 \tTraining Loss: 0.000222\n",
      "Epoch: 2 \tTraining Loss: 0.000222\n",
      "Epoch: 3 \tTraining Loss: 0.000222\n",
      "Epoch: 4 \tTraining Loss: 0.000222\n",
      "Epoch: 5 \tTraining Loss: 0.000222\n",
      "Epoch: 6 \tTraining Loss: 0.000222\n",
      "Epoch: 7 \tTraining Loss: 0.000222\n",
      "Epoch: 8 \tTraining Loss: 0.000222\n",
      "Epoch: 9 \tTraining Loss: 0.000222\n",
      "Epoch: 10 \tTraining Loss: 0.000222\n",
      "Epoch: 11 \tTraining Loss: 0.000222\n",
      "Epoch: 12 \tTraining Loss: 0.000222\n",
      "Epoch: 13 \tTraining Loss: 0.000222\n",
      "Epoch: 14 \tTraining Loss: 0.000222\n",
      "Epoch: 15 \tTraining Loss: 0.000222\n",
      "Epoch: 16 \tTraining Loss: 0.000222\n",
      "Epoch: 17 \tTraining Loss: 0.000222\n",
      "Epoch: 18 \tTraining Loss: 0.000222\n",
      "Epoch: 1 \tTraining Loss: 0.000211\n",
      "Epoch: 2 \tTraining Loss: 0.000211\n",
      "Epoch: 3 \tTraining Loss: 0.000211\n",
      "Epoch: 4 \tTraining Loss: 0.000211\n",
      "Epoch: 5 \tTraining Loss: 0.000211\n",
      "Epoch: 6 \tTraining Loss: 0.000211\n",
      "Epoch: 7 \tTraining Loss: 0.000211\n",
      "Epoch: 8 \tTraining Loss: 0.000211\n",
      "Epoch: 9 \tTraining Loss: 0.000211\n",
      "Epoch: 10 \tTraining Loss: 0.000211\n",
      "Epoch: 11 \tTraining Loss: 0.000211\n",
      "Epoch: 12 \tTraining Loss: 0.000211\n",
      "Epoch: 13 \tTraining Loss: 0.000211\n",
      "Epoch: 14 \tTraining Loss: 0.000211\n",
      "Epoch: 15 \tTraining Loss: 0.000211\n",
      "Epoch: 16 \tTraining Loss: 0.000211\n",
      "Epoch: 17 \tTraining Loss: 0.000211\n",
      "Epoch: 18 \tTraining Loss: 0.000211\n",
      "Epoch: 1 \tTraining Loss: 0.000172\n",
      "Epoch: 2 \tTraining Loss: 0.000172\n",
      "Epoch: 3 \tTraining Loss: 0.000172\n",
      "Epoch: 4 \tTraining Loss: 0.000172\n",
      "Epoch: 5 \tTraining Loss: 0.000172\n",
      "Epoch: 6 \tTraining Loss: 0.000172\n",
      "Epoch: 7 \tTraining Loss: 0.000172\n",
      "Epoch: 8 \tTraining Loss: 0.000172\n",
      "Epoch: 9 \tTraining Loss: 0.000172\n",
      "Epoch: 10 \tTraining Loss: 0.000172\n",
      "Epoch: 11 \tTraining Loss: 0.000172\n",
      "Epoch: 12 \tTraining Loss: 0.000172\n",
      "Epoch: 13 \tTraining Loss: 0.000172\n",
      "Epoch: 14 \tTraining Loss: 0.000172\n",
      "Epoch: 15 \tTraining Loss: 0.000172\n",
      "Epoch: 16 \tTraining Loss: 0.000172\n",
      "Epoch: 17 \tTraining Loss: 0.000172\n",
      "Epoch: 18 \tTraining Loss: 0.000172\n",
      "Epoch: 1 \tTraining Loss: 0.000255\n",
      "Epoch: 2 \tTraining Loss: 0.000255\n",
      "Epoch: 3 \tTraining Loss: 0.000255\n",
      "Epoch: 4 \tTraining Loss: 0.000255\n",
      "Epoch: 5 \tTraining Loss: 0.000255\n",
      "Epoch: 6 \tTraining Loss: 0.000255\n",
      "Epoch: 7 \tTraining Loss: 0.000255\n",
      "Epoch: 8 \tTraining Loss: 0.000255\n",
      "Epoch: 9 \tTraining Loss: 0.000255\n",
      "Epoch: 10 \tTraining Loss: 0.000255\n",
      "Epoch: 11 \tTraining Loss: 0.000255\n",
      "Epoch: 12 \tTraining Loss: 0.000255\n",
      "Epoch: 13 \tTraining Loss: 0.000255\n",
      "Epoch: 14 \tTraining Loss: 0.000255\n",
      "Epoch: 15 \tTraining Loss: 0.000255\n",
      "Epoch: 16 \tTraining Loss: 0.000255\n",
      "Epoch: 17 \tTraining Loss: 0.000255\n",
      "Epoch: 18 \tTraining Loss: 0.000255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000171\n",
      "Epoch: 2 \tTraining Loss: 0.000171\n",
      "Epoch: 3 \tTraining Loss: 0.000171\n",
      "Epoch: 4 \tTraining Loss: 0.000171\n",
      "Epoch: 5 \tTraining Loss: 0.000171\n",
      "Epoch: 6 \tTraining Loss: 0.000171\n",
      "Epoch: 7 \tTraining Loss: 0.000171\n",
      "Epoch: 8 \tTraining Loss: 0.000171\n",
      "Epoch: 9 \tTraining Loss: 0.000171\n",
      "Epoch: 10 \tTraining Loss: 0.000171\n",
      "Epoch: 11 \tTraining Loss: 0.000171\n",
      "Epoch: 12 \tTraining Loss: 0.000171\n",
      "Epoch: 13 \tTraining Loss: 0.000171\n",
      "Epoch: 14 \tTraining Loss: 0.000171\n",
      "Epoch: 15 \tTraining Loss: 0.000171\n",
      "Epoch: 16 \tTraining Loss: 0.000171\n",
      "Epoch: 17 \tTraining Loss: 0.000171\n",
      "Epoch: 18 \tTraining Loss: 0.000171\n",
      "Epoch: 1 \tTraining Loss: 0.000171\n",
      "Epoch: 2 \tTraining Loss: 0.000171\n",
      "Epoch: 3 \tTraining Loss: 0.000171\n",
      "Epoch: 4 \tTraining Loss: 0.000171\n",
      "Epoch: 5 \tTraining Loss: 0.000171\n",
      "Epoch: 6 \tTraining Loss: 0.000171\n",
      "Epoch: 7 \tTraining Loss: 0.000171\n",
      "Epoch: 8 \tTraining Loss: 0.000171\n",
      "Epoch: 9 \tTraining Loss: 0.000171\n",
      "Epoch: 10 \tTraining Loss: 0.000171\n",
      "Epoch: 11 \tTraining Loss: 0.000171\n",
      "Epoch: 12 \tTraining Loss: 0.000171\n",
      "Epoch: 13 \tTraining Loss: 0.000171\n",
      "Epoch: 14 \tTraining Loss: 0.000171\n",
      "Epoch: 15 \tTraining Loss: 0.000171\n",
      "Epoch: 16 \tTraining Loss: 0.000171\n",
      "Epoch: 17 \tTraining Loss: 0.000171\n",
      "Epoch: 18 \tTraining Loss: 0.000171\n",
      "Epoch: 1 \tTraining Loss: 0.000267\n",
      "Epoch: 2 \tTraining Loss: 0.000267\n",
      "Epoch: 3 \tTraining Loss: 0.000267\n",
      "Epoch: 4 \tTraining Loss: 0.000267\n",
      "Epoch: 5 \tTraining Loss: 0.000267\n",
      "Epoch: 6 \tTraining Loss: 0.000267\n",
      "Epoch: 7 \tTraining Loss: 0.000267\n",
      "Epoch: 8 \tTraining Loss: 0.000267\n",
      "Epoch: 9 \tTraining Loss: 0.000267\n",
      "Epoch: 10 \tTraining Loss: 0.000267\n",
      "Epoch: 11 \tTraining Loss: 0.000267\n",
      "Epoch: 12 \tTraining Loss: 0.000267\n",
      "Epoch: 13 \tTraining Loss: 0.000267\n",
      "Epoch: 14 \tTraining Loss: 0.000267\n",
      "Epoch: 15 \tTraining Loss: 0.000267\n",
      "Epoch: 16 \tTraining Loss: 0.000267\n",
      "Epoch: 17 \tTraining Loss: 0.000267\n",
      "Epoch: 18 \tTraining Loss: 0.000267\n",
      "Epoch: 1 \tTraining Loss: 0.000112\n",
      "Epoch: 2 \tTraining Loss: 0.000112\n",
      "Epoch: 3 \tTraining Loss: 0.000112\n",
      "Epoch: 4 \tTraining Loss: 0.000112\n",
      "Epoch: 5 \tTraining Loss: 0.000112\n",
      "Epoch: 6 \tTraining Loss: 0.000112\n",
      "Epoch: 7 \tTraining Loss: 0.000112\n",
      "Epoch: 8 \tTraining Loss: 0.000112\n",
      "Epoch: 9 \tTraining Loss: 0.000112\n",
      "Epoch: 10 \tTraining Loss: 0.000112\n",
      "Epoch: 11 \tTraining Loss: 0.000112\n",
      "Epoch: 12 \tTraining Loss: 0.000112\n",
      "Epoch: 13 \tTraining Loss: 0.000112\n",
      "Epoch: 14 \tTraining Loss: 0.000112\n",
      "Epoch: 15 \tTraining Loss: 0.000112\n",
      "Epoch: 16 \tTraining Loss: 0.000112\n",
      "Epoch: 17 \tTraining Loss: 0.000112\n",
      "Epoch: 18 \tTraining Loss: 0.000112\n",
      "Epoch: 1 \tTraining Loss: 0.000188\n",
      "Epoch: 2 \tTraining Loss: 0.000188\n",
      "Epoch: 3 \tTraining Loss: 0.000188\n",
      "Epoch: 4 \tTraining Loss: 0.000188\n",
      "Epoch: 5 \tTraining Loss: 0.000188\n",
      "Epoch: 6 \tTraining Loss: 0.000188\n",
      "Epoch: 7 \tTraining Loss: 0.000188\n",
      "Epoch: 8 \tTraining Loss: 0.000188\n",
      "Epoch: 9 \tTraining Loss: 0.000188\n",
      "Epoch: 10 \tTraining Loss: 0.000188\n",
      "Epoch: 11 \tTraining Loss: 0.000188\n",
      "Epoch: 12 \tTraining Loss: 0.000188\n",
      "Epoch: 13 \tTraining Loss: 0.000188\n",
      "Epoch: 14 \tTraining Loss: 0.000188\n",
      "Epoch: 15 \tTraining Loss: 0.000188\n",
      "Epoch: 16 \tTraining Loss: 0.000188\n",
      "Epoch: 17 \tTraining Loss: 0.000188\n",
      "Epoch: 18 \tTraining Loss: 0.000188\n",
      "Epoch: 1 \tTraining Loss: 0.000181\n",
      "Epoch: 2 \tTraining Loss: 0.000181\n",
      "Epoch: 3 \tTraining Loss: 0.000181\n",
      "Epoch: 4 \tTraining Loss: 0.000181\n",
      "Epoch: 5 \tTraining Loss: 0.000181\n",
      "Epoch: 6 \tTraining Loss: 0.000181\n",
      "Epoch: 7 \tTraining Loss: 0.000181\n",
      "Epoch: 8 \tTraining Loss: 0.000181\n",
      "Epoch: 9 \tTraining Loss: 0.000181\n",
      "Epoch: 10 \tTraining Loss: 0.000181\n",
      "Epoch: 11 \tTraining Loss: 0.000181\n",
      "Epoch: 12 \tTraining Loss: 0.000181\n",
      "Epoch: 13 \tTraining Loss: 0.000181\n",
      "Epoch: 14 \tTraining Loss: 0.000181\n",
      "Epoch: 15 \tTraining Loss: 0.000181\n",
      "Epoch: 16 \tTraining Loss: 0.000181\n",
      "Epoch: 17 \tTraining Loss: 0.000181\n",
      "Epoch: 18 \tTraining Loss: 0.000181\n",
      "Epoch: 1 \tTraining Loss: 0.000221\n",
      "Epoch: 2 \tTraining Loss: 0.000221\n",
      "Epoch: 3 \tTraining Loss: 0.000221\n",
      "Epoch: 4 \tTraining Loss: 0.000221\n",
      "Epoch: 5 \tTraining Loss: 0.000221\n",
      "Epoch: 6 \tTraining Loss: 0.000221\n",
      "Epoch: 7 \tTraining Loss: 0.000221\n",
      "Epoch: 8 \tTraining Loss: 0.000221\n",
      "Epoch: 9 \tTraining Loss: 0.000221\n",
      "Epoch: 10 \tTraining Loss: 0.000221\n",
      "Epoch: 11 \tTraining Loss: 0.000221\n",
      "Epoch: 12 \tTraining Loss: 0.000221\n",
      "Epoch: 13 \tTraining Loss: 0.000221\n",
      "Epoch: 14 \tTraining Loss: 0.000221\n",
      "Epoch: 15 \tTraining Loss: 0.000221\n",
      "Epoch: 16 \tTraining Loss: 0.000221\n",
      "Epoch: 17 \tTraining Loss: 0.000221\n",
      "Epoch: 18 \tTraining Loss: 0.000221\n",
      "Epoch: 1 \tTraining Loss: 0.000161\n",
      "Epoch: 2 \tTraining Loss: 0.000161\n",
      "Epoch: 3 \tTraining Loss: 0.000161\n",
      "Epoch: 4 \tTraining Loss: 0.000161\n",
      "Epoch: 5 \tTraining Loss: 0.000161\n",
      "Epoch: 6 \tTraining Loss: 0.000161\n",
      "Epoch: 7 \tTraining Loss: 0.000161\n",
      "Epoch: 8 \tTraining Loss: 0.000161\n",
      "Epoch: 9 \tTraining Loss: 0.000161\n",
      "Epoch: 10 \tTraining Loss: 0.000161\n",
      "Epoch: 11 \tTraining Loss: 0.000161\n",
      "Epoch: 12 \tTraining Loss: 0.000161\n",
      "Epoch: 13 \tTraining Loss: 0.000161\n",
      "Epoch: 14 \tTraining Loss: 0.000161\n",
      "Epoch: 15 \tTraining Loss: 0.000161\n",
      "Epoch: 16 \tTraining Loss: 0.000161\n",
      "Epoch: 17 \tTraining Loss: 0.000161\n",
      "Epoch: 18 \tTraining Loss: 0.000161\n",
      "Epoch: 1 \tTraining Loss: 0.000125\n",
      "Epoch: 2 \tTraining Loss: 0.000125\n",
      "Epoch: 3 \tTraining Loss: 0.000125\n",
      "Epoch: 4 \tTraining Loss: 0.000125\n",
      "Epoch: 5 \tTraining Loss: 0.000125\n",
      "Epoch: 6 \tTraining Loss: 0.000125\n",
      "Epoch: 7 \tTraining Loss: 0.000125\n",
      "Epoch: 8 \tTraining Loss: 0.000125\n",
      "Epoch: 9 \tTraining Loss: 0.000125\n",
      "Epoch: 10 \tTraining Loss: 0.000125\n",
      "Epoch: 11 \tTraining Loss: 0.000125\n",
      "Epoch: 12 \tTraining Loss: 0.000125\n",
      "Epoch: 13 \tTraining Loss: 0.000125\n",
      "Epoch: 14 \tTraining Loss: 0.000125\n",
      "Epoch: 15 \tTraining Loss: 0.000125\n",
      "Epoch: 16 \tTraining Loss: 0.000125\n",
      "Epoch: 17 \tTraining Loss: 0.000125\n",
      "Epoch: 18 \tTraining Loss: 0.000125\n",
      "Epoch: 1 \tTraining Loss: 0.000099\n",
      "Epoch: 2 \tTraining Loss: 0.000099\n",
      "Epoch: 3 \tTraining Loss: 0.000099\n",
      "Epoch: 4 \tTraining Loss: 0.000099\n",
      "Epoch: 5 \tTraining Loss: 0.000099\n",
      "Epoch: 6 \tTraining Loss: 0.000099\n",
      "Epoch: 7 \tTraining Loss: 0.000099\n",
      "Epoch: 8 \tTraining Loss: 0.000099\n",
      "Epoch: 9 \tTraining Loss: 0.000099\n",
      "Epoch: 10 \tTraining Loss: 0.000099\n",
      "Epoch: 11 \tTraining Loss: 0.000099\n",
      "Epoch: 12 \tTraining Loss: 0.000099\n",
      "Epoch: 13 \tTraining Loss: 0.000099\n",
      "Epoch: 14 \tTraining Loss: 0.000099\n",
      "Epoch: 15 \tTraining Loss: 0.000099\n",
      "Epoch: 16 \tTraining Loss: 0.000099\n",
      "Epoch: 17 \tTraining Loss: 0.000099\n",
      "Epoch: 18 \tTraining Loss: 0.000099\n",
      "Epoch: 1 \tTraining Loss: 0.000161\n",
      "Epoch: 2 \tTraining Loss: 0.000161\n",
      "Epoch: 3 \tTraining Loss: 0.000161\n",
      "Epoch: 4 \tTraining Loss: 0.000161\n",
      "Epoch: 5 \tTraining Loss: 0.000161\n",
      "Epoch: 6 \tTraining Loss: 0.000161\n",
      "Epoch: 7 \tTraining Loss: 0.000161\n",
      "Epoch: 8 \tTraining Loss: 0.000161\n",
      "Epoch: 9 \tTraining Loss: 0.000161\n",
      "Epoch: 10 \tTraining Loss: 0.000161\n",
      "Epoch: 11 \tTraining Loss: 0.000161\n",
      "Epoch: 12 \tTraining Loss: 0.000161\n",
      "Epoch: 13 \tTraining Loss: 0.000161\n",
      "Epoch: 14 \tTraining Loss: 0.000161\n",
      "Epoch: 15 \tTraining Loss: 0.000161\n",
      "Epoch: 16 \tTraining Loss: 0.000161\n",
      "Epoch: 17 \tTraining Loss: 0.000161\n",
      "Epoch: 18 \tTraining Loss: 0.000161\n",
      "Epoch: 1 \tTraining Loss: 0.000390\n",
      "Epoch: 2 \tTraining Loss: 0.000390\n",
      "Epoch: 3 \tTraining Loss: 0.000390\n",
      "Epoch: 4 \tTraining Loss: 0.000390\n",
      "Epoch: 5 \tTraining Loss: 0.000390\n",
      "Epoch: 6 \tTraining Loss: 0.000390\n",
      "Epoch: 7 \tTraining Loss: 0.000390\n",
      "Epoch: 8 \tTraining Loss: 0.000390\n",
      "Epoch: 9 \tTraining Loss: 0.000390\n",
      "Epoch: 10 \tTraining Loss: 0.000390\n",
      "Epoch: 11 \tTraining Loss: 0.000390\n",
      "Epoch: 12 \tTraining Loss: 0.000390\n",
      "Epoch: 13 \tTraining Loss: 0.000390\n",
      "Epoch: 14 \tTraining Loss: 0.000390\n",
      "Epoch: 15 \tTraining Loss: 0.000390\n",
      "Epoch: 16 \tTraining Loss: 0.000390\n",
      "Epoch: 17 \tTraining Loss: 0.000390\n",
      "Epoch: 18 \tTraining Loss: 0.000390\n",
      "Epoch: 1 \tTraining Loss: 0.000253\n",
      "Epoch: 2 \tTraining Loss: 0.000253\n",
      "Epoch: 3 \tTraining Loss: 0.000253\n",
      "Epoch: 4 \tTraining Loss: 0.000253\n",
      "Epoch: 5 \tTraining Loss: 0.000253\n",
      "Epoch: 6 \tTraining Loss: 0.000253\n",
      "Epoch: 7 \tTraining Loss: 0.000253\n",
      "Epoch: 8 \tTraining Loss: 0.000253\n",
      "Epoch: 9 \tTraining Loss: 0.000253\n",
      "Epoch: 10 \tTraining Loss: 0.000253\n",
      "Epoch: 11 \tTraining Loss: 0.000253\n",
      "Epoch: 12 \tTraining Loss: 0.000253\n",
      "Epoch: 13 \tTraining Loss: 0.000253\n",
      "Epoch: 14 \tTraining Loss: 0.000253\n",
      "Epoch: 15 \tTraining Loss: 0.000253\n",
      "Epoch: 16 \tTraining Loss: 0.000253\n",
      "Epoch: 17 \tTraining Loss: 0.000253\n",
      "Epoch: 18 \tTraining Loss: 0.000253\n",
      "Epoch: 1 \tTraining Loss: 0.000350\n",
      "Epoch: 2 \tTraining Loss: 0.000350\n",
      "Epoch: 3 \tTraining Loss: 0.000350\n",
      "Epoch: 4 \tTraining Loss: 0.000350\n",
      "Epoch: 5 \tTraining Loss: 0.000350\n",
      "Epoch: 6 \tTraining Loss: 0.000350\n",
      "Epoch: 7 \tTraining Loss: 0.000350\n",
      "Epoch: 8 \tTraining Loss: 0.000350\n",
      "Epoch: 9 \tTraining Loss: 0.000350\n",
      "Epoch: 10 \tTraining Loss: 0.000350\n",
      "Epoch: 11 \tTraining Loss: 0.000350\n",
      "Epoch: 12 \tTraining Loss: 0.000350\n",
      "Epoch: 13 \tTraining Loss: 0.000350\n",
      "Epoch: 14 \tTraining Loss: 0.000350\n",
      "Epoch: 15 \tTraining Loss: 0.000350\n",
      "Epoch: 16 \tTraining Loss: 0.000350\n",
      "Epoch: 17 \tTraining Loss: 0.000350\n",
      "Epoch: 18 \tTraining Loss: 0.000350\n",
      "Epoch: 1 \tTraining Loss: 0.000121\n",
      "Epoch: 2 \tTraining Loss: 0.000121\n",
      "Epoch: 3 \tTraining Loss: 0.000121\n",
      "Epoch: 4 \tTraining Loss: 0.000121\n",
      "Epoch: 5 \tTraining Loss: 0.000121\n",
      "Epoch: 6 \tTraining Loss: 0.000121\n",
      "Epoch: 7 \tTraining Loss: 0.000121\n",
      "Epoch: 8 \tTraining Loss: 0.000121\n",
      "Epoch: 9 \tTraining Loss: 0.000121\n",
      "Epoch: 10 \tTraining Loss: 0.000121\n",
      "Epoch: 11 \tTraining Loss: 0.000121\n",
      "Epoch: 12 \tTraining Loss: 0.000121\n",
      "Epoch: 13 \tTraining Loss: 0.000121\n",
      "Epoch: 14 \tTraining Loss: 0.000121\n",
      "Epoch: 15 \tTraining Loss: 0.000121\n",
      "Epoch: 16 \tTraining Loss: 0.000121\n",
      "Epoch: 17 \tTraining Loss: 0.000121\n",
      "Epoch: 18 \tTraining Loss: 0.000121\n",
      "Epoch: 1 \tTraining Loss: 0.000222\n",
      "Epoch: 2 \tTraining Loss: 0.000222\n",
      "Epoch: 3 \tTraining Loss: 0.000222\n",
      "Epoch: 4 \tTraining Loss: 0.000222\n",
      "Epoch: 5 \tTraining Loss: 0.000222\n",
      "Epoch: 6 \tTraining Loss: 0.000222\n",
      "Epoch: 7 \tTraining Loss: 0.000222\n",
      "Epoch: 8 \tTraining Loss: 0.000222\n",
      "Epoch: 9 \tTraining Loss: 0.000222\n",
      "Epoch: 10 \tTraining Loss: 0.000222\n",
      "Epoch: 11 \tTraining Loss: 0.000222\n",
      "Epoch: 12 \tTraining Loss: 0.000222\n",
      "Epoch: 13 \tTraining Loss: 0.000222\n",
      "Epoch: 14 \tTraining Loss: 0.000222\n",
      "Epoch: 15 \tTraining Loss: 0.000222\n",
      "Epoch: 16 \tTraining Loss: 0.000222\n",
      "Epoch: 17 \tTraining Loss: 0.000222\n",
      "Epoch: 18 \tTraining Loss: 0.000222\n",
      "Epoch: 1 \tTraining Loss: 0.000220\n",
      "Epoch: 2 \tTraining Loss: 0.000220\n",
      "Epoch: 3 \tTraining Loss: 0.000220\n",
      "Epoch: 4 \tTraining Loss: 0.000220\n",
      "Epoch: 5 \tTraining Loss: 0.000220\n",
      "Epoch: 6 \tTraining Loss: 0.000220\n",
      "Epoch: 7 \tTraining Loss: 0.000220\n",
      "Epoch: 8 \tTraining Loss: 0.000220\n",
      "Epoch: 9 \tTraining Loss: 0.000220\n",
      "Epoch: 10 \tTraining Loss: 0.000220\n",
      "Epoch: 11 \tTraining Loss: 0.000220\n",
      "Epoch: 12 \tTraining Loss: 0.000220\n",
      "Epoch: 13 \tTraining Loss: 0.000220\n",
      "Epoch: 14 \tTraining Loss: 0.000220\n",
      "Epoch: 15 \tTraining Loss: 0.000220\n",
      "Epoch: 16 \tTraining Loss: 0.000220\n",
      "Epoch: 17 \tTraining Loss: 0.000220\n",
      "Epoch: 18 \tTraining Loss: 0.000220\n",
      "Epoch: 1 \tTraining Loss: 0.000145\n",
      "Epoch: 2 \tTraining Loss: 0.000145\n",
      "Epoch: 3 \tTraining Loss: 0.000145\n",
      "Epoch: 4 \tTraining Loss: 0.000145\n",
      "Epoch: 5 \tTraining Loss: 0.000145\n",
      "Epoch: 6 \tTraining Loss: 0.000145\n",
      "Epoch: 7 \tTraining Loss: 0.000145\n",
      "Epoch: 8 \tTraining Loss: 0.000145\n",
      "Epoch: 9 \tTraining Loss: 0.000145\n",
      "Epoch: 10 \tTraining Loss: 0.000145\n",
      "Epoch: 11 \tTraining Loss: 0.000145\n",
      "Epoch: 12 \tTraining Loss: 0.000145\n",
      "Epoch: 13 \tTraining Loss: 0.000145\n",
      "Epoch: 14 \tTraining Loss: 0.000145\n",
      "Epoch: 15 \tTraining Loss: 0.000145\n",
      "Epoch: 16 \tTraining Loss: 0.000145\n",
      "Epoch: 17 \tTraining Loss: 0.000145\n",
      "Epoch: 18 \tTraining Loss: 0.000145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000280\n",
      "Epoch: 2 \tTraining Loss: 0.000280\n",
      "Epoch: 3 \tTraining Loss: 0.000280\n",
      "Epoch: 4 \tTraining Loss: 0.000280\n",
      "Epoch: 5 \tTraining Loss: 0.000280\n",
      "Epoch: 6 \tTraining Loss: 0.000280\n",
      "Epoch: 7 \tTraining Loss: 0.000280\n",
      "Epoch: 8 \tTraining Loss: 0.000280\n",
      "Epoch: 9 \tTraining Loss: 0.000280\n",
      "Epoch: 10 \tTraining Loss: 0.000280\n",
      "Epoch: 11 \tTraining Loss: 0.000280\n",
      "Epoch: 12 \tTraining Loss: 0.000280\n",
      "Epoch: 13 \tTraining Loss: 0.000280\n",
      "Epoch: 14 \tTraining Loss: 0.000280\n",
      "Epoch: 15 \tTraining Loss: 0.000280\n",
      "Epoch: 16 \tTraining Loss: 0.000280\n",
      "Epoch: 17 \tTraining Loss: 0.000280\n",
      "Epoch: 18 \tTraining Loss: 0.000280\n",
      "Epoch: 1 \tTraining Loss: 0.000147\n",
      "Epoch: 2 \tTraining Loss: 0.000147\n",
      "Epoch: 3 \tTraining Loss: 0.000147\n",
      "Epoch: 4 \tTraining Loss: 0.000147\n",
      "Epoch: 5 \tTraining Loss: 0.000147\n",
      "Epoch: 6 \tTraining Loss: 0.000147\n",
      "Epoch: 7 \tTraining Loss: 0.000147\n",
      "Epoch: 8 \tTraining Loss: 0.000147\n",
      "Epoch: 9 \tTraining Loss: 0.000147\n",
      "Epoch: 10 \tTraining Loss: 0.000147\n",
      "Epoch: 11 \tTraining Loss: 0.000147\n",
      "Epoch: 12 \tTraining Loss: 0.000147\n",
      "Epoch: 13 \tTraining Loss: 0.000147\n",
      "Epoch: 14 \tTraining Loss: 0.000147\n",
      "Epoch: 15 \tTraining Loss: 0.000147\n",
      "Epoch: 16 \tTraining Loss: 0.000147\n",
      "Epoch: 17 \tTraining Loss: 0.000147\n",
      "Epoch: 18 \tTraining Loss: 0.000147\n",
      "Epoch: 1 \tTraining Loss: 0.000156\n",
      "Epoch: 2 \tTraining Loss: 0.000156\n",
      "Epoch: 3 \tTraining Loss: 0.000156\n",
      "Epoch: 4 \tTraining Loss: 0.000156\n",
      "Epoch: 5 \tTraining Loss: 0.000156\n",
      "Epoch: 6 \tTraining Loss: 0.000156\n",
      "Epoch: 7 \tTraining Loss: 0.000156\n",
      "Epoch: 8 \tTraining Loss: 0.000156\n",
      "Epoch: 9 \tTraining Loss: 0.000156\n",
      "Epoch: 10 \tTraining Loss: 0.000156\n",
      "Epoch: 11 \tTraining Loss: 0.000156\n",
      "Epoch: 12 \tTraining Loss: 0.000156\n",
      "Epoch: 13 \tTraining Loss: 0.000156\n",
      "Epoch: 14 \tTraining Loss: 0.000156\n",
      "Epoch: 15 \tTraining Loss: 0.000156\n",
      "Epoch: 16 \tTraining Loss: 0.000156\n",
      "Epoch: 17 \tTraining Loss: 0.000156\n",
      "Epoch: 18 \tTraining Loss: 0.000156\n",
      "Epoch: 1 \tTraining Loss: 0.000103\n",
      "Epoch: 2 \tTraining Loss: 0.000103\n",
      "Epoch: 3 \tTraining Loss: 0.000103\n",
      "Epoch: 4 \tTraining Loss: 0.000103\n",
      "Epoch: 5 \tTraining Loss: 0.000103\n",
      "Epoch: 6 \tTraining Loss: 0.000103\n",
      "Epoch: 7 \tTraining Loss: 0.000103\n",
      "Epoch: 8 \tTraining Loss: 0.000103\n",
      "Epoch: 9 \tTraining Loss: 0.000103\n",
      "Epoch: 10 \tTraining Loss: 0.000103\n",
      "Epoch: 11 \tTraining Loss: 0.000103\n",
      "Epoch: 12 \tTraining Loss: 0.000103\n",
      "Epoch: 13 \tTraining Loss: 0.000103\n",
      "Epoch: 14 \tTraining Loss: 0.000103\n",
      "Epoch: 15 \tTraining Loss: 0.000103\n",
      "Epoch: 16 \tTraining Loss: 0.000103\n",
      "Epoch: 17 \tTraining Loss: 0.000103\n",
      "Epoch: 18 \tTraining Loss: 0.000103\n",
      "Epoch: 1 \tTraining Loss: 0.000080\n",
      "Epoch: 2 \tTraining Loss: 0.000080\n",
      "Epoch: 3 \tTraining Loss: 0.000080\n",
      "Epoch: 4 \tTraining Loss: 0.000080\n",
      "Epoch: 5 \tTraining Loss: 0.000080\n",
      "Epoch: 6 \tTraining Loss: 0.000080\n",
      "Epoch: 7 \tTraining Loss: 0.000080\n",
      "Epoch: 8 \tTraining Loss: 0.000080\n",
      "Epoch: 9 \tTraining Loss: 0.000080\n",
      "Epoch: 10 \tTraining Loss: 0.000080\n",
      "Epoch: 11 \tTraining Loss: 0.000080\n",
      "Epoch: 12 \tTraining Loss: 0.000080\n",
      "Epoch: 13 \tTraining Loss: 0.000080\n",
      "Epoch: 14 \tTraining Loss: 0.000080\n",
      "Epoch: 15 \tTraining Loss: 0.000080\n",
      "Epoch: 16 \tTraining Loss: 0.000080\n",
      "Epoch: 17 \tTraining Loss: 0.000080\n",
      "Epoch: 18 \tTraining Loss: 0.000080\n",
      "Epoch: 1 \tTraining Loss: 0.000221\n",
      "Epoch: 2 \tTraining Loss: 0.000221\n",
      "Epoch: 3 \tTraining Loss: 0.000221\n",
      "Epoch: 4 \tTraining Loss: 0.000221\n",
      "Epoch: 5 \tTraining Loss: 0.000221\n",
      "Epoch: 6 \tTraining Loss: 0.000221\n",
      "Epoch: 7 \tTraining Loss: 0.000221\n",
      "Epoch: 8 \tTraining Loss: 0.000221\n",
      "Epoch: 9 \tTraining Loss: 0.000221\n",
      "Epoch: 10 \tTraining Loss: 0.000221\n",
      "Epoch: 11 \tTraining Loss: 0.000221\n",
      "Epoch: 12 \tTraining Loss: 0.000221\n",
      "Epoch: 13 \tTraining Loss: 0.000221\n",
      "Epoch: 14 \tTraining Loss: 0.000221\n",
      "Epoch: 15 \tTraining Loss: 0.000221\n",
      "Epoch: 16 \tTraining Loss: 0.000221\n",
      "Epoch: 17 \tTraining Loss: 0.000221\n",
      "Epoch: 18 \tTraining Loss: 0.000221\n",
      "Epoch: 1 \tTraining Loss: 0.000188\n",
      "Epoch: 2 \tTraining Loss: 0.000188\n",
      "Epoch: 3 \tTraining Loss: 0.000188\n",
      "Epoch: 4 \tTraining Loss: 0.000188\n",
      "Epoch: 5 \tTraining Loss: 0.000188\n",
      "Epoch: 6 \tTraining Loss: 0.000188\n",
      "Epoch: 7 \tTraining Loss: 0.000188\n",
      "Epoch: 8 \tTraining Loss: 0.000188\n",
      "Epoch: 9 \tTraining Loss: 0.000188\n",
      "Epoch: 10 \tTraining Loss: 0.000188\n",
      "Epoch: 11 \tTraining Loss: 0.000188\n",
      "Epoch: 12 \tTraining Loss: 0.000188\n",
      "Epoch: 13 \tTraining Loss: 0.000188\n",
      "Epoch: 14 \tTraining Loss: 0.000188\n",
      "Epoch: 15 \tTraining Loss: 0.000188\n",
      "Epoch: 16 \tTraining Loss: 0.000188\n",
      "Epoch: 17 \tTraining Loss: 0.000188\n",
      "Epoch: 18 \tTraining Loss: 0.000188\n",
      "Epoch: 1 \tTraining Loss: 0.000165\n",
      "Epoch: 2 \tTraining Loss: 0.000165\n",
      "Epoch: 3 \tTraining Loss: 0.000165\n",
      "Epoch: 4 \tTraining Loss: 0.000165\n",
      "Epoch: 5 \tTraining Loss: 0.000165\n",
      "Epoch: 6 \tTraining Loss: 0.000165\n",
      "Epoch: 7 \tTraining Loss: 0.000165\n",
      "Epoch: 8 \tTraining Loss: 0.000165\n",
      "Epoch: 9 \tTraining Loss: 0.000165\n",
      "Epoch: 10 \tTraining Loss: 0.000165\n",
      "Epoch: 11 \tTraining Loss: 0.000165\n",
      "Epoch: 12 \tTraining Loss: 0.000165\n",
      "Epoch: 13 \tTraining Loss: 0.000165\n",
      "Epoch: 14 \tTraining Loss: 0.000165\n",
      "Epoch: 15 \tTraining Loss: 0.000165\n",
      "Epoch: 16 \tTraining Loss: 0.000165\n",
      "Epoch: 17 \tTraining Loss: 0.000165\n",
      "Epoch: 18 \tTraining Loss: 0.000165\n",
      "Epoch: 1 \tTraining Loss: 0.000220\n",
      "Epoch: 2 \tTraining Loss: 0.000220\n",
      "Epoch: 3 \tTraining Loss: 0.000220\n",
      "Epoch: 4 \tTraining Loss: 0.000220\n",
      "Epoch: 5 \tTraining Loss: 0.000220\n",
      "Epoch: 6 \tTraining Loss: 0.000220\n",
      "Epoch: 7 \tTraining Loss: 0.000220\n",
      "Epoch: 8 \tTraining Loss: 0.000220\n",
      "Epoch: 9 \tTraining Loss: 0.000220\n",
      "Epoch: 10 \tTraining Loss: 0.000220\n",
      "Epoch: 11 \tTraining Loss: 0.000220\n",
      "Epoch: 12 \tTraining Loss: 0.000220\n",
      "Epoch: 13 \tTraining Loss: 0.000220\n",
      "Epoch: 14 \tTraining Loss: 0.000220\n",
      "Epoch: 15 \tTraining Loss: 0.000220\n",
      "Epoch: 16 \tTraining Loss: 0.000220\n",
      "Epoch: 17 \tTraining Loss: 0.000220\n",
      "Epoch: 18 \tTraining Loss: 0.000220\n",
      "Epoch: 1 \tTraining Loss: 0.000247\n",
      "Epoch: 2 \tTraining Loss: 0.000247\n",
      "Epoch: 3 \tTraining Loss: 0.000247\n",
      "Epoch: 4 \tTraining Loss: 0.000247\n",
      "Epoch: 5 \tTraining Loss: 0.000247\n",
      "Epoch: 6 \tTraining Loss: 0.000247\n",
      "Epoch: 7 \tTraining Loss: 0.000247\n",
      "Epoch: 8 \tTraining Loss: 0.000247\n",
      "Epoch: 9 \tTraining Loss: 0.000247\n",
      "Epoch: 10 \tTraining Loss: 0.000247\n",
      "Epoch: 11 \tTraining Loss: 0.000247\n",
      "Epoch: 12 \tTraining Loss: 0.000247\n",
      "Epoch: 13 \tTraining Loss: 0.000247\n",
      "Epoch: 14 \tTraining Loss: 0.000247\n",
      "Epoch: 15 \tTraining Loss: 0.000247\n",
      "Epoch: 16 \tTraining Loss: 0.000247\n",
      "Epoch: 17 \tTraining Loss: 0.000247\n",
      "Epoch: 18 \tTraining Loss: 0.000247\n",
      "Epoch: 1 \tTraining Loss: 0.000069\n",
      "Epoch: 2 \tTraining Loss: 0.000069\n",
      "Epoch: 3 \tTraining Loss: 0.000069\n",
      "Epoch: 4 \tTraining Loss: 0.000069\n",
      "Epoch: 5 \tTraining Loss: 0.000069\n",
      "Epoch: 6 \tTraining Loss: 0.000069\n",
      "Epoch: 7 \tTraining Loss: 0.000069\n",
      "Epoch: 8 \tTraining Loss: 0.000069\n",
      "Epoch: 9 \tTraining Loss: 0.000069\n",
      "Epoch: 10 \tTraining Loss: 0.000069\n",
      "Epoch: 11 \tTraining Loss: 0.000069\n",
      "Epoch: 12 \tTraining Loss: 0.000069\n",
      "Epoch: 13 \tTraining Loss: 0.000069\n",
      "Epoch: 14 \tTraining Loss: 0.000069\n",
      "Epoch: 15 \tTraining Loss: 0.000069\n",
      "Epoch: 16 \tTraining Loss: 0.000069\n",
      "Epoch: 17 \tTraining Loss: 0.000069\n",
      "Epoch: 18 \tTraining Loss: 0.000069\n",
      "Epoch: 1 \tTraining Loss: 0.000142\n",
      "Epoch: 2 \tTraining Loss: 0.000142\n",
      "Epoch: 3 \tTraining Loss: 0.000142\n",
      "Epoch: 4 \tTraining Loss: 0.000142\n",
      "Epoch: 5 \tTraining Loss: 0.000142\n",
      "Epoch: 6 \tTraining Loss: 0.000142\n",
      "Epoch: 7 \tTraining Loss: 0.000142\n",
      "Epoch: 8 \tTraining Loss: 0.000142\n",
      "Epoch: 9 \tTraining Loss: 0.000142\n",
      "Epoch: 10 \tTraining Loss: 0.000142\n",
      "Epoch: 11 \tTraining Loss: 0.000142\n",
      "Epoch: 12 \tTraining Loss: 0.000142\n",
      "Epoch: 13 \tTraining Loss: 0.000142\n",
      "Epoch: 14 \tTraining Loss: 0.000142\n",
      "Epoch: 15 \tTraining Loss: 0.000142\n",
      "Epoch: 16 \tTraining Loss: 0.000142\n",
      "Epoch: 17 \tTraining Loss: 0.000142\n",
      "Epoch: 18 \tTraining Loss: 0.000142\n",
      "Epoch: 1 \tTraining Loss: 0.000161\n",
      "Epoch: 2 \tTraining Loss: 0.000161\n",
      "Epoch: 3 \tTraining Loss: 0.000161\n",
      "Epoch: 4 \tTraining Loss: 0.000161\n",
      "Epoch: 5 \tTraining Loss: 0.000161\n",
      "Epoch: 6 \tTraining Loss: 0.000161\n",
      "Epoch: 7 \tTraining Loss: 0.000161\n",
      "Epoch: 8 \tTraining Loss: 0.000161\n",
      "Epoch: 9 \tTraining Loss: 0.000161\n",
      "Epoch: 10 \tTraining Loss: 0.000161\n",
      "Epoch: 11 \tTraining Loss: 0.000161\n",
      "Epoch: 12 \tTraining Loss: 0.000161\n",
      "Epoch: 13 \tTraining Loss: 0.000161\n",
      "Epoch: 14 \tTraining Loss: 0.000161\n",
      "Epoch: 15 \tTraining Loss: 0.000161\n",
      "Epoch: 16 \tTraining Loss: 0.000161\n",
      "Epoch: 17 \tTraining Loss: 0.000161\n",
      "Epoch: 18 \tTraining Loss: 0.000161\n",
      "Epoch: 1 \tTraining Loss: 0.000202\n",
      "Epoch: 2 \tTraining Loss: 0.000202\n",
      "Epoch: 3 \tTraining Loss: 0.000202\n",
      "Epoch: 4 \tTraining Loss: 0.000202\n",
      "Epoch: 5 \tTraining Loss: 0.000202\n",
      "Epoch: 6 \tTraining Loss: 0.000202\n",
      "Epoch: 7 \tTraining Loss: 0.000202\n",
      "Epoch: 8 \tTraining Loss: 0.000202\n",
      "Epoch: 9 \tTraining Loss: 0.000202\n",
      "Epoch: 10 \tTraining Loss: 0.000202\n",
      "Epoch: 11 \tTraining Loss: 0.000202\n",
      "Epoch: 12 \tTraining Loss: 0.000202\n",
      "Epoch: 13 \tTraining Loss: 0.000202\n",
      "Epoch: 14 \tTraining Loss: 0.000202\n",
      "Epoch: 15 \tTraining Loss: 0.000202\n",
      "Epoch: 16 \tTraining Loss: 0.000202\n",
      "Epoch: 17 \tTraining Loss: 0.000202\n",
      "Epoch: 18 \tTraining Loss: 0.000202\n",
      "Epoch: 1 \tTraining Loss: 0.000207\n",
      "Epoch: 2 \tTraining Loss: 0.000207\n",
      "Epoch: 3 \tTraining Loss: 0.000207\n",
      "Epoch: 4 \tTraining Loss: 0.000207\n",
      "Epoch: 5 \tTraining Loss: 0.000207\n",
      "Epoch: 6 \tTraining Loss: 0.000207\n",
      "Epoch: 7 \tTraining Loss: 0.000207\n",
      "Epoch: 8 \tTraining Loss: 0.000207\n",
      "Epoch: 9 \tTraining Loss: 0.000207\n",
      "Epoch: 10 \tTraining Loss: 0.000207\n",
      "Epoch: 11 \tTraining Loss: 0.000207\n",
      "Epoch: 12 \tTraining Loss: 0.000207\n",
      "Epoch: 13 \tTraining Loss: 0.000207\n",
      "Epoch: 14 \tTraining Loss: 0.000207\n",
      "Epoch: 15 \tTraining Loss: 0.000207\n",
      "Epoch: 16 \tTraining Loss: 0.000207\n",
      "Epoch: 17 \tTraining Loss: 0.000207\n",
      "Epoch: 18 \tTraining Loss: 0.000207\n",
      "Epoch: 1 \tTraining Loss: 0.000109\n",
      "Epoch: 2 \tTraining Loss: 0.000109\n",
      "Epoch: 3 \tTraining Loss: 0.000109\n",
      "Epoch: 4 \tTraining Loss: 0.000109\n",
      "Epoch: 5 \tTraining Loss: 0.000109\n",
      "Epoch: 6 \tTraining Loss: 0.000109\n",
      "Epoch: 7 \tTraining Loss: 0.000109\n",
      "Epoch: 8 \tTraining Loss: 0.000109\n",
      "Epoch: 9 \tTraining Loss: 0.000109\n",
      "Epoch: 10 \tTraining Loss: 0.000109\n",
      "Epoch: 11 \tTraining Loss: 0.000109\n",
      "Epoch: 12 \tTraining Loss: 0.000109\n",
      "Epoch: 13 \tTraining Loss: 0.000109\n",
      "Epoch: 14 \tTraining Loss: 0.000109\n",
      "Epoch: 15 \tTraining Loss: 0.000109\n",
      "Epoch: 16 \tTraining Loss: 0.000109\n",
      "Epoch: 17 \tTraining Loss: 0.000109\n",
      "Epoch: 18 \tTraining Loss: 0.000109\n",
      "Epoch: 1 \tTraining Loss: 0.000166\n",
      "Epoch: 2 \tTraining Loss: 0.000166\n",
      "Epoch: 3 \tTraining Loss: 0.000166\n",
      "Epoch: 4 \tTraining Loss: 0.000166\n",
      "Epoch: 5 \tTraining Loss: 0.000166\n",
      "Epoch: 6 \tTraining Loss: 0.000166\n",
      "Epoch: 7 \tTraining Loss: 0.000166\n",
      "Epoch: 8 \tTraining Loss: 0.000166\n",
      "Epoch: 9 \tTraining Loss: 0.000166\n",
      "Epoch: 10 \tTraining Loss: 0.000166\n",
      "Epoch: 11 \tTraining Loss: 0.000166\n",
      "Epoch: 12 \tTraining Loss: 0.000166\n",
      "Epoch: 13 \tTraining Loss: 0.000166\n",
      "Epoch: 14 \tTraining Loss: 0.000166\n",
      "Epoch: 15 \tTraining Loss: 0.000166\n",
      "Epoch: 16 \tTraining Loss: 0.000166\n",
      "Epoch: 17 \tTraining Loss: 0.000166\n",
      "Epoch: 18 \tTraining Loss: 0.000166\n",
      "Epoch: 1 \tTraining Loss: 0.000212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \tTraining Loss: 0.000212\n",
      "Epoch: 3 \tTraining Loss: 0.000212\n",
      "Epoch: 4 \tTraining Loss: 0.000212\n",
      "Epoch: 5 \tTraining Loss: 0.000212\n",
      "Epoch: 6 \tTraining Loss: 0.000212\n",
      "Epoch: 7 \tTraining Loss: 0.000212\n",
      "Epoch: 8 \tTraining Loss: 0.000212\n",
      "Epoch: 9 \tTraining Loss: 0.000212\n",
      "Epoch: 10 \tTraining Loss: 0.000212\n",
      "Epoch: 11 \tTraining Loss: 0.000212\n",
      "Epoch: 12 \tTraining Loss: 0.000212\n",
      "Epoch: 13 \tTraining Loss: 0.000212\n",
      "Epoch: 14 \tTraining Loss: 0.000212\n",
      "Epoch: 15 \tTraining Loss: 0.000212\n",
      "Epoch: 16 \tTraining Loss: 0.000212\n",
      "Epoch: 17 \tTraining Loss: 0.000212\n",
      "Epoch: 18 \tTraining Loss: 0.000212\n",
      "Epoch: 1 \tTraining Loss: 0.000246\n",
      "Epoch: 2 \tTraining Loss: 0.000246\n",
      "Epoch: 3 \tTraining Loss: 0.000246\n",
      "Epoch: 4 \tTraining Loss: 0.000246\n",
      "Epoch: 5 \tTraining Loss: 0.000246\n",
      "Epoch: 6 \tTraining Loss: 0.000246\n",
      "Epoch: 7 \tTraining Loss: 0.000246\n",
      "Epoch: 8 \tTraining Loss: 0.000246\n",
      "Epoch: 9 \tTraining Loss: 0.000246\n",
      "Epoch: 10 \tTraining Loss: 0.000246\n",
      "Epoch: 11 \tTraining Loss: 0.000246\n",
      "Epoch: 12 \tTraining Loss: 0.000246\n",
      "Epoch: 13 \tTraining Loss: 0.000246\n",
      "Epoch: 14 \tTraining Loss: 0.000246\n",
      "Epoch: 15 \tTraining Loss: 0.000246\n",
      "Epoch: 16 \tTraining Loss: 0.000246\n",
      "Epoch: 17 \tTraining Loss: 0.000246\n",
      "Epoch: 18 \tTraining Loss: 0.000246\n",
      "Epoch: 1 \tTraining Loss: 0.000287\n",
      "Epoch: 2 \tTraining Loss: 0.000287\n",
      "Epoch: 3 \tTraining Loss: 0.000287\n",
      "Epoch: 4 \tTraining Loss: 0.000287\n",
      "Epoch: 5 \tTraining Loss: 0.000287\n",
      "Epoch: 6 \tTraining Loss: 0.000287\n",
      "Epoch: 7 \tTraining Loss: 0.000287\n",
      "Epoch: 8 \tTraining Loss: 0.000287\n",
      "Epoch: 9 \tTraining Loss: 0.000287\n",
      "Epoch: 10 \tTraining Loss: 0.000287\n",
      "Epoch: 11 \tTraining Loss: 0.000287\n",
      "Epoch: 12 \tTraining Loss: 0.000287\n",
      "Epoch: 13 \tTraining Loss: 0.000287\n",
      "Epoch: 14 \tTraining Loss: 0.000287\n",
      "Epoch: 15 \tTraining Loss: 0.000287\n",
      "Epoch: 16 \tTraining Loss: 0.000287\n",
      "Epoch: 17 \tTraining Loss: 0.000287\n",
      "Epoch: 18 \tTraining Loss: 0.000287\n",
      "Epoch: 1 \tTraining Loss: 0.000139\n",
      "Epoch: 2 \tTraining Loss: 0.000139\n",
      "Epoch: 3 \tTraining Loss: 0.000139\n",
      "Epoch: 4 \tTraining Loss: 0.000139\n",
      "Epoch: 5 \tTraining Loss: 0.000139\n",
      "Epoch: 6 \tTraining Loss: 0.000139\n",
      "Epoch: 7 \tTraining Loss: 0.000139\n",
      "Epoch: 8 \tTraining Loss: 0.000139\n",
      "Epoch: 9 \tTraining Loss: 0.000139\n",
      "Epoch: 10 \tTraining Loss: 0.000139\n",
      "Epoch: 11 \tTraining Loss: 0.000139\n",
      "Epoch: 12 \tTraining Loss: 0.000139\n",
      "Epoch: 13 \tTraining Loss: 0.000139\n",
      "Epoch: 14 \tTraining Loss: 0.000139\n",
      "Epoch: 15 \tTraining Loss: 0.000139\n",
      "Epoch: 16 \tTraining Loss: 0.000139\n",
      "Epoch: 17 \tTraining Loss: 0.000139\n",
      "Epoch: 18 \tTraining Loss: 0.000139\n",
      "Epoch: 1 \tTraining Loss: 0.000287\n",
      "Epoch: 2 \tTraining Loss: 0.000287\n",
      "Epoch: 3 \tTraining Loss: 0.000287\n",
      "Epoch: 4 \tTraining Loss: 0.000287\n",
      "Epoch: 5 \tTraining Loss: 0.000287\n",
      "Epoch: 6 \tTraining Loss: 0.000287\n",
      "Epoch: 7 \tTraining Loss: 0.000287\n",
      "Epoch: 8 \tTraining Loss: 0.000287\n",
      "Epoch: 9 \tTraining Loss: 0.000287\n",
      "Epoch: 10 \tTraining Loss: 0.000287\n",
      "Epoch: 11 \tTraining Loss: 0.000287\n",
      "Epoch: 12 \tTraining Loss: 0.000287\n",
      "Epoch: 13 \tTraining Loss: 0.000287\n",
      "Epoch: 14 \tTraining Loss: 0.000287\n",
      "Epoch: 15 \tTraining Loss: 0.000287\n",
      "Epoch: 16 \tTraining Loss: 0.000287\n",
      "Epoch: 17 \tTraining Loss: 0.000287\n",
      "Epoch: 18 \tTraining Loss: 0.000287\n",
      "Epoch: 1 \tTraining Loss: 0.000217\n",
      "Epoch: 2 \tTraining Loss: 0.000217\n",
      "Epoch: 3 \tTraining Loss: 0.000217\n",
      "Epoch: 4 \tTraining Loss: 0.000217\n",
      "Epoch: 5 \tTraining Loss: 0.000217\n",
      "Epoch: 6 \tTraining Loss: 0.000217\n",
      "Epoch: 7 \tTraining Loss: 0.000217\n",
      "Epoch: 8 \tTraining Loss: 0.000217\n",
      "Epoch: 9 \tTraining Loss: 0.000217\n",
      "Epoch: 10 \tTraining Loss: 0.000217\n",
      "Epoch: 11 \tTraining Loss: 0.000217\n",
      "Epoch: 12 \tTraining Loss: 0.000217\n",
      "Epoch: 13 \tTraining Loss: 0.000217\n",
      "Epoch: 14 \tTraining Loss: 0.000217\n",
      "Epoch: 15 \tTraining Loss: 0.000217\n",
      "Epoch: 16 \tTraining Loss: 0.000217\n",
      "Epoch: 17 \tTraining Loss: 0.000217\n",
      "Epoch: 18 \tTraining Loss: 0.000217\n",
      "Epoch: 1 \tTraining Loss: 0.000167\n",
      "Epoch: 2 \tTraining Loss: 0.000167\n",
      "Epoch: 3 \tTraining Loss: 0.000167\n",
      "Epoch: 4 \tTraining Loss: 0.000167\n",
      "Epoch: 5 \tTraining Loss: 0.000167\n",
      "Epoch: 6 \tTraining Loss: 0.000167\n",
      "Epoch: 7 \tTraining Loss: 0.000167\n",
      "Epoch: 8 \tTraining Loss: 0.000167\n",
      "Epoch: 9 \tTraining Loss: 0.000167\n",
      "Epoch: 10 \tTraining Loss: 0.000167\n",
      "Epoch: 11 \tTraining Loss: 0.000167\n",
      "Epoch: 12 \tTraining Loss: 0.000167\n",
      "Epoch: 13 \tTraining Loss: 0.000167\n",
      "Epoch: 14 \tTraining Loss: 0.000167\n",
      "Epoch: 15 \tTraining Loss: 0.000167\n",
      "Epoch: 16 \tTraining Loss: 0.000167\n",
      "Epoch: 17 \tTraining Loss: 0.000167\n",
      "Epoch: 18 \tTraining Loss: 0.000167\n",
      "Epoch: 1 \tTraining Loss: 0.000101\n",
      "Epoch: 2 \tTraining Loss: 0.000101\n",
      "Epoch: 3 \tTraining Loss: 0.000101\n",
      "Epoch: 4 \tTraining Loss: 0.000101\n",
      "Epoch: 5 \tTraining Loss: 0.000101\n",
      "Epoch: 6 \tTraining Loss: 0.000101\n",
      "Epoch: 7 \tTraining Loss: 0.000101\n",
      "Epoch: 8 \tTraining Loss: 0.000101\n",
      "Epoch: 9 \tTraining Loss: 0.000101\n",
      "Epoch: 10 \tTraining Loss: 0.000101\n",
      "Epoch: 11 \tTraining Loss: 0.000101\n",
      "Epoch: 12 \tTraining Loss: 0.000101\n",
      "Epoch: 13 \tTraining Loss: 0.000101\n",
      "Epoch: 14 \tTraining Loss: 0.000101\n",
      "Epoch: 15 \tTraining Loss: 0.000101\n",
      "Epoch: 16 \tTraining Loss: 0.000101\n",
      "Epoch: 17 \tTraining Loss: 0.000101\n",
      "Epoch: 18 \tTraining Loss: 0.000101\n",
      "Epoch: 1 \tTraining Loss: 0.000158\n",
      "Epoch: 2 \tTraining Loss: 0.000158\n",
      "Epoch: 3 \tTraining Loss: 0.000158\n",
      "Epoch: 4 \tTraining Loss: 0.000158\n",
      "Epoch: 5 \tTraining Loss: 0.000158\n",
      "Epoch: 6 \tTraining Loss: 0.000158\n",
      "Epoch: 7 \tTraining Loss: 0.000158\n",
      "Epoch: 8 \tTraining Loss: 0.000158\n",
      "Epoch: 9 \tTraining Loss: 0.000158\n",
      "Epoch: 10 \tTraining Loss: 0.000158\n",
      "Epoch: 11 \tTraining Loss: 0.000158\n",
      "Epoch: 12 \tTraining Loss: 0.000158\n",
      "Epoch: 13 \tTraining Loss: 0.000158\n",
      "Epoch: 14 \tTraining Loss: 0.000158\n",
      "Epoch: 15 \tTraining Loss: 0.000158\n",
      "Epoch: 16 \tTraining Loss: 0.000158\n",
      "Epoch: 17 \tTraining Loss: 0.000158\n",
      "Epoch: 18 \tTraining Loss: 0.000158\n",
      "Epoch: 1 \tTraining Loss: 0.000247\n",
      "Epoch: 2 \tTraining Loss: 0.000247\n",
      "Epoch: 3 \tTraining Loss: 0.000247\n",
      "Epoch: 4 \tTraining Loss: 0.000247\n",
      "Epoch: 5 \tTraining Loss: 0.000247\n",
      "Epoch: 6 \tTraining Loss: 0.000247\n",
      "Epoch: 7 \tTraining Loss: 0.000247\n",
      "Epoch: 8 \tTraining Loss: 0.000247\n",
      "Epoch: 9 \tTraining Loss: 0.000247\n",
      "Epoch: 10 \tTraining Loss: 0.000247\n",
      "Epoch: 11 \tTraining Loss: 0.000247\n",
      "Epoch: 12 \tTraining Loss: 0.000247\n",
      "Epoch: 13 \tTraining Loss: 0.000247\n",
      "Epoch: 14 \tTraining Loss: 0.000247\n",
      "Epoch: 15 \tTraining Loss: 0.000247\n",
      "Epoch: 16 \tTraining Loss: 0.000247\n",
      "Epoch: 17 \tTraining Loss: 0.000247\n",
      "Epoch: 18 \tTraining Loss: 0.000247\n",
      "Epoch: 1 \tTraining Loss: 0.000265\n",
      "Epoch: 2 \tTraining Loss: 0.000265\n",
      "Epoch: 3 \tTraining Loss: 0.000265\n",
      "Epoch: 4 \tTraining Loss: 0.000265\n",
      "Epoch: 5 \tTraining Loss: 0.000265\n",
      "Epoch: 6 \tTraining Loss: 0.000265\n",
      "Epoch: 7 \tTraining Loss: 0.000265\n",
      "Epoch: 8 \tTraining Loss: 0.000265\n",
      "Epoch: 9 \tTraining Loss: 0.000265\n",
      "Epoch: 10 \tTraining Loss: 0.000265\n",
      "Epoch: 11 \tTraining Loss: 0.000265\n",
      "Epoch: 12 \tTraining Loss: 0.000265\n",
      "Epoch: 13 \tTraining Loss: 0.000265\n",
      "Epoch: 14 \tTraining Loss: 0.000265\n",
      "Epoch: 15 \tTraining Loss: 0.000265\n",
      "Epoch: 16 \tTraining Loss: 0.000265\n",
      "Epoch: 17 \tTraining Loss: 0.000265\n",
      "Epoch: 18 \tTraining Loss: 0.000265\n",
      "Epoch: 1 \tTraining Loss: 0.000206\n",
      "Epoch: 2 \tTraining Loss: 0.000206\n",
      "Epoch: 3 \tTraining Loss: 0.000206\n",
      "Epoch: 4 \tTraining Loss: 0.000206\n",
      "Epoch: 5 \tTraining Loss: 0.000206\n",
      "Epoch: 6 \tTraining Loss: 0.000206\n",
      "Epoch: 7 \tTraining Loss: 0.000206\n",
      "Epoch: 8 \tTraining Loss: 0.000206\n",
      "Epoch: 9 \tTraining Loss: 0.000206\n",
      "Epoch: 10 \tTraining Loss: 0.000206\n",
      "Epoch: 11 \tTraining Loss: 0.000206\n",
      "Epoch: 12 \tTraining Loss: 0.000206\n",
      "Epoch: 13 \tTraining Loss: 0.000206\n",
      "Epoch: 14 \tTraining Loss: 0.000206\n",
      "Epoch: 15 \tTraining Loss: 0.000206\n",
      "Epoch: 16 \tTraining Loss: 0.000206\n",
      "Epoch: 17 \tTraining Loss: 0.000206\n",
      "Epoch: 18 \tTraining Loss: 0.000206\n",
      "Epoch: 1 \tTraining Loss: 0.000178\n",
      "Epoch: 2 \tTraining Loss: 0.000178\n",
      "Epoch: 3 \tTraining Loss: 0.000178\n",
      "Epoch: 4 \tTraining Loss: 0.000178\n",
      "Epoch: 5 \tTraining Loss: 0.000178\n",
      "Epoch: 6 \tTraining Loss: 0.000178\n",
      "Epoch: 7 \tTraining Loss: 0.000178\n",
      "Epoch: 8 \tTraining Loss: 0.000178\n",
      "Epoch: 9 \tTraining Loss: 0.000178\n",
      "Epoch: 10 \tTraining Loss: 0.000178\n",
      "Epoch: 11 \tTraining Loss: 0.000178\n",
      "Epoch: 12 \tTraining Loss: 0.000178\n",
      "Epoch: 13 \tTraining Loss: 0.000178\n",
      "Epoch: 14 \tTraining Loss: 0.000178\n",
      "Epoch: 15 \tTraining Loss: 0.000178\n",
      "Epoch: 16 \tTraining Loss: 0.000178\n",
      "Epoch: 17 \tTraining Loss: 0.000178\n",
      "Epoch: 18 \tTraining Loss: 0.000178\n",
      "Epoch: 1 \tTraining Loss: 0.000240\n",
      "Epoch: 2 \tTraining Loss: 0.000240\n",
      "Epoch: 3 \tTraining Loss: 0.000240\n",
      "Epoch: 4 \tTraining Loss: 0.000240\n",
      "Epoch: 5 \tTraining Loss: 0.000240\n",
      "Epoch: 6 \tTraining Loss: 0.000240\n",
      "Epoch: 7 \tTraining Loss: 0.000240\n",
      "Epoch: 8 \tTraining Loss: 0.000240\n",
      "Epoch: 9 \tTraining Loss: 0.000240\n",
      "Epoch: 10 \tTraining Loss: 0.000240\n",
      "Epoch: 11 \tTraining Loss: 0.000240\n",
      "Epoch: 12 \tTraining Loss: 0.000240\n",
      "Epoch: 13 \tTraining Loss: 0.000240\n",
      "Epoch: 14 \tTraining Loss: 0.000240\n",
      "Epoch: 15 \tTraining Loss: 0.000240\n",
      "Epoch: 16 \tTraining Loss: 0.000240\n",
      "Epoch: 17 \tTraining Loss: 0.000240\n",
      "Epoch: 18 \tTraining Loss: 0.000240\n",
      "Epoch: 1 \tTraining Loss: 0.000151\n",
      "Epoch: 2 \tTraining Loss: 0.000151\n",
      "Epoch: 3 \tTraining Loss: 0.000151\n",
      "Epoch: 4 \tTraining Loss: 0.000151\n",
      "Epoch: 5 \tTraining Loss: 0.000151\n",
      "Epoch: 6 \tTraining Loss: 0.000151\n",
      "Epoch: 7 \tTraining Loss: 0.000151\n",
      "Epoch: 8 \tTraining Loss: 0.000151\n",
      "Epoch: 9 \tTraining Loss: 0.000151\n",
      "Epoch: 10 \tTraining Loss: 0.000151\n",
      "Epoch: 11 \tTraining Loss: 0.000151\n",
      "Epoch: 12 \tTraining Loss: 0.000151\n",
      "Epoch: 13 \tTraining Loss: 0.000151\n",
      "Epoch: 14 \tTraining Loss: 0.000151\n",
      "Epoch: 15 \tTraining Loss: 0.000151\n",
      "Epoch: 16 \tTraining Loss: 0.000151\n",
      "Epoch: 17 \tTraining Loss: 0.000151\n",
      "Epoch: 18 \tTraining Loss: 0.000151\n",
      "Epoch: 1 \tTraining Loss: 0.000066\n",
      "Epoch: 2 \tTraining Loss: 0.000066\n",
      "Epoch: 3 \tTraining Loss: 0.000066\n",
      "Epoch: 4 \tTraining Loss: 0.000066\n",
      "Epoch: 5 \tTraining Loss: 0.000066\n",
      "Epoch: 6 \tTraining Loss: 0.000066\n",
      "Epoch: 7 \tTraining Loss: 0.000066\n",
      "Epoch: 8 \tTraining Loss: 0.000066\n",
      "Epoch: 9 \tTraining Loss: 0.000066\n",
      "Epoch: 10 \tTraining Loss: 0.000066\n",
      "Epoch: 11 \tTraining Loss: 0.000066\n",
      "Epoch: 12 \tTraining Loss: 0.000066\n",
      "Epoch: 13 \tTraining Loss: 0.000066\n",
      "Epoch: 14 \tTraining Loss: 0.000066\n",
      "Epoch: 15 \tTraining Loss: 0.000066\n",
      "Epoch: 16 \tTraining Loss: 0.000066\n",
      "Epoch: 17 \tTraining Loss: 0.000066\n",
      "Epoch: 18 \tTraining Loss: 0.000066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000073\n",
      "Epoch: 2 \tTraining Loss: 0.000073\n",
      "Epoch: 3 \tTraining Loss: 0.000073\n",
      "Epoch: 4 \tTraining Loss: 0.000073\n",
      "Epoch: 5 \tTraining Loss: 0.000073\n",
      "Epoch: 6 \tTraining Loss: 0.000073\n",
      "Epoch: 7 \tTraining Loss: 0.000073\n",
      "Epoch: 8 \tTraining Loss: 0.000073\n",
      "Epoch: 9 \tTraining Loss: 0.000073\n",
      "Epoch: 10 \tTraining Loss: 0.000073\n",
      "Epoch: 11 \tTraining Loss: 0.000073\n",
      "Epoch: 12 \tTraining Loss: 0.000073\n",
      "Epoch: 13 \tTraining Loss: 0.000073\n",
      "Epoch: 14 \tTraining Loss: 0.000073\n",
      "Epoch: 15 \tTraining Loss: 0.000073\n",
      "Epoch: 16 \tTraining Loss: 0.000073\n",
      "Epoch: 17 \tTraining Loss: 0.000073\n",
      "Epoch: 18 \tTraining Loss: 0.000073\n",
      "Epoch: 1 \tTraining Loss: 0.000264\n",
      "Epoch: 2 \tTraining Loss: 0.000264\n",
      "Epoch: 3 \tTraining Loss: 0.000264\n",
      "Epoch: 4 \tTraining Loss: 0.000264\n",
      "Epoch: 5 \tTraining Loss: 0.000264\n",
      "Epoch: 6 \tTraining Loss: 0.000264\n",
      "Epoch: 7 \tTraining Loss: 0.000264\n",
      "Epoch: 8 \tTraining Loss: 0.000264\n",
      "Epoch: 9 \tTraining Loss: 0.000264\n",
      "Epoch: 10 \tTraining Loss: 0.000264\n",
      "Epoch: 11 \tTraining Loss: 0.000264\n",
      "Epoch: 12 \tTraining Loss: 0.000264\n",
      "Epoch: 13 \tTraining Loss: 0.000264\n",
      "Epoch: 14 \tTraining Loss: 0.000264\n",
      "Epoch: 15 \tTraining Loss: 0.000264\n",
      "Epoch: 16 \tTraining Loss: 0.000264\n",
      "Epoch: 17 \tTraining Loss: 0.000264\n",
      "Epoch: 18 \tTraining Loss: 0.000264\n",
      "Epoch: 1 \tTraining Loss: 0.000196\n",
      "Epoch: 2 \tTraining Loss: 0.000196\n",
      "Epoch: 3 \tTraining Loss: 0.000196\n",
      "Epoch: 4 \tTraining Loss: 0.000196\n",
      "Epoch: 5 \tTraining Loss: 0.000196\n",
      "Epoch: 6 \tTraining Loss: 0.000196\n",
      "Epoch: 7 \tTraining Loss: 0.000196\n",
      "Epoch: 8 \tTraining Loss: 0.000196\n",
      "Epoch: 9 \tTraining Loss: 0.000196\n",
      "Epoch: 10 \tTraining Loss: 0.000196\n",
      "Epoch: 11 \tTraining Loss: 0.000196\n",
      "Epoch: 12 \tTraining Loss: 0.000196\n",
      "Epoch: 13 \tTraining Loss: 0.000196\n",
      "Epoch: 14 \tTraining Loss: 0.000196\n",
      "Epoch: 15 \tTraining Loss: 0.000196\n",
      "Epoch: 16 \tTraining Loss: 0.000196\n",
      "Epoch: 17 \tTraining Loss: 0.000196\n",
      "Epoch: 18 \tTraining Loss: 0.000196\n",
      "Epoch: 1 \tTraining Loss: 0.000242\n",
      "Epoch: 2 \tTraining Loss: 0.000242\n",
      "Epoch: 3 \tTraining Loss: 0.000242\n",
      "Epoch: 4 \tTraining Loss: 0.000242\n",
      "Epoch: 5 \tTraining Loss: 0.000242\n",
      "Epoch: 6 \tTraining Loss: 0.000242\n",
      "Epoch: 7 \tTraining Loss: 0.000242\n",
      "Epoch: 8 \tTraining Loss: 0.000242\n",
      "Epoch: 9 \tTraining Loss: 0.000242\n",
      "Epoch: 10 \tTraining Loss: 0.000242\n",
      "Epoch: 11 \tTraining Loss: 0.000242\n",
      "Epoch: 12 \tTraining Loss: 0.000242\n",
      "Epoch: 13 \tTraining Loss: 0.000242\n",
      "Epoch: 14 \tTraining Loss: 0.000242\n",
      "Epoch: 15 \tTraining Loss: 0.000242\n",
      "Epoch: 16 \tTraining Loss: 0.000242\n",
      "Epoch: 17 \tTraining Loss: 0.000242\n",
      "Epoch: 18 \tTraining Loss: 0.000242\n",
      "Epoch: 1 \tTraining Loss: 0.000203\n",
      "Epoch: 2 \tTraining Loss: 0.000203\n",
      "Epoch: 3 \tTraining Loss: 0.000203\n",
      "Epoch: 4 \tTraining Loss: 0.000203\n",
      "Epoch: 5 \tTraining Loss: 0.000203\n",
      "Epoch: 6 \tTraining Loss: 0.000203\n",
      "Epoch: 7 \tTraining Loss: 0.000203\n",
      "Epoch: 8 \tTraining Loss: 0.000203\n",
      "Epoch: 9 \tTraining Loss: 0.000203\n",
      "Epoch: 10 \tTraining Loss: 0.000203\n",
      "Epoch: 11 \tTraining Loss: 0.000203\n",
      "Epoch: 12 \tTraining Loss: 0.000203\n",
      "Epoch: 13 \tTraining Loss: 0.000203\n",
      "Epoch: 14 \tTraining Loss: 0.000203\n",
      "Epoch: 15 \tTraining Loss: 0.000203\n",
      "Epoch: 16 \tTraining Loss: 0.000203\n",
      "Epoch: 17 \tTraining Loss: 0.000203\n",
      "Epoch: 18 \tTraining Loss: 0.000203\n",
      "Epoch: 1 \tTraining Loss: 0.000114\n",
      "Epoch: 2 \tTraining Loss: 0.000114\n",
      "Epoch: 3 \tTraining Loss: 0.000114\n",
      "Epoch: 4 \tTraining Loss: 0.000114\n",
      "Epoch: 5 \tTraining Loss: 0.000114\n",
      "Epoch: 6 \tTraining Loss: 0.000114\n",
      "Epoch: 7 \tTraining Loss: 0.000114\n",
      "Epoch: 8 \tTraining Loss: 0.000114\n",
      "Epoch: 9 \tTraining Loss: 0.000114\n",
      "Epoch: 10 \tTraining Loss: 0.000114\n",
      "Epoch: 11 \tTraining Loss: 0.000114\n",
      "Epoch: 12 \tTraining Loss: 0.000114\n",
      "Epoch: 13 \tTraining Loss: 0.000114\n",
      "Epoch: 14 \tTraining Loss: 0.000114\n",
      "Epoch: 15 \tTraining Loss: 0.000114\n",
      "Epoch: 16 \tTraining Loss: 0.000114\n",
      "Epoch: 17 \tTraining Loss: 0.000114\n",
      "Epoch: 18 \tTraining Loss: 0.000114\n",
      "Epoch: 1 \tTraining Loss: 0.000180\n",
      "Epoch: 2 \tTraining Loss: 0.000180\n",
      "Epoch: 3 \tTraining Loss: 0.000180\n",
      "Epoch: 4 \tTraining Loss: 0.000180\n",
      "Epoch: 5 \tTraining Loss: 0.000180\n",
      "Epoch: 6 \tTraining Loss: 0.000180\n",
      "Epoch: 7 \tTraining Loss: 0.000180\n",
      "Epoch: 8 \tTraining Loss: 0.000180\n",
      "Epoch: 9 \tTraining Loss: 0.000180\n",
      "Epoch: 10 \tTraining Loss: 0.000180\n",
      "Epoch: 11 \tTraining Loss: 0.000180\n",
      "Epoch: 12 \tTraining Loss: 0.000180\n",
      "Epoch: 13 \tTraining Loss: 0.000180\n",
      "Epoch: 14 \tTraining Loss: 0.000180\n",
      "Epoch: 15 \tTraining Loss: 0.000180\n",
      "Epoch: 16 \tTraining Loss: 0.000180\n",
      "Epoch: 17 \tTraining Loss: 0.000180\n",
      "Epoch: 18 \tTraining Loss: 0.000180\n",
      "Epoch: 1 \tTraining Loss: 0.000130\n",
      "Epoch: 2 \tTraining Loss: 0.000130\n",
      "Epoch: 3 \tTraining Loss: 0.000130\n",
      "Epoch: 4 \tTraining Loss: 0.000130\n",
      "Epoch: 5 \tTraining Loss: 0.000130\n",
      "Epoch: 6 \tTraining Loss: 0.000130\n",
      "Epoch: 7 \tTraining Loss: 0.000130\n",
      "Epoch: 8 \tTraining Loss: 0.000130\n",
      "Epoch: 9 \tTraining Loss: 0.000130\n",
      "Epoch: 10 \tTraining Loss: 0.000130\n",
      "Epoch: 11 \tTraining Loss: 0.000130\n",
      "Epoch: 12 \tTraining Loss: 0.000130\n",
      "Epoch: 13 \tTraining Loss: 0.000130\n",
      "Epoch: 14 \tTraining Loss: 0.000130\n",
      "Epoch: 15 \tTraining Loss: 0.000130\n",
      "Epoch: 16 \tTraining Loss: 0.000130\n",
      "Epoch: 17 \tTraining Loss: 0.000130\n",
      "Epoch: 18 \tTraining Loss: 0.000130\n",
      "Epoch: 1 \tTraining Loss: 0.000223\n",
      "Epoch: 2 \tTraining Loss: 0.000223\n",
      "Epoch: 3 \tTraining Loss: 0.000223\n",
      "Epoch: 4 \tTraining Loss: 0.000223\n",
      "Epoch: 5 \tTraining Loss: 0.000223\n",
      "Epoch: 6 \tTraining Loss: 0.000223\n",
      "Epoch: 7 \tTraining Loss: 0.000223\n",
      "Epoch: 8 \tTraining Loss: 0.000223\n",
      "Epoch: 9 \tTraining Loss: 0.000223\n",
      "Epoch: 10 \tTraining Loss: 0.000223\n",
      "Epoch: 11 \tTraining Loss: 0.000223\n",
      "Epoch: 12 \tTraining Loss: 0.000223\n",
      "Epoch: 13 \tTraining Loss: 0.000223\n",
      "Epoch: 14 \tTraining Loss: 0.000223\n",
      "Epoch: 15 \tTraining Loss: 0.000223\n",
      "Epoch: 16 \tTraining Loss: 0.000223\n",
      "Epoch: 17 \tTraining Loss: 0.000223\n",
      "Epoch: 18 \tTraining Loss: 0.000223\n",
      "Epoch: 1 \tTraining Loss: 0.000156\n",
      "Epoch: 2 \tTraining Loss: 0.000156\n",
      "Epoch: 3 \tTraining Loss: 0.000156\n",
      "Epoch: 4 \tTraining Loss: 0.000156\n",
      "Epoch: 5 \tTraining Loss: 0.000156\n",
      "Epoch: 6 \tTraining Loss: 0.000156\n",
      "Epoch: 7 \tTraining Loss: 0.000156\n",
      "Epoch: 8 \tTraining Loss: 0.000156\n",
      "Epoch: 9 \tTraining Loss: 0.000156\n",
      "Epoch: 10 \tTraining Loss: 0.000156\n",
      "Epoch: 11 \tTraining Loss: 0.000156\n",
      "Epoch: 12 \tTraining Loss: 0.000156\n",
      "Epoch: 13 \tTraining Loss: 0.000156\n",
      "Epoch: 14 \tTraining Loss: 0.000156\n",
      "Epoch: 15 \tTraining Loss: 0.000156\n",
      "Epoch: 16 \tTraining Loss: 0.000156\n",
      "Epoch: 17 \tTraining Loss: 0.000156\n",
      "Epoch: 18 \tTraining Loss: 0.000156\n",
      "Epoch: 1 \tTraining Loss: 0.000304\n",
      "Epoch: 2 \tTraining Loss: 0.000304\n",
      "Epoch: 3 \tTraining Loss: 0.000304\n",
      "Epoch: 4 \tTraining Loss: 0.000304\n",
      "Epoch: 5 \tTraining Loss: 0.000304\n",
      "Epoch: 6 \tTraining Loss: 0.000304\n",
      "Epoch: 7 \tTraining Loss: 0.000304\n",
      "Epoch: 8 \tTraining Loss: 0.000304\n",
      "Epoch: 9 \tTraining Loss: 0.000304\n",
      "Epoch: 10 \tTraining Loss: 0.000304\n",
      "Epoch: 11 \tTraining Loss: 0.000304\n",
      "Epoch: 12 \tTraining Loss: 0.000304\n",
      "Epoch: 13 \tTraining Loss: 0.000304\n",
      "Epoch: 14 \tTraining Loss: 0.000304\n",
      "Epoch: 15 \tTraining Loss: 0.000304\n",
      "Epoch: 16 \tTraining Loss: 0.000304\n",
      "Epoch: 17 \tTraining Loss: 0.000304\n",
      "Epoch: 18 \tTraining Loss: 0.000304\n",
      "Epoch: 1 \tTraining Loss: 0.000113\n",
      "Epoch: 2 \tTraining Loss: 0.000113\n",
      "Epoch: 3 \tTraining Loss: 0.000113\n",
      "Epoch: 4 \tTraining Loss: 0.000113\n",
      "Epoch: 5 \tTraining Loss: 0.000113\n",
      "Epoch: 6 \tTraining Loss: 0.000113\n",
      "Epoch: 7 \tTraining Loss: 0.000113\n",
      "Epoch: 8 \tTraining Loss: 0.000113\n",
      "Epoch: 9 \tTraining Loss: 0.000113\n",
      "Epoch: 10 \tTraining Loss: 0.000113\n",
      "Epoch: 11 \tTraining Loss: 0.000113\n",
      "Epoch: 12 \tTraining Loss: 0.000113\n",
      "Epoch: 13 \tTraining Loss: 0.000113\n",
      "Epoch: 14 \tTraining Loss: 0.000113\n",
      "Epoch: 15 \tTraining Loss: 0.000113\n",
      "Epoch: 16 \tTraining Loss: 0.000113\n",
      "Epoch: 17 \tTraining Loss: 0.000113\n",
      "Epoch: 18 \tTraining Loss: 0.000113\n",
      "Epoch: 1 \tTraining Loss: 0.000196\n",
      "Epoch: 2 \tTraining Loss: 0.000196\n",
      "Epoch: 3 \tTraining Loss: 0.000196\n",
      "Epoch: 4 \tTraining Loss: 0.000196\n",
      "Epoch: 5 \tTraining Loss: 0.000196\n",
      "Epoch: 6 \tTraining Loss: 0.000196\n",
      "Epoch: 7 \tTraining Loss: 0.000196\n",
      "Epoch: 8 \tTraining Loss: 0.000196\n",
      "Epoch: 9 \tTraining Loss: 0.000196\n",
      "Epoch: 10 \tTraining Loss: 0.000196\n",
      "Epoch: 11 \tTraining Loss: 0.000196\n",
      "Epoch: 12 \tTraining Loss: 0.000196\n",
      "Epoch: 13 \tTraining Loss: 0.000196\n",
      "Epoch: 14 \tTraining Loss: 0.000196\n",
      "Epoch: 15 \tTraining Loss: 0.000196\n",
      "Epoch: 16 \tTraining Loss: 0.000196\n",
      "Epoch: 17 \tTraining Loss: 0.000196\n",
      "Epoch: 18 \tTraining Loss: 0.000196\n",
      "Epoch: 1 \tTraining Loss: 0.000187\n",
      "Epoch: 2 \tTraining Loss: 0.000187\n",
      "Epoch: 3 \tTraining Loss: 0.000187\n",
      "Epoch: 4 \tTraining Loss: 0.000187\n",
      "Epoch: 5 \tTraining Loss: 0.000187\n",
      "Epoch: 6 \tTraining Loss: 0.000187\n",
      "Epoch: 7 \tTraining Loss: 0.000187\n",
      "Epoch: 8 \tTraining Loss: 0.000187\n",
      "Epoch: 9 \tTraining Loss: 0.000187\n",
      "Epoch: 10 \tTraining Loss: 0.000187\n",
      "Epoch: 11 \tTraining Loss: 0.000187\n",
      "Epoch: 12 \tTraining Loss: 0.000187\n",
      "Epoch: 13 \tTraining Loss: 0.000187\n",
      "Epoch: 14 \tTraining Loss: 0.000187\n",
      "Epoch: 15 \tTraining Loss: 0.000187\n",
      "Epoch: 16 \tTraining Loss: 0.000187\n",
      "Epoch: 17 \tTraining Loss: 0.000187\n",
      "Epoch: 18 \tTraining Loss: 0.000187\n",
      "Epoch: 1 \tTraining Loss: 0.000250\n",
      "Epoch: 2 \tTraining Loss: 0.000250\n",
      "Epoch: 3 \tTraining Loss: 0.000250\n",
      "Epoch: 4 \tTraining Loss: 0.000250\n",
      "Epoch: 5 \tTraining Loss: 0.000250\n",
      "Epoch: 6 \tTraining Loss: 0.000250\n",
      "Epoch: 7 \tTraining Loss: 0.000250\n",
      "Epoch: 8 \tTraining Loss: 0.000250\n",
      "Epoch: 9 \tTraining Loss: 0.000250\n",
      "Epoch: 10 \tTraining Loss: 0.000250\n",
      "Epoch: 11 \tTraining Loss: 0.000250\n",
      "Epoch: 12 \tTraining Loss: 0.000250\n",
      "Epoch: 13 \tTraining Loss: 0.000250\n",
      "Epoch: 14 \tTraining Loss: 0.000250\n",
      "Epoch: 15 \tTraining Loss: 0.000250\n",
      "Epoch: 16 \tTraining Loss: 0.000250\n",
      "Epoch: 17 \tTraining Loss: 0.000250\n",
      "Epoch: 18 \tTraining Loss: 0.000250\n",
      "Epoch: 1 \tTraining Loss: 0.000225\n",
      "Epoch: 2 \tTraining Loss: 0.000225\n",
      "Epoch: 3 \tTraining Loss: 0.000225\n",
      "Epoch: 4 \tTraining Loss: 0.000225\n",
      "Epoch: 5 \tTraining Loss: 0.000225\n",
      "Epoch: 6 \tTraining Loss: 0.000225\n",
      "Epoch: 7 \tTraining Loss: 0.000225\n",
      "Epoch: 8 \tTraining Loss: 0.000225\n",
      "Epoch: 9 \tTraining Loss: 0.000225\n",
      "Epoch: 10 \tTraining Loss: 0.000225\n",
      "Epoch: 11 \tTraining Loss: 0.000225\n",
      "Epoch: 12 \tTraining Loss: 0.000225\n",
      "Epoch: 13 \tTraining Loss: 0.000225\n",
      "Epoch: 14 \tTraining Loss: 0.000225\n",
      "Epoch: 15 \tTraining Loss: 0.000225\n",
      "Epoch: 16 \tTraining Loss: 0.000225\n",
      "Epoch: 17 \tTraining Loss: 0.000225\n",
      "Epoch: 18 \tTraining Loss: 0.000225\n",
      "Epoch: 1 \tTraining Loss: 0.000153\n",
      "Epoch: 2 \tTraining Loss: 0.000153\n",
      "Epoch: 3 \tTraining Loss: 0.000153\n",
      "Epoch: 4 \tTraining Loss: 0.000153\n",
      "Epoch: 5 \tTraining Loss: 0.000153\n",
      "Epoch: 6 \tTraining Loss: 0.000153\n",
      "Epoch: 7 \tTraining Loss: 0.000153\n",
      "Epoch: 8 \tTraining Loss: 0.000153\n",
      "Epoch: 9 \tTraining Loss: 0.000153\n",
      "Epoch: 10 \tTraining Loss: 0.000153\n",
      "Epoch: 11 \tTraining Loss: 0.000153\n",
      "Epoch: 12 \tTraining Loss: 0.000153\n",
      "Epoch: 13 \tTraining Loss: 0.000153\n",
      "Epoch: 14 \tTraining Loss: 0.000153\n",
      "Epoch: 15 \tTraining Loss: 0.000153\n",
      "Epoch: 16 \tTraining Loss: 0.000153\n",
      "Epoch: 17 \tTraining Loss: 0.000153\n",
      "Epoch: 18 \tTraining Loss: 0.000153\n",
      "Epoch: 1 \tTraining Loss: 0.000225\n",
      "Epoch: 2 \tTraining Loss: 0.000225\n",
      "Epoch: 3 \tTraining Loss: 0.000225\n",
      "Epoch: 4 \tTraining Loss: 0.000225\n",
      "Epoch: 5 \tTraining Loss: 0.000225\n",
      "Epoch: 6 \tTraining Loss: 0.000225\n",
      "Epoch: 7 \tTraining Loss: 0.000225\n",
      "Epoch: 8 \tTraining Loss: 0.000225\n",
      "Epoch: 9 \tTraining Loss: 0.000225\n",
      "Epoch: 10 \tTraining Loss: 0.000225\n",
      "Epoch: 11 \tTraining Loss: 0.000225\n",
      "Epoch: 12 \tTraining Loss: 0.000225\n",
      "Epoch: 13 \tTraining Loss: 0.000225\n",
      "Epoch: 14 \tTraining Loss: 0.000225\n",
      "Epoch: 15 \tTraining Loss: 0.000225\n",
      "Epoch: 16 \tTraining Loss: 0.000225\n",
      "Epoch: 17 \tTraining Loss: 0.000225\n",
      "Epoch: 18 \tTraining Loss: 0.000225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000220\n",
      "Epoch: 2 \tTraining Loss: 0.000220\n",
      "Epoch: 3 \tTraining Loss: 0.000220\n",
      "Epoch: 4 \tTraining Loss: 0.000220\n",
      "Epoch: 5 \tTraining Loss: 0.000220\n",
      "Epoch: 6 \tTraining Loss: 0.000220\n",
      "Epoch: 7 \tTraining Loss: 0.000220\n",
      "Epoch: 8 \tTraining Loss: 0.000220\n",
      "Epoch: 9 \tTraining Loss: 0.000220\n",
      "Epoch: 10 \tTraining Loss: 0.000220\n",
      "Epoch: 11 \tTraining Loss: 0.000220\n",
      "Epoch: 12 \tTraining Loss: 0.000220\n",
      "Epoch: 13 \tTraining Loss: 0.000220\n",
      "Epoch: 14 \tTraining Loss: 0.000220\n",
      "Epoch: 15 \tTraining Loss: 0.000220\n",
      "Epoch: 16 \tTraining Loss: 0.000220\n",
      "Epoch: 17 \tTraining Loss: 0.000220\n",
      "Epoch: 18 \tTraining Loss: 0.000220\n",
      "Epoch: 1 \tTraining Loss: 0.000200\n",
      "Epoch: 2 \tTraining Loss: 0.000200\n",
      "Epoch: 3 \tTraining Loss: 0.000200\n",
      "Epoch: 4 \tTraining Loss: 0.000200\n",
      "Epoch: 5 \tTraining Loss: 0.000200\n",
      "Epoch: 6 \tTraining Loss: 0.000200\n",
      "Epoch: 7 \tTraining Loss: 0.000200\n",
      "Epoch: 8 \tTraining Loss: 0.000200\n",
      "Epoch: 9 \tTraining Loss: 0.000200\n",
      "Epoch: 10 \tTraining Loss: 0.000200\n",
      "Epoch: 11 \tTraining Loss: 0.000200\n",
      "Epoch: 12 \tTraining Loss: 0.000200\n",
      "Epoch: 13 \tTraining Loss: 0.000200\n",
      "Epoch: 14 \tTraining Loss: 0.000200\n",
      "Epoch: 15 \tTraining Loss: 0.000200\n",
      "Epoch: 16 \tTraining Loss: 0.000200\n",
      "Epoch: 17 \tTraining Loss: 0.000200\n",
      "Epoch: 18 \tTraining Loss: 0.000200\n",
      "Epoch: 1 \tTraining Loss: 0.000273\n",
      "Epoch: 2 \tTraining Loss: 0.000273\n",
      "Epoch: 3 \tTraining Loss: 0.000273\n",
      "Epoch: 4 \tTraining Loss: 0.000273\n",
      "Epoch: 5 \tTraining Loss: 0.000273\n",
      "Epoch: 6 \tTraining Loss: 0.000273\n",
      "Epoch: 7 \tTraining Loss: 0.000273\n",
      "Epoch: 8 \tTraining Loss: 0.000273\n",
      "Epoch: 9 \tTraining Loss: 0.000273\n",
      "Epoch: 10 \tTraining Loss: 0.000273\n",
      "Epoch: 11 \tTraining Loss: 0.000273\n",
      "Epoch: 12 \tTraining Loss: 0.000273\n",
      "Epoch: 13 \tTraining Loss: 0.000273\n",
      "Epoch: 14 \tTraining Loss: 0.000273\n",
      "Epoch: 15 \tTraining Loss: 0.000273\n",
      "Epoch: 16 \tTraining Loss: 0.000273\n",
      "Epoch: 17 \tTraining Loss: 0.000273\n",
      "Epoch: 18 \tTraining Loss: 0.000273\n",
      "Epoch: 1 \tTraining Loss: 0.000205\n",
      "Epoch: 2 \tTraining Loss: 0.000205\n",
      "Epoch: 3 \tTraining Loss: 0.000205\n",
      "Epoch: 4 \tTraining Loss: 0.000205\n",
      "Epoch: 5 \tTraining Loss: 0.000205\n",
      "Epoch: 6 \tTraining Loss: 0.000205\n",
      "Epoch: 7 \tTraining Loss: 0.000205\n",
      "Epoch: 8 \tTraining Loss: 0.000205\n",
      "Epoch: 9 \tTraining Loss: 0.000205\n",
      "Epoch: 10 \tTraining Loss: 0.000205\n",
      "Epoch: 11 \tTraining Loss: 0.000205\n",
      "Epoch: 12 \tTraining Loss: 0.000205\n",
      "Epoch: 13 \tTraining Loss: 0.000205\n",
      "Epoch: 14 \tTraining Loss: 0.000205\n",
      "Epoch: 15 \tTraining Loss: 0.000205\n",
      "Epoch: 16 \tTraining Loss: 0.000205\n",
      "Epoch: 17 \tTraining Loss: 0.000205\n",
      "Epoch: 18 \tTraining Loss: 0.000205\n",
      "Epoch: 1 \tTraining Loss: 0.000211\n",
      "Epoch: 2 \tTraining Loss: 0.000211\n",
      "Epoch: 3 \tTraining Loss: 0.000211\n",
      "Epoch: 4 \tTraining Loss: 0.000211\n",
      "Epoch: 5 \tTraining Loss: 0.000211\n",
      "Epoch: 6 \tTraining Loss: 0.000211\n",
      "Epoch: 7 \tTraining Loss: 0.000211\n",
      "Epoch: 8 \tTraining Loss: 0.000211\n",
      "Epoch: 9 \tTraining Loss: 0.000211\n",
      "Epoch: 10 \tTraining Loss: 0.000211\n",
      "Epoch: 11 \tTraining Loss: 0.000211\n",
      "Epoch: 12 \tTraining Loss: 0.000211\n",
      "Epoch: 13 \tTraining Loss: 0.000211\n",
      "Epoch: 14 \tTraining Loss: 0.000211\n",
      "Epoch: 15 \tTraining Loss: 0.000211\n",
      "Epoch: 16 \tTraining Loss: 0.000211\n",
      "Epoch: 17 \tTraining Loss: 0.000211\n",
      "Epoch: 18 \tTraining Loss: 0.000211\n",
      "Epoch: 1 \tTraining Loss: 0.000090\n",
      "Epoch: 2 \tTraining Loss: 0.000090\n",
      "Epoch: 3 \tTraining Loss: 0.000090\n",
      "Epoch: 4 \tTraining Loss: 0.000090\n",
      "Epoch: 5 \tTraining Loss: 0.000090\n",
      "Epoch: 6 \tTraining Loss: 0.000090\n",
      "Epoch: 7 \tTraining Loss: 0.000090\n",
      "Epoch: 8 \tTraining Loss: 0.000090\n",
      "Epoch: 9 \tTraining Loss: 0.000090\n",
      "Epoch: 10 \tTraining Loss: 0.000090\n",
      "Epoch: 11 \tTraining Loss: 0.000090\n",
      "Epoch: 12 \tTraining Loss: 0.000090\n",
      "Epoch: 13 \tTraining Loss: 0.000090\n",
      "Epoch: 14 \tTraining Loss: 0.000090\n",
      "Epoch: 15 \tTraining Loss: 0.000090\n",
      "Epoch: 16 \tTraining Loss: 0.000090\n",
      "Epoch: 17 \tTraining Loss: 0.000090\n",
      "Epoch: 18 \tTraining Loss: 0.000090\n",
      "Epoch: 1 \tTraining Loss: 0.000213\n",
      "Epoch: 2 \tTraining Loss: 0.000213\n",
      "Epoch: 3 \tTraining Loss: 0.000213\n",
      "Epoch: 4 \tTraining Loss: 0.000213\n",
      "Epoch: 5 \tTraining Loss: 0.000213\n",
      "Epoch: 6 \tTraining Loss: 0.000213\n",
      "Epoch: 7 \tTraining Loss: 0.000213\n",
      "Epoch: 8 \tTraining Loss: 0.000213\n",
      "Epoch: 9 \tTraining Loss: 0.000213\n",
      "Epoch: 10 \tTraining Loss: 0.000213\n",
      "Epoch: 11 \tTraining Loss: 0.000213\n",
      "Epoch: 12 \tTraining Loss: 0.000213\n",
      "Epoch: 13 \tTraining Loss: 0.000213\n",
      "Epoch: 14 \tTraining Loss: 0.000213\n",
      "Epoch: 15 \tTraining Loss: 0.000213\n",
      "Epoch: 16 \tTraining Loss: 0.000213\n",
      "Epoch: 17 \tTraining Loss: 0.000213\n",
      "Epoch: 18 \tTraining Loss: 0.000213\n",
      "Epoch: 1 \tTraining Loss: 0.000328\n",
      "Epoch: 2 \tTraining Loss: 0.000328\n",
      "Epoch: 3 \tTraining Loss: 0.000328\n",
      "Epoch: 4 \tTraining Loss: 0.000328\n",
      "Epoch: 5 \tTraining Loss: 0.000328\n",
      "Epoch: 6 \tTraining Loss: 0.000328\n",
      "Epoch: 7 \tTraining Loss: 0.000328\n",
      "Epoch: 8 \tTraining Loss: 0.000328\n",
      "Epoch: 9 \tTraining Loss: 0.000328\n",
      "Epoch: 10 \tTraining Loss: 0.000328\n",
      "Epoch: 11 \tTraining Loss: 0.000328\n",
      "Epoch: 12 \tTraining Loss: 0.000328\n",
      "Epoch: 13 \tTraining Loss: 0.000328\n",
      "Epoch: 14 \tTraining Loss: 0.000328\n",
      "Epoch: 15 \tTraining Loss: 0.000328\n",
      "Epoch: 16 \tTraining Loss: 0.000328\n",
      "Epoch: 17 \tTraining Loss: 0.000328\n",
      "Epoch: 18 \tTraining Loss: 0.000328\n",
      "Epoch: 1 \tTraining Loss: 0.000123\n",
      "Epoch: 2 \tTraining Loss: 0.000123\n",
      "Epoch: 3 \tTraining Loss: 0.000123\n",
      "Epoch: 4 \tTraining Loss: 0.000123\n",
      "Epoch: 5 \tTraining Loss: 0.000123\n",
      "Epoch: 6 \tTraining Loss: 0.000123\n",
      "Epoch: 7 \tTraining Loss: 0.000123\n",
      "Epoch: 8 \tTraining Loss: 0.000123\n",
      "Epoch: 9 \tTraining Loss: 0.000123\n",
      "Epoch: 10 \tTraining Loss: 0.000123\n",
      "Epoch: 11 \tTraining Loss: 0.000123\n",
      "Epoch: 12 \tTraining Loss: 0.000123\n",
      "Epoch: 13 \tTraining Loss: 0.000123\n",
      "Epoch: 14 \tTraining Loss: 0.000123\n",
      "Epoch: 15 \tTraining Loss: 0.000123\n",
      "Epoch: 16 \tTraining Loss: 0.000123\n",
      "Epoch: 17 \tTraining Loss: 0.000123\n",
      "Epoch: 18 \tTraining Loss: 0.000123\n",
      "Epoch: 1 \tTraining Loss: 0.000171\n",
      "Epoch: 2 \tTraining Loss: 0.000171\n",
      "Epoch: 3 \tTraining Loss: 0.000171\n",
      "Epoch: 4 \tTraining Loss: 0.000171\n",
      "Epoch: 5 \tTraining Loss: 0.000171\n",
      "Epoch: 6 \tTraining Loss: 0.000171\n",
      "Epoch: 7 \tTraining Loss: 0.000171\n",
      "Epoch: 8 \tTraining Loss: 0.000171\n",
      "Epoch: 9 \tTraining Loss: 0.000171\n",
      "Epoch: 10 \tTraining Loss: 0.000171\n",
      "Epoch: 11 \tTraining Loss: 0.000171\n",
      "Epoch: 12 \tTraining Loss: 0.000171\n",
      "Epoch: 13 \tTraining Loss: 0.000171\n",
      "Epoch: 14 \tTraining Loss: 0.000171\n",
      "Epoch: 15 \tTraining Loss: 0.000171\n",
      "Epoch: 16 \tTraining Loss: 0.000171\n",
      "Epoch: 17 \tTraining Loss: 0.000171\n",
      "Epoch: 18 \tTraining Loss: 0.000171\n",
      "Epoch: 1 \tTraining Loss: 0.000218\n",
      "Epoch: 2 \tTraining Loss: 0.000218\n",
      "Epoch: 3 \tTraining Loss: 0.000218\n",
      "Epoch: 4 \tTraining Loss: 0.000218\n",
      "Epoch: 5 \tTraining Loss: 0.000218\n",
      "Epoch: 6 \tTraining Loss: 0.000218\n",
      "Epoch: 7 \tTraining Loss: 0.000218\n",
      "Epoch: 8 \tTraining Loss: 0.000218\n",
      "Epoch: 9 \tTraining Loss: 0.000218\n",
      "Epoch: 10 \tTraining Loss: 0.000218\n",
      "Epoch: 11 \tTraining Loss: 0.000218\n",
      "Epoch: 12 \tTraining Loss: 0.000218\n",
      "Epoch: 13 \tTraining Loss: 0.000218\n",
      "Epoch: 14 \tTraining Loss: 0.000218\n",
      "Epoch: 15 \tTraining Loss: 0.000218\n",
      "Epoch: 16 \tTraining Loss: 0.000218\n",
      "Epoch: 17 \tTraining Loss: 0.000218\n",
      "Epoch: 18 \tTraining Loss: 0.000218\n",
      "Epoch: 1 \tTraining Loss: 0.000111\n",
      "Epoch: 2 \tTraining Loss: 0.000111\n",
      "Epoch: 3 \tTraining Loss: 0.000111\n",
      "Epoch: 4 \tTraining Loss: 0.000111\n",
      "Epoch: 5 \tTraining Loss: 0.000111\n",
      "Epoch: 6 \tTraining Loss: 0.000111\n",
      "Epoch: 7 \tTraining Loss: 0.000111\n",
      "Epoch: 8 \tTraining Loss: 0.000111\n",
      "Epoch: 9 \tTraining Loss: 0.000111\n",
      "Epoch: 10 \tTraining Loss: 0.000111\n",
      "Epoch: 11 \tTraining Loss: 0.000111\n",
      "Epoch: 12 \tTraining Loss: 0.000111\n",
      "Epoch: 13 \tTraining Loss: 0.000111\n",
      "Epoch: 14 \tTraining Loss: 0.000111\n",
      "Epoch: 15 \tTraining Loss: 0.000111\n",
      "Epoch: 16 \tTraining Loss: 0.000111\n",
      "Epoch: 17 \tTraining Loss: 0.000111\n",
      "Epoch: 18 \tTraining Loss: 0.000111\n",
      "Epoch: 1 \tTraining Loss: 0.000159\n",
      "Epoch: 2 \tTraining Loss: 0.000159\n",
      "Epoch: 3 \tTraining Loss: 0.000159\n",
      "Epoch: 4 \tTraining Loss: 0.000159\n",
      "Epoch: 5 \tTraining Loss: 0.000159\n",
      "Epoch: 6 \tTraining Loss: 0.000159\n",
      "Epoch: 7 \tTraining Loss: 0.000159\n",
      "Epoch: 8 \tTraining Loss: 0.000159\n",
      "Epoch: 9 \tTraining Loss: 0.000159\n",
      "Epoch: 10 \tTraining Loss: 0.000159\n",
      "Epoch: 11 \tTraining Loss: 0.000159\n",
      "Epoch: 12 \tTraining Loss: 0.000159\n",
      "Epoch: 13 \tTraining Loss: 0.000159\n",
      "Epoch: 14 \tTraining Loss: 0.000159\n",
      "Epoch: 15 \tTraining Loss: 0.000159\n",
      "Epoch: 16 \tTraining Loss: 0.000159\n",
      "Epoch: 17 \tTraining Loss: 0.000159\n",
      "Epoch: 18 \tTraining Loss: 0.000159\n",
      "Epoch: 1 \tTraining Loss: 0.000179\n",
      "Epoch: 2 \tTraining Loss: 0.000179\n",
      "Epoch: 3 \tTraining Loss: 0.000179\n",
      "Epoch: 4 \tTraining Loss: 0.000179\n",
      "Epoch: 5 \tTraining Loss: 0.000179\n",
      "Epoch: 6 \tTraining Loss: 0.000179\n",
      "Epoch: 7 \tTraining Loss: 0.000179\n",
      "Epoch: 8 \tTraining Loss: 0.000179\n",
      "Epoch: 9 \tTraining Loss: 0.000179\n",
      "Epoch: 10 \tTraining Loss: 0.000179\n",
      "Epoch: 11 \tTraining Loss: 0.000179\n",
      "Epoch: 12 \tTraining Loss: 0.000179\n",
      "Epoch: 13 \tTraining Loss: 0.000179\n",
      "Epoch: 14 \tTraining Loss: 0.000179\n",
      "Epoch: 15 \tTraining Loss: 0.000179\n",
      "Epoch: 16 \tTraining Loss: 0.000179\n",
      "Epoch: 17 \tTraining Loss: 0.000179\n",
      "Epoch: 18 \tTraining Loss: 0.000179\n",
      "Epoch: 1 \tTraining Loss: 0.000120\n",
      "Epoch: 2 \tTraining Loss: 0.000120\n",
      "Epoch: 3 \tTraining Loss: 0.000120\n",
      "Epoch: 4 \tTraining Loss: 0.000120\n",
      "Epoch: 5 \tTraining Loss: 0.000120\n",
      "Epoch: 6 \tTraining Loss: 0.000120\n",
      "Epoch: 7 \tTraining Loss: 0.000120\n",
      "Epoch: 8 \tTraining Loss: 0.000120\n",
      "Epoch: 9 \tTraining Loss: 0.000120\n",
      "Epoch: 10 \tTraining Loss: 0.000120\n",
      "Epoch: 11 \tTraining Loss: 0.000120\n",
      "Epoch: 12 \tTraining Loss: 0.000120\n",
      "Epoch: 13 \tTraining Loss: 0.000120\n",
      "Epoch: 14 \tTraining Loss: 0.000120\n",
      "Epoch: 15 \tTraining Loss: 0.000120\n",
      "Epoch: 16 \tTraining Loss: 0.000120\n",
      "Epoch: 17 \tTraining Loss: 0.000120\n",
      "Epoch: 18 \tTraining Loss: 0.000120\n",
      "Epoch: 1 \tTraining Loss: 0.000120\n",
      "Epoch: 2 \tTraining Loss: 0.000120\n",
      "Epoch: 3 \tTraining Loss: 0.000120\n",
      "Epoch: 4 \tTraining Loss: 0.000120\n",
      "Epoch: 5 \tTraining Loss: 0.000120\n",
      "Epoch: 6 \tTraining Loss: 0.000120\n",
      "Epoch: 7 \tTraining Loss: 0.000120\n",
      "Epoch: 8 \tTraining Loss: 0.000120\n",
      "Epoch: 9 \tTraining Loss: 0.000120\n",
      "Epoch: 10 \tTraining Loss: 0.000120\n",
      "Epoch: 11 \tTraining Loss: 0.000120\n",
      "Epoch: 12 \tTraining Loss: 0.000120\n",
      "Epoch: 13 \tTraining Loss: 0.000120\n",
      "Epoch: 14 \tTraining Loss: 0.000120\n",
      "Epoch: 15 \tTraining Loss: 0.000120\n",
      "Epoch: 16 \tTraining Loss: 0.000120\n",
      "Epoch: 17 \tTraining Loss: 0.000120\n",
      "Epoch: 18 \tTraining Loss: 0.000120\n",
      "Epoch: 1 \tTraining Loss: 0.000126\n",
      "Epoch: 2 \tTraining Loss: 0.000126\n",
      "Epoch: 3 \tTraining Loss: 0.000126\n",
      "Epoch: 4 \tTraining Loss: 0.000126\n",
      "Epoch: 5 \tTraining Loss: 0.000126\n",
      "Epoch: 6 \tTraining Loss: 0.000126\n",
      "Epoch: 7 \tTraining Loss: 0.000126\n",
      "Epoch: 8 \tTraining Loss: 0.000126\n",
      "Epoch: 9 \tTraining Loss: 0.000126\n",
      "Epoch: 10 \tTraining Loss: 0.000126\n",
      "Epoch: 11 \tTraining Loss: 0.000126\n",
      "Epoch: 12 \tTraining Loss: 0.000126\n",
      "Epoch: 13 \tTraining Loss: 0.000126\n",
      "Epoch: 14 \tTraining Loss: 0.000126\n",
      "Epoch: 15 \tTraining Loss: 0.000126\n",
      "Epoch: 16 \tTraining Loss: 0.000126\n",
      "Epoch: 17 \tTraining Loss: 0.000126\n",
      "Epoch: 18 \tTraining Loss: 0.000126\n",
      "Epoch: 1 \tTraining Loss: 0.000164\n",
      "Epoch: 2 \tTraining Loss: 0.000164\n",
      "Epoch: 3 \tTraining Loss: 0.000164\n",
      "Epoch: 4 \tTraining Loss: 0.000164\n",
      "Epoch: 5 \tTraining Loss: 0.000164\n",
      "Epoch: 6 \tTraining Loss: 0.000164\n",
      "Epoch: 7 \tTraining Loss: 0.000164\n",
      "Epoch: 8 \tTraining Loss: 0.000164\n",
      "Epoch: 9 \tTraining Loss: 0.000164\n",
      "Epoch: 10 \tTraining Loss: 0.000164\n",
      "Epoch: 11 \tTraining Loss: 0.000164\n",
      "Epoch: 12 \tTraining Loss: 0.000164\n",
      "Epoch: 13 \tTraining Loss: 0.000164\n",
      "Epoch: 14 \tTraining Loss: 0.000164\n",
      "Epoch: 15 \tTraining Loss: 0.000164\n",
      "Epoch: 16 \tTraining Loss: 0.000164\n",
      "Epoch: 17 \tTraining Loss: 0.000164\n",
      "Epoch: 18 \tTraining Loss: 0.000164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000184\n",
      "Epoch: 2 \tTraining Loss: 0.000184\n",
      "Epoch: 3 \tTraining Loss: 0.000184\n",
      "Epoch: 4 \tTraining Loss: 0.000184\n",
      "Epoch: 5 \tTraining Loss: 0.000184\n",
      "Epoch: 6 \tTraining Loss: 0.000184\n",
      "Epoch: 7 \tTraining Loss: 0.000184\n",
      "Epoch: 8 \tTraining Loss: 0.000184\n",
      "Epoch: 9 \tTraining Loss: 0.000184\n",
      "Epoch: 10 \tTraining Loss: 0.000184\n",
      "Epoch: 11 \tTraining Loss: 0.000184\n",
      "Epoch: 12 \tTraining Loss: 0.000184\n",
      "Epoch: 13 \tTraining Loss: 0.000184\n",
      "Epoch: 14 \tTraining Loss: 0.000184\n",
      "Epoch: 15 \tTraining Loss: 0.000184\n",
      "Epoch: 16 \tTraining Loss: 0.000184\n",
      "Epoch: 17 \tTraining Loss: 0.000184\n",
      "Epoch: 18 \tTraining Loss: 0.000184\n",
      "Epoch: 1 \tTraining Loss: 0.000236\n",
      "Epoch: 2 \tTraining Loss: 0.000236\n",
      "Epoch: 3 \tTraining Loss: 0.000236\n",
      "Epoch: 4 \tTraining Loss: 0.000236\n",
      "Epoch: 5 \tTraining Loss: 0.000236\n",
      "Epoch: 6 \tTraining Loss: 0.000236\n",
      "Epoch: 7 \tTraining Loss: 0.000236\n",
      "Epoch: 8 \tTraining Loss: 0.000236\n",
      "Epoch: 9 \tTraining Loss: 0.000236\n",
      "Epoch: 10 \tTraining Loss: 0.000236\n",
      "Epoch: 11 \tTraining Loss: 0.000236\n",
      "Epoch: 12 \tTraining Loss: 0.000236\n",
      "Epoch: 13 \tTraining Loss: 0.000236\n",
      "Epoch: 14 \tTraining Loss: 0.000236\n",
      "Epoch: 15 \tTraining Loss: 0.000236\n",
      "Epoch: 16 \tTraining Loss: 0.000236\n",
      "Epoch: 17 \tTraining Loss: 0.000236\n",
      "Epoch: 18 \tTraining Loss: 0.000236\n",
      "Epoch: 1 \tTraining Loss: 0.000142\n",
      "Epoch: 2 \tTraining Loss: 0.000142\n",
      "Epoch: 3 \tTraining Loss: 0.000142\n",
      "Epoch: 4 \tTraining Loss: 0.000142\n",
      "Epoch: 5 \tTraining Loss: 0.000142\n",
      "Epoch: 6 \tTraining Loss: 0.000142\n",
      "Epoch: 7 \tTraining Loss: 0.000142\n",
      "Epoch: 8 \tTraining Loss: 0.000142\n",
      "Epoch: 9 \tTraining Loss: 0.000142\n",
      "Epoch: 10 \tTraining Loss: 0.000142\n",
      "Epoch: 11 \tTraining Loss: 0.000142\n",
      "Epoch: 12 \tTraining Loss: 0.000142\n",
      "Epoch: 13 \tTraining Loss: 0.000142\n",
      "Epoch: 14 \tTraining Loss: 0.000142\n",
      "Epoch: 15 \tTraining Loss: 0.000142\n",
      "Epoch: 16 \tTraining Loss: 0.000142\n",
      "Epoch: 17 \tTraining Loss: 0.000142\n",
      "Epoch: 18 \tTraining Loss: 0.000142\n",
      "Epoch: 1 \tTraining Loss: 0.000175\n",
      "Epoch: 2 \tTraining Loss: 0.000175\n",
      "Epoch: 3 \tTraining Loss: 0.000175\n",
      "Epoch: 4 \tTraining Loss: 0.000175\n",
      "Epoch: 5 \tTraining Loss: 0.000175\n",
      "Epoch: 6 \tTraining Loss: 0.000175\n",
      "Epoch: 7 \tTraining Loss: 0.000175\n",
      "Epoch: 8 \tTraining Loss: 0.000175\n",
      "Epoch: 9 \tTraining Loss: 0.000175\n",
      "Epoch: 10 \tTraining Loss: 0.000175\n",
      "Epoch: 11 \tTraining Loss: 0.000175\n",
      "Epoch: 12 \tTraining Loss: 0.000175\n",
      "Epoch: 13 \tTraining Loss: 0.000175\n",
      "Epoch: 14 \tTraining Loss: 0.000175\n",
      "Epoch: 15 \tTraining Loss: 0.000175\n",
      "Epoch: 16 \tTraining Loss: 0.000175\n",
      "Epoch: 17 \tTraining Loss: 0.000175\n",
      "Epoch: 18 \tTraining Loss: 0.000175\n",
      "Epoch: 1 \tTraining Loss: 0.000106\n",
      "Epoch: 2 \tTraining Loss: 0.000106\n",
      "Epoch: 3 \tTraining Loss: 0.000106\n",
      "Epoch: 4 \tTraining Loss: 0.000106\n",
      "Epoch: 5 \tTraining Loss: 0.000106\n",
      "Epoch: 6 \tTraining Loss: 0.000106\n",
      "Epoch: 7 \tTraining Loss: 0.000106\n",
      "Epoch: 8 \tTraining Loss: 0.000106\n",
      "Epoch: 9 \tTraining Loss: 0.000106\n",
      "Epoch: 10 \tTraining Loss: 0.000106\n",
      "Epoch: 11 \tTraining Loss: 0.000106\n",
      "Epoch: 12 \tTraining Loss: 0.000106\n",
      "Epoch: 13 \tTraining Loss: 0.000106\n",
      "Epoch: 14 \tTraining Loss: 0.000106\n",
      "Epoch: 15 \tTraining Loss: 0.000106\n",
      "Epoch: 16 \tTraining Loss: 0.000106\n",
      "Epoch: 17 \tTraining Loss: 0.000106\n",
      "Epoch: 18 \tTraining Loss: 0.000106\n",
      "Epoch: 1 \tTraining Loss: 0.000230\n",
      "Epoch: 2 \tTraining Loss: 0.000230\n",
      "Epoch: 3 \tTraining Loss: 0.000230\n",
      "Epoch: 4 \tTraining Loss: 0.000230\n",
      "Epoch: 5 \tTraining Loss: 0.000230\n",
      "Epoch: 6 \tTraining Loss: 0.000230\n",
      "Epoch: 7 \tTraining Loss: 0.000230\n",
      "Epoch: 8 \tTraining Loss: 0.000230\n",
      "Epoch: 9 \tTraining Loss: 0.000230\n",
      "Epoch: 10 \tTraining Loss: 0.000230\n",
      "Epoch: 11 \tTraining Loss: 0.000230\n",
      "Epoch: 12 \tTraining Loss: 0.000230\n",
      "Epoch: 13 \tTraining Loss: 0.000230\n",
      "Epoch: 14 \tTraining Loss: 0.000230\n",
      "Epoch: 15 \tTraining Loss: 0.000230\n",
      "Epoch: 16 \tTraining Loss: 0.000230\n",
      "Epoch: 17 \tTraining Loss: 0.000230\n",
      "Epoch: 18 \tTraining Loss: 0.000230\n",
      "Epoch: 1 \tTraining Loss: 0.000212\n",
      "Epoch: 2 \tTraining Loss: 0.000212\n",
      "Epoch: 3 \tTraining Loss: 0.000212\n",
      "Epoch: 4 \tTraining Loss: 0.000212\n",
      "Epoch: 5 \tTraining Loss: 0.000212\n",
      "Epoch: 6 \tTraining Loss: 0.000212\n",
      "Epoch: 7 \tTraining Loss: 0.000212\n",
      "Epoch: 8 \tTraining Loss: 0.000212\n",
      "Epoch: 9 \tTraining Loss: 0.000212\n",
      "Epoch: 10 \tTraining Loss: 0.000212\n",
      "Epoch: 11 \tTraining Loss: 0.000212\n",
      "Epoch: 12 \tTraining Loss: 0.000212\n",
      "Epoch: 13 \tTraining Loss: 0.000212\n",
      "Epoch: 14 \tTraining Loss: 0.000212\n",
      "Epoch: 15 \tTraining Loss: 0.000212\n",
      "Epoch: 16 \tTraining Loss: 0.000212\n",
      "Epoch: 17 \tTraining Loss: 0.000212\n",
      "Epoch: 18 \tTraining Loss: 0.000212\n",
      "Epoch: 1 \tTraining Loss: 0.000255\n",
      "Epoch: 2 \tTraining Loss: 0.000255\n",
      "Epoch: 3 \tTraining Loss: 0.000255\n",
      "Epoch: 4 \tTraining Loss: 0.000255\n",
      "Epoch: 5 \tTraining Loss: 0.000255\n",
      "Epoch: 6 \tTraining Loss: 0.000255\n",
      "Epoch: 7 \tTraining Loss: 0.000255\n",
      "Epoch: 8 \tTraining Loss: 0.000255\n",
      "Epoch: 9 \tTraining Loss: 0.000255\n",
      "Epoch: 10 \tTraining Loss: 0.000255\n",
      "Epoch: 11 \tTraining Loss: 0.000255\n",
      "Epoch: 12 \tTraining Loss: 0.000255\n",
      "Epoch: 13 \tTraining Loss: 0.000255\n",
      "Epoch: 14 \tTraining Loss: 0.000255\n",
      "Epoch: 15 \tTraining Loss: 0.000255\n",
      "Epoch: 16 \tTraining Loss: 0.000255\n",
      "Epoch: 17 \tTraining Loss: 0.000255\n",
      "Epoch: 18 \tTraining Loss: 0.000255\n",
      "Epoch: 1 \tTraining Loss: 0.000214\n",
      "Epoch: 2 \tTraining Loss: 0.000214\n",
      "Epoch: 3 \tTraining Loss: 0.000214\n",
      "Epoch: 4 \tTraining Loss: 0.000214\n",
      "Epoch: 5 \tTraining Loss: 0.000214\n",
      "Epoch: 6 \tTraining Loss: 0.000214\n",
      "Epoch: 7 \tTraining Loss: 0.000214\n",
      "Epoch: 8 \tTraining Loss: 0.000214\n",
      "Epoch: 9 \tTraining Loss: 0.000214\n",
      "Epoch: 10 \tTraining Loss: 0.000214\n",
      "Epoch: 11 \tTraining Loss: 0.000214\n",
      "Epoch: 12 \tTraining Loss: 0.000214\n",
      "Epoch: 13 \tTraining Loss: 0.000214\n",
      "Epoch: 14 \tTraining Loss: 0.000214\n",
      "Epoch: 15 \tTraining Loss: 0.000214\n",
      "Epoch: 16 \tTraining Loss: 0.000214\n",
      "Epoch: 17 \tTraining Loss: 0.000214\n",
      "Epoch: 18 \tTraining Loss: 0.000214\n",
      "Epoch: 1 \tTraining Loss: 0.000246\n",
      "Epoch: 2 \tTraining Loss: 0.000246\n",
      "Epoch: 3 \tTraining Loss: 0.000246\n",
      "Epoch: 4 \tTraining Loss: 0.000246\n",
      "Epoch: 5 \tTraining Loss: 0.000246\n",
      "Epoch: 6 \tTraining Loss: 0.000246\n",
      "Epoch: 7 \tTraining Loss: 0.000246\n",
      "Epoch: 8 \tTraining Loss: 0.000246\n",
      "Epoch: 9 \tTraining Loss: 0.000246\n",
      "Epoch: 10 \tTraining Loss: 0.000246\n",
      "Epoch: 11 \tTraining Loss: 0.000246\n",
      "Epoch: 12 \tTraining Loss: 0.000246\n",
      "Epoch: 13 \tTraining Loss: 0.000246\n",
      "Epoch: 14 \tTraining Loss: 0.000246\n",
      "Epoch: 15 \tTraining Loss: 0.000246\n",
      "Epoch: 16 \tTraining Loss: 0.000246\n",
      "Epoch: 17 \tTraining Loss: 0.000246\n",
      "Epoch: 18 \tTraining Loss: 0.000246\n",
      "Epoch: 1 \tTraining Loss: 0.000155\n",
      "Epoch: 2 \tTraining Loss: 0.000155\n",
      "Epoch: 3 \tTraining Loss: 0.000155\n",
      "Epoch: 4 \tTraining Loss: 0.000155\n",
      "Epoch: 5 \tTraining Loss: 0.000155\n",
      "Epoch: 6 \tTraining Loss: 0.000155\n",
      "Epoch: 7 \tTraining Loss: 0.000155\n",
      "Epoch: 8 \tTraining Loss: 0.000155\n",
      "Epoch: 9 \tTraining Loss: 0.000155\n",
      "Epoch: 10 \tTraining Loss: 0.000155\n",
      "Epoch: 11 \tTraining Loss: 0.000155\n",
      "Epoch: 12 \tTraining Loss: 0.000155\n",
      "Epoch: 13 \tTraining Loss: 0.000155\n",
      "Epoch: 14 \tTraining Loss: 0.000155\n",
      "Epoch: 15 \tTraining Loss: 0.000155\n",
      "Epoch: 16 \tTraining Loss: 0.000155\n",
      "Epoch: 17 \tTraining Loss: 0.000155\n",
      "Epoch: 18 \tTraining Loss: 0.000155\n",
      "Epoch: 1 \tTraining Loss: 0.000272\n",
      "Epoch: 2 \tTraining Loss: 0.000272\n",
      "Epoch: 3 \tTraining Loss: 0.000272\n",
      "Epoch: 4 \tTraining Loss: 0.000272\n",
      "Epoch: 5 \tTraining Loss: 0.000272\n",
      "Epoch: 6 \tTraining Loss: 0.000272\n",
      "Epoch: 7 \tTraining Loss: 0.000272\n",
      "Epoch: 8 \tTraining Loss: 0.000272\n",
      "Epoch: 9 \tTraining Loss: 0.000272\n",
      "Epoch: 10 \tTraining Loss: 0.000272\n",
      "Epoch: 11 \tTraining Loss: 0.000272\n",
      "Epoch: 12 \tTraining Loss: 0.000272\n",
      "Epoch: 13 \tTraining Loss: 0.000272\n",
      "Epoch: 14 \tTraining Loss: 0.000272\n",
      "Epoch: 15 \tTraining Loss: 0.000272\n",
      "Epoch: 16 \tTraining Loss: 0.000272\n",
      "Epoch: 17 \tTraining Loss: 0.000272\n",
      "Epoch: 18 \tTraining Loss: 0.000272\n",
      "Epoch: 1 \tTraining Loss: 0.000239\n",
      "Epoch: 2 \tTraining Loss: 0.000239\n",
      "Epoch: 3 \tTraining Loss: 0.000239\n",
      "Epoch: 4 \tTraining Loss: 0.000239\n",
      "Epoch: 5 \tTraining Loss: 0.000239\n",
      "Epoch: 6 \tTraining Loss: 0.000239\n",
      "Epoch: 7 \tTraining Loss: 0.000239\n",
      "Epoch: 8 \tTraining Loss: 0.000239\n",
      "Epoch: 9 \tTraining Loss: 0.000239\n",
      "Epoch: 10 \tTraining Loss: 0.000239\n",
      "Epoch: 11 \tTraining Loss: 0.000239\n",
      "Epoch: 12 \tTraining Loss: 0.000239\n",
      "Epoch: 13 \tTraining Loss: 0.000239\n",
      "Epoch: 14 \tTraining Loss: 0.000239\n",
      "Epoch: 15 \tTraining Loss: 0.000239\n",
      "Epoch: 16 \tTraining Loss: 0.000239\n",
      "Epoch: 17 \tTraining Loss: 0.000239\n",
      "Epoch: 18 \tTraining Loss: 0.000239\n",
      "Epoch: 1 \tTraining Loss: 0.000193\n",
      "Epoch: 2 \tTraining Loss: 0.000193\n",
      "Epoch: 3 \tTraining Loss: 0.000193\n",
      "Epoch: 4 \tTraining Loss: 0.000193\n",
      "Epoch: 5 \tTraining Loss: 0.000193\n",
      "Epoch: 6 \tTraining Loss: 0.000193\n",
      "Epoch: 7 \tTraining Loss: 0.000193\n",
      "Epoch: 8 \tTraining Loss: 0.000193\n",
      "Epoch: 9 \tTraining Loss: 0.000193\n",
      "Epoch: 10 \tTraining Loss: 0.000193\n",
      "Epoch: 11 \tTraining Loss: 0.000193\n",
      "Epoch: 12 \tTraining Loss: 0.000193\n",
      "Epoch: 13 \tTraining Loss: 0.000193\n",
      "Epoch: 14 \tTraining Loss: 0.000193\n",
      "Epoch: 15 \tTraining Loss: 0.000193\n",
      "Epoch: 16 \tTraining Loss: 0.000193\n",
      "Epoch: 17 \tTraining Loss: 0.000193\n",
      "Epoch: 18 \tTraining Loss: 0.000193\n",
      "Epoch: 1 \tTraining Loss: 0.000179\n",
      "Epoch: 2 \tTraining Loss: 0.000179\n",
      "Epoch: 3 \tTraining Loss: 0.000179\n",
      "Epoch: 4 \tTraining Loss: 0.000179\n",
      "Epoch: 5 \tTraining Loss: 0.000179\n",
      "Epoch: 6 \tTraining Loss: 0.000179\n",
      "Epoch: 7 \tTraining Loss: 0.000179\n",
      "Epoch: 8 \tTraining Loss: 0.000179\n",
      "Epoch: 9 \tTraining Loss: 0.000179\n",
      "Epoch: 10 \tTraining Loss: 0.000179\n",
      "Epoch: 11 \tTraining Loss: 0.000179\n",
      "Epoch: 12 \tTraining Loss: 0.000179\n",
      "Epoch: 13 \tTraining Loss: 0.000179\n",
      "Epoch: 14 \tTraining Loss: 0.000179\n",
      "Epoch: 15 \tTraining Loss: 0.000179\n",
      "Epoch: 16 \tTraining Loss: 0.000179\n",
      "Epoch: 17 \tTraining Loss: 0.000179\n",
      "Epoch: 18 \tTraining Loss: 0.000179\n",
      "Epoch: 1 \tTraining Loss: 0.000177\n",
      "Epoch: 2 \tTraining Loss: 0.000177\n",
      "Epoch: 3 \tTraining Loss: 0.000177\n",
      "Epoch: 4 \tTraining Loss: 0.000177\n",
      "Epoch: 5 \tTraining Loss: 0.000177\n",
      "Epoch: 6 \tTraining Loss: 0.000177\n",
      "Epoch: 7 \tTraining Loss: 0.000177\n",
      "Epoch: 8 \tTraining Loss: 0.000177\n",
      "Epoch: 9 \tTraining Loss: 0.000177\n",
      "Epoch: 10 \tTraining Loss: 0.000177\n",
      "Epoch: 11 \tTraining Loss: 0.000177\n",
      "Epoch: 12 \tTraining Loss: 0.000177\n",
      "Epoch: 13 \tTraining Loss: 0.000177\n",
      "Epoch: 14 \tTraining Loss: 0.000177\n",
      "Epoch: 15 \tTraining Loss: 0.000177\n",
      "Epoch: 16 \tTraining Loss: 0.000177\n",
      "Epoch: 17 \tTraining Loss: 0.000177\n",
      "Epoch: 18 \tTraining Loss: 0.000177\n",
      "Epoch: 1 \tTraining Loss: 0.000265\n",
      "Epoch: 2 \tTraining Loss: 0.000265\n",
      "Epoch: 3 \tTraining Loss: 0.000265\n",
      "Epoch: 4 \tTraining Loss: 0.000265\n",
      "Epoch: 5 \tTraining Loss: 0.000265\n",
      "Epoch: 6 \tTraining Loss: 0.000265\n",
      "Epoch: 7 \tTraining Loss: 0.000265\n",
      "Epoch: 8 \tTraining Loss: 0.000265\n",
      "Epoch: 9 \tTraining Loss: 0.000265\n",
      "Epoch: 10 \tTraining Loss: 0.000265\n",
      "Epoch: 11 \tTraining Loss: 0.000265\n",
      "Epoch: 12 \tTraining Loss: 0.000265\n",
      "Epoch: 13 \tTraining Loss: 0.000265\n",
      "Epoch: 14 \tTraining Loss: 0.000265\n",
      "Epoch: 15 \tTraining Loss: 0.000265\n",
      "Epoch: 16 \tTraining Loss: 0.000265\n",
      "Epoch: 17 \tTraining Loss: 0.000265\n",
      "Epoch: 18 \tTraining Loss: 0.000265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000173\n",
      "Epoch: 2 \tTraining Loss: 0.000173\n",
      "Epoch: 3 \tTraining Loss: 0.000173\n",
      "Epoch: 4 \tTraining Loss: 0.000173\n",
      "Epoch: 5 \tTraining Loss: 0.000173\n",
      "Epoch: 6 \tTraining Loss: 0.000173\n",
      "Epoch: 7 \tTraining Loss: 0.000173\n",
      "Epoch: 8 \tTraining Loss: 0.000173\n",
      "Epoch: 9 \tTraining Loss: 0.000173\n",
      "Epoch: 10 \tTraining Loss: 0.000173\n",
      "Epoch: 11 \tTraining Loss: 0.000173\n",
      "Epoch: 12 \tTraining Loss: 0.000173\n",
      "Epoch: 13 \tTraining Loss: 0.000173\n",
      "Epoch: 14 \tTraining Loss: 0.000173\n",
      "Epoch: 15 \tTraining Loss: 0.000173\n",
      "Epoch: 16 \tTraining Loss: 0.000173\n",
      "Epoch: 17 \tTraining Loss: 0.000173\n",
      "Epoch: 18 \tTraining Loss: 0.000173\n",
      "Epoch: 1 \tTraining Loss: 0.000068\n",
      "Epoch: 2 \tTraining Loss: 0.000068\n",
      "Epoch: 3 \tTraining Loss: 0.000068\n",
      "Epoch: 4 \tTraining Loss: 0.000068\n",
      "Epoch: 5 \tTraining Loss: 0.000068\n",
      "Epoch: 6 \tTraining Loss: 0.000068\n",
      "Epoch: 7 \tTraining Loss: 0.000068\n",
      "Epoch: 8 \tTraining Loss: 0.000068\n",
      "Epoch: 9 \tTraining Loss: 0.000068\n",
      "Epoch: 10 \tTraining Loss: 0.000068\n",
      "Epoch: 11 \tTraining Loss: 0.000068\n",
      "Epoch: 12 \tTraining Loss: 0.000068\n",
      "Epoch: 13 \tTraining Loss: 0.000068\n",
      "Epoch: 14 \tTraining Loss: 0.000068\n",
      "Epoch: 15 \tTraining Loss: 0.000068\n",
      "Epoch: 16 \tTraining Loss: 0.000068\n",
      "Epoch: 17 \tTraining Loss: 0.000068\n",
      "Epoch: 18 \tTraining Loss: 0.000068\n",
      "Epoch: 1 \tTraining Loss: 0.000099\n",
      "Epoch: 2 \tTraining Loss: 0.000099\n",
      "Epoch: 3 \tTraining Loss: 0.000099\n",
      "Epoch: 4 \tTraining Loss: 0.000099\n",
      "Epoch: 5 \tTraining Loss: 0.000099\n",
      "Epoch: 6 \tTraining Loss: 0.000099\n",
      "Epoch: 7 \tTraining Loss: 0.000099\n",
      "Epoch: 8 \tTraining Loss: 0.000099\n",
      "Epoch: 9 \tTraining Loss: 0.000099\n",
      "Epoch: 10 \tTraining Loss: 0.000099\n",
      "Epoch: 11 \tTraining Loss: 0.000099\n",
      "Epoch: 12 \tTraining Loss: 0.000099\n",
      "Epoch: 13 \tTraining Loss: 0.000099\n",
      "Epoch: 14 \tTraining Loss: 0.000099\n",
      "Epoch: 15 \tTraining Loss: 0.000099\n",
      "Epoch: 16 \tTraining Loss: 0.000099\n",
      "Epoch: 17 \tTraining Loss: 0.000099\n",
      "Epoch: 18 \tTraining Loss: 0.000099\n",
      "Epoch: 1 \tTraining Loss: 0.000093\n",
      "Epoch: 2 \tTraining Loss: 0.000093\n",
      "Epoch: 3 \tTraining Loss: 0.000093\n",
      "Epoch: 4 \tTraining Loss: 0.000093\n",
      "Epoch: 5 \tTraining Loss: 0.000093\n",
      "Epoch: 6 \tTraining Loss: 0.000093\n",
      "Epoch: 7 \tTraining Loss: 0.000093\n",
      "Epoch: 8 \tTraining Loss: 0.000093\n",
      "Epoch: 9 \tTraining Loss: 0.000093\n",
      "Epoch: 10 \tTraining Loss: 0.000093\n",
      "Epoch: 11 \tTraining Loss: 0.000093\n",
      "Epoch: 12 \tTraining Loss: 0.000093\n",
      "Epoch: 13 \tTraining Loss: 0.000093\n",
      "Epoch: 14 \tTraining Loss: 0.000093\n",
      "Epoch: 15 \tTraining Loss: 0.000093\n",
      "Epoch: 16 \tTraining Loss: 0.000093\n",
      "Epoch: 17 \tTraining Loss: 0.000093\n",
      "Epoch: 18 \tTraining Loss: 0.000093\n",
      "Epoch: 1 \tTraining Loss: 0.000116\n",
      "Epoch: 2 \tTraining Loss: 0.000116\n",
      "Epoch: 3 \tTraining Loss: 0.000116\n",
      "Epoch: 4 \tTraining Loss: 0.000116\n",
      "Epoch: 5 \tTraining Loss: 0.000116\n",
      "Epoch: 6 \tTraining Loss: 0.000116\n",
      "Epoch: 7 \tTraining Loss: 0.000116\n",
      "Epoch: 8 \tTraining Loss: 0.000116\n",
      "Epoch: 9 \tTraining Loss: 0.000116\n",
      "Epoch: 10 \tTraining Loss: 0.000116\n",
      "Epoch: 11 \tTraining Loss: 0.000116\n",
      "Epoch: 12 \tTraining Loss: 0.000116\n",
      "Epoch: 13 \tTraining Loss: 0.000116\n",
      "Epoch: 14 \tTraining Loss: 0.000116\n",
      "Epoch: 15 \tTraining Loss: 0.000116\n",
      "Epoch: 16 \tTraining Loss: 0.000116\n",
      "Epoch: 17 \tTraining Loss: 0.000116\n",
      "Epoch: 18 \tTraining Loss: 0.000116\n",
      "Epoch: 1 \tTraining Loss: 0.000269\n",
      "Epoch: 2 \tTraining Loss: 0.000269\n",
      "Epoch: 3 \tTraining Loss: 0.000269\n",
      "Epoch: 4 \tTraining Loss: 0.000269\n",
      "Epoch: 5 \tTraining Loss: 0.000269\n",
      "Epoch: 6 \tTraining Loss: 0.000269\n",
      "Epoch: 7 \tTraining Loss: 0.000269\n",
      "Epoch: 8 \tTraining Loss: 0.000269\n",
      "Epoch: 9 \tTraining Loss: 0.000269\n",
      "Epoch: 10 \tTraining Loss: 0.000269\n",
      "Epoch: 11 \tTraining Loss: 0.000269\n",
      "Epoch: 12 \tTraining Loss: 0.000269\n",
      "Epoch: 13 \tTraining Loss: 0.000269\n",
      "Epoch: 14 \tTraining Loss: 0.000269\n",
      "Epoch: 15 \tTraining Loss: 0.000269\n",
      "Epoch: 16 \tTraining Loss: 0.000269\n",
      "Epoch: 17 \tTraining Loss: 0.000269\n",
      "Epoch: 18 \tTraining Loss: 0.000269\n",
      "Epoch: 1 \tTraining Loss: 0.000228\n",
      "Epoch: 2 \tTraining Loss: 0.000228\n",
      "Epoch: 3 \tTraining Loss: 0.000228\n",
      "Epoch: 4 \tTraining Loss: 0.000228\n",
      "Epoch: 5 \tTraining Loss: 0.000228\n",
      "Epoch: 6 \tTraining Loss: 0.000228\n",
      "Epoch: 7 \tTraining Loss: 0.000228\n",
      "Epoch: 8 \tTraining Loss: 0.000228\n",
      "Epoch: 9 \tTraining Loss: 0.000228\n",
      "Epoch: 10 \tTraining Loss: 0.000228\n",
      "Epoch: 11 \tTraining Loss: 0.000228\n",
      "Epoch: 12 \tTraining Loss: 0.000228\n",
      "Epoch: 13 \tTraining Loss: 0.000228\n",
      "Epoch: 14 \tTraining Loss: 0.000228\n",
      "Epoch: 15 \tTraining Loss: 0.000228\n",
      "Epoch: 16 \tTraining Loss: 0.000228\n",
      "Epoch: 17 \tTraining Loss: 0.000228\n",
      "Epoch: 18 \tTraining Loss: 0.000228\n",
      "Epoch: 1 \tTraining Loss: 0.000275\n",
      "Epoch: 2 \tTraining Loss: 0.000275\n",
      "Epoch: 3 \tTraining Loss: 0.000275\n",
      "Epoch: 4 \tTraining Loss: 0.000275\n",
      "Epoch: 5 \tTraining Loss: 0.000275\n",
      "Epoch: 6 \tTraining Loss: 0.000275\n",
      "Epoch: 7 \tTraining Loss: 0.000275\n",
      "Epoch: 8 \tTraining Loss: 0.000275\n",
      "Epoch: 9 \tTraining Loss: 0.000275\n",
      "Epoch: 10 \tTraining Loss: 0.000275\n",
      "Epoch: 11 \tTraining Loss: 0.000275\n",
      "Epoch: 12 \tTraining Loss: 0.000275\n",
      "Epoch: 13 \tTraining Loss: 0.000275\n",
      "Epoch: 14 \tTraining Loss: 0.000275\n",
      "Epoch: 15 \tTraining Loss: 0.000275\n",
      "Epoch: 16 \tTraining Loss: 0.000275\n",
      "Epoch: 17 \tTraining Loss: 0.000275\n",
      "Epoch: 18 \tTraining Loss: 0.000275\n",
      "Epoch: 1 \tTraining Loss: 0.000198\n",
      "Epoch: 2 \tTraining Loss: 0.000198\n",
      "Epoch: 3 \tTraining Loss: 0.000198\n",
      "Epoch: 4 \tTraining Loss: 0.000198\n",
      "Epoch: 5 \tTraining Loss: 0.000198\n",
      "Epoch: 6 \tTraining Loss: 0.000198\n",
      "Epoch: 7 \tTraining Loss: 0.000198\n",
      "Epoch: 8 \tTraining Loss: 0.000198\n",
      "Epoch: 9 \tTraining Loss: 0.000198\n",
      "Epoch: 10 \tTraining Loss: 0.000198\n",
      "Epoch: 11 \tTraining Loss: 0.000198\n",
      "Epoch: 12 \tTraining Loss: 0.000198\n",
      "Epoch: 13 \tTraining Loss: 0.000198\n",
      "Epoch: 14 \tTraining Loss: 0.000198\n",
      "Epoch: 15 \tTraining Loss: 0.000198\n",
      "Epoch: 16 \tTraining Loss: 0.000198\n",
      "Epoch: 17 \tTraining Loss: 0.000198\n",
      "Epoch: 18 \tTraining Loss: 0.000198\n",
      "Epoch: 1 \tTraining Loss: 0.000209\n",
      "Epoch: 2 \tTraining Loss: 0.000209\n",
      "Epoch: 3 \tTraining Loss: 0.000209\n",
      "Epoch: 4 \tTraining Loss: 0.000209\n",
      "Epoch: 5 \tTraining Loss: 0.000209\n",
      "Epoch: 6 \tTraining Loss: 0.000209\n",
      "Epoch: 7 \tTraining Loss: 0.000209\n",
      "Epoch: 8 \tTraining Loss: 0.000209\n",
      "Epoch: 9 \tTraining Loss: 0.000209\n",
      "Epoch: 10 \tTraining Loss: 0.000209\n",
      "Epoch: 11 \tTraining Loss: 0.000209\n",
      "Epoch: 12 \tTraining Loss: 0.000209\n",
      "Epoch: 13 \tTraining Loss: 0.000209\n",
      "Epoch: 14 \tTraining Loss: 0.000209\n",
      "Epoch: 15 \tTraining Loss: 0.000209\n",
      "Epoch: 16 \tTraining Loss: 0.000209\n",
      "Epoch: 17 \tTraining Loss: 0.000209\n",
      "Epoch: 18 \tTraining Loss: 0.000209\n",
      "Epoch: 1 \tTraining Loss: 0.000219\n",
      "Epoch: 2 \tTraining Loss: 0.000219\n",
      "Epoch: 3 \tTraining Loss: 0.000219\n",
      "Epoch: 4 \tTraining Loss: 0.000219\n",
      "Epoch: 5 \tTraining Loss: 0.000219\n",
      "Epoch: 6 \tTraining Loss: 0.000219\n",
      "Epoch: 7 \tTraining Loss: 0.000219\n",
      "Epoch: 8 \tTraining Loss: 0.000219\n",
      "Epoch: 9 \tTraining Loss: 0.000219\n",
      "Epoch: 10 \tTraining Loss: 0.000219\n",
      "Epoch: 11 \tTraining Loss: 0.000219\n",
      "Epoch: 12 \tTraining Loss: 0.000219\n",
      "Epoch: 13 \tTraining Loss: 0.000219\n",
      "Epoch: 14 \tTraining Loss: 0.000219\n",
      "Epoch: 15 \tTraining Loss: 0.000219\n",
      "Epoch: 16 \tTraining Loss: 0.000219\n",
      "Epoch: 17 \tTraining Loss: 0.000219\n",
      "Epoch: 18 \tTraining Loss: 0.000219\n",
      "Epoch: 1 \tTraining Loss: 0.000298\n",
      "Epoch: 2 \tTraining Loss: 0.000298\n",
      "Epoch: 3 \tTraining Loss: 0.000298\n",
      "Epoch: 4 \tTraining Loss: 0.000298\n",
      "Epoch: 5 \tTraining Loss: 0.000298\n",
      "Epoch: 6 \tTraining Loss: 0.000298\n",
      "Epoch: 7 \tTraining Loss: 0.000298\n",
      "Epoch: 8 \tTraining Loss: 0.000298\n",
      "Epoch: 9 \tTraining Loss: 0.000298\n",
      "Epoch: 10 \tTraining Loss: 0.000298\n",
      "Epoch: 11 \tTraining Loss: 0.000298\n",
      "Epoch: 12 \tTraining Loss: 0.000298\n",
      "Epoch: 13 \tTraining Loss: 0.000298\n",
      "Epoch: 14 \tTraining Loss: 0.000298\n",
      "Epoch: 15 \tTraining Loss: 0.000298\n",
      "Epoch: 16 \tTraining Loss: 0.000298\n",
      "Epoch: 17 \tTraining Loss: 0.000298\n",
      "Epoch: 18 \tTraining Loss: 0.000298\n",
      "Epoch: 1 \tTraining Loss: 0.000338\n",
      "Epoch: 2 \tTraining Loss: 0.000338\n",
      "Epoch: 3 \tTraining Loss: 0.000338\n",
      "Epoch: 4 \tTraining Loss: 0.000338\n",
      "Epoch: 5 \tTraining Loss: 0.000338\n",
      "Epoch: 6 \tTraining Loss: 0.000338\n",
      "Epoch: 7 \tTraining Loss: 0.000338\n",
      "Epoch: 8 \tTraining Loss: 0.000338\n",
      "Epoch: 9 \tTraining Loss: 0.000338\n",
      "Epoch: 10 \tTraining Loss: 0.000338\n",
      "Epoch: 11 \tTraining Loss: 0.000338\n",
      "Epoch: 12 \tTraining Loss: 0.000338\n",
      "Epoch: 13 \tTraining Loss: 0.000338\n",
      "Epoch: 14 \tTraining Loss: 0.000338\n",
      "Epoch: 15 \tTraining Loss: 0.000338\n",
      "Epoch: 16 \tTraining Loss: 0.000338\n",
      "Epoch: 17 \tTraining Loss: 0.000338\n",
      "Epoch: 18 \tTraining Loss: 0.000338\n",
      "Epoch: 1 \tTraining Loss: 0.000171\n",
      "Epoch: 2 \tTraining Loss: 0.000171\n",
      "Epoch: 3 \tTraining Loss: 0.000171\n",
      "Epoch: 4 \tTraining Loss: 0.000171\n",
      "Epoch: 5 \tTraining Loss: 0.000171\n",
      "Epoch: 6 \tTraining Loss: 0.000171\n",
      "Epoch: 7 \tTraining Loss: 0.000171\n",
      "Epoch: 8 \tTraining Loss: 0.000171\n",
      "Epoch: 9 \tTraining Loss: 0.000171\n",
      "Epoch: 10 \tTraining Loss: 0.000171\n",
      "Epoch: 11 \tTraining Loss: 0.000171\n",
      "Epoch: 12 \tTraining Loss: 0.000171\n",
      "Epoch: 13 \tTraining Loss: 0.000171\n",
      "Epoch: 14 \tTraining Loss: 0.000171\n",
      "Epoch: 15 \tTraining Loss: 0.000171\n",
      "Epoch: 16 \tTraining Loss: 0.000171\n",
      "Epoch: 17 \tTraining Loss: 0.000171\n",
      "Epoch: 18 \tTraining Loss: 0.000171\n",
      "Epoch: 1 \tTraining Loss: 0.000220\n",
      "Epoch: 2 \tTraining Loss: 0.000220\n",
      "Epoch: 3 \tTraining Loss: 0.000220\n",
      "Epoch: 4 \tTraining Loss: 0.000220\n",
      "Epoch: 5 \tTraining Loss: 0.000220\n",
      "Epoch: 6 \tTraining Loss: 0.000220\n",
      "Epoch: 7 \tTraining Loss: 0.000220\n",
      "Epoch: 8 \tTraining Loss: 0.000220\n",
      "Epoch: 9 \tTraining Loss: 0.000220\n",
      "Epoch: 10 \tTraining Loss: 0.000220\n",
      "Epoch: 11 \tTraining Loss: 0.000220\n",
      "Epoch: 12 \tTraining Loss: 0.000220\n",
      "Epoch: 13 \tTraining Loss: 0.000220\n",
      "Epoch: 14 \tTraining Loss: 0.000220\n",
      "Epoch: 15 \tTraining Loss: 0.000220\n",
      "Epoch: 16 \tTraining Loss: 0.000220\n",
      "Epoch: 17 \tTraining Loss: 0.000220\n",
      "Epoch: 18 \tTraining Loss: 0.000220\n",
      "Epoch: 1 \tTraining Loss: 0.000308\n",
      "Epoch: 2 \tTraining Loss: 0.000308\n",
      "Epoch: 3 \tTraining Loss: 0.000308\n",
      "Epoch: 4 \tTraining Loss: 0.000308\n",
      "Epoch: 5 \tTraining Loss: 0.000308\n",
      "Epoch: 6 \tTraining Loss: 0.000308\n",
      "Epoch: 7 \tTraining Loss: 0.000308\n",
      "Epoch: 8 \tTraining Loss: 0.000308\n",
      "Epoch: 9 \tTraining Loss: 0.000308\n",
      "Epoch: 10 \tTraining Loss: 0.000308\n",
      "Epoch: 11 \tTraining Loss: 0.000308\n",
      "Epoch: 12 \tTraining Loss: 0.000308\n",
      "Epoch: 13 \tTraining Loss: 0.000308\n",
      "Epoch: 14 \tTraining Loss: 0.000308\n",
      "Epoch: 15 \tTraining Loss: 0.000308\n",
      "Epoch: 16 \tTraining Loss: 0.000308\n",
      "Epoch: 17 \tTraining Loss: 0.000308\n",
      "Epoch: 18 \tTraining Loss: 0.000308\n",
      "Epoch: 1 \tTraining Loss: 0.000208\n",
      "Epoch: 2 \tTraining Loss: 0.000208\n",
      "Epoch: 3 \tTraining Loss: 0.000208\n",
      "Epoch: 4 \tTraining Loss: 0.000208\n",
      "Epoch: 5 \tTraining Loss: 0.000208\n",
      "Epoch: 6 \tTraining Loss: 0.000208\n",
      "Epoch: 7 \tTraining Loss: 0.000208\n",
      "Epoch: 8 \tTraining Loss: 0.000208\n",
      "Epoch: 9 \tTraining Loss: 0.000208\n",
      "Epoch: 10 \tTraining Loss: 0.000208\n",
      "Epoch: 11 \tTraining Loss: 0.000208\n",
      "Epoch: 12 \tTraining Loss: 0.000208\n",
      "Epoch: 13 \tTraining Loss: 0.000208\n",
      "Epoch: 14 \tTraining Loss: 0.000208\n",
      "Epoch: 15 \tTraining Loss: 0.000208\n",
      "Epoch: 16 \tTraining Loss: 0.000208\n",
      "Epoch: 17 \tTraining Loss: 0.000208\n",
      "Epoch: 18 \tTraining Loss: 0.000208\n",
      "Epoch: 1 \tTraining Loss: 0.000155\n",
      "Epoch: 2 \tTraining Loss: 0.000155\n",
      "Epoch: 3 \tTraining Loss: 0.000155\n",
      "Epoch: 4 \tTraining Loss: 0.000155\n",
      "Epoch: 5 \tTraining Loss: 0.000155\n",
      "Epoch: 6 \tTraining Loss: 0.000155\n",
      "Epoch: 7 \tTraining Loss: 0.000155\n",
      "Epoch: 8 \tTraining Loss: 0.000155\n",
      "Epoch: 9 \tTraining Loss: 0.000155\n",
      "Epoch: 10 \tTraining Loss: 0.000155\n",
      "Epoch: 11 \tTraining Loss: 0.000155\n",
      "Epoch: 12 \tTraining Loss: 0.000155\n",
      "Epoch: 13 \tTraining Loss: 0.000155\n",
      "Epoch: 14 \tTraining Loss: 0.000155\n",
      "Epoch: 15 \tTraining Loss: 0.000155\n",
      "Epoch: 16 \tTraining Loss: 0.000155\n",
      "Epoch: 17 \tTraining Loss: 0.000155\n",
      "Epoch: 18 \tTraining Loss: 0.000155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000416\n",
      "Epoch: 2 \tTraining Loss: 0.000416\n",
      "Epoch: 3 \tTraining Loss: 0.000416\n",
      "Epoch: 4 \tTraining Loss: 0.000416\n",
      "Epoch: 5 \tTraining Loss: 0.000416\n",
      "Epoch: 6 \tTraining Loss: 0.000416\n",
      "Epoch: 7 \tTraining Loss: 0.000416\n",
      "Epoch: 8 \tTraining Loss: 0.000416\n",
      "Epoch: 9 \tTraining Loss: 0.000416\n",
      "Epoch: 10 \tTraining Loss: 0.000416\n",
      "Epoch: 11 \tTraining Loss: 0.000416\n",
      "Epoch: 12 \tTraining Loss: 0.000416\n",
      "Epoch: 13 \tTraining Loss: 0.000416\n",
      "Epoch: 14 \tTraining Loss: 0.000416\n",
      "Epoch: 15 \tTraining Loss: 0.000416\n",
      "Epoch: 16 \tTraining Loss: 0.000416\n",
      "Epoch: 17 \tTraining Loss: 0.000416\n",
      "Epoch: 18 \tTraining Loss: 0.000416\n",
      "Epoch: 1 \tTraining Loss: 0.000294\n",
      "Epoch: 2 \tTraining Loss: 0.000294\n",
      "Epoch: 3 \tTraining Loss: 0.000294\n",
      "Epoch: 4 \tTraining Loss: 0.000294\n",
      "Epoch: 5 \tTraining Loss: 0.000294\n",
      "Epoch: 6 \tTraining Loss: 0.000294\n",
      "Epoch: 7 \tTraining Loss: 0.000294\n",
      "Epoch: 8 \tTraining Loss: 0.000294\n",
      "Epoch: 9 \tTraining Loss: 0.000294\n",
      "Epoch: 10 \tTraining Loss: 0.000294\n",
      "Epoch: 11 \tTraining Loss: 0.000294\n",
      "Epoch: 12 \tTraining Loss: 0.000294\n",
      "Epoch: 13 \tTraining Loss: 0.000294\n",
      "Epoch: 14 \tTraining Loss: 0.000294\n",
      "Epoch: 15 \tTraining Loss: 0.000294\n",
      "Epoch: 16 \tTraining Loss: 0.000294\n",
      "Epoch: 17 \tTraining Loss: 0.000294\n",
      "Epoch: 18 \tTraining Loss: 0.000294\n",
      "Epoch: 1 \tTraining Loss: 0.000300\n",
      "Epoch: 2 \tTraining Loss: 0.000300\n",
      "Epoch: 3 \tTraining Loss: 0.000300\n",
      "Epoch: 4 \tTraining Loss: 0.000300\n",
      "Epoch: 5 \tTraining Loss: 0.000300\n",
      "Epoch: 6 \tTraining Loss: 0.000300\n",
      "Epoch: 7 \tTraining Loss: 0.000300\n",
      "Epoch: 8 \tTraining Loss: 0.000300\n",
      "Epoch: 9 \tTraining Loss: 0.000300\n",
      "Epoch: 10 \tTraining Loss: 0.000300\n",
      "Epoch: 11 \tTraining Loss: 0.000300\n",
      "Epoch: 12 \tTraining Loss: 0.000300\n",
      "Epoch: 13 \tTraining Loss: 0.000300\n",
      "Epoch: 14 \tTraining Loss: 0.000300\n",
      "Epoch: 15 \tTraining Loss: 0.000300\n",
      "Epoch: 16 \tTraining Loss: 0.000300\n",
      "Epoch: 17 \tTraining Loss: 0.000300\n",
      "Epoch: 18 \tTraining Loss: 0.000300\n",
      "Epoch: 1 \tTraining Loss: 0.000073\n",
      "Epoch: 2 \tTraining Loss: 0.000073\n",
      "Epoch: 3 \tTraining Loss: 0.000073\n",
      "Epoch: 4 \tTraining Loss: 0.000073\n",
      "Epoch: 5 \tTraining Loss: 0.000073\n",
      "Epoch: 6 \tTraining Loss: 0.000073\n",
      "Epoch: 7 \tTraining Loss: 0.000073\n",
      "Epoch: 8 \tTraining Loss: 0.000073\n",
      "Epoch: 9 \tTraining Loss: 0.000073\n",
      "Epoch: 10 \tTraining Loss: 0.000073\n",
      "Epoch: 11 \tTraining Loss: 0.000073\n",
      "Epoch: 12 \tTraining Loss: 0.000073\n",
      "Epoch: 13 \tTraining Loss: 0.000073\n",
      "Epoch: 14 \tTraining Loss: 0.000073\n",
      "Epoch: 15 \tTraining Loss: 0.000073\n",
      "Epoch: 16 \tTraining Loss: 0.000073\n",
      "Epoch: 17 \tTraining Loss: 0.000073\n",
      "Epoch: 18 \tTraining Loss: 0.000073\n",
      "Epoch: 1 \tTraining Loss: 0.000107\n",
      "Epoch: 2 \tTraining Loss: 0.000107\n",
      "Epoch: 3 \tTraining Loss: 0.000107\n",
      "Epoch: 4 \tTraining Loss: 0.000107\n",
      "Epoch: 5 \tTraining Loss: 0.000107\n",
      "Epoch: 6 \tTraining Loss: 0.000107\n",
      "Epoch: 7 \tTraining Loss: 0.000107\n",
      "Epoch: 8 \tTraining Loss: 0.000107\n",
      "Epoch: 9 \tTraining Loss: 0.000107\n",
      "Epoch: 10 \tTraining Loss: 0.000107\n",
      "Epoch: 11 \tTraining Loss: 0.000107\n",
      "Epoch: 12 \tTraining Loss: 0.000107\n",
      "Epoch: 13 \tTraining Loss: 0.000107\n",
      "Epoch: 14 \tTraining Loss: 0.000107\n",
      "Epoch: 15 \tTraining Loss: 0.000107\n",
      "Epoch: 16 \tTraining Loss: 0.000107\n",
      "Epoch: 17 \tTraining Loss: 0.000107\n",
      "Epoch: 18 \tTraining Loss: 0.000107\n",
      "Epoch: 1 \tTraining Loss: 0.000109\n",
      "Epoch: 2 \tTraining Loss: 0.000109\n",
      "Epoch: 3 \tTraining Loss: 0.000109\n",
      "Epoch: 4 \tTraining Loss: 0.000109\n",
      "Epoch: 5 \tTraining Loss: 0.000109\n",
      "Epoch: 6 \tTraining Loss: 0.000109\n",
      "Epoch: 7 \tTraining Loss: 0.000109\n",
      "Epoch: 8 \tTraining Loss: 0.000109\n",
      "Epoch: 9 \tTraining Loss: 0.000109\n",
      "Epoch: 10 \tTraining Loss: 0.000109\n",
      "Epoch: 11 \tTraining Loss: 0.000109\n",
      "Epoch: 12 \tTraining Loss: 0.000109\n",
      "Epoch: 13 \tTraining Loss: 0.000109\n",
      "Epoch: 14 \tTraining Loss: 0.000109\n",
      "Epoch: 15 \tTraining Loss: 0.000109\n",
      "Epoch: 16 \tTraining Loss: 0.000109\n",
      "Epoch: 17 \tTraining Loss: 0.000109\n",
      "Epoch: 18 \tTraining Loss: 0.000109\n",
      "Epoch: 1 \tTraining Loss: 0.000239\n",
      "Epoch: 2 \tTraining Loss: 0.000239\n",
      "Epoch: 3 \tTraining Loss: 0.000239\n",
      "Epoch: 4 \tTraining Loss: 0.000239\n",
      "Epoch: 5 \tTraining Loss: 0.000239\n",
      "Epoch: 6 \tTraining Loss: 0.000239\n",
      "Epoch: 7 \tTraining Loss: 0.000239\n",
      "Epoch: 8 \tTraining Loss: 0.000239\n",
      "Epoch: 9 \tTraining Loss: 0.000239\n",
      "Epoch: 10 \tTraining Loss: 0.000239\n",
      "Epoch: 11 \tTraining Loss: 0.000239\n",
      "Epoch: 12 \tTraining Loss: 0.000239\n",
      "Epoch: 13 \tTraining Loss: 0.000239\n",
      "Epoch: 14 \tTraining Loss: 0.000239\n",
      "Epoch: 15 \tTraining Loss: 0.000239\n",
      "Epoch: 16 \tTraining Loss: 0.000239\n",
      "Epoch: 17 \tTraining Loss: 0.000239\n",
      "Epoch: 18 \tTraining Loss: 0.000239\n",
      "Epoch: 1 \tTraining Loss: 0.000212\n",
      "Epoch: 2 \tTraining Loss: 0.000212\n",
      "Epoch: 3 \tTraining Loss: 0.000212\n",
      "Epoch: 4 \tTraining Loss: 0.000212\n",
      "Epoch: 5 \tTraining Loss: 0.000212\n",
      "Epoch: 6 \tTraining Loss: 0.000212\n",
      "Epoch: 7 \tTraining Loss: 0.000212\n",
      "Epoch: 8 \tTraining Loss: 0.000212\n",
      "Epoch: 9 \tTraining Loss: 0.000212\n",
      "Epoch: 10 \tTraining Loss: 0.000212\n",
      "Epoch: 11 \tTraining Loss: 0.000212\n",
      "Epoch: 12 \tTraining Loss: 0.000212\n",
      "Epoch: 13 \tTraining Loss: 0.000212\n",
      "Epoch: 14 \tTraining Loss: 0.000212\n",
      "Epoch: 15 \tTraining Loss: 0.000212\n",
      "Epoch: 16 \tTraining Loss: 0.000212\n",
      "Epoch: 17 \tTraining Loss: 0.000212\n",
      "Epoch: 18 \tTraining Loss: 0.000212\n",
      "Epoch: 1 \tTraining Loss: 0.000150\n",
      "Epoch: 2 \tTraining Loss: 0.000150\n",
      "Epoch: 3 \tTraining Loss: 0.000150\n",
      "Epoch: 4 \tTraining Loss: 0.000150\n",
      "Epoch: 5 \tTraining Loss: 0.000150\n",
      "Epoch: 6 \tTraining Loss: 0.000150\n",
      "Epoch: 7 \tTraining Loss: 0.000150\n",
      "Epoch: 8 \tTraining Loss: 0.000150\n",
      "Epoch: 9 \tTraining Loss: 0.000150\n",
      "Epoch: 10 \tTraining Loss: 0.000150\n",
      "Epoch: 11 \tTraining Loss: 0.000150\n",
      "Epoch: 12 \tTraining Loss: 0.000150\n",
      "Epoch: 13 \tTraining Loss: 0.000150\n",
      "Epoch: 14 \tTraining Loss: 0.000150\n",
      "Epoch: 15 \tTraining Loss: 0.000150\n",
      "Epoch: 16 \tTraining Loss: 0.000150\n",
      "Epoch: 17 \tTraining Loss: 0.000150\n",
      "Epoch: 18 \tTraining Loss: 0.000150\n",
      "Epoch: 1 \tTraining Loss: 0.000125\n",
      "Epoch: 2 \tTraining Loss: 0.000125\n",
      "Epoch: 3 \tTraining Loss: 0.000125\n",
      "Epoch: 4 \tTraining Loss: 0.000125\n",
      "Epoch: 5 \tTraining Loss: 0.000125\n",
      "Epoch: 6 \tTraining Loss: 0.000125\n",
      "Epoch: 7 \tTraining Loss: 0.000125\n",
      "Epoch: 8 \tTraining Loss: 0.000125\n",
      "Epoch: 9 \tTraining Loss: 0.000125\n",
      "Epoch: 10 \tTraining Loss: 0.000125\n",
      "Epoch: 11 \tTraining Loss: 0.000125\n",
      "Epoch: 12 \tTraining Loss: 0.000125\n",
      "Epoch: 13 \tTraining Loss: 0.000125\n",
      "Epoch: 14 \tTraining Loss: 0.000125\n",
      "Epoch: 15 \tTraining Loss: 0.000125\n",
      "Epoch: 16 \tTraining Loss: 0.000125\n",
      "Epoch: 17 \tTraining Loss: 0.000125\n",
      "Epoch: 18 \tTraining Loss: 0.000125\n",
      "Epoch: 1 \tTraining Loss: 0.000155\n",
      "Epoch: 2 \tTraining Loss: 0.000155\n",
      "Epoch: 3 \tTraining Loss: 0.000155\n",
      "Epoch: 4 \tTraining Loss: 0.000155\n",
      "Epoch: 5 \tTraining Loss: 0.000155\n",
      "Epoch: 6 \tTraining Loss: 0.000155\n",
      "Epoch: 7 \tTraining Loss: 0.000155\n",
      "Epoch: 8 \tTraining Loss: 0.000155\n",
      "Epoch: 9 \tTraining Loss: 0.000155\n",
      "Epoch: 10 \tTraining Loss: 0.000155\n",
      "Epoch: 11 \tTraining Loss: 0.000155\n",
      "Epoch: 12 \tTraining Loss: 0.000155\n",
      "Epoch: 13 \tTraining Loss: 0.000155\n",
      "Epoch: 14 \tTraining Loss: 0.000155\n",
      "Epoch: 15 \tTraining Loss: 0.000155\n",
      "Epoch: 16 \tTraining Loss: 0.000155\n",
      "Epoch: 17 \tTraining Loss: 0.000155\n",
      "Epoch: 18 \tTraining Loss: 0.000155\n",
      "Epoch: 1 \tTraining Loss: 0.000090\n",
      "Epoch: 2 \tTraining Loss: 0.000090\n",
      "Epoch: 3 \tTraining Loss: 0.000090\n",
      "Epoch: 4 \tTraining Loss: 0.000090\n",
      "Epoch: 5 \tTraining Loss: 0.000090\n",
      "Epoch: 6 \tTraining Loss: 0.000090\n",
      "Epoch: 7 \tTraining Loss: 0.000090\n",
      "Epoch: 8 \tTraining Loss: 0.000090\n",
      "Epoch: 9 \tTraining Loss: 0.000090\n",
      "Epoch: 10 \tTraining Loss: 0.000090\n",
      "Epoch: 11 \tTraining Loss: 0.000090\n",
      "Epoch: 12 \tTraining Loss: 0.000090\n",
      "Epoch: 13 \tTraining Loss: 0.000090\n",
      "Epoch: 14 \tTraining Loss: 0.000090\n",
      "Epoch: 15 \tTraining Loss: 0.000090\n",
      "Epoch: 16 \tTraining Loss: 0.000090\n",
      "Epoch: 17 \tTraining Loss: 0.000090\n",
      "Epoch: 18 \tTraining Loss: 0.000090\n",
      "Epoch: 1 \tTraining Loss: 0.000247\n",
      "Epoch: 2 \tTraining Loss: 0.000247\n",
      "Epoch: 3 \tTraining Loss: 0.000247\n",
      "Epoch: 4 \tTraining Loss: 0.000247\n",
      "Epoch: 5 \tTraining Loss: 0.000247\n",
      "Epoch: 6 \tTraining Loss: 0.000247\n",
      "Epoch: 7 \tTraining Loss: 0.000247\n",
      "Epoch: 8 \tTraining Loss: 0.000247\n",
      "Epoch: 9 \tTraining Loss: 0.000247\n",
      "Epoch: 10 \tTraining Loss: 0.000247\n",
      "Epoch: 11 \tTraining Loss: 0.000247\n",
      "Epoch: 12 \tTraining Loss: 0.000247\n",
      "Epoch: 13 \tTraining Loss: 0.000247\n",
      "Epoch: 14 \tTraining Loss: 0.000247\n",
      "Epoch: 15 \tTraining Loss: 0.000247\n",
      "Epoch: 16 \tTraining Loss: 0.000247\n",
      "Epoch: 17 \tTraining Loss: 0.000247\n",
      "Epoch: 18 \tTraining Loss: 0.000247\n",
      "Epoch: 1 \tTraining Loss: 0.000212\n",
      "Epoch: 2 \tTraining Loss: 0.000212\n",
      "Epoch: 3 \tTraining Loss: 0.000212\n",
      "Epoch: 4 \tTraining Loss: 0.000212\n",
      "Epoch: 5 \tTraining Loss: 0.000212\n",
      "Epoch: 6 \tTraining Loss: 0.000212\n",
      "Epoch: 7 \tTraining Loss: 0.000212\n",
      "Epoch: 8 \tTraining Loss: 0.000212\n",
      "Epoch: 9 \tTraining Loss: 0.000212\n",
      "Epoch: 10 \tTraining Loss: 0.000212\n",
      "Epoch: 11 \tTraining Loss: 0.000212\n",
      "Epoch: 12 \tTraining Loss: 0.000212\n",
      "Epoch: 13 \tTraining Loss: 0.000212\n",
      "Epoch: 14 \tTraining Loss: 0.000212\n",
      "Epoch: 15 \tTraining Loss: 0.000212\n",
      "Epoch: 16 \tTraining Loss: 0.000212\n",
      "Epoch: 17 \tTraining Loss: 0.000212\n",
      "Epoch: 18 \tTraining Loss: 0.000212\n",
      "Epoch: 1 \tTraining Loss: 0.000234\n",
      "Epoch: 2 \tTraining Loss: 0.000234\n",
      "Epoch: 3 \tTraining Loss: 0.000234\n",
      "Epoch: 4 \tTraining Loss: 0.000234\n",
      "Epoch: 5 \tTraining Loss: 0.000234\n",
      "Epoch: 6 \tTraining Loss: 0.000234\n",
      "Epoch: 7 \tTraining Loss: 0.000234\n",
      "Epoch: 8 \tTraining Loss: 0.000234\n",
      "Epoch: 9 \tTraining Loss: 0.000234\n",
      "Epoch: 10 \tTraining Loss: 0.000234\n",
      "Epoch: 11 \tTraining Loss: 0.000234\n",
      "Epoch: 12 \tTraining Loss: 0.000234\n",
      "Epoch: 13 \tTraining Loss: 0.000234\n",
      "Epoch: 14 \tTraining Loss: 0.000234\n",
      "Epoch: 15 \tTraining Loss: 0.000234\n",
      "Epoch: 16 \tTraining Loss: 0.000234\n",
      "Epoch: 17 \tTraining Loss: 0.000234\n",
      "Epoch: 18 \tTraining Loss: 0.000234\n",
      "Epoch: 1 \tTraining Loss: 0.000121\n",
      "Epoch: 2 \tTraining Loss: 0.000121\n",
      "Epoch: 3 \tTraining Loss: 0.000121\n",
      "Epoch: 4 \tTraining Loss: 0.000121\n",
      "Epoch: 5 \tTraining Loss: 0.000121\n",
      "Epoch: 6 \tTraining Loss: 0.000121\n",
      "Epoch: 7 \tTraining Loss: 0.000121\n",
      "Epoch: 8 \tTraining Loss: 0.000121\n",
      "Epoch: 9 \tTraining Loss: 0.000121\n",
      "Epoch: 10 \tTraining Loss: 0.000121\n",
      "Epoch: 11 \tTraining Loss: 0.000121\n",
      "Epoch: 12 \tTraining Loss: 0.000121\n",
      "Epoch: 13 \tTraining Loss: 0.000121\n",
      "Epoch: 14 \tTraining Loss: 0.000121\n",
      "Epoch: 15 \tTraining Loss: 0.000121\n",
      "Epoch: 16 \tTraining Loss: 0.000121\n",
      "Epoch: 17 \tTraining Loss: 0.000121\n",
      "Epoch: 18 \tTraining Loss: 0.000121\n",
      "Epoch: 1 \tTraining Loss: 0.000455\n",
      "Epoch: 2 \tTraining Loss: 0.000455\n",
      "Epoch: 3 \tTraining Loss: 0.000455\n",
      "Epoch: 4 \tTraining Loss: 0.000455\n",
      "Epoch: 5 \tTraining Loss: 0.000455\n",
      "Epoch: 6 \tTraining Loss: 0.000455\n",
      "Epoch: 7 \tTraining Loss: 0.000455\n",
      "Epoch: 8 \tTraining Loss: 0.000455\n",
      "Epoch: 9 \tTraining Loss: 0.000455\n",
      "Epoch: 10 \tTraining Loss: 0.000455\n",
      "Epoch: 11 \tTraining Loss: 0.000455\n",
      "Epoch: 12 \tTraining Loss: 0.000455\n",
      "Epoch: 13 \tTraining Loss: 0.000455\n",
      "Epoch: 14 \tTraining Loss: 0.000455\n",
      "Epoch: 15 \tTraining Loss: 0.000455\n",
      "Epoch: 16 \tTraining Loss: 0.000455\n",
      "Epoch: 17 \tTraining Loss: 0.000455\n",
      "Epoch: 18 \tTraining Loss: 0.000455\n",
      "Epoch: 1 \tTraining Loss: 0.000176\n",
      "Epoch: 2 \tTraining Loss: 0.000176\n",
      "Epoch: 3 \tTraining Loss: 0.000176\n",
      "Epoch: 4 \tTraining Loss: 0.000176\n",
      "Epoch: 5 \tTraining Loss: 0.000176\n",
      "Epoch: 6 \tTraining Loss: 0.000176\n",
      "Epoch: 7 \tTraining Loss: 0.000176\n",
      "Epoch: 8 \tTraining Loss: 0.000176\n",
      "Epoch: 9 \tTraining Loss: 0.000176\n",
      "Epoch: 10 \tTraining Loss: 0.000176\n",
      "Epoch: 11 \tTraining Loss: 0.000176\n",
      "Epoch: 12 \tTraining Loss: 0.000176\n",
      "Epoch: 13 \tTraining Loss: 0.000176\n",
      "Epoch: 14 \tTraining Loss: 0.000176\n",
      "Epoch: 15 \tTraining Loss: 0.000176\n",
      "Epoch: 16 \tTraining Loss: 0.000176\n",
      "Epoch: 17 \tTraining Loss: 0.000176\n",
      "Epoch: 18 \tTraining Loss: 0.000176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000206\n",
      "Epoch: 2 \tTraining Loss: 0.000206\n",
      "Epoch: 3 \tTraining Loss: 0.000206\n",
      "Epoch: 4 \tTraining Loss: 0.000206\n",
      "Epoch: 5 \tTraining Loss: 0.000206\n",
      "Epoch: 6 \tTraining Loss: 0.000206\n",
      "Epoch: 7 \tTraining Loss: 0.000206\n",
      "Epoch: 8 \tTraining Loss: 0.000206\n",
      "Epoch: 9 \tTraining Loss: 0.000206\n",
      "Epoch: 10 \tTraining Loss: 0.000206\n",
      "Epoch: 11 \tTraining Loss: 0.000206\n",
      "Epoch: 12 \tTraining Loss: 0.000206\n",
      "Epoch: 13 \tTraining Loss: 0.000206\n",
      "Epoch: 14 \tTraining Loss: 0.000206\n",
      "Epoch: 15 \tTraining Loss: 0.000206\n",
      "Epoch: 16 \tTraining Loss: 0.000206\n",
      "Epoch: 17 \tTraining Loss: 0.000206\n",
      "Epoch: 18 \tTraining Loss: 0.000206\n",
      "Epoch: 1 \tTraining Loss: 0.000111\n",
      "Epoch: 2 \tTraining Loss: 0.000111\n",
      "Epoch: 3 \tTraining Loss: 0.000111\n",
      "Epoch: 4 \tTraining Loss: 0.000111\n",
      "Epoch: 5 \tTraining Loss: 0.000111\n",
      "Epoch: 6 \tTraining Loss: 0.000111\n",
      "Epoch: 7 \tTraining Loss: 0.000111\n",
      "Epoch: 8 \tTraining Loss: 0.000111\n",
      "Epoch: 9 \tTraining Loss: 0.000111\n",
      "Epoch: 10 \tTraining Loss: 0.000111\n",
      "Epoch: 11 \tTraining Loss: 0.000111\n",
      "Epoch: 12 \tTraining Loss: 0.000111\n",
      "Epoch: 13 \tTraining Loss: 0.000111\n",
      "Epoch: 14 \tTraining Loss: 0.000111\n",
      "Epoch: 15 \tTraining Loss: 0.000111\n",
      "Epoch: 16 \tTraining Loss: 0.000111\n",
      "Epoch: 17 \tTraining Loss: 0.000111\n",
      "Epoch: 18 \tTraining Loss: 0.000111\n",
      "Epoch: 1 \tTraining Loss: 0.000124\n",
      "Epoch: 2 \tTraining Loss: 0.000124\n",
      "Epoch: 3 \tTraining Loss: 0.000124\n",
      "Epoch: 4 \tTraining Loss: 0.000124\n",
      "Epoch: 5 \tTraining Loss: 0.000124\n",
      "Epoch: 6 \tTraining Loss: 0.000124\n",
      "Epoch: 7 \tTraining Loss: 0.000124\n",
      "Epoch: 8 \tTraining Loss: 0.000124\n",
      "Epoch: 9 \tTraining Loss: 0.000124\n",
      "Epoch: 10 \tTraining Loss: 0.000124\n",
      "Epoch: 11 \tTraining Loss: 0.000124\n",
      "Epoch: 12 \tTraining Loss: 0.000124\n",
      "Epoch: 13 \tTraining Loss: 0.000124\n",
      "Epoch: 14 \tTraining Loss: 0.000124\n",
      "Epoch: 15 \tTraining Loss: 0.000124\n",
      "Epoch: 16 \tTraining Loss: 0.000124\n",
      "Epoch: 17 \tTraining Loss: 0.000124\n",
      "Epoch: 18 \tTraining Loss: 0.000124\n",
      "Epoch: 1 \tTraining Loss: 0.000160\n",
      "Epoch: 2 \tTraining Loss: 0.000160\n",
      "Epoch: 3 \tTraining Loss: 0.000160\n",
      "Epoch: 4 \tTraining Loss: 0.000160\n",
      "Epoch: 5 \tTraining Loss: 0.000160\n",
      "Epoch: 6 \tTraining Loss: 0.000160\n",
      "Epoch: 7 \tTraining Loss: 0.000160\n",
      "Epoch: 8 \tTraining Loss: 0.000160\n",
      "Epoch: 9 \tTraining Loss: 0.000160\n",
      "Epoch: 10 \tTraining Loss: 0.000160\n",
      "Epoch: 11 \tTraining Loss: 0.000160\n",
      "Epoch: 12 \tTraining Loss: 0.000160\n",
      "Epoch: 13 \tTraining Loss: 0.000160\n",
      "Epoch: 14 \tTraining Loss: 0.000160\n",
      "Epoch: 15 \tTraining Loss: 0.000160\n",
      "Epoch: 16 \tTraining Loss: 0.000160\n",
      "Epoch: 17 \tTraining Loss: 0.000160\n",
      "Epoch: 18 \tTraining Loss: 0.000160\n",
      "Epoch: 1 \tTraining Loss: 0.000174\n",
      "Epoch: 2 \tTraining Loss: 0.000174\n",
      "Epoch: 3 \tTraining Loss: 0.000174\n",
      "Epoch: 4 \tTraining Loss: 0.000174\n",
      "Epoch: 5 \tTraining Loss: 0.000174\n",
      "Epoch: 6 \tTraining Loss: 0.000174\n",
      "Epoch: 7 \tTraining Loss: 0.000174\n",
      "Epoch: 8 \tTraining Loss: 0.000174\n",
      "Epoch: 9 \tTraining Loss: 0.000174\n",
      "Epoch: 10 \tTraining Loss: 0.000174\n",
      "Epoch: 11 \tTraining Loss: 0.000174\n",
      "Epoch: 12 \tTraining Loss: 0.000174\n",
      "Epoch: 13 \tTraining Loss: 0.000174\n",
      "Epoch: 14 \tTraining Loss: 0.000174\n",
      "Epoch: 15 \tTraining Loss: 0.000174\n",
      "Epoch: 16 \tTraining Loss: 0.000174\n",
      "Epoch: 17 \tTraining Loss: 0.000174\n",
      "Epoch: 18 \tTraining Loss: 0.000174\n",
      "Epoch: 1 \tTraining Loss: 0.000319\n",
      "Epoch: 2 \tTraining Loss: 0.000319\n",
      "Epoch: 3 \tTraining Loss: 0.000319\n",
      "Epoch: 4 \tTraining Loss: 0.000319\n",
      "Epoch: 5 \tTraining Loss: 0.000319\n",
      "Epoch: 6 \tTraining Loss: 0.000319\n",
      "Epoch: 7 \tTraining Loss: 0.000319\n",
      "Epoch: 8 \tTraining Loss: 0.000319\n",
      "Epoch: 9 \tTraining Loss: 0.000319\n",
      "Epoch: 10 \tTraining Loss: 0.000319\n",
      "Epoch: 11 \tTraining Loss: 0.000319\n",
      "Epoch: 12 \tTraining Loss: 0.000319\n",
      "Epoch: 13 \tTraining Loss: 0.000319\n",
      "Epoch: 14 \tTraining Loss: 0.000319\n",
      "Epoch: 15 \tTraining Loss: 0.000319\n",
      "Epoch: 16 \tTraining Loss: 0.000319\n",
      "Epoch: 17 \tTraining Loss: 0.000319\n",
      "Epoch: 18 \tTraining Loss: 0.000319\n",
      "Epoch: 1 \tTraining Loss: 0.000115\n",
      "Epoch: 2 \tTraining Loss: 0.000115\n",
      "Epoch: 3 \tTraining Loss: 0.000115\n",
      "Epoch: 4 \tTraining Loss: 0.000115\n",
      "Epoch: 5 \tTraining Loss: 0.000115\n",
      "Epoch: 6 \tTraining Loss: 0.000115\n",
      "Epoch: 7 \tTraining Loss: 0.000115\n",
      "Epoch: 8 \tTraining Loss: 0.000115\n",
      "Epoch: 9 \tTraining Loss: 0.000115\n",
      "Epoch: 10 \tTraining Loss: 0.000115\n",
      "Epoch: 11 \tTraining Loss: 0.000115\n",
      "Epoch: 12 \tTraining Loss: 0.000115\n",
      "Epoch: 13 \tTraining Loss: 0.000115\n",
      "Epoch: 14 \tTraining Loss: 0.000115\n",
      "Epoch: 15 \tTraining Loss: 0.000115\n",
      "Epoch: 16 \tTraining Loss: 0.000115\n",
      "Epoch: 17 \tTraining Loss: 0.000115\n",
      "Epoch: 18 \tTraining Loss: 0.000115\n",
      "Epoch: 1 \tTraining Loss: 0.000170\n",
      "Epoch: 2 \tTraining Loss: 0.000170\n",
      "Epoch: 3 \tTraining Loss: 0.000170\n",
      "Epoch: 4 \tTraining Loss: 0.000170\n",
      "Epoch: 5 \tTraining Loss: 0.000170\n",
      "Epoch: 6 \tTraining Loss: 0.000170\n",
      "Epoch: 7 \tTraining Loss: 0.000170\n",
      "Epoch: 8 \tTraining Loss: 0.000170\n",
      "Epoch: 9 \tTraining Loss: 0.000170\n",
      "Epoch: 10 \tTraining Loss: 0.000170\n",
      "Epoch: 11 \tTraining Loss: 0.000170\n",
      "Epoch: 12 \tTraining Loss: 0.000170\n",
      "Epoch: 13 \tTraining Loss: 0.000170\n",
      "Epoch: 14 \tTraining Loss: 0.000170\n",
      "Epoch: 15 \tTraining Loss: 0.000170\n",
      "Epoch: 16 \tTraining Loss: 0.000170\n",
      "Epoch: 17 \tTraining Loss: 0.000170\n",
      "Epoch: 18 \tTraining Loss: 0.000170\n",
      "Epoch: 1 \tTraining Loss: 0.000081\n",
      "Epoch: 2 \tTraining Loss: 0.000081\n",
      "Epoch: 3 \tTraining Loss: 0.000081\n",
      "Epoch: 4 \tTraining Loss: 0.000081\n",
      "Epoch: 5 \tTraining Loss: 0.000081\n",
      "Epoch: 6 \tTraining Loss: 0.000081\n",
      "Epoch: 7 \tTraining Loss: 0.000081\n",
      "Epoch: 8 \tTraining Loss: 0.000081\n",
      "Epoch: 9 \tTraining Loss: 0.000081\n",
      "Epoch: 10 \tTraining Loss: 0.000081\n",
      "Epoch: 11 \tTraining Loss: 0.000081\n",
      "Epoch: 12 \tTraining Loss: 0.000081\n",
      "Epoch: 13 \tTraining Loss: 0.000081\n",
      "Epoch: 14 \tTraining Loss: 0.000081\n",
      "Epoch: 15 \tTraining Loss: 0.000081\n",
      "Epoch: 16 \tTraining Loss: 0.000081\n",
      "Epoch: 17 \tTraining Loss: 0.000081\n",
      "Epoch: 18 \tTraining Loss: 0.000081\n",
      "Epoch: 1 \tTraining Loss: 0.000189\n",
      "Epoch: 2 \tTraining Loss: 0.000189\n",
      "Epoch: 3 \tTraining Loss: 0.000189\n",
      "Epoch: 4 \tTraining Loss: 0.000189\n",
      "Epoch: 5 \tTraining Loss: 0.000189\n",
      "Epoch: 6 \tTraining Loss: 0.000189\n",
      "Epoch: 7 \tTraining Loss: 0.000189\n",
      "Epoch: 8 \tTraining Loss: 0.000189\n",
      "Epoch: 9 \tTraining Loss: 0.000189\n",
      "Epoch: 10 \tTraining Loss: 0.000189\n",
      "Epoch: 11 \tTraining Loss: 0.000189\n",
      "Epoch: 12 \tTraining Loss: 0.000189\n",
      "Epoch: 13 \tTraining Loss: 0.000189\n",
      "Epoch: 14 \tTraining Loss: 0.000189\n",
      "Epoch: 15 \tTraining Loss: 0.000189\n",
      "Epoch: 16 \tTraining Loss: 0.000189\n",
      "Epoch: 17 \tTraining Loss: 0.000189\n",
      "Epoch: 18 \tTraining Loss: 0.000189\n",
      "Epoch: 1 \tTraining Loss: 0.000284\n",
      "Epoch: 2 \tTraining Loss: 0.000284\n",
      "Epoch: 3 \tTraining Loss: 0.000284\n",
      "Epoch: 4 \tTraining Loss: 0.000284\n",
      "Epoch: 5 \tTraining Loss: 0.000284\n",
      "Epoch: 6 \tTraining Loss: 0.000284\n",
      "Epoch: 7 \tTraining Loss: 0.000284\n",
      "Epoch: 8 \tTraining Loss: 0.000284\n",
      "Epoch: 9 \tTraining Loss: 0.000284\n",
      "Epoch: 10 \tTraining Loss: 0.000284\n",
      "Epoch: 11 \tTraining Loss: 0.000284\n",
      "Epoch: 12 \tTraining Loss: 0.000284\n",
      "Epoch: 13 \tTraining Loss: 0.000284\n",
      "Epoch: 14 \tTraining Loss: 0.000284\n",
      "Epoch: 15 \tTraining Loss: 0.000284\n",
      "Epoch: 16 \tTraining Loss: 0.000284\n",
      "Epoch: 17 \tTraining Loss: 0.000284\n",
      "Epoch: 18 \tTraining Loss: 0.000284\n",
      "Epoch: 1 \tTraining Loss: 0.000102\n",
      "Epoch: 2 \tTraining Loss: 0.000102\n",
      "Epoch: 3 \tTraining Loss: 0.000102\n",
      "Epoch: 4 \tTraining Loss: 0.000102\n",
      "Epoch: 5 \tTraining Loss: 0.000102\n",
      "Epoch: 6 \tTraining Loss: 0.000102\n",
      "Epoch: 7 \tTraining Loss: 0.000102\n",
      "Epoch: 8 \tTraining Loss: 0.000102\n",
      "Epoch: 9 \tTraining Loss: 0.000102\n",
      "Epoch: 10 \tTraining Loss: 0.000102\n",
      "Epoch: 11 \tTraining Loss: 0.000102\n",
      "Epoch: 12 \tTraining Loss: 0.000102\n",
      "Epoch: 13 \tTraining Loss: 0.000102\n",
      "Epoch: 14 \tTraining Loss: 0.000102\n",
      "Epoch: 15 \tTraining Loss: 0.000102\n",
      "Epoch: 16 \tTraining Loss: 0.000102\n",
      "Epoch: 17 \tTraining Loss: 0.000102\n",
      "Epoch: 18 \tTraining Loss: 0.000102\n",
      "Epoch: 1 \tTraining Loss: 0.000228\n",
      "Epoch: 2 \tTraining Loss: 0.000228\n",
      "Epoch: 3 \tTraining Loss: 0.000228\n",
      "Epoch: 4 \tTraining Loss: 0.000228\n",
      "Epoch: 5 \tTraining Loss: 0.000228\n",
      "Epoch: 6 \tTraining Loss: 0.000228\n",
      "Epoch: 7 \tTraining Loss: 0.000228\n",
      "Epoch: 8 \tTraining Loss: 0.000228\n",
      "Epoch: 9 \tTraining Loss: 0.000228\n",
      "Epoch: 10 \tTraining Loss: 0.000228\n",
      "Epoch: 11 \tTraining Loss: 0.000228\n",
      "Epoch: 12 \tTraining Loss: 0.000228\n",
      "Epoch: 13 \tTraining Loss: 0.000228\n",
      "Epoch: 14 \tTraining Loss: 0.000228\n",
      "Epoch: 15 \tTraining Loss: 0.000228\n",
      "Epoch: 16 \tTraining Loss: 0.000228\n",
      "Epoch: 17 \tTraining Loss: 0.000228\n",
      "Epoch: 18 \tTraining Loss: 0.000228\n",
      "Epoch: 1 \tTraining Loss: 0.000114\n",
      "Epoch: 2 \tTraining Loss: 0.000114\n",
      "Epoch: 3 \tTraining Loss: 0.000114\n",
      "Epoch: 4 \tTraining Loss: 0.000114\n",
      "Epoch: 5 \tTraining Loss: 0.000114\n",
      "Epoch: 6 \tTraining Loss: 0.000114\n",
      "Epoch: 7 \tTraining Loss: 0.000114\n",
      "Epoch: 8 \tTraining Loss: 0.000114\n",
      "Epoch: 9 \tTraining Loss: 0.000114\n",
      "Epoch: 10 \tTraining Loss: 0.000114\n",
      "Epoch: 11 \tTraining Loss: 0.000114\n",
      "Epoch: 12 \tTraining Loss: 0.000114\n",
      "Epoch: 13 \tTraining Loss: 0.000114\n",
      "Epoch: 14 \tTraining Loss: 0.000114\n",
      "Epoch: 15 \tTraining Loss: 0.000114\n",
      "Epoch: 16 \tTraining Loss: 0.000114\n",
      "Epoch: 17 \tTraining Loss: 0.000114\n",
      "Epoch: 18 \tTraining Loss: 0.000114\n",
      "Epoch: 1 \tTraining Loss: 0.000128\n",
      "Epoch: 2 \tTraining Loss: 0.000128\n",
      "Epoch: 3 \tTraining Loss: 0.000128\n",
      "Epoch: 4 \tTraining Loss: 0.000128\n",
      "Epoch: 5 \tTraining Loss: 0.000128\n",
      "Epoch: 6 \tTraining Loss: 0.000128\n",
      "Epoch: 7 \tTraining Loss: 0.000128\n",
      "Epoch: 8 \tTraining Loss: 0.000128\n",
      "Epoch: 9 \tTraining Loss: 0.000128\n",
      "Epoch: 10 \tTraining Loss: 0.000128\n",
      "Epoch: 11 \tTraining Loss: 0.000128\n",
      "Epoch: 12 \tTraining Loss: 0.000128\n",
      "Epoch: 13 \tTraining Loss: 0.000128\n",
      "Epoch: 14 \tTraining Loss: 0.000128\n",
      "Epoch: 15 \tTraining Loss: 0.000128\n",
      "Epoch: 16 \tTraining Loss: 0.000128\n",
      "Epoch: 17 \tTraining Loss: 0.000128\n",
      "Epoch: 18 \tTraining Loss: 0.000128\n",
      "Epoch: 1 \tTraining Loss: 0.000195\n",
      "Epoch: 2 \tTraining Loss: 0.000195\n",
      "Epoch: 3 \tTraining Loss: 0.000195\n",
      "Epoch: 4 \tTraining Loss: 0.000195\n",
      "Epoch: 5 \tTraining Loss: 0.000195\n",
      "Epoch: 6 \tTraining Loss: 0.000195\n",
      "Epoch: 7 \tTraining Loss: 0.000195\n",
      "Epoch: 8 \tTraining Loss: 0.000195\n",
      "Epoch: 9 \tTraining Loss: 0.000195\n",
      "Epoch: 10 \tTraining Loss: 0.000195\n",
      "Epoch: 11 \tTraining Loss: 0.000195\n",
      "Epoch: 12 \tTraining Loss: 0.000195\n",
      "Epoch: 13 \tTraining Loss: 0.000195\n",
      "Epoch: 14 \tTraining Loss: 0.000195\n",
      "Epoch: 15 \tTraining Loss: 0.000195\n",
      "Epoch: 16 \tTraining Loss: 0.000195\n",
      "Epoch: 17 \tTraining Loss: 0.000195\n",
      "Epoch: 18 \tTraining Loss: 0.000195\n",
      "Epoch: 1 \tTraining Loss: 0.000121\n",
      "Epoch: 2 \tTraining Loss: 0.000121\n",
      "Epoch: 3 \tTraining Loss: 0.000121\n",
      "Epoch: 4 \tTraining Loss: 0.000121\n",
      "Epoch: 5 \tTraining Loss: 0.000121\n",
      "Epoch: 6 \tTraining Loss: 0.000121\n",
      "Epoch: 7 \tTraining Loss: 0.000121\n",
      "Epoch: 8 \tTraining Loss: 0.000121\n",
      "Epoch: 9 \tTraining Loss: 0.000121\n",
      "Epoch: 10 \tTraining Loss: 0.000121\n",
      "Epoch: 11 \tTraining Loss: 0.000121\n",
      "Epoch: 12 \tTraining Loss: 0.000121\n",
      "Epoch: 13 \tTraining Loss: 0.000121\n",
      "Epoch: 14 \tTraining Loss: 0.000121\n",
      "Epoch: 15 \tTraining Loss: 0.000121\n",
      "Epoch: 16 \tTraining Loss: 0.000121\n",
      "Epoch: 17 \tTraining Loss: 0.000121\n",
      "Epoch: 18 \tTraining Loss: 0.000121\n",
      "Epoch: 1 \tTraining Loss: 0.000063\n",
      "Epoch: 2 \tTraining Loss: 0.000063\n",
      "Epoch: 3 \tTraining Loss: 0.000063\n",
      "Epoch: 4 \tTraining Loss: 0.000063\n",
      "Epoch: 5 \tTraining Loss: 0.000063\n",
      "Epoch: 6 \tTraining Loss: 0.000063\n",
      "Epoch: 7 \tTraining Loss: 0.000063\n",
      "Epoch: 8 \tTraining Loss: 0.000063\n",
      "Epoch: 9 \tTraining Loss: 0.000063\n",
      "Epoch: 10 \tTraining Loss: 0.000063\n",
      "Epoch: 11 \tTraining Loss: 0.000063\n",
      "Epoch: 12 \tTraining Loss: 0.000063\n",
      "Epoch: 13 \tTraining Loss: 0.000063\n",
      "Epoch: 14 \tTraining Loss: 0.000063\n",
      "Epoch: 15 \tTraining Loss: 0.000063\n",
      "Epoch: 16 \tTraining Loss: 0.000063\n",
      "Epoch: 17 \tTraining Loss: 0.000063\n",
      "Epoch: 18 \tTraining Loss: 0.000063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000196\n",
      "Epoch: 2 \tTraining Loss: 0.000196\n",
      "Epoch: 3 \tTraining Loss: 0.000196\n",
      "Epoch: 4 \tTraining Loss: 0.000196\n",
      "Epoch: 5 \tTraining Loss: 0.000196\n",
      "Epoch: 6 \tTraining Loss: 0.000196\n",
      "Epoch: 7 \tTraining Loss: 0.000196\n",
      "Epoch: 8 \tTraining Loss: 0.000196\n",
      "Epoch: 9 \tTraining Loss: 0.000196\n",
      "Epoch: 10 \tTraining Loss: 0.000196\n",
      "Epoch: 11 \tTraining Loss: 0.000196\n",
      "Epoch: 12 \tTraining Loss: 0.000196\n",
      "Epoch: 13 \tTraining Loss: 0.000196\n",
      "Epoch: 14 \tTraining Loss: 0.000196\n",
      "Epoch: 15 \tTraining Loss: 0.000196\n",
      "Epoch: 16 \tTraining Loss: 0.000196\n",
      "Epoch: 17 \tTraining Loss: 0.000196\n",
      "Epoch: 18 \tTraining Loss: 0.000196\n",
      "Epoch: 1 \tTraining Loss: 0.000196\n",
      "Epoch: 2 \tTraining Loss: 0.000196\n",
      "Epoch: 3 \tTraining Loss: 0.000196\n",
      "Epoch: 4 \tTraining Loss: 0.000196\n",
      "Epoch: 5 \tTraining Loss: 0.000196\n",
      "Epoch: 6 \tTraining Loss: 0.000196\n",
      "Epoch: 7 \tTraining Loss: 0.000196\n",
      "Epoch: 8 \tTraining Loss: 0.000196\n",
      "Epoch: 9 \tTraining Loss: 0.000196\n",
      "Epoch: 10 \tTraining Loss: 0.000196\n",
      "Epoch: 11 \tTraining Loss: 0.000196\n",
      "Epoch: 12 \tTraining Loss: 0.000196\n",
      "Epoch: 13 \tTraining Loss: 0.000196\n",
      "Epoch: 14 \tTraining Loss: 0.000196\n",
      "Epoch: 15 \tTraining Loss: 0.000196\n",
      "Epoch: 16 \tTraining Loss: 0.000196\n",
      "Epoch: 17 \tTraining Loss: 0.000196\n",
      "Epoch: 18 \tTraining Loss: 0.000196\n",
      "Epoch: 1 \tTraining Loss: 0.000422\n",
      "Epoch: 2 \tTraining Loss: 0.000422\n",
      "Epoch: 3 \tTraining Loss: 0.000422\n",
      "Epoch: 4 \tTraining Loss: 0.000422\n",
      "Epoch: 5 \tTraining Loss: 0.000422\n",
      "Epoch: 6 \tTraining Loss: 0.000422\n",
      "Epoch: 7 \tTraining Loss: 0.000422\n",
      "Epoch: 8 \tTraining Loss: 0.000422\n",
      "Epoch: 9 \tTraining Loss: 0.000422\n",
      "Epoch: 10 \tTraining Loss: 0.000422\n",
      "Epoch: 11 \tTraining Loss: 0.000422\n",
      "Epoch: 12 \tTraining Loss: 0.000422\n",
      "Epoch: 13 \tTraining Loss: 0.000422\n",
      "Epoch: 14 \tTraining Loss: 0.000422\n",
      "Epoch: 15 \tTraining Loss: 0.000422\n",
      "Epoch: 16 \tTraining Loss: 0.000422\n",
      "Epoch: 17 \tTraining Loss: 0.000422\n",
      "Epoch: 18 \tTraining Loss: 0.000422\n",
      "Epoch: 1 \tTraining Loss: 0.000370\n",
      "Epoch: 2 \tTraining Loss: 0.000370\n",
      "Epoch: 3 \tTraining Loss: 0.000370\n",
      "Epoch: 4 \tTraining Loss: 0.000370\n",
      "Epoch: 5 \tTraining Loss: 0.000370\n",
      "Epoch: 6 \tTraining Loss: 0.000370\n",
      "Epoch: 7 \tTraining Loss: 0.000370\n",
      "Epoch: 8 \tTraining Loss: 0.000370\n",
      "Epoch: 9 \tTraining Loss: 0.000370\n",
      "Epoch: 10 \tTraining Loss: 0.000370\n",
      "Epoch: 11 \tTraining Loss: 0.000370\n",
      "Epoch: 12 \tTraining Loss: 0.000370\n",
      "Epoch: 13 \tTraining Loss: 0.000370\n",
      "Epoch: 14 \tTraining Loss: 0.000370\n",
      "Epoch: 15 \tTraining Loss: 0.000370\n",
      "Epoch: 16 \tTraining Loss: 0.000370\n",
      "Epoch: 17 \tTraining Loss: 0.000370\n",
      "Epoch: 18 \tTraining Loss: 0.000370\n",
      "Epoch: 1 \tTraining Loss: 0.000063\n",
      "Epoch: 2 \tTraining Loss: 0.000063\n",
      "Epoch: 3 \tTraining Loss: 0.000063\n",
      "Epoch: 4 \tTraining Loss: 0.000063\n",
      "Epoch: 5 \tTraining Loss: 0.000063\n",
      "Epoch: 6 \tTraining Loss: 0.000063\n",
      "Epoch: 7 \tTraining Loss: 0.000063\n",
      "Epoch: 8 \tTraining Loss: 0.000063\n",
      "Epoch: 9 \tTraining Loss: 0.000063\n",
      "Epoch: 10 \tTraining Loss: 0.000063\n",
      "Epoch: 11 \tTraining Loss: 0.000063\n",
      "Epoch: 12 \tTraining Loss: 0.000063\n",
      "Epoch: 13 \tTraining Loss: 0.000063\n",
      "Epoch: 14 \tTraining Loss: 0.000063\n",
      "Epoch: 15 \tTraining Loss: 0.000063\n",
      "Epoch: 16 \tTraining Loss: 0.000063\n",
      "Epoch: 17 \tTraining Loss: 0.000063\n",
      "Epoch: 18 \tTraining Loss: 0.000063\n",
      "Epoch: 1 \tTraining Loss: 0.000119\n",
      "Epoch: 2 \tTraining Loss: 0.000119\n",
      "Epoch: 3 \tTraining Loss: 0.000119\n",
      "Epoch: 4 \tTraining Loss: 0.000119\n",
      "Epoch: 5 \tTraining Loss: 0.000119\n",
      "Epoch: 6 \tTraining Loss: 0.000119\n",
      "Epoch: 7 \tTraining Loss: 0.000119\n",
      "Epoch: 8 \tTraining Loss: 0.000119\n",
      "Epoch: 9 \tTraining Loss: 0.000119\n",
      "Epoch: 10 \tTraining Loss: 0.000119\n",
      "Epoch: 11 \tTraining Loss: 0.000119\n",
      "Epoch: 12 \tTraining Loss: 0.000119\n",
      "Epoch: 13 \tTraining Loss: 0.000119\n",
      "Epoch: 14 \tTraining Loss: 0.000119\n",
      "Epoch: 15 \tTraining Loss: 0.000119\n",
      "Epoch: 16 \tTraining Loss: 0.000119\n",
      "Epoch: 17 \tTraining Loss: 0.000119\n",
      "Epoch: 18 \tTraining Loss: 0.000119\n",
      "Epoch: 1 \tTraining Loss: 0.000240\n",
      "Epoch: 2 \tTraining Loss: 0.000240\n",
      "Epoch: 3 \tTraining Loss: 0.000240\n",
      "Epoch: 4 \tTraining Loss: 0.000240\n",
      "Epoch: 5 \tTraining Loss: 0.000240\n",
      "Epoch: 6 \tTraining Loss: 0.000240\n",
      "Epoch: 7 \tTraining Loss: 0.000240\n",
      "Epoch: 8 \tTraining Loss: 0.000240\n",
      "Epoch: 9 \tTraining Loss: 0.000240\n",
      "Epoch: 10 \tTraining Loss: 0.000240\n",
      "Epoch: 11 \tTraining Loss: 0.000240\n",
      "Epoch: 12 \tTraining Loss: 0.000240\n",
      "Epoch: 13 \tTraining Loss: 0.000240\n",
      "Epoch: 14 \tTraining Loss: 0.000240\n",
      "Epoch: 15 \tTraining Loss: 0.000240\n",
      "Epoch: 16 \tTraining Loss: 0.000240\n",
      "Epoch: 17 \tTraining Loss: 0.000240\n",
      "Epoch: 18 \tTraining Loss: 0.000240\n",
      "Epoch: 1 \tTraining Loss: 0.000203\n",
      "Epoch: 2 \tTraining Loss: 0.000203\n",
      "Epoch: 3 \tTraining Loss: 0.000203\n",
      "Epoch: 4 \tTraining Loss: 0.000203\n",
      "Epoch: 5 \tTraining Loss: 0.000203\n",
      "Epoch: 6 \tTraining Loss: 0.000203\n",
      "Epoch: 7 \tTraining Loss: 0.000203\n",
      "Epoch: 8 \tTraining Loss: 0.000203\n",
      "Epoch: 9 \tTraining Loss: 0.000203\n",
      "Epoch: 10 \tTraining Loss: 0.000203\n",
      "Epoch: 11 \tTraining Loss: 0.000203\n",
      "Epoch: 12 \tTraining Loss: 0.000203\n",
      "Epoch: 13 \tTraining Loss: 0.000203\n",
      "Epoch: 14 \tTraining Loss: 0.000203\n",
      "Epoch: 15 \tTraining Loss: 0.000203\n",
      "Epoch: 16 \tTraining Loss: 0.000203\n",
      "Epoch: 17 \tTraining Loss: 0.000203\n",
      "Epoch: 18 \tTraining Loss: 0.000203\n",
      "Epoch: 1 \tTraining Loss: 0.000113\n",
      "Epoch: 2 \tTraining Loss: 0.000113\n",
      "Epoch: 3 \tTraining Loss: 0.000113\n",
      "Epoch: 4 \tTraining Loss: 0.000113\n",
      "Epoch: 5 \tTraining Loss: 0.000113\n",
      "Epoch: 6 \tTraining Loss: 0.000113\n",
      "Epoch: 7 \tTraining Loss: 0.000113\n",
      "Epoch: 8 \tTraining Loss: 0.000113\n",
      "Epoch: 9 \tTraining Loss: 0.000113\n",
      "Epoch: 10 \tTraining Loss: 0.000113\n",
      "Epoch: 11 \tTraining Loss: 0.000113\n",
      "Epoch: 12 \tTraining Loss: 0.000113\n",
      "Epoch: 13 \tTraining Loss: 0.000113\n",
      "Epoch: 14 \tTraining Loss: 0.000113\n",
      "Epoch: 15 \tTraining Loss: 0.000113\n",
      "Epoch: 16 \tTraining Loss: 0.000113\n",
      "Epoch: 17 \tTraining Loss: 0.000113\n",
      "Epoch: 18 \tTraining Loss: 0.000113\n",
      "Epoch: 1 \tTraining Loss: 0.000280\n",
      "Epoch: 2 \tTraining Loss: 0.000280\n",
      "Epoch: 3 \tTraining Loss: 0.000280\n",
      "Epoch: 4 \tTraining Loss: 0.000280\n",
      "Epoch: 5 \tTraining Loss: 0.000280\n",
      "Epoch: 6 \tTraining Loss: 0.000280\n",
      "Epoch: 7 \tTraining Loss: 0.000280\n",
      "Epoch: 8 \tTraining Loss: 0.000280\n",
      "Epoch: 9 \tTraining Loss: 0.000280\n",
      "Epoch: 10 \tTraining Loss: 0.000280\n",
      "Epoch: 11 \tTraining Loss: 0.000280\n",
      "Epoch: 12 \tTraining Loss: 0.000280\n",
      "Epoch: 13 \tTraining Loss: 0.000280\n",
      "Epoch: 14 \tTraining Loss: 0.000280\n",
      "Epoch: 15 \tTraining Loss: 0.000280\n",
      "Epoch: 16 \tTraining Loss: 0.000280\n",
      "Epoch: 17 \tTraining Loss: 0.000280\n",
      "Epoch: 18 \tTraining Loss: 0.000280\n",
      "Epoch: 1 \tTraining Loss: 0.000137\n",
      "Epoch: 2 \tTraining Loss: 0.000137\n",
      "Epoch: 3 \tTraining Loss: 0.000137\n",
      "Epoch: 4 \tTraining Loss: 0.000137\n",
      "Epoch: 5 \tTraining Loss: 0.000137\n",
      "Epoch: 6 \tTraining Loss: 0.000137\n",
      "Epoch: 7 \tTraining Loss: 0.000137\n",
      "Epoch: 8 \tTraining Loss: 0.000137\n",
      "Epoch: 9 \tTraining Loss: 0.000137\n",
      "Epoch: 10 \tTraining Loss: 0.000137\n",
      "Epoch: 11 \tTraining Loss: 0.000137\n",
      "Epoch: 12 \tTraining Loss: 0.000137\n",
      "Epoch: 13 \tTraining Loss: 0.000137\n",
      "Epoch: 14 \tTraining Loss: 0.000137\n",
      "Epoch: 15 \tTraining Loss: 0.000137\n",
      "Epoch: 16 \tTraining Loss: 0.000137\n",
      "Epoch: 17 \tTraining Loss: 0.000137\n",
      "Epoch: 18 \tTraining Loss: 0.000137\n",
      "Epoch: 1 \tTraining Loss: 0.000218\n",
      "Epoch: 2 \tTraining Loss: 0.000218\n",
      "Epoch: 3 \tTraining Loss: 0.000218\n",
      "Epoch: 4 \tTraining Loss: 0.000218\n",
      "Epoch: 5 \tTraining Loss: 0.000218\n",
      "Epoch: 6 \tTraining Loss: 0.000218\n",
      "Epoch: 7 \tTraining Loss: 0.000218\n",
      "Epoch: 8 \tTraining Loss: 0.000218\n",
      "Epoch: 9 \tTraining Loss: 0.000218\n",
      "Epoch: 10 \tTraining Loss: 0.000218\n",
      "Epoch: 11 \tTraining Loss: 0.000218\n",
      "Epoch: 12 \tTraining Loss: 0.000218\n",
      "Epoch: 13 \tTraining Loss: 0.000218\n",
      "Epoch: 14 \tTraining Loss: 0.000218\n",
      "Epoch: 15 \tTraining Loss: 0.000218\n",
      "Epoch: 16 \tTraining Loss: 0.000218\n",
      "Epoch: 17 \tTraining Loss: 0.000218\n",
      "Epoch: 18 \tTraining Loss: 0.000218\n",
      "Epoch: 1 \tTraining Loss: 0.000274\n",
      "Epoch: 2 \tTraining Loss: 0.000274\n",
      "Epoch: 3 \tTraining Loss: 0.000274\n",
      "Epoch: 4 \tTraining Loss: 0.000274\n",
      "Epoch: 5 \tTraining Loss: 0.000274\n",
      "Epoch: 6 \tTraining Loss: 0.000274\n",
      "Epoch: 7 \tTraining Loss: 0.000274\n",
      "Epoch: 8 \tTraining Loss: 0.000274\n",
      "Epoch: 9 \tTraining Loss: 0.000274\n",
      "Epoch: 10 \tTraining Loss: 0.000274\n",
      "Epoch: 11 \tTraining Loss: 0.000274\n",
      "Epoch: 12 \tTraining Loss: 0.000274\n",
      "Epoch: 13 \tTraining Loss: 0.000274\n",
      "Epoch: 14 \tTraining Loss: 0.000274\n",
      "Epoch: 15 \tTraining Loss: 0.000274\n",
      "Epoch: 16 \tTraining Loss: 0.000274\n",
      "Epoch: 17 \tTraining Loss: 0.000274\n",
      "Epoch: 18 \tTraining Loss: 0.000274\n",
      "Epoch: 1 \tTraining Loss: 0.000253\n",
      "Epoch: 2 \tTraining Loss: 0.000253\n",
      "Epoch: 3 \tTraining Loss: 0.000253\n",
      "Epoch: 4 \tTraining Loss: 0.000253\n",
      "Epoch: 5 \tTraining Loss: 0.000253\n",
      "Epoch: 6 \tTraining Loss: 0.000253\n",
      "Epoch: 7 \tTraining Loss: 0.000253\n",
      "Epoch: 8 \tTraining Loss: 0.000253\n",
      "Epoch: 9 \tTraining Loss: 0.000253\n",
      "Epoch: 10 \tTraining Loss: 0.000253\n",
      "Epoch: 11 \tTraining Loss: 0.000253\n",
      "Epoch: 12 \tTraining Loss: 0.000253\n",
      "Epoch: 13 \tTraining Loss: 0.000253\n",
      "Epoch: 14 \tTraining Loss: 0.000253\n",
      "Epoch: 15 \tTraining Loss: 0.000253\n",
      "Epoch: 16 \tTraining Loss: 0.000253\n",
      "Epoch: 17 \tTraining Loss: 0.000253\n",
      "Epoch: 18 \tTraining Loss: 0.000253\n",
      "Epoch: 1 \tTraining Loss: 0.000128\n",
      "Epoch: 2 \tTraining Loss: 0.000128\n",
      "Epoch: 3 \tTraining Loss: 0.000128\n",
      "Epoch: 4 \tTraining Loss: 0.000128\n",
      "Epoch: 5 \tTraining Loss: 0.000128\n",
      "Epoch: 6 \tTraining Loss: 0.000128\n",
      "Epoch: 7 \tTraining Loss: 0.000128\n",
      "Epoch: 8 \tTraining Loss: 0.000128\n",
      "Epoch: 9 \tTraining Loss: 0.000128\n",
      "Epoch: 10 \tTraining Loss: 0.000128\n",
      "Epoch: 11 \tTraining Loss: 0.000128\n",
      "Epoch: 12 \tTraining Loss: 0.000128\n",
      "Epoch: 13 \tTraining Loss: 0.000128\n",
      "Epoch: 14 \tTraining Loss: 0.000128\n",
      "Epoch: 15 \tTraining Loss: 0.000128\n",
      "Epoch: 16 \tTraining Loss: 0.000128\n",
      "Epoch: 17 \tTraining Loss: 0.000128\n",
      "Epoch: 18 \tTraining Loss: 0.000128\n",
      "Epoch: 1 \tTraining Loss: 0.000347\n",
      "Epoch: 2 \tTraining Loss: 0.000347\n",
      "Epoch: 3 \tTraining Loss: 0.000347\n",
      "Epoch: 4 \tTraining Loss: 0.000347\n",
      "Epoch: 5 \tTraining Loss: 0.000347\n",
      "Epoch: 6 \tTraining Loss: 0.000347\n",
      "Epoch: 7 \tTraining Loss: 0.000347\n",
      "Epoch: 8 \tTraining Loss: 0.000347\n",
      "Epoch: 9 \tTraining Loss: 0.000347\n",
      "Epoch: 10 \tTraining Loss: 0.000347\n",
      "Epoch: 11 \tTraining Loss: 0.000347\n",
      "Epoch: 12 \tTraining Loss: 0.000347\n",
      "Epoch: 13 \tTraining Loss: 0.000347\n",
      "Epoch: 14 \tTraining Loss: 0.000347\n",
      "Epoch: 15 \tTraining Loss: 0.000347\n",
      "Epoch: 16 \tTraining Loss: 0.000347\n",
      "Epoch: 17 \tTraining Loss: 0.000347\n",
      "Epoch: 18 \tTraining Loss: 0.000347\n",
      "Epoch: 1 \tTraining Loss: 0.000160\n",
      "Epoch: 2 \tTraining Loss: 0.000160\n",
      "Epoch: 3 \tTraining Loss: 0.000160\n",
      "Epoch: 4 \tTraining Loss: 0.000160\n",
      "Epoch: 5 \tTraining Loss: 0.000160\n",
      "Epoch: 6 \tTraining Loss: 0.000160\n",
      "Epoch: 7 \tTraining Loss: 0.000160\n",
      "Epoch: 8 \tTraining Loss: 0.000160\n",
      "Epoch: 9 \tTraining Loss: 0.000160\n",
      "Epoch: 10 \tTraining Loss: 0.000160\n",
      "Epoch: 11 \tTraining Loss: 0.000160\n",
      "Epoch: 12 \tTraining Loss: 0.000160\n",
      "Epoch: 13 \tTraining Loss: 0.000160\n",
      "Epoch: 14 \tTraining Loss: 0.000160\n",
      "Epoch: 15 \tTraining Loss: 0.000160\n",
      "Epoch: 16 \tTraining Loss: 0.000160\n",
      "Epoch: 17 \tTraining Loss: 0.000160\n",
      "Epoch: 18 \tTraining Loss: 0.000160\n",
      "Epoch: 1 \tTraining Loss: 0.000111\n",
      "Epoch: 2 \tTraining Loss: 0.000111\n",
      "Epoch: 3 \tTraining Loss: 0.000111\n",
      "Epoch: 4 \tTraining Loss: 0.000111\n",
      "Epoch: 5 \tTraining Loss: 0.000111\n",
      "Epoch: 6 \tTraining Loss: 0.000111\n",
      "Epoch: 7 \tTraining Loss: 0.000111\n",
      "Epoch: 8 \tTraining Loss: 0.000111\n",
      "Epoch: 9 \tTraining Loss: 0.000111\n",
      "Epoch: 10 \tTraining Loss: 0.000111\n",
      "Epoch: 11 \tTraining Loss: 0.000111\n",
      "Epoch: 12 \tTraining Loss: 0.000111\n",
      "Epoch: 13 \tTraining Loss: 0.000111\n",
      "Epoch: 14 \tTraining Loss: 0.000111\n",
      "Epoch: 15 \tTraining Loss: 0.000111\n",
      "Epoch: 16 \tTraining Loss: 0.000111\n",
      "Epoch: 17 \tTraining Loss: 0.000111\n",
      "Epoch: 18 \tTraining Loss: 0.000111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000150\n",
      "Epoch: 2 \tTraining Loss: 0.000150\n",
      "Epoch: 3 \tTraining Loss: 0.000150\n",
      "Epoch: 4 \tTraining Loss: 0.000150\n",
      "Epoch: 5 \tTraining Loss: 0.000150\n",
      "Epoch: 6 \tTraining Loss: 0.000150\n",
      "Epoch: 7 \tTraining Loss: 0.000150\n",
      "Epoch: 8 \tTraining Loss: 0.000150\n",
      "Epoch: 9 \tTraining Loss: 0.000150\n",
      "Epoch: 10 \tTraining Loss: 0.000150\n",
      "Epoch: 11 \tTraining Loss: 0.000150\n",
      "Epoch: 12 \tTraining Loss: 0.000150\n",
      "Epoch: 13 \tTraining Loss: 0.000150\n",
      "Epoch: 14 \tTraining Loss: 0.000150\n",
      "Epoch: 15 \tTraining Loss: 0.000150\n",
      "Epoch: 16 \tTraining Loss: 0.000150\n",
      "Epoch: 17 \tTraining Loss: 0.000150\n",
      "Epoch: 18 \tTraining Loss: 0.000150\n",
      "Epoch: 1 \tTraining Loss: 0.000237\n",
      "Epoch: 2 \tTraining Loss: 0.000237\n",
      "Epoch: 3 \tTraining Loss: 0.000237\n",
      "Epoch: 4 \tTraining Loss: 0.000237\n",
      "Epoch: 5 \tTraining Loss: 0.000237\n",
      "Epoch: 6 \tTraining Loss: 0.000237\n",
      "Epoch: 7 \tTraining Loss: 0.000237\n",
      "Epoch: 8 \tTraining Loss: 0.000237\n",
      "Epoch: 9 \tTraining Loss: 0.000237\n",
      "Epoch: 10 \tTraining Loss: 0.000237\n",
      "Epoch: 11 \tTraining Loss: 0.000237\n",
      "Epoch: 12 \tTraining Loss: 0.000237\n",
      "Epoch: 13 \tTraining Loss: 0.000237\n",
      "Epoch: 14 \tTraining Loss: 0.000237\n",
      "Epoch: 15 \tTraining Loss: 0.000237\n",
      "Epoch: 16 \tTraining Loss: 0.000237\n",
      "Epoch: 17 \tTraining Loss: 0.000237\n",
      "Epoch: 18 \tTraining Loss: 0.000237\n",
      "Epoch: 1 \tTraining Loss: 0.000093\n",
      "Epoch: 2 \tTraining Loss: 0.000093\n",
      "Epoch: 3 \tTraining Loss: 0.000093\n",
      "Epoch: 4 \tTraining Loss: 0.000093\n",
      "Epoch: 5 \tTraining Loss: 0.000093\n",
      "Epoch: 6 \tTraining Loss: 0.000093\n",
      "Epoch: 7 \tTraining Loss: 0.000093\n",
      "Epoch: 8 \tTraining Loss: 0.000093\n",
      "Epoch: 9 \tTraining Loss: 0.000093\n",
      "Epoch: 10 \tTraining Loss: 0.000093\n",
      "Epoch: 11 \tTraining Loss: 0.000093\n",
      "Epoch: 12 \tTraining Loss: 0.000093\n",
      "Epoch: 13 \tTraining Loss: 0.000093\n",
      "Epoch: 14 \tTraining Loss: 0.000093\n",
      "Epoch: 15 \tTraining Loss: 0.000093\n",
      "Epoch: 16 \tTraining Loss: 0.000093\n",
      "Epoch: 17 \tTraining Loss: 0.000093\n",
      "Epoch: 18 \tTraining Loss: 0.000093\n",
      "Epoch: 1 \tTraining Loss: 0.000209\n",
      "Epoch: 2 \tTraining Loss: 0.000209\n",
      "Epoch: 3 \tTraining Loss: 0.000209\n",
      "Epoch: 4 \tTraining Loss: 0.000209\n",
      "Epoch: 5 \tTraining Loss: 0.000209\n",
      "Epoch: 6 \tTraining Loss: 0.000209\n",
      "Epoch: 7 \tTraining Loss: 0.000209\n",
      "Epoch: 8 \tTraining Loss: 0.000209\n",
      "Epoch: 9 \tTraining Loss: 0.000209\n",
      "Epoch: 10 \tTraining Loss: 0.000209\n",
      "Epoch: 11 \tTraining Loss: 0.000209\n",
      "Epoch: 12 \tTraining Loss: 0.000209\n",
      "Epoch: 13 \tTraining Loss: 0.000209\n",
      "Epoch: 14 \tTraining Loss: 0.000209\n",
      "Epoch: 15 \tTraining Loss: 0.000209\n",
      "Epoch: 16 \tTraining Loss: 0.000209\n",
      "Epoch: 17 \tTraining Loss: 0.000209\n",
      "Epoch: 18 \tTraining Loss: 0.000209\n",
      "Epoch: 1 \tTraining Loss: 0.000279\n",
      "Epoch: 2 \tTraining Loss: 0.000279\n",
      "Epoch: 3 \tTraining Loss: 0.000279\n",
      "Epoch: 4 \tTraining Loss: 0.000279\n",
      "Epoch: 5 \tTraining Loss: 0.000279\n",
      "Epoch: 6 \tTraining Loss: 0.000279\n",
      "Epoch: 7 \tTraining Loss: 0.000279\n",
      "Epoch: 8 \tTraining Loss: 0.000279\n",
      "Epoch: 9 \tTraining Loss: 0.000279\n",
      "Epoch: 10 \tTraining Loss: 0.000279\n",
      "Epoch: 11 \tTraining Loss: 0.000279\n",
      "Epoch: 12 \tTraining Loss: 0.000279\n",
      "Epoch: 13 \tTraining Loss: 0.000279\n",
      "Epoch: 14 \tTraining Loss: 0.000279\n",
      "Epoch: 15 \tTraining Loss: 0.000279\n",
      "Epoch: 16 \tTraining Loss: 0.000279\n",
      "Epoch: 17 \tTraining Loss: 0.000279\n",
      "Epoch: 18 \tTraining Loss: 0.000279\n",
      "Epoch: 1 \tTraining Loss: 0.000127\n",
      "Epoch: 2 \tTraining Loss: 0.000127\n",
      "Epoch: 3 \tTraining Loss: 0.000127\n",
      "Epoch: 4 \tTraining Loss: 0.000127\n",
      "Epoch: 5 \tTraining Loss: 0.000127\n",
      "Epoch: 6 \tTraining Loss: 0.000127\n",
      "Epoch: 7 \tTraining Loss: 0.000127\n",
      "Epoch: 8 \tTraining Loss: 0.000127\n",
      "Epoch: 9 \tTraining Loss: 0.000127\n",
      "Epoch: 10 \tTraining Loss: 0.000127\n",
      "Epoch: 11 \tTraining Loss: 0.000127\n",
      "Epoch: 12 \tTraining Loss: 0.000127\n",
      "Epoch: 13 \tTraining Loss: 0.000127\n",
      "Epoch: 14 \tTraining Loss: 0.000127\n",
      "Epoch: 15 \tTraining Loss: 0.000127\n",
      "Epoch: 16 \tTraining Loss: 0.000127\n",
      "Epoch: 17 \tTraining Loss: 0.000127\n",
      "Epoch: 18 \tTraining Loss: 0.000127\n",
      "Epoch: 1 \tTraining Loss: 0.000203\n",
      "Epoch: 2 \tTraining Loss: 0.000203\n",
      "Epoch: 3 \tTraining Loss: 0.000203\n",
      "Epoch: 4 \tTraining Loss: 0.000203\n",
      "Epoch: 5 \tTraining Loss: 0.000203\n",
      "Epoch: 6 \tTraining Loss: 0.000203\n",
      "Epoch: 7 \tTraining Loss: 0.000203\n",
      "Epoch: 8 \tTraining Loss: 0.000203\n",
      "Epoch: 9 \tTraining Loss: 0.000203\n",
      "Epoch: 10 \tTraining Loss: 0.000203\n",
      "Epoch: 11 \tTraining Loss: 0.000203\n",
      "Epoch: 12 \tTraining Loss: 0.000203\n",
      "Epoch: 13 \tTraining Loss: 0.000203\n",
      "Epoch: 14 \tTraining Loss: 0.000203\n",
      "Epoch: 15 \tTraining Loss: 0.000203\n",
      "Epoch: 16 \tTraining Loss: 0.000203\n",
      "Epoch: 17 \tTraining Loss: 0.000203\n",
      "Epoch: 18 \tTraining Loss: 0.000203\n",
      "Epoch: 1 \tTraining Loss: 0.000271\n",
      "Epoch: 2 \tTraining Loss: 0.000271\n",
      "Epoch: 3 \tTraining Loss: 0.000271\n",
      "Epoch: 4 \tTraining Loss: 0.000271\n",
      "Epoch: 5 \tTraining Loss: 0.000271\n",
      "Epoch: 6 \tTraining Loss: 0.000271\n",
      "Epoch: 7 \tTraining Loss: 0.000271\n",
      "Epoch: 8 \tTraining Loss: 0.000271\n",
      "Epoch: 9 \tTraining Loss: 0.000271\n",
      "Epoch: 10 \tTraining Loss: 0.000271\n",
      "Epoch: 11 \tTraining Loss: 0.000271\n",
      "Epoch: 12 \tTraining Loss: 0.000271\n",
      "Epoch: 13 \tTraining Loss: 0.000271\n",
      "Epoch: 14 \tTraining Loss: 0.000271\n",
      "Epoch: 15 \tTraining Loss: 0.000271\n",
      "Epoch: 16 \tTraining Loss: 0.000271\n",
      "Epoch: 17 \tTraining Loss: 0.000271\n",
      "Epoch: 18 \tTraining Loss: 0.000271\n",
      "Epoch: 1 \tTraining Loss: 0.000122\n",
      "Epoch: 2 \tTraining Loss: 0.000122\n",
      "Epoch: 3 \tTraining Loss: 0.000122\n",
      "Epoch: 4 \tTraining Loss: 0.000122\n",
      "Epoch: 5 \tTraining Loss: 0.000122\n",
      "Epoch: 6 \tTraining Loss: 0.000122\n",
      "Epoch: 7 \tTraining Loss: 0.000122\n",
      "Epoch: 8 \tTraining Loss: 0.000122\n",
      "Epoch: 9 \tTraining Loss: 0.000122\n",
      "Epoch: 10 \tTraining Loss: 0.000122\n",
      "Epoch: 11 \tTraining Loss: 0.000122\n",
      "Epoch: 12 \tTraining Loss: 0.000122\n",
      "Epoch: 13 \tTraining Loss: 0.000122\n",
      "Epoch: 14 \tTraining Loss: 0.000122\n",
      "Epoch: 15 \tTraining Loss: 0.000122\n",
      "Epoch: 16 \tTraining Loss: 0.000122\n",
      "Epoch: 17 \tTraining Loss: 0.000122\n",
      "Epoch: 18 \tTraining Loss: 0.000122\n",
      "Epoch: 1 \tTraining Loss: 0.000089\n",
      "Epoch: 2 \tTraining Loss: 0.000089\n",
      "Epoch: 3 \tTraining Loss: 0.000089\n",
      "Epoch: 4 \tTraining Loss: 0.000089\n",
      "Epoch: 5 \tTraining Loss: 0.000089\n",
      "Epoch: 6 \tTraining Loss: 0.000089\n",
      "Epoch: 7 \tTraining Loss: 0.000089\n",
      "Epoch: 8 \tTraining Loss: 0.000089\n",
      "Epoch: 9 \tTraining Loss: 0.000089\n",
      "Epoch: 10 \tTraining Loss: 0.000089\n",
      "Epoch: 11 \tTraining Loss: 0.000089\n",
      "Epoch: 12 \tTraining Loss: 0.000089\n",
      "Epoch: 13 \tTraining Loss: 0.000089\n",
      "Epoch: 14 \tTraining Loss: 0.000089\n",
      "Epoch: 15 \tTraining Loss: 0.000089\n",
      "Epoch: 16 \tTraining Loss: 0.000089\n",
      "Epoch: 17 \tTraining Loss: 0.000089\n",
      "Epoch: 18 \tTraining Loss: 0.000089\n",
      "Epoch: 1 \tTraining Loss: 0.000261\n",
      "Epoch: 2 \tTraining Loss: 0.000261\n",
      "Epoch: 3 \tTraining Loss: 0.000261\n",
      "Epoch: 4 \tTraining Loss: 0.000261\n",
      "Epoch: 5 \tTraining Loss: 0.000261\n",
      "Epoch: 6 \tTraining Loss: 0.000261\n",
      "Epoch: 7 \tTraining Loss: 0.000261\n",
      "Epoch: 8 \tTraining Loss: 0.000261\n",
      "Epoch: 9 \tTraining Loss: 0.000261\n",
      "Epoch: 10 \tTraining Loss: 0.000261\n",
      "Epoch: 11 \tTraining Loss: 0.000261\n",
      "Epoch: 12 \tTraining Loss: 0.000261\n",
      "Epoch: 13 \tTraining Loss: 0.000261\n",
      "Epoch: 14 \tTraining Loss: 0.000261\n",
      "Epoch: 15 \tTraining Loss: 0.000261\n",
      "Epoch: 16 \tTraining Loss: 0.000261\n",
      "Epoch: 17 \tTraining Loss: 0.000261\n",
      "Epoch: 18 \tTraining Loss: 0.000261\n",
      "Epoch: 1 \tTraining Loss: 0.000235\n",
      "Epoch: 2 \tTraining Loss: 0.000235\n",
      "Epoch: 3 \tTraining Loss: 0.000235\n",
      "Epoch: 4 \tTraining Loss: 0.000235\n",
      "Epoch: 5 \tTraining Loss: 0.000235\n",
      "Epoch: 6 \tTraining Loss: 0.000235\n",
      "Epoch: 7 \tTraining Loss: 0.000235\n",
      "Epoch: 8 \tTraining Loss: 0.000235\n",
      "Epoch: 9 \tTraining Loss: 0.000235\n",
      "Epoch: 10 \tTraining Loss: 0.000235\n",
      "Epoch: 11 \tTraining Loss: 0.000235\n",
      "Epoch: 12 \tTraining Loss: 0.000235\n",
      "Epoch: 13 \tTraining Loss: 0.000235\n",
      "Epoch: 14 \tTraining Loss: 0.000235\n",
      "Epoch: 15 \tTraining Loss: 0.000235\n",
      "Epoch: 16 \tTraining Loss: 0.000235\n",
      "Epoch: 17 \tTraining Loss: 0.000235\n",
      "Epoch: 18 \tTraining Loss: 0.000235\n",
      "Epoch: 1 \tTraining Loss: 0.000156\n",
      "Epoch: 2 \tTraining Loss: 0.000156\n",
      "Epoch: 3 \tTraining Loss: 0.000156\n",
      "Epoch: 4 \tTraining Loss: 0.000156\n",
      "Epoch: 5 \tTraining Loss: 0.000156\n",
      "Epoch: 6 \tTraining Loss: 0.000156\n",
      "Epoch: 7 \tTraining Loss: 0.000156\n",
      "Epoch: 8 \tTraining Loss: 0.000156\n",
      "Epoch: 9 \tTraining Loss: 0.000156\n",
      "Epoch: 10 \tTraining Loss: 0.000156\n",
      "Epoch: 11 \tTraining Loss: 0.000156\n",
      "Epoch: 12 \tTraining Loss: 0.000156\n",
      "Epoch: 13 \tTraining Loss: 0.000156\n",
      "Epoch: 14 \tTraining Loss: 0.000156\n",
      "Epoch: 15 \tTraining Loss: 0.000156\n",
      "Epoch: 16 \tTraining Loss: 0.000156\n",
      "Epoch: 17 \tTraining Loss: 0.000156\n",
      "Epoch: 18 \tTraining Loss: 0.000156\n",
      "Epoch: 1 \tTraining Loss: 0.000177\n",
      "Epoch: 2 \tTraining Loss: 0.000177\n",
      "Epoch: 3 \tTraining Loss: 0.000177\n",
      "Epoch: 4 \tTraining Loss: 0.000177\n",
      "Epoch: 5 \tTraining Loss: 0.000177\n",
      "Epoch: 6 \tTraining Loss: 0.000177\n",
      "Epoch: 7 \tTraining Loss: 0.000177\n",
      "Epoch: 8 \tTraining Loss: 0.000177\n",
      "Epoch: 9 \tTraining Loss: 0.000177\n",
      "Epoch: 10 \tTraining Loss: 0.000177\n",
      "Epoch: 11 \tTraining Loss: 0.000177\n",
      "Epoch: 12 \tTraining Loss: 0.000177\n",
      "Epoch: 13 \tTraining Loss: 0.000177\n",
      "Epoch: 14 \tTraining Loss: 0.000177\n",
      "Epoch: 15 \tTraining Loss: 0.000177\n",
      "Epoch: 16 \tTraining Loss: 0.000177\n",
      "Epoch: 17 \tTraining Loss: 0.000177\n",
      "Epoch: 18 \tTraining Loss: 0.000177\n",
      "Epoch: 1 \tTraining Loss: 0.000168\n",
      "Epoch: 2 \tTraining Loss: 0.000168\n",
      "Epoch: 3 \tTraining Loss: 0.000168\n",
      "Epoch: 4 \tTraining Loss: 0.000168\n",
      "Epoch: 5 \tTraining Loss: 0.000168\n",
      "Epoch: 6 \tTraining Loss: 0.000168\n",
      "Epoch: 7 \tTraining Loss: 0.000168\n",
      "Epoch: 8 \tTraining Loss: 0.000168\n",
      "Epoch: 9 \tTraining Loss: 0.000168\n",
      "Epoch: 10 \tTraining Loss: 0.000168\n",
      "Epoch: 11 \tTraining Loss: 0.000168\n",
      "Epoch: 12 \tTraining Loss: 0.000168\n",
      "Epoch: 13 \tTraining Loss: 0.000168\n",
      "Epoch: 14 \tTraining Loss: 0.000168\n",
      "Epoch: 15 \tTraining Loss: 0.000168\n",
      "Epoch: 16 \tTraining Loss: 0.000168\n",
      "Epoch: 17 \tTraining Loss: 0.000168\n",
      "Epoch: 18 \tTraining Loss: 0.000168\n",
      "Epoch: 1 \tTraining Loss: 0.000379\n",
      "Epoch: 2 \tTraining Loss: 0.000379\n",
      "Epoch: 3 \tTraining Loss: 0.000379\n",
      "Epoch: 4 \tTraining Loss: 0.000379\n",
      "Epoch: 5 \tTraining Loss: 0.000379\n",
      "Epoch: 6 \tTraining Loss: 0.000379\n",
      "Epoch: 7 \tTraining Loss: 0.000379\n",
      "Epoch: 8 \tTraining Loss: 0.000379\n",
      "Epoch: 9 \tTraining Loss: 0.000379\n",
      "Epoch: 10 \tTraining Loss: 0.000379\n",
      "Epoch: 11 \tTraining Loss: 0.000379\n",
      "Epoch: 12 \tTraining Loss: 0.000379\n",
      "Epoch: 13 \tTraining Loss: 0.000379\n",
      "Epoch: 14 \tTraining Loss: 0.000379\n",
      "Epoch: 15 \tTraining Loss: 0.000379\n",
      "Epoch: 16 \tTraining Loss: 0.000379\n",
      "Epoch: 17 \tTraining Loss: 0.000379\n",
      "Epoch: 18 \tTraining Loss: 0.000379\n",
      "Epoch: 1 \tTraining Loss: 0.000232\n",
      "Epoch: 2 \tTraining Loss: 0.000232\n",
      "Epoch: 3 \tTraining Loss: 0.000232\n",
      "Epoch: 4 \tTraining Loss: 0.000232\n",
      "Epoch: 5 \tTraining Loss: 0.000232\n",
      "Epoch: 6 \tTraining Loss: 0.000232\n",
      "Epoch: 7 \tTraining Loss: 0.000232\n",
      "Epoch: 8 \tTraining Loss: 0.000232\n",
      "Epoch: 9 \tTraining Loss: 0.000232\n",
      "Epoch: 10 \tTraining Loss: 0.000232\n",
      "Epoch: 11 \tTraining Loss: 0.000232\n",
      "Epoch: 12 \tTraining Loss: 0.000232\n",
      "Epoch: 13 \tTraining Loss: 0.000232\n",
      "Epoch: 14 \tTraining Loss: 0.000232\n",
      "Epoch: 15 \tTraining Loss: 0.000232\n",
      "Epoch: 16 \tTraining Loss: 0.000232\n",
      "Epoch: 17 \tTraining Loss: 0.000232\n",
      "Epoch: 18 \tTraining Loss: 0.000232\n",
      "Epoch: 1 \tTraining Loss: 0.000318\n",
      "Epoch: 2 \tTraining Loss: 0.000318\n",
      "Epoch: 3 \tTraining Loss: 0.000318\n",
      "Epoch: 4 \tTraining Loss: 0.000318\n",
      "Epoch: 5 \tTraining Loss: 0.000318\n",
      "Epoch: 6 \tTraining Loss: 0.000318\n",
      "Epoch: 7 \tTraining Loss: 0.000318\n",
      "Epoch: 8 \tTraining Loss: 0.000318\n",
      "Epoch: 9 \tTraining Loss: 0.000318\n",
      "Epoch: 10 \tTraining Loss: 0.000318\n",
      "Epoch: 11 \tTraining Loss: 0.000318\n",
      "Epoch: 12 \tTraining Loss: 0.000318\n",
      "Epoch: 13 \tTraining Loss: 0.000318\n",
      "Epoch: 14 \tTraining Loss: 0.000318\n",
      "Epoch: 15 \tTraining Loss: 0.000318\n",
      "Epoch: 16 \tTraining Loss: 0.000318\n",
      "Epoch: 17 \tTraining Loss: 0.000318\n",
      "Epoch: 18 \tTraining Loss: 0.000318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000156\n",
      "Epoch: 2 \tTraining Loss: 0.000156\n",
      "Epoch: 3 \tTraining Loss: 0.000156\n",
      "Epoch: 4 \tTraining Loss: 0.000156\n",
      "Epoch: 5 \tTraining Loss: 0.000156\n",
      "Epoch: 6 \tTraining Loss: 0.000156\n",
      "Epoch: 7 \tTraining Loss: 0.000156\n",
      "Epoch: 8 \tTraining Loss: 0.000156\n",
      "Epoch: 9 \tTraining Loss: 0.000156\n",
      "Epoch: 10 \tTraining Loss: 0.000156\n",
      "Epoch: 11 \tTraining Loss: 0.000156\n",
      "Epoch: 12 \tTraining Loss: 0.000156\n",
      "Epoch: 13 \tTraining Loss: 0.000156\n",
      "Epoch: 14 \tTraining Loss: 0.000156\n",
      "Epoch: 15 \tTraining Loss: 0.000156\n",
      "Epoch: 16 \tTraining Loss: 0.000156\n",
      "Epoch: 17 \tTraining Loss: 0.000156\n",
      "Epoch: 18 \tTraining Loss: 0.000156\n",
      "Epoch: 1 \tTraining Loss: 0.000105\n",
      "Epoch: 2 \tTraining Loss: 0.000105\n",
      "Epoch: 3 \tTraining Loss: 0.000105\n",
      "Epoch: 4 \tTraining Loss: 0.000105\n",
      "Epoch: 5 \tTraining Loss: 0.000105\n",
      "Epoch: 6 \tTraining Loss: 0.000105\n",
      "Epoch: 7 \tTraining Loss: 0.000105\n",
      "Epoch: 8 \tTraining Loss: 0.000105\n",
      "Epoch: 9 \tTraining Loss: 0.000105\n",
      "Epoch: 10 \tTraining Loss: 0.000105\n",
      "Epoch: 11 \tTraining Loss: 0.000105\n",
      "Epoch: 12 \tTraining Loss: 0.000105\n",
      "Epoch: 13 \tTraining Loss: 0.000105\n",
      "Epoch: 14 \tTraining Loss: 0.000105\n",
      "Epoch: 15 \tTraining Loss: 0.000105\n",
      "Epoch: 16 \tTraining Loss: 0.000105\n",
      "Epoch: 17 \tTraining Loss: 0.000105\n",
      "Epoch: 18 \tTraining Loss: 0.000105\n",
      "Epoch: 1 \tTraining Loss: 0.000148\n",
      "Epoch: 2 \tTraining Loss: 0.000148\n",
      "Epoch: 3 \tTraining Loss: 0.000148\n",
      "Epoch: 4 \tTraining Loss: 0.000148\n",
      "Epoch: 5 \tTraining Loss: 0.000148\n",
      "Epoch: 6 \tTraining Loss: 0.000148\n",
      "Epoch: 7 \tTraining Loss: 0.000148\n",
      "Epoch: 8 \tTraining Loss: 0.000148\n",
      "Epoch: 9 \tTraining Loss: 0.000148\n",
      "Epoch: 10 \tTraining Loss: 0.000148\n",
      "Epoch: 11 \tTraining Loss: 0.000148\n",
      "Epoch: 12 \tTraining Loss: 0.000148\n",
      "Epoch: 13 \tTraining Loss: 0.000148\n",
      "Epoch: 14 \tTraining Loss: 0.000148\n",
      "Epoch: 15 \tTraining Loss: 0.000148\n",
      "Epoch: 16 \tTraining Loss: 0.000148\n",
      "Epoch: 17 \tTraining Loss: 0.000148\n",
      "Epoch: 18 \tTraining Loss: 0.000148\n",
      "Epoch: 1 \tTraining Loss: 0.000246\n",
      "Epoch: 2 \tTraining Loss: 0.000246\n",
      "Epoch: 3 \tTraining Loss: 0.000246\n",
      "Epoch: 4 \tTraining Loss: 0.000246\n",
      "Epoch: 5 \tTraining Loss: 0.000246\n",
      "Epoch: 6 \tTraining Loss: 0.000246\n",
      "Epoch: 7 \tTraining Loss: 0.000246\n",
      "Epoch: 8 \tTraining Loss: 0.000246\n",
      "Epoch: 9 \tTraining Loss: 0.000246\n",
      "Epoch: 10 \tTraining Loss: 0.000246\n",
      "Epoch: 11 \tTraining Loss: 0.000246\n",
      "Epoch: 12 \tTraining Loss: 0.000246\n",
      "Epoch: 13 \tTraining Loss: 0.000246\n",
      "Epoch: 14 \tTraining Loss: 0.000246\n",
      "Epoch: 15 \tTraining Loss: 0.000246\n",
      "Epoch: 16 \tTraining Loss: 0.000246\n",
      "Epoch: 17 \tTraining Loss: 0.000246\n",
      "Epoch: 18 \tTraining Loss: 0.000246\n",
      "Epoch: 1 \tTraining Loss: 0.000097\n",
      "Epoch: 2 \tTraining Loss: 0.000097\n",
      "Epoch: 3 \tTraining Loss: 0.000097\n",
      "Epoch: 4 \tTraining Loss: 0.000097\n",
      "Epoch: 5 \tTraining Loss: 0.000097\n",
      "Epoch: 6 \tTraining Loss: 0.000097\n",
      "Epoch: 7 \tTraining Loss: 0.000097\n",
      "Epoch: 8 \tTraining Loss: 0.000097\n",
      "Epoch: 9 \tTraining Loss: 0.000097\n",
      "Epoch: 10 \tTraining Loss: 0.000097\n",
      "Epoch: 11 \tTraining Loss: 0.000097\n",
      "Epoch: 12 \tTraining Loss: 0.000097\n",
      "Epoch: 13 \tTraining Loss: 0.000097\n",
      "Epoch: 14 \tTraining Loss: 0.000097\n",
      "Epoch: 15 \tTraining Loss: 0.000097\n",
      "Epoch: 16 \tTraining Loss: 0.000097\n",
      "Epoch: 17 \tTraining Loss: 0.000097\n",
      "Epoch: 18 \tTraining Loss: 0.000097\n",
      "Epoch: 1 \tTraining Loss: 0.000181\n",
      "Epoch: 2 \tTraining Loss: 0.000181\n",
      "Epoch: 3 \tTraining Loss: 0.000181\n",
      "Epoch: 4 \tTraining Loss: 0.000181\n",
      "Epoch: 5 \tTraining Loss: 0.000181\n",
      "Epoch: 6 \tTraining Loss: 0.000181\n",
      "Epoch: 7 \tTraining Loss: 0.000181\n",
      "Epoch: 8 \tTraining Loss: 0.000181\n",
      "Epoch: 9 \tTraining Loss: 0.000181\n",
      "Epoch: 10 \tTraining Loss: 0.000181\n",
      "Epoch: 11 \tTraining Loss: 0.000181\n",
      "Epoch: 12 \tTraining Loss: 0.000181\n",
      "Epoch: 13 \tTraining Loss: 0.000181\n",
      "Epoch: 14 \tTraining Loss: 0.000181\n",
      "Epoch: 15 \tTraining Loss: 0.000181\n",
      "Epoch: 16 \tTraining Loss: 0.000181\n",
      "Epoch: 17 \tTraining Loss: 0.000181\n",
      "Epoch: 18 \tTraining Loss: 0.000181\n",
      "Epoch: 1 \tTraining Loss: 0.000235\n",
      "Epoch: 2 \tTraining Loss: 0.000235\n",
      "Epoch: 3 \tTraining Loss: 0.000235\n",
      "Epoch: 4 \tTraining Loss: 0.000235\n",
      "Epoch: 5 \tTraining Loss: 0.000235\n",
      "Epoch: 6 \tTraining Loss: 0.000235\n",
      "Epoch: 7 \tTraining Loss: 0.000235\n",
      "Epoch: 8 \tTraining Loss: 0.000235\n",
      "Epoch: 9 \tTraining Loss: 0.000235\n",
      "Epoch: 10 \tTraining Loss: 0.000235\n",
      "Epoch: 11 \tTraining Loss: 0.000235\n",
      "Epoch: 12 \tTraining Loss: 0.000235\n",
      "Epoch: 13 \tTraining Loss: 0.000235\n",
      "Epoch: 14 \tTraining Loss: 0.000235\n",
      "Epoch: 15 \tTraining Loss: 0.000235\n",
      "Epoch: 16 \tTraining Loss: 0.000235\n",
      "Epoch: 17 \tTraining Loss: 0.000235\n",
      "Epoch: 18 \tTraining Loss: 0.000235\n",
      "Epoch: 1 \tTraining Loss: 0.000245\n",
      "Epoch: 2 \tTraining Loss: 0.000245\n",
      "Epoch: 3 \tTraining Loss: 0.000245\n",
      "Epoch: 4 \tTraining Loss: 0.000245\n",
      "Epoch: 5 \tTraining Loss: 0.000245\n",
      "Epoch: 6 \tTraining Loss: 0.000245\n",
      "Epoch: 7 \tTraining Loss: 0.000245\n",
      "Epoch: 8 \tTraining Loss: 0.000245\n",
      "Epoch: 9 \tTraining Loss: 0.000245\n",
      "Epoch: 10 \tTraining Loss: 0.000245\n",
      "Epoch: 11 \tTraining Loss: 0.000245\n",
      "Epoch: 12 \tTraining Loss: 0.000245\n",
      "Epoch: 13 \tTraining Loss: 0.000245\n",
      "Epoch: 14 \tTraining Loss: 0.000245\n",
      "Epoch: 15 \tTraining Loss: 0.000245\n",
      "Epoch: 16 \tTraining Loss: 0.000245\n",
      "Epoch: 17 \tTraining Loss: 0.000245\n",
      "Epoch: 18 \tTraining Loss: 0.000245\n",
      "Epoch: 1 \tTraining Loss: 0.000165\n",
      "Epoch: 2 \tTraining Loss: 0.000165\n",
      "Epoch: 3 \tTraining Loss: 0.000165\n",
      "Epoch: 4 \tTraining Loss: 0.000165\n",
      "Epoch: 5 \tTraining Loss: 0.000165\n",
      "Epoch: 6 \tTraining Loss: 0.000165\n",
      "Epoch: 7 \tTraining Loss: 0.000165\n",
      "Epoch: 8 \tTraining Loss: 0.000165\n",
      "Epoch: 9 \tTraining Loss: 0.000165\n",
      "Epoch: 10 \tTraining Loss: 0.000165\n",
      "Epoch: 11 \tTraining Loss: 0.000165\n",
      "Epoch: 12 \tTraining Loss: 0.000165\n",
      "Epoch: 13 \tTraining Loss: 0.000165\n",
      "Epoch: 14 \tTraining Loss: 0.000165\n",
      "Epoch: 15 \tTraining Loss: 0.000165\n",
      "Epoch: 16 \tTraining Loss: 0.000165\n",
      "Epoch: 17 \tTraining Loss: 0.000165\n",
      "Epoch: 18 \tTraining Loss: 0.000165\n",
      "Epoch: 1 \tTraining Loss: 0.000246\n",
      "Epoch: 2 \tTraining Loss: 0.000246\n",
      "Epoch: 3 \tTraining Loss: 0.000246\n",
      "Epoch: 4 \tTraining Loss: 0.000246\n",
      "Epoch: 5 \tTraining Loss: 0.000246\n",
      "Epoch: 6 \tTraining Loss: 0.000246\n",
      "Epoch: 7 \tTraining Loss: 0.000246\n",
      "Epoch: 8 \tTraining Loss: 0.000246\n",
      "Epoch: 9 \tTraining Loss: 0.000246\n",
      "Epoch: 10 \tTraining Loss: 0.000246\n",
      "Epoch: 11 \tTraining Loss: 0.000246\n",
      "Epoch: 12 \tTraining Loss: 0.000246\n",
      "Epoch: 13 \tTraining Loss: 0.000246\n",
      "Epoch: 14 \tTraining Loss: 0.000246\n",
      "Epoch: 15 \tTraining Loss: 0.000246\n",
      "Epoch: 16 \tTraining Loss: 0.000246\n",
      "Epoch: 17 \tTraining Loss: 0.000246\n",
      "Epoch: 18 \tTraining Loss: 0.000246\n",
      "Epoch: 1 \tTraining Loss: 0.000247\n",
      "Epoch: 2 \tTraining Loss: 0.000247\n",
      "Epoch: 3 \tTraining Loss: 0.000247\n",
      "Epoch: 4 \tTraining Loss: 0.000247\n",
      "Epoch: 5 \tTraining Loss: 0.000247\n",
      "Epoch: 6 \tTraining Loss: 0.000247\n",
      "Epoch: 7 \tTraining Loss: 0.000247\n",
      "Epoch: 8 \tTraining Loss: 0.000247\n",
      "Epoch: 9 \tTraining Loss: 0.000247\n",
      "Epoch: 10 \tTraining Loss: 0.000247\n",
      "Epoch: 11 \tTraining Loss: 0.000247\n",
      "Epoch: 12 \tTraining Loss: 0.000247\n",
      "Epoch: 13 \tTraining Loss: 0.000247\n",
      "Epoch: 14 \tTraining Loss: 0.000247\n",
      "Epoch: 15 \tTraining Loss: 0.000247\n",
      "Epoch: 16 \tTraining Loss: 0.000247\n",
      "Epoch: 17 \tTraining Loss: 0.000247\n",
      "Epoch: 18 \tTraining Loss: 0.000247\n",
      "Epoch: 1 \tTraining Loss: 0.000286\n",
      "Epoch: 2 \tTraining Loss: 0.000286\n",
      "Epoch: 3 \tTraining Loss: 0.000286\n",
      "Epoch: 4 \tTraining Loss: 0.000286\n",
      "Epoch: 5 \tTraining Loss: 0.000286\n",
      "Epoch: 6 \tTraining Loss: 0.000286\n",
      "Epoch: 7 \tTraining Loss: 0.000286\n",
      "Epoch: 8 \tTraining Loss: 0.000286\n",
      "Epoch: 9 \tTraining Loss: 0.000286\n",
      "Epoch: 10 \tTraining Loss: 0.000286\n",
      "Epoch: 11 \tTraining Loss: 0.000286\n",
      "Epoch: 12 \tTraining Loss: 0.000286\n",
      "Epoch: 13 \tTraining Loss: 0.000286\n",
      "Epoch: 14 \tTraining Loss: 0.000286\n",
      "Epoch: 15 \tTraining Loss: 0.000286\n",
      "Epoch: 16 \tTraining Loss: 0.000286\n",
      "Epoch: 17 \tTraining Loss: 0.000286\n",
      "Epoch: 18 \tTraining Loss: 0.000286\n",
      "Epoch: 1 \tTraining Loss: 0.000349\n",
      "Epoch: 2 \tTraining Loss: 0.000349\n",
      "Epoch: 3 \tTraining Loss: 0.000349\n",
      "Epoch: 4 \tTraining Loss: 0.000349\n",
      "Epoch: 5 \tTraining Loss: 0.000349\n",
      "Epoch: 6 \tTraining Loss: 0.000349\n",
      "Epoch: 7 \tTraining Loss: 0.000349\n",
      "Epoch: 8 \tTraining Loss: 0.000349\n",
      "Epoch: 9 \tTraining Loss: 0.000349\n",
      "Epoch: 10 \tTraining Loss: 0.000349\n",
      "Epoch: 11 \tTraining Loss: 0.000349\n",
      "Epoch: 12 \tTraining Loss: 0.000349\n",
      "Epoch: 13 \tTraining Loss: 0.000349\n",
      "Epoch: 14 \tTraining Loss: 0.000349\n",
      "Epoch: 15 \tTraining Loss: 0.000349\n",
      "Epoch: 16 \tTraining Loss: 0.000349\n",
      "Epoch: 17 \tTraining Loss: 0.000349\n",
      "Epoch: 18 \tTraining Loss: 0.000349\n",
      "Epoch: 1 \tTraining Loss: 0.000162\n",
      "Epoch: 2 \tTraining Loss: 0.000162\n",
      "Epoch: 3 \tTraining Loss: 0.000162\n",
      "Epoch: 4 \tTraining Loss: 0.000162\n",
      "Epoch: 5 \tTraining Loss: 0.000162\n",
      "Epoch: 6 \tTraining Loss: 0.000162\n",
      "Epoch: 7 \tTraining Loss: 0.000162\n",
      "Epoch: 8 \tTraining Loss: 0.000162\n",
      "Epoch: 9 \tTraining Loss: 0.000162\n",
      "Epoch: 10 \tTraining Loss: 0.000162\n",
      "Epoch: 11 \tTraining Loss: 0.000162\n",
      "Epoch: 12 \tTraining Loss: 0.000162\n",
      "Epoch: 13 \tTraining Loss: 0.000162\n",
      "Epoch: 14 \tTraining Loss: 0.000162\n",
      "Epoch: 15 \tTraining Loss: 0.000162\n",
      "Epoch: 16 \tTraining Loss: 0.000162\n",
      "Epoch: 17 \tTraining Loss: 0.000162\n",
      "Epoch: 18 \tTraining Loss: 0.000162\n",
      "Epoch: 1 \tTraining Loss: 0.000132\n",
      "Epoch: 2 \tTraining Loss: 0.000132\n",
      "Epoch: 3 \tTraining Loss: 0.000132\n",
      "Epoch: 4 \tTraining Loss: 0.000132\n",
      "Epoch: 5 \tTraining Loss: 0.000132\n",
      "Epoch: 6 \tTraining Loss: 0.000132\n",
      "Epoch: 7 \tTraining Loss: 0.000132\n",
      "Epoch: 8 \tTraining Loss: 0.000132\n",
      "Epoch: 9 \tTraining Loss: 0.000132\n",
      "Epoch: 10 \tTraining Loss: 0.000132\n",
      "Epoch: 11 \tTraining Loss: 0.000132\n",
      "Epoch: 12 \tTraining Loss: 0.000132\n",
      "Epoch: 13 \tTraining Loss: 0.000132\n",
      "Epoch: 14 \tTraining Loss: 0.000132\n",
      "Epoch: 15 \tTraining Loss: 0.000132\n",
      "Epoch: 16 \tTraining Loss: 0.000132\n",
      "Epoch: 17 \tTraining Loss: 0.000132\n",
      "Epoch: 18 \tTraining Loss: 0.000132\n",
      "Epoch: 1 \tTraining Loss: 0.000088\n",
      "Epoch: 2 \tTraining Loss: 0.000088\n",
      "Epoch: 3 \tTraining Loss: 0.000088\n",
      "Epoch: 4 \tTraining Loss: 0.000088\n",
      "Epoch: 5 \tTraining Loss: 0.000088\n",
      "Epoch: 6 \tTraining Loss: 0.000088\n",
      "Epoch: 7 \tTraining Loss: 0.000088\n",
      "Epoch: 8 \tTraining Loss: 0.000088\n",
      "Epoch: 9 \tTraining Loss: 0.000088\n",
      "Epoch: 10 \tTraining Loss: 0.000088\n",
      "Epoch: 11 \tTraining Loss: 0.000088\n",
      "Epoch: 12 \tTraining Loss: 0.000088\n",
      "Epoch: 13 \tTraining Loss: 0.000088\n",
      "Epoch: 14 \tTraining Loss: 0.000088\n",
      "Epoch: 15 \tTraining Loss: 0.000088\n",
      "Epoch: 16 \tTraining Loss: 0.000088\n",
      "Epoch: 17 \tTraining Loss: 0.000088\n",
      "Epoch: 18 \tTraining Loss: 0.000088\n",
      "Epoch: 1 \tTraining Loss: 0.000159\n",
      "Epoch: 2 \tTraining Loss: 0.000159\n",
      "Epoch: 3 \tTraining Loss: 0.000159\n",
      "Epoch: 4 \tTraining Loss: 0.000159\n",
      "Epoch: 5 \tTraining Loss: 0.000159\n",
      "Epoch: 6 \tTraining Loss: 0.000159\n",
      "Epoch: 7 \tTraining Loss: 0.000159\n",
      "Epoch: 8 \tTraining Loss: 0.000159\n",
      "Epoch: 9 \tTraining Loss: 0.000159\n",
      "Epoch: 10 \tTraining Loss: 0.000159\n",
      "Epoch: 11 \tTraining Loss: 0.000159\n",
      "Epoch: 12 \tTraining Loss: 0.000159\n",
      "Epoch: 13 \tTraining Loss: 0.000159\n",
      "Epoch: 14 \tTraining Loss: 0.000159\n",
      "Epoch: 15 \tTraining Loss: 0.000159\n",
      "Epoch: 16 \tTraining Loss: 0.000159\n",
      "Epoch: 17 \tTraining Loss: 0.000159\n",
      "Epoch: 18 \tTraining Loss: 0.000159\n",
      "Epoch: 1 \tTraining Loss: 0.000131\n",
      "Epoch: 2 \tTraining Loss: 0.000131\n",
      "Epoch: 3 \tTraining Loss: 0.000131\n",
      "Epoch: 4 \tTraining Loss: 0.000131\n",
      "Epoch: 5 \tTraining Loss: 0.000131\n",
      "Epoch: 6 \tTraining Loss: 0.000131\n",
      "Epoch: 7 \tTraining Loss: 0.000131\n",
      "Epoch: 8 \tTraining Loss: 0.000131\n",
      "Epoch: 9 \tTraining Loss: 0.000131\n",
      "Epoch: 10 \tTraining Loss: 0.000131\n",
      "Epoch: 11 \tTraining Loss: 0.000131\n",
      "Epoch: 12 \tTraining Loss: 0.000131\n",
      "Epoch: 13 \tTraining Loss: 0.000131\n",
      "Epoch: 14 \tTraining Loss: 0.000131\n",
      "Epoch: 15 \tTraining Loss: 0.000131\n",
      "Epoch: 16 \tTraining Loss: 0.000131\n",
      "Epoch: 17 \tTraining Loss: 0.000131\n",
      "Epoch: 18 \tTraining Loss: 0.000131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000242\n",
      "Epoch: 2 \tTraining Loss: 0.000242\n",
      "Epoch: 3 \tTraining Loss: 0.000242\n",
      "Epoch: 4 \tTraining Loss: 0.000242\n",
      "Epoch: 5 \tTraining Loss: 0.000242\n",
      "Epoch: 6 \tTraining Loss: 0.000242\n",
      "Epoch: 7 \tTraining Loss: 0.000242\n",
      "Epoch: 8 \tTraining Loss: 0.000242\n",
      "Epoch: 9 \tTraining Loss: 0.000242\n",
      "Epoch: 10 \tTraining Loss: 0.000242\n",
      "Epoch: 11 \tTraining Loss: 0.000242\n",
      "Epoch: 12 \tTraining Loss: 0.000242\n",
      "Epoch: 13 \tTraining Loss: 0.000242\n",
      "Epoch: 14 \tTraining Loss: 0.000242\n",
      "Epoch: 15 \tTraining Loss: 0.000242\n",
      "Epoch: 16 \tTraining Loss: 0.000242\n",
      "Epoch: 17 \tTraining Loss: 0.000242\n",
      "Epoch: 18 \tTraining Loss: 0.000242\n",
      "Epoch: 1 \tTraining Loss: 0.000106\n",
      "Epoch: 2 \tTraining Loss: 0.000106\n",
      "Epoch: 3 \tTraining Loss: 0.000106\n",
      "Epoch: 4 \tTraining Loss: 0.000106\n",
      "Epoch: 5 \tTraining Loss: 0.000106\n",
      "Epoch: 6 \tTraining Loss: 0.000106\n",
      "Epoch: 7 \tTraining Loss: 0.000106\n",
      "Epoch: 8 \tTraining Loss: 0.000106\n",
      "Epoch: 9 \tTraining Loss: 0.000106\n",
      "Epoch: 10 \tTraining Loss: 0.000106\n",
      "Epoch: 11 \tTraining Loss: 0.000106\n",
      "Epoch: 12 \tTraining Loss: 0.000106\n",
      "Epoch: 13 \tTraining Loss: 0.000106\n",
      "Epoch: 14 \tTraining Loss: 0.000106\n",
      "Epoch: 15 \tTraining Loss: 0.000106\n",
      "Epoch: 16 \tTraining Loss: 0.000106\n",
      "Epoch: 17 \tTraining Loss: 0.000106\n",
      "Epoch: 18 \tTraining Loss: 0.000106\n",
      "Epoch: 1 \tTraining Loss: 0.000163\n",
      "Epoch: 2 \tTraining Loss: 0.000163\n",
      "Epoch: 3 \tTraining Loss: 0.000163\n",
      "Epoch: 4 \tTraining Loss: 0.000163\n",
      "Epoch: 5 \tTraining Loss: 0.000163\n",
      "Epoch: 6 \tTraining Loss: 0.000163\n",
      "Epoch: 7 \tTraining Loss: 0.000163\n",
      "Epoch: 8 \tTraining Loss: 0.000163\n",
      "Epoch: 9 \tTraining Loss: 0.000163\n",
      "Epoch: 10 \tTraining Loss: 0.000163\n",
      "Epoch: 11 \tTraining Loss: 0.000163\n",
      "Epoch: 12 \tTraining Loss: 0.000163\n",
      "Epoch: 13 \tTraining Loss: 0.000163\n",
      "Epoch: 14 \tTraining Loss: 0.000163\n",
      "Epoch: 15 \tTraining Loss: 0.000163\n",
      "Epoch: 16 \tTraining Loss: 0.000163\n",
      "Epoch: 17 \tTraining Loss: 0.000163\n",
      "Epoch: 18 \tTraining Loss: 0.000163\n",
      "Epoch: 1 \tTraining Loss: 0.000241\n",
      "Epoch: 2 \tTraining Loss: 0.000241\n",
      "Epoch: 3 \tTraining Loss: 0.000241\n",
      "Epoch: 4 \tTraining Loss: 0.000241\n",
      "Epoch: 5 \tTraining Loss: 0.000241\n",
      "Epoch: 6 \tTraining Loss: 0.000241\n",
      "Epoch: 7 \tTraining Loss: 0.000241\n",
      "Epoch: 8 \tTraining Loss: 0.000241\n",
      "Epoch: 9 \tTraining Loss: 0.000241\n",
      "Epoch: 10 \tTraining Loss: 0.000241\n",
      "Epoch: 11 \tTraining Loss: 0.000241\n",
      "Epoch: 12 \tTraining Loss: 0.000241\n",
      "Epoch: 13 \tTraining Loss: 0.000241\n",
      "Epoch: 14 \tTraining Loss: 0.000241\n",
      "Epoch: 15 \tTraining Loss: 0.000241\n",
      "Epoch: 16 \tTraining Loss: 0.000241\n",
      "Epoch: 17 \tTraining Loss: 0.000241\n",
      "Epoch: 18 \tTraining Loss: 0.000241\n",
      "Epoch: 1 \tTraining Loss: 0.000292\n",
      "Epoch: 2 \tTraining Loss: 0.000292\n",
      "Epoch: 3 \tTraining Loss: 0.000292\n",
      "Epoch: 4 \tTraining Loss: 0.000292\n",
      "Epoch: 5 \tTraining Loss: 0.000292\n",
      "Epoch: 6 \tTraining Loss: 0.000292\n",
      "Epoch: 7 \tTraining Loss: 0.000292\n",
      "Epoch: 8 \tTraining Loss: 0.000292\n",
      "Epoch: 9 \tTraining Loss: 0.000292\n",
      "Epoch: 10 \tTraining Loss: 0.000292\n",
      "Epoch: 11 \tTraining Loss: 0.000292\n",
      "Epoch: 12 \tTraining Loss: 0.000292\n",
      "Epoch: 13 \tTraining Loss: 0.000292\n",
      "Epoch: 14 \tTraining Loss: 0.000292\n",
      "Epoch: 15 \tTraining Loss: 0.000292\n",
      "Epoch: 16 \tTraining Loss: 0.000292\n",
      "Epoch: 17 \tTraining Loss: 0.000292\n",
      "Epoch: 18 \tTraining Loss: 0.000292\n",
      "Epoch: 1 \tTraining Loss: 0.000147\n",
      "Epoch: 2 \tTraining Loss: 0.000147\n",
      "Epoch: 3 \tTraining Loss: 0.000147\n",
      "Epoch: 4 \tTraining Loss: 0.000147\n",
      "Epoch: 5 \tTraining Loss: 0.000147\n",
      "Epoch: 6 \tTraining Loss: 0.000147\n",
      "Epoch: 7 \tTraining Loss: 0.000147\n",
      "Epoch: 8 \tTraining Loss: 0.000147\n",
      "Epoch: 9 \tTraining Loss: 0.000147\n",
      "Epoch: 10 \tTraining Loss: 0.000147\n",
      "Epoch: 11 \tTraining Loss: 0.000147\n",
      "Epoch: 12 \tTraining Loss: 0.000147\n",
      "Epoch: 13 \tTraining Loss: 0.000147\n",
      "Epoch: 14 \tTraining Loss: 0.000147\n",
      "Epoch: 15 \tTraining Loss: 0.000147\n",
      "Epoch: 16 \tTraining Loss: 0.000147\n",
      "Epoch: 17 \tTraining Loss: 0.000147\n",
      "Epoch: 18 \tTraining Loss: 0.000147\n",
      "Epoch: 1 \tTraining Loss: 0.000280\n",
      "Epoch: 2 \tTraining Loss: 0.000280\n",
      "Epoch: 3 \tTraining Loss: 0.000280\n",
      "Epoch: 4 \tTraining Loss: 0.000280\n",
      "Epoch: 5 \tTraining Loss: 0.000280\n",
      "Epoch: 6 \tTraining Loss: 0.000280\n",
      "Epoch: 7 \tTraining Loss: 0.000280\n",
      "Epoch: 8 \tTraining Loss: 0.000280\n",
      "Epoch: 9 \tTraining Loss: 0.000280\n",
      "Epoch: 10 \tTraining Loss: 0.000280\n",
      "Epoch: 11 \tTraining Loss: 0.000280\n",
      "Epoch: 12 \tTraining Loss: 0.000280\n",
      "Epoch: 13 \tTraining Loss: 0.000280\n",
      "Epoch: 14 \tTraining Loss: 0.000280\n",
      "Epoch: 15 \tTraining Loss: 0.000280\n",
      "Epoch: 16 \tTraining Loss: 0.000280\n",
      "Epoch: 17 \tTraining Loss: 0.000280\n",
      "Epoch: 18 \tTraining Loss: 0.000280\n",
      "Epoch: 1 \tTraining Loss: 0.000105\n",
      "Epoch: 2 \tTraining Loss: 0.000105\n",
      "Epoch: 3 \tTraining Loss: 0.000105\n",
      "Epoch: 4 \tTraining Loss: 0.000105\n",
      "Epoch: 5 \tTraining Loss: 0.000105\n",
      "Epoch: 6 \tTraining Loss: 0.000105\n",
      "Epoch: 7 \tTraining Loss: 0.000105\n",
      "Epoch: 8 \tTraining Loss: 0.000105\n",
      "Epoch: 9 \tTraining Loss: 0.000105\n",
      "Epoch: 10 \tTraining Loss: 0.000105\n",
      "Epoch: 11 \tTraining Loss: 0.000105\n",
      "Epoch: 12 \tTraining Loss: 0.000105\n",
      "Epoch: 13 \tTraining Loss: 0.000105\n",
      "Epoch: 14 \tTraining Loss: 0.000105\n",
      "Epoch: 15 \tTraining Loss: 0.000105\n",
      "Epoch: 16 \tTraining Loss: 0.000105\n",
      "Epoch: 17 \tTraining Loss: 0.000105\n",
      "Epoch: 18 \tTraining Loss: 0.000105\n",
      "Epoch: 1 \tTraining Loss: 0.000179\n",
      "Epoch: 2 \tTraining Loss: 0.000179\n",
      "Epoch: 3 \tTraining Loss: 0.000179\n",
      "Epoch: 4 \tTraining Loss: 0.000179\n",
      "Epoch: 5 \tTraining Loss: 0.000179\n",
      "Epoch: 6 \tTraining Loss: 0.000179\n",
      "Epoch: 7 \tTraining Loss: 0.000179\n",
      "Epoch: 8 \tTraining Loss: 0.000179\n",
      "Epoch: 9 \tTraining Loss: 0.000179\n",
      "Epoch: 10 \tTraining Loss: 0.000179\n",
      "Epoch: 11 \tTraining Loss: 0.000179\n",
      "Epoch: 12 \tTraining Loss: 0.000179\n",
      "Epoch: 13 \tTraining Loss: 0.000179\n",
      "Epoch: 14 \tTraining Loss: 0.000179\n",
      "Epoch: 15 \tTraining Loss: 0.000179\n",
      "Epoch: 16 \tTraining Loss: 0.000179\n",
      "Epoch: 17 \tTraining Loss: 0.000179\n",
      "Epoch: 18 \tTraining Loss: 0.000179\n",
      "Epoch: 1 \tTraining Loss: 0.000146\n",
      "Epoch: 2 \tTraining Loss: 0.000146\n",
      "Epoch: 3 \tTraining Loss: 0.000146\n",
      "Epoch: 4 \tTraining Loss: 0.000146\n",
      "Epoch: 5 \tTraining Loss: 0.000146\n",
      "Epoch: 6 \tTraining Loss: 0.000146\n",
      "Epoch: 7 \tTraining Loss: 0.000146\n",
      "Epoch: 8 \tTraining Loss: 0.000146\n",
      "Epoch: 9 \tTraining Loss: 0.000146\n",
      "Epoch: 10 \tTraining Loss: 0.000146\n",
      "Epoch: 11 \tTraining Loss: 0.000146\n",
      "Epoch: 12 \tTraining Loss: 0.000146\n",
      "Epoch: 13 \tTraining Loss: 0.000146\n",
      "Epoch: 14 \tTraining Loss: 0.000146\n",
      "Epoch: 15 \tTraining Loss: 0.000146\n",
      "Epoch: 16 \tTraining Loss: 0.000146\n",
      "Epoch: 17 \tTraining Loss: 0.000146\n",
      "Epoch: 18 \tTraining Loss: 0.000146\n",
      "Epoch: 1 \tTraining Loss: 0.000140\n",
      "Epoch: 2 \tTraining Loss: 0.000140\n",
      "Epoch: 3 \tTraining Loss: 0.000140\n",
      "Epoch: 4 \tTraining Loss: 0.000140\n",
      "Epoch: 5 \tTraining Loss: 0.000140\n",
      "Epoch: 6 \tTraining Loss: 0.000140\n",
      "Epoch: 7 \tTraining Loss: 0.000140\n",
      "Epoch: 8 \tTraining Loss: 0.000140\n",
      "Epoch: 9 \tTraining Loss: 0.000140\n",
      "Epoch: 10 \tTraining Loss: 0.000140\n",
      "Epoch: 11 \tTraining Loss: 0.000140\n",
      "Epoch: 12 \tTraining Loss: 0.000140\n",
      "Epoch: 13 \tTraining Loss: 0.000140\n",
      "Epoch: 14 \tTraining Loss: 0.000140\n",
      "Epoch: 15 \tTraining Loss: 0.000140\n",
      "Epoch: 16 \tTraining Loss: 0.000140\n",
      "Epoch: 17 \tTraining Loss: 0.000140\n",
      "Epoch: 18 \tTraining Loss: 0.000140\n",
      "Epoch: 1 \tTraining Loss: 0.000192\n",
      "Epoch: 2 \tTraining Loss: 0.000192\n",
      "Epoch: 3 \tTraining Loss: 0.000192\n",
      "Epoch: 4 \tTraining Loss: 0.000192\n",
      "Epoch: 5 \tTraining Loss: 0.000192\n",
      "Epoch: 6 \tTraining Loss: 0.000192\n",
      "Epoch: 7 \tTraining Loss: 0.000192\n",
      "Epoch: 8 \tTraining Loss: 0.000192\n",
      "Epoch: 9 \tTraining Loss: 0.000192\n",
      "Epoch: 10 \tTraining Loss: 0.000192\n",
      "Epoch: 11 \tTraining Loss: 0.000192\n",
      "Epoch: 12 \tTraining Loss: 0.000192\n",
      "Epoch: 13 \tTraining Loss: 0.000192\n",
      "Epoch: 14 \tTraining Loss: 0.000192\n",
      "Epoch: 15 \tTraining Loss: 0.000192\n",
      "Epoch: 16 \tTraining Loss: 0.000192\n",
      "Epoch: 17 \tTraining Loss: 0.000192\n",
      "Epoch: 18 \tTraining Loss: 0.000192\n",
      "Epoch: 1 \tTraining Loss: 0.000187\n",
      "Epoch: 2 \tTraining Loss: 0.000187\n",
      "Epoch: 3 \tTraining Loss: 0.000187\n",
      "Epoch: 4 \tTraining Loss: 0.000187\n",
      "Epoch: 5 \tTraining Loss: 0.000187\n",
      "Epoch: 6 \tTraining Loss: 0.000187\n",
      "Epoch: 7 \tTraining Loss: 0.000187\n",
      "Epoch: 8 \tTraining Loss: 0.000187\n",
      "Epoch: 9 \tTraining Loss: 0.000187\n",
      "Epoch: 10 \tTraining Loss: 0.000187\n",
      "Epoch: 11 \tTraining Loss: 0.000187\n",
      "Epoch: 12 \tTraining Loss: 0.000187\n",
      "Epoch: 13 \tTraining Loss: 0.000187\n",
      "Epoch: 14 \tTraining Loss: 0.000187\n",
      "Epoch: 15 \tTraining Loss: 0.000187\n",
      "Epoch: 16 \tTraining Loss: 0.000187\n",
      "Epoch: 17 \tTraining Loss: 0.000187\n",
      "Epoch: 18 \tTraining Loss: 0.000187\n",
      "Epoch: 1 \tTraining Loss: 0.000198\n",
      "Epoch: 2 \tTraining Loss: 0.000198\n",
      "Epoch: 3 \tTraining Loss: 0.000198\n",
      "Epoch: 4 \tTraining Loss: 0.000198\n",
      "Epoch: 5 \tTraining Loss: 0.000198\n",
      "Epoch: 6 \tTraining Loss: 0.000198\n",
      "Epoch: 7 \tTraining Loss: 0.000198\n",
      "Epoch: 8 \tTraining Loss: 0.000198\n",
      "Epoch: 9 \tTraining Loss: 0.000198\n",
      "Epoch: 10 \tTraining Loss: 0.000198\n",
      "Epoch: 11 \tTraining Loss: 0.000198\n",
      "Epoch: 12 \tTraining Loss: 0.000198\n",
      "Epoch: 13 \tTraining Loss: 0.000198\n",
      "Epoch: 14 \tTraining Loss: 0.000198\n",
      "Epoch: 15 \tTraining Loss: 0.000198\n",
      "Epoch: 16 \tTraining Loss: 0.000198\n",
      "Epoch: 17 \tTraining Loss: 0.000198\n",
      "Epoch: 18 \tTraining Loss: 0.000198\n",
      "Epoch: 1 \tTraining Loss: 0.000145\n",
      "Epoch: 2 \tTraining Loss: 0.000145\n",
      "Epoch: 3 \tTraining Loss: 0.000145\n",
      "Epoch: 4 \tTraining Loss: 0.000145\n",
      "Epoch: 5 \tTraining Loss: 0.000145\n",
      "Epoch: 6 \tTraining Loss: 0.000145\n",
      "Epoch: 7 \tTraining Loss: 0.000145\n",
      "Epoch: 8 \tTraining Loss: 0.000145\n",
      "Epoch: 9 \tTraining Loss: 0.000145\n",
      "Epoch: 10 \tTraining Loss: 0.000145\n",
      "Epoch: 11 \tTraining Loss: 0.000145\n",
      "Epoch: 12 \tTraining Loss: 0.000145\n",
      "Epoch: 13 \tTraining Loss: 0.000145\n",
      "Epoch: 14 \tTraining Loss: 0.000145\n",
      "Epoch: 15 \tTraining Loss: 0.000145\n",
      "Epoch: 16 \tTraining Loss: 0.000145\n",
      "Epoch: 17 \tTraining Loss: 0.000145\n",
      "Epoch: 18 \tTraining Loss: 0.000145\n",
      "Epoch: 1 \tTraining Loss: 0.000130\n",
      "Epoch: 2 \tTraining Loss: 0.000130\n",
      "Epoch: 3 \tTraining Loss: 0.000130\n",
      "Epoch: 4 \tTraining Loss: 0.000130\n",
      "Epoch: 5 \tTraining Loss: 0.000130\n",
      "Epoch: 6 \tTraining Loss: 0.000130\n",
      "Epoch: 7 \tTraining Loss: 0.000130\n",
      "Epoch: 8 \tTraining Loss: 0.000130\n",
      "Epoch: 9 \tTraining Loss: 0.000130\n",
      "Epoch: 10 \tTraining Loss: 0.000130\n",
      "Epoch: 11 \tTraining Loss: 0.000130\n",
      "Epoch: 12 \tTraining Loss: 0.000130\n",
      "Epoch: 13 \tTraining Loss: 0.000130\n",
      "Epoch: 14 \tTraining Loss: 0.000130\n",
      "Epoch: 15 \tTraining Loss: 0.000130\n",
      "Epoch: 16 \tTraining Loss: 0.000130\n",
      "Epoch: 17 \tTraining Loss: 0.000130\n",
      "Epoch: 18 \tTraining Loss: 0.000130\n",
      "Epoch: 1 \tTraining Loss: 0.000156\n",
      "Epoch: 2 \tTraining Loss: 0.000156\n",
      "Epoch: 3 \tTraining Loss: 0.000156\n",
      "Epoch: 4 \tTraining Loss: 0.000156\n",
      "Epoch: 5 \tTraining Loss: 0.000156\n",
      "Epoch: 6 \tTraining Loss: 0.000156\n",
      "Epoch: 7 \tTraining Loss: 0.000156\n",
      "Epoch: 8 \tTraining Loss: 0.000156\n",
      "Epoch: 9 \tTraining Loss: 0.000156\n",
      "Epoch: 10 \tTraining Loss: 0.000156\n",
      "Epoch: 11 \tTraining Loss: 0.000156\n",
      "Epoch: 12 \tTraining Loss: 0.000156\n",
      "Epoch: 13 \tTraining Loss: 0.000156\n",
      "Epoch: 14 \tTraining Loss: 0.000156\n",
      "Epoch: 15 \tTraining Loss: 0.000156\n",
      "Epoch: 16 \tTraining Loss: 0.000156\n",
      "Epoch: 17 \tTraining Loss: 0.000156\n",
      "Epoch: 18 \tTraining Loss: 0.000156\n",
      "Epoch: 1 \tTraining Loss: 0.000119\n",
      "Epoch: 2 \tTraining Loss: 0.000119\n",
      "Epoch: 3 \tTraining Loss: 0.000119\n",
      "Epoch: 4 \tTraining Loss: 0.000119\n",
      "Epoch: 5 \tTraining Loss: 0.000119\n",
      "Epoch: 6 \tTraining Loss: 0.000119\n",
      "Epoch: 7 \tTraining Loss: 0.000119\n",
      "Epoch: 8 \tTraining Loss: 0.000119\n",
      "Epoch: 9 \tTraining Loss: 0.000119\n",
      "Epoch: 10 \tTraining Loss: 0.000119\n",
      "Epoch: 11 \tTraining Loss: 0.000119\n",
      "Epoch: 12 \tTraining Loss: 0.000119\n",
      "Epoch: 13 \tTraining Loss: 0.000119\n",
      "Epoch: 14 \tTraining Loss: 0.000119\n",
      "Epoch: 15 \tTraining Loss: 0.000119\n",
      "Epoch: 16 \tTraining Loss: 0.000119\n",
      "Epoch: 17 \tTraining Loss: 0.000119\n",
      "Epoch: 18 \tTraining Loss: 0.000119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000221\n",
      "Epoch: 2 \tTraining Loss: 0.000221\n",
      "Epoch: 3 \tTraining Loss: 0.000221\n",
      "Epoch: 4 \tTraining Loss: 0.000221\n",
      "Epoch: 5 \tTraining Loss: 0.000221\n",
      "Epoch: 6 \tTraining Loss: 0.000221\n",
      "Epoch: 7 \tTraining Loss: 0.000221\n",
      "Epoch: 8 \tTraining Loss: 0.000221\n",
      "Epoch: 9 \tTraining Loss: 0.000221\n",
      "Epoch: 10 \tTraining Loss: 0.000221\n",
      "Epoch: 11 \tTraining Loss: 0.000221\n",
      "Epoch: 12 \tTraining Loss: 0.000221\n",
      "Epoch: 13 \tTraining Loss: 0.000221\n",
      "Epoch: 14 \tTraining Loss: 0.000221\n",
      "Epoch: 15 \tTraining Loss: 0.000221\n",
      "Epoch: 16 \tTraining Loss: 0.000221\n",
      "Epoch: 17 \tTraining Loss: 0.000221\n",
      "Epoch: 18 \tTraining Loss: 0.000221\n",
      "Epoch: 1 \tTraining Loss: 0.000182\n",
      "Epoch: 2 \tTraining Loss: 0.000182\n",
      "Epoch: 3 \tTraining Loss: 0.000182\n",
      "Epoch: 4 \tTraining Loss: 0.000182\n",
      "Epoch: 5 \tTraining Loss: 0.000182\n",
      "Epoch: 6 \tTraining Loss: 0.000182\n",
      "Epoch: 7 \tTraining Loss: 0.000182\n",
      "Epoch: 8 \tTraining Loss: 0.000182\n",
      "Epoch: 9 \tTraining Loss: 0.000182\n",
      "Epoch: 10 \tTraining Loss: 0.000182\n",
      "Epoch: 11 \tTraining Loss: 0.000182\n",
      "Epoch: 12 \tTraining Loss: 0.000182\n",
      "Epoch: 13 \tTraining Loss: 0.000182\n",
      "Epoch: 14 \tTraining Loss: 0.000182\n",
      "Epoch: 15 \tTraining Loss: 0.000182\n",
      "Epoch: 16 \tTraining Loss: 0.000182\n",
      "Epoch: 17 \tTraining Loss: 0.000182\n",
      "Epoch: 18 \tTraining Loss: 0.000182\n",
      "Epoch: 1 \tTraining Loss: 0.000207\n",
      "Epoch: 2 \tTraining Loss: 0.000207\n",
      "Epoch: 3 \tTraining Loss: 0.000207\n",
      "Epoch: 4 \tTraining Loss: 0.000207\n",
      "Epoch: 5 \tTraining Loss: 0.000207\n",
      "Epoch: 6 \tTraining Loss: 0.000207\n",
      "Epoch: 7 \tTraining Loss: 0.000207\n",
      "Epoch: 8 \tTraining Loss: 0.000207\n",
      "Epoch: 9 \tTraining Loss: 0.000207\n",
      "Epoch: 10 \tTraining Loss: 0.000207\n",
      "Epoch: 11 \tTraining Loss: 0.000207\n",
      "Epoch: 12 \tTraining Loss: 0.000207\n",
      "Epoch: 13 \tTraining Loss: 0.000207\n",
      "Epoch: 14 \tTraining Loss: 0.000207\n",
      "Epoch: 15 \tTraining Loss: 0.000207\n",
      "Epoch: 16 \tTraining Loss: 0.000207\n",
      "Epoch: 17 \tTraining Loss: 0.000207\n",
      "Epoch: 18 \tTraining Loss: 0.000207\n",
      "Epoch: 1 \tTraining Loss: 0.000407\n",
      "Epoch: 2 \tTraining Loss: 0.000407\n",
      "Epoch: 3 \tTraining Loss: 0.000407\n",
      "Epoch: 4 \tTraining Loss: 0.000407\n",
      "Epoch: 5 \tTraining Loss: 0.000407\n",
      "Epoch: 6 \tTraining Loss: 0.000407\n",
      "Epoch: 7 \tTraining Loss: 0.000407\n",
      "Epoch: 8 \tTraining Loss: 0.000407\n",
      "Epoch: 9 \tTraining Loss: 0.000407\n",
      "Epoch: 10 \tTraining Loss: 0.000407\n",
      "Epoch: 11 \tTraining Loss: 0.000407\n",
      "Epoch: 12 \tTraining Loss: 0.000407\n",
      "Epoch: 13 \tTraining Loss: 0.000407\n",
      "Epoch: 14 \tTraining Loss: 0.000407\n",
      "Epoch: 15 \tTraining Loss: 0.000407\n",
      "Epoch: 16 \tTraining Loss: 0.000407\n",
      "Epoch: 17 \tTraining Loss: 0.000407\n",
      "Epoch: 18 \tTraining Loss: 0.000407\n",
      "Epoch: 1 \tTraining Loss: 0.000186\n",
      "Epoch: 2 \tTraining Loss: 0.000186\n",
      "Epoch: 3 \tTraining Loss: 0.000186\n",
      "Epoch: 4 \tTraining Loss: 0.000186\n",
      "Epoch: 5 \tTraining Loss: 0.000186\n",
      "Epoch: 6 \tTraining Loss: 0.000186\n",
      "Epoch: 7 \tTraining Loss: 0.000186\n",
      "Epoch: 8 \tTraining Loss: 0.000186\n",
      "Epoch: 9 \tTraining Loss: 0.000186\n",
      "Epoch: 10 \tTraining Loss: 0.000186\n",
      "Epoch: 11 \tTraining Loss: 0.000186\n",
      "Epoch: 12 \tTraining Loss: 0.000186\n",
      "Epoch: 13 \tTraining Loss: 0.000186\n",
      "Epoch: 14 \tTraining Loss: 0.000186\n",
      "Epoch: 15 \tTraining Loss: 0.000186\n",
      "Epoch: 16 \tTraining Loss: 0.000186\n",
      "Epoch: 17 \tTraining Loss: 0.000186\n",
      "Epoch: 18 \tTraining Loss: 0.000186\n",
      "Epoch: 1 \tTraining Loss: 0.000084\n",
      "Epoch: 2 \tTraining Loss: 0.000084\n",
      "Epoch: 3 \tTraining Loss: 0.000084\n",
      "Epoch: 4 \tTraining Loss: 0.000084\n",
      "Epoch: 5 \tTraining Loss: 0.000084\n",
      "Epoch: 6 \tTraining Loss: 0.000084\n",
      "Epoch: 7 \tTraining Loss: 0.000084\n",
      "Epoch: 8 \tTraining Loss: 0.000084\n",
      "Epoch: 9 \tTraining Loss: 0.000084\n",
      "Epoch: 10 \tTraining Loss: 0.000084\n",
      "Epoch: 11 \tTraining Loss: 0.000084\n",
      "Epoch: 12 \tTraining Loss: 0.000084\n",
      "Epoch: 13 \tTraining Loss: 0.000084\n",
      "Epoch: 14 \tTraining Loss: 0.000084\n",
      "Epoch: 15 \tTraining Loss: 0.000084\n",
      "Epoch: 16 \tTraining Loss: 0.000084\n",
      "Epoch: 17 \tTraining Loss: 0.000084\n",
      "Epoch: 18 \tTraining Loss: 0.000084\n",
      "Epoch: 1 \tTraining Loss: 0.000228\n",
      "Epoch: 2 \tTraining Loss: 0.000228\n",
      "Epoch: 3 \tTraining Loss: 0.000228\n",
      "Epoch: 4 \tTraining Loss: 0.000228\n",
      "Epoch: 5 \tTraining Loss: 0.000228\n",
      "Epoch: 6 \tTraining Loss: 0.000228\n",
      "Epoch: 7 \tTraining Loss: 0.000228\n",
      "Epoch: 8 \tTraining Loss: 0.000228\n",
      "Epoch: 9 \tTraining Loss: 0.000228\n",
      "Epoch: 10 \tTraining Loss: 0.000228\n",
      "Epoch: 11 \tTraining Loss: 0.000228\n",
      "Epoch: 12 \tTraining Loss: 0.000228\n",
      "Epoch: 13 \tTraining Loss: 0.000228\n",
      "Epoch: 14 \tTraining Loss: 0.000228\n",
      "Epoch: 15 \tTraining Loss: 0.000228\n",
      "Epoch: 16 \tTraining Loss: 0.000228\n",
      "Epoch: 17 \tTraining Loss: 0.000228\n",
      "Epoch: 18 \tTraining Loss: 0.000228\n",
      "Epoch: 1 \tTraining Loss: 0.000150\n",
      "Epoch: 2 \tTraining Loss: 0.000150\n",
      "Epoch: 3 \tTraining Loss: 0.000150\n",
      "Epoch: 4 \tTraining Loss: 0.000150\n",
      "Epoch: 5 \tTraining Loss: 0.000150\n",
      "Epoch: 6 \tTraining Loss: 0.000150\n",
      "Epoch: 7 \tTraining Loss: 0.000150\n",
      "Epoch: 8 \tTraining Loss: 0.000150\n",
      "Epoch: 9 \tTraining Loss: 0.000150\n",
      "Epoch: 10 \tTraining Loss: 0.000150\n",
      "Epoch: 11 \tTraining Loss: 0.000150\n",
      "Epoch: 12 \tTraining Loss: 0.000150\n",
      "Epoch: 13 \tTraining Loss: 0.000150\n",
      "Epoch: 14 \tTraining Loss: 0.000150\n",
      "Epoch: 15 \tTraining Loss: 0.000150\n",
      "Epoch: 16 \tTraining Loss: 0.000150\n",
      "Epoch: 17 \tTraining Loss: 0.000150\n",
      "Epoch: 18 \tTraining Loss: 0.000150\n",
      "Epoch: 1 \tTraining Loss: 0.000161\n",
      "Epoch: 2 \tTraining Loss: 0.000161\n",
      "Epoch: 3 \tTraining Loss: 0.000161\n",
      "Epoch: 4 \tTraining Loss: 0.000161\n",
      "Epoch: 5 \tTraining Loss: 0.000161\n",
      "Epoch: 6 \tTraining Loss: 0.000161\n",
      "Epoch: 7 \tTraining Loss: 0.000161\n",
      "Epoch: 8 \tTraining Loss: 0.000161\n",
      "Epoch: 9 \tTraining Loss: 0.000161\n",
      "Epoch: 10 \tTraining Loss: 0.000161\n",
      "Epoch: 11 \tTraining Loss: 0.000161\n",
      "Epoch: 12 \tTraining Loss: 0.000161\n",
      "Epoch: 13 \tTraining Loss: 0.000161\n",
      "Epoch: 14 \tTraining Loss: 0.000161\n",
      "Epoch: 15 \tTraining Loss: 0.000161\n",
      "Epoch: 16 \tTraining Loss: 0.000161\n",
      "Epoch: 17 \tTraining Loss: 0.000161\n",
      "Epoch: 18 \tTraining Loss: 0.000161\n",
      "Epoch: 1 \tTraining Loss: 0.000159\n",
      "Epoch: 2 \tTraining Loss: 0.000159\n",
      "Epoch: 3 \tTraining Loss: 0.000159\n",
      "Epoch: 4 \tTraining Loss: 0.000159\n",
      "Epoch: 5 \tTraining Loss: 0.000159\n",
      "Epoch: 6 \tTraining Loss: 0.000159\n",
      "Epoch: 7 \tTraining Loss: 0.000159\n",
      "Epoch: 8 \tTraining Loss: 0.000159\n",
      "Epoch: 9 \tTraining Loss: 0.000159\n",
      "Epoch: 10 \tTraining Loss: 0.000159\n",
      "Epoch: 11 \tTraining Loss: 0.000159\n",
      "Epoch: 12 \tTraining Loss: 0.000159\n",
      "Epoch: 13 \tTraining Loss: 0.000159\n",
      "Epoch: 14 \tTraining Loss: 0.000159\n",
      "Epoch: 15 \tTraining Loss: 0.000159\n",
      "Epoch: 16 \tTraining Loss: 0.000159\n",
      "Epoch: 17 \tTraining Loss: 0.000159\n",
      "Epoch: 18 \tTraining Loss: 0.000159\n",
      "Epoch: 1 \tTraining Loss: 0.000180\n",
      "Epoch: 2 \tTraining Loss: 0.000180\n",
      "Epoch: 3 \tTraining Loss: 0.000180\n",
      "Epoch: 4 \tTraining Loss: 0.000180\n",
      "Epoch: 5 \tTraining Loss: 0.000180\n",
      "Epoch: 6 \tTraining Loss: 0.000180\n",
      "Epoch: 7 \tTraining Loss: 0.000180\n",
      "Epoch: 8 \tTraining Loss: 0.000180\n",
      "Epoch: 9 \tTraining Loss: 0.000180\n",
      "Epoch: 10 \tTraining Loss: 0.000180\n",
      "Epoch: 11 \tTraining Loss: 0.000180\n",
      "Epoch: 12 \tTraining Loss: 0.000180\n",
      "Epoch: 13 \tTraining Loss: 0.000180\n",
      "Epoch: 14 \tTraining Loss: 0.000180\n",
      "Epoch: 15 \tTraining Loss: 0.000180\n",
      "Epoch: 16 \tTraining Loss: 0.000180\n",
      "Epoch: 17 \tTraining Loss: 0.000180\n",
      "Epoch: 18 \tTraining Loss: 0.000180\n",
      "Epoch: 1 \tTraining Loss: 0.000389\n",
      "Epoch: 2 \tTraining Loss: 0.000389\n",
      "Epoch: 3 \tTraining Loss: 0.000389\n",
      "Epoch: 4 \tTraining Loss: 0.000389\n",
      "Epoch: 5 \tTraining Loss: 0.000389\n",
      "Epoch: 6 \tTraining Loss: 0.000389\n",
      "Epoch: 7 \tTraining Loss: 0.000389\n",
      "Epoch: 8 \tTraining Loss: 0.000389\n",
      "Epoch: 9 \tTraining Loss: 0.000389\n",
      "Epoch: 10 \tTraining Loss: 0.000389\n",
      "Epoch: 11 \tTraining Loss: 0.000389\n",
      "Epoch: 12 \tTraining Loss: 0.000389\n",
      "Epoch: 13 \tTraining Loss: 0.000389\n",
      "Epoch: 14 \tTraining Loss: 0.000389\n",
      "Epoch: 15 \tTraining Loss: 0.000389\n",
      "Epoch: 16 \tTraining Loss: 0.000389\n",
      "Epoch: 17 \tTraining Loss: 0.000389\n",
      "Epoch: 18 \tTraining Loss: 0.000389\n",
      "Epoch: 1 \tTraining Loss: 0.000145\n",
      "Epoch: 2 \tTraining Loss: 0.000145\n",
      "Epoch: 3 \tTraining Loss: 0.000145\n",
      "Epoch: 4 \tTraining Loss: 0.000145\n",
      "Epoch: 5 \tTraining Loss: 0.000145\n",
      "Epoch: 6 \tTraining Loss: 0.000145\n",
      "Epoch: 7 \tTraining Loss: 0.000145\n",
      "Epoch: 8 \tTraining Loss: 0.000145\n",
      "Epoch: 9 \tTraining Loss: 0.000145\n",
      "Epoch: 10 \tTraining Loss: 0.000145\n",
      "Epoch: 11 \tTraining Loss: 0.000145\n",
      "Epoch: 12 \tTraining Loss: 0.000145\n",
      "Epoch: 13 \tTraining Loss: 0.000145\n",
      "Epoch: 14 \tTraining Loss: 0.000145\n",
      "Epoch: 15 \tTraining Loss: 0.000145\n",
      "Epoch: 16 \tTraining Loss: 0.000145\n",
      "Epoch: 17 \tTraining Loss: 0.000145\n",
      "Epoch: 18 \tTraining Loss: 0.000145\n",
      "Epoch: 1 \tTraining Loss: 0.000202\n",
      "Epoch: 2 \tTraining Loss: 0.000202\n",
      "Epoch: 3 \tTraining Loss: 0.000202\n",
      "Epoch: 4 \tTraining Loss: 0.000202\n",
      "Epoch: 5 \tTraining Loss: 0.000202\n",
      "Epoch: 6 \tTraining Loss: 0.000202\n",
      "Epoch: 7 \tTraining Loss: 0.000202\n",
      "Epoch: 8 \tTraining Loss: 0.000202\n",
      "Epoch: 9 \tTraining Loss: 0.000202\n",
      "Epoch: 10 \tTraining Loss: 0.000202\n",
      "Epoch: 11 \tTraining Loss: 0.000202\n",
      "Epoch: 12 \tTraining Loss: 0.000202\n",
      "Epoch: 13 \tTraining Loss: 0.000202\n",
      "Epoch: 14 \tTraining Loss: 0.000202\n",
      "Epoch: 15 \tTraining Loss: 0.000202\n",
      "Epoch: 16 \tTraining Loss: 0.000202\n",
      "Epoch: 17 \tTraining Loss: 0.000202\n",
      "Epoch: 18 \tTraining Loss: 0.000202\n",
      "Epoch: 1 \tTraining Loss: 0.000070\n",
      "Epoch: 2 \tTraining Loss: 0.000070\n",
      "Epoch: 3 \tTraining Loss: 0.000070\n",
      "Epoch: 4 \tTraining Loss: 0.000070\n",
      "Epoch: 5 \tTraining Loss: 0.000070\n",
      "Epoch: 6 \tTraining Loss: 0.000070\n",
      "Epoch: 7 \tTraining Loss: 0.000070\n",
      "Epoch: 8 \tTraining Loss: 0.000070\n",
      "Epoch: 9 \tTraining Loss: 0.000070\n",
      "Epoch: 10 \tTraining Loss: 0.000070\n",
      "Epoch: 11 \tTraining Loss: 0.000070\n",
      "Epoch: 12 \tTraining Loss: 0.000070\n",
      "Epoch: 13 \tTraining Loss: 0.000070\n",
      "Epoch: 14 \tTraining Loss: 0.000070\n",
      "Epoch: 15 \tTraining Loss: 0.000070\n",
      "Epoch: 16 \tTraining Loss: 0.000070\n",
      "Epoch: 17 \tTraining Loss: 0.000070\n",
      "Epoch: 18 \tTraining Loss: 0.000070\n",
      "Epoch: 1 \tTraining Loss: 0.000157\n",
      "Epoch: 2 \tTraining Loss: 0.000157\n",
      "Epoch: 3 \tTraining Loss: 0.000157\n",
      "Epoch: 4 \tTraining Loss: 0.000157\n",
      "Epoch: 5 \tTraining Loss: 0.000157\n",
      "Epoch: 6 \tTraining Loss: 0.000157\n",
      "Epoch: 7 \tTraining Loss: 0.000157\n",
      "Epoch: 8 \tTraining Loss: 0.000157\n",
      "Epoch: 9 \tTraining Loss: 0.000157\n",
      "Epoch: 10 \tTraining Loss: 0.000157\n",
      "Epoch: 11 \tTraining Loss: 0.000157\n",
      "Epoch: 12 \tTraining Loss: 0.000157\n",
      "Epoch: 13 \tTraining Loss: 0.000157\n",
      "Epoch: 14 \tTraining Loss: 0.000157\n",
      "Epoch: 15 \tTraining Loss: 0.000157\n",
      "Epoch: 16 \tTraining Loss: 0.000157\n",
      "Epoch: 17 \tTraining Loss: 0.000157\n",
      "Epoch: 18 \tTraining Loss: 0.000157\n",
      "Epoch: 1 \tTraining Loss: 0.000172\n",
      "Epoch: 2 \tTraining Loss: 0.000172\n",
      "Epoch: 3 \tTraining Loss: 0.000172\n",
      "Epoch: 4 \tTraining Loss: 0.000172\n",
      "Epoch: 5 \tTraining Loss: 0.000172\n",
      "Epoch: 6 \tTraining Loss: 0.000172\n",
      "Epoch: 7 \tTraining Loss: 0.000172\n",
      "Epoch: 8 \tTraining Loss: 0.000172\n",
      "Epoch: 9 \tTraining Loss: 0.000172\n",
      "Epoch: 10 \tTraining Loss: 0.000172\n",
      "Epoch: 11 \tTraining Loss: 0.000172\n",
      "Epoch: 12 \tTraining Loss: 0.000172\n",
      "Epoch: 13 \tTraining Loss: 0.000172\n",
      "Epoch: 14 \tTraining Loss: 0.000172\n",
      "Epoch: 15 \tTraining Loss: 0.000172\n",
      "Epoch: 16 \tTraining Loss: 0.000172\n",
      "Epoch: 17 \tTraining Loss: 0.000172\n",
      "Epoch: 18 \tTraining Loss: 0.000172\n",
      "Epoch: 1 \tTraining Loss: 0.000240\n",
      "Epoch: 2 \tTraining Loss: 0.000240\n",
      "Epoch: 3 \tTraining Loss: 0.000240\n",
      "Epoch: 4 \tTraining Loss: 0.000240\n",
      "Epoch: 5 \tTraining Loss: 0.000240\n",
      "Epoch: 6 \tTraining Loss: 0.000240\n",
      "Epoch: 7 \tTraining Loss: 0.000240\n",
      "Epoch: 8 \tTraining Loss: 0.000240\n",
      "Epoch: 9 \tTraining Loss: 0.000240\n",
      "Epoch: 10 \tTraining Loss: 0.000240\n",
      "Epoch: 11 \tTraining Loss: 0.000240\n",
      "Epoch: 12 \tTraining Loss: 0.000240\n",
      "Epoch: 13 \tTraining Loss: 0.000240\n",
      "Epoch: 14 \tTraining Loss: 0.000240\n",
      "Epoch: 15 \tTraining Loss: 0.000240\n",
      "Epoch: 16 \tTraining Loss: 0.000240\n",
      "Epoch: 17 \tTraining Loss: 0.000240\n",
      "Epoch: 18 \tTraining Loss: 0.000240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000137\n",
      "Epoch: 2 \tTraining Loss: 0.000137\n",
      "Epoch: 3 \tTraining Loss: 0.000137\n",
      "Epoch: 4 \tTraining Loss: 0.000137\n",
      "Epoch: 5 \tTraining Loss: 0.000137\n",
      "Epoch: 6 \tTraining Loss: 0.000137\n",
      "Epoch: 7 \tTraining Loss: 0.000137\n",
      "Epoch: 8 \tTraining Loss: 0.000137\n",
      "Epoch: 9 \tTraining Loss: 0.000137\n",
      "Epoch: 10 \tTraining Loss: 0.000137\n",
      "Epoch: 11 \tTraining Loss: 0.000137\n",
      "Epoch: 12 \tTraining Loss: 0.000137\n",
      "Epoch: 13 \tTraining Loss: 0.000137\n",
      "Epoch: 14 \tTraining Loss: 0.000137\n",
      "Epoch: 15 \tTraining Loss: 0.000137\n",
      "Epoch: 16 \tTraining Loss: 0.000137\n",
      "Epoch: 17 \tTraining Loss: 0.000137\n",
      "Epoch: 18 \tTraining Loss: 0.000137\n",
      "Epoch: 1 \tTraining Loss: 0.000073\n",
      "Epoch: 2 \tTraining Loss: 0.000073\n",
      "Epoch: 3 \tTraining Loss: 0.000073\n",
      "Epoch: 4 \tTraining Loss: 0.000073\n",
      "Epoch: 5 \tTraining Loss: 0.000073\n",
      "Epoch: 6 \tTraining Loss: 0.000073\n",
      "Epoch: 7 \tTraining Loss: 0.000073\n",
      "Epoch: 8 \tTraining Loss: 0.000073\n",
      "Epoch: 9 \tTraining Loss: 0.000073\n",
      "Epoch: 10 \tTraining Loss: 0.000073\n",
      "Epoch: 11 \tTraining Loss: 0.000073\n",
      "Epoch: 12 \tTraining Loss: 0.000073\n",
      "Epoch: 13 \tTraining Loss: 0.000073\n",
      "Epoch: 14 \tTraining Loss: 0.000073\n",
      "Epoch: 15 \tTraining Loss: 0.000073\n",
      "Epoch: 16 \tTraining Loss: 0.000073\n",
      "Epoch: 17 \tTraining Loss: 0.000073\n",
      "Epoch: 18 \tTraining Loss: 0.000073\n",
      "Epoch: 1 \tTraining Loss: 0.000243\n",
      "Epoch: 2 \tTraining Loss: 0.000243\n",
      "Epoch: 3 \tTraining Loss: 0.000243\n",
      "Epoch: 4 \tTraining Loss: 0.000243\n",
      "Epoch: 5 \tTraining Loss: 0.000243\n",
      "Epoch: 6 \tTraining Loss: 0.000243\n",
      "Epoch: 7 \tTraining Loss: 0.000243\n",
      "Epoch: 8 \tTraining Loss: 0.000243\n",
      "Epoch: 9 \tTraining Loss: 0.000243\n",
      "Epoch: 10 \tTraining Loss: 0.000243\n",
      "Epoch: 11 \tTraining Loss: 0.000243\n",
      "Epoch: 12 \tTraining Loss: 0.000243\n",
      "Epoch: 13 \tTraining Loss: 0.000243\n",
      "Epoch: 14 \tTraining Loss: 0.000243\n",
      "Epoch: 15 \tTraining Loss: 0.000243\n",
      "Epoch: 16 \tTraining Loss: 0.000243\n",
      "Epoch: 17 \tTraining Loss: 0.000243\n",
      "Epoch: 18 \tTraining Loss: 0.000243\n",
      "Epoch: 1 \tTraining Loss: 0.000089\n",
      "Epoch: 2 \tTraining Loss: 0.000089\n",
      "Epoch: 3 \tTraining Loss: 0.000089\n",
      "Epoch: 4 \tTraining Loss: 0.000089\n",
      "Epoch: 5 \tTraining Loss: 0.000089\n",
      "Epoch: 6 \tTraining Loss: 0.000089\n",
      "Epoch: 7 \tTraining Loss: 0.000089\n",
      "Epoch: 8 \tTraining Loss: 0.000089\n",
      "Epoch: 9 \tTraining Loss: 0.000089\n",
      "Epoch: 10 \tTraining Loss: 0.000089\n",
      "Epoch: 11 \tTraining Loss: 0.000089\n",
      "Epoch: 12 \tTraining Loss: 0.000089\n",
      "Epoch: 13 \tTraining Loss: 0.000089\n",
      "Epoch: 14 \tTraining Loss: 0.000089\n",
      "Epoch: 15 \tTraining Loss: 0.000089\n",
      "Epoch: 16 \tTraining Loss: 0.000089\n",
      "Epoch: 17 \tTraining Loss: 0.000089\n",
      "Epoch: 18 \tTraining Loss: 0.000089\n",
      "Epoch: 1 \tTraining Loss: 0.000142\n",
      "Epoch: 2 \tTraining Loss: 0.000142\n",
      "Epoch: 3 \tTraining Loss: 0.000142\n",
      "Epoch: 4 \tTraining Loss: 0.000142\n",
      "Epoch: 5 \tTraining Loss: 0.000142\n",
      "Epoch: 6 \tTraining Loss: 0.000142\n",
      "Epoch: 7 \tTraining Loss: 0.000142\n",
      "Epoch: 8 \tTraining Loss: 0.000142\n",
      "Epoch: 9 \tTraining Loss: 0.000142\n",
      "Epoch: 10 \tTraining Loss: 0.000142\n",
      "Epoch: 11 \tTraining Loss: 0.000142\n",
      "Epoch: 12 \tTraining Loss: 0.000142\n",
      "Epoch: 13 \tTraining Loss: 0.000142\n",
      "Epoch: 14 \tTraining Loss: 0.000142\n",
      "Epoch: 15 \tTraining Loss: 0.000142\n",
      "Epoch: 16 \tTraining Loss: 0.000142\n",
      "Epoch: 17 \tTraining Loss: 0.000142\n",
      "Epoch: 18 \tTraining Loss: 0.000142\n",
      "Epoch: 1 \tTraining Loss: 0.000160\n",
      "Epoch: 2 \tTraining Loss: 0.000160\n",
      "Epoch: 3 \tTraining Loss: 0.000160\n",
      "Epoch: 4 \tTraining Loss: 0.000160\n",
      "Epoch: 5 \tTraining Loss: 0.000160\n",
      "Epoch: 6 \tTraining Loss: 0.000160\n",
      "Epoch: 7 \tTraining Loss: 0.000160\n",
      "Epoch: 8 \tTraining Loss: 0.000160\n",
      "Epoch: 9 \tTraining Loss: 0.000160\n",
      "Epoch: 10 \tTraining Loss: 0.000160\n",
      "Epoch: 11 \tTraining Loss: 0.000160\n",
      "Epoch: 12 \tTraining Loss: 0.000160\n",
      "Epoch: 13 \tTraining Loss: 0.000160\n",
      "Epoch: 14 \tTraining Loss: 0.000160\n",
      "Epoch: 15 \tTraining Loss: 0.000160\n",
      "Epoch: 16 \tTraining Loss: 0.000160\n",
      "Epoch: 17 \tTraining Loss: 0.000160\n",
      "Epoch: 18 \tTraining Loss: 0.000160\n",
      "Epoch: 1 \tTraining Loss: 0.000124\n",
      "Epoch: 2 \tTraining Loss: 0.000124\n",
      "Epoch: 3 \tTraining Loss: 0.000124\n",
      "Epoch: 4 \tTraining Loss: 0.000124\n",
      "Epoch: 5 \tTraining Loss: 0.000124\n",
      "Epoch: 6 \tTraining Loss: 0.000124\n",
      "Epoch: 7 \tTraining Loss: 0.000124\n",
      "Epoch: 8 \tTraining Loss: 0.000124\n",
      "Epoch: 9 \tTraining Loss: 0.000124\n",
      "Epoch: 10 \tTraining Loss: 0.000124\n",
      "Epoch: 11 \tTraining Loss: 0.000124\n",
      "Epoch: 12 \tTraining Loss: 0.000124\n",
      "Epoch: 13 \tTraining Loss: 0.000124\n",
      "Epoch: 14 \tTraining Loss: 0.000124\n",
      "Epoch: 15 \tTraining Loss: 0.000124\n",
      "Epoch: 16 \tTraining Loss: 0.000124\n",
      "Epoch: 17 \tTraining Loss: 0.000124\n",
      "Epoch: 18 \tTraining Loss: 0.000124\n",
      "Epoch: 1 \tTraining Loss: 0.000123\n",
      "Epoch: 2 \tTraining Loss: 0.000123\n",
      "Epoch: 3 \tTraining Loss: 0.000123\n",
      "Epoch: 4 \tTraining Loss: 0.000123\n",
      "Epoch: 5 \tTraining Loss: 0.000123\n",
      "Epoch: 6 \tTraining Loss: 0.000123\n",
      "Epoch: 7 \tTraining Loss: 0.000123\n",
      "Epoch: 8 \tTraining Loss: 0.000123\n",
      "Epoch: 9 \tTraining Loss: 0.000123\n",
      "Epoch: 10 \tTraining Loss: 0.000123\n",
      "Epoch: 11 \tTraining Loss: 0.000123\n",
      "Epoch: 12 \tTraining Loss: 0.000123\n",
      "Epoch: 13 \tTraining Loss: 0.000123\n",
      "Epoch: 14 \tTraining Loss: 0.000123\n",
      "Epoch: 15 \tTraining Loss: 0.000123\n",
      "Epoch: 16 \tTraining Loss: 0.000123\n",
      "Epoch: 17 \tTraining Loss: 0.000123\n",
      "Epoch: 18 \tTraining Loss: 0.000123\n",
      "Epoch: 1 \tTraining Loss: 0.000326\n",
      "Epoch: 2 \tTraining Loss: 0.000326\n",
      "Epoch: 3 \tTraining Loss: 0.000326\n",
      "Epoch: 4 \tTraining Loss: 0.000326\n",
      "Epoch: 5 \tTraining Loss: 0.000326\n",
      "Epoch: 6 \tTraining Loss: 0.000326\n",
      "Epoch: 7 \tTraining Loss: 0.000326\n",
      "Epoch: 8 \tTraining Loss: 0.000326\n",
      "Epoch: 9 \tTraining Loss: 0.000326\n",
      "Epoch: 10 \tTraining Loss: 0.000326\n",
      "Epoch: 11 \tTraining Loss: 0.000326\n",
      "Epoch: 12 \tTraining Loss: 0.000326\n",
      "Epoch: 13 \tTraining Loss: 0.000326\n",
      "Epoch: 14 \tTraining Loss: 0.000326\n",
      "Epoch: 15 \tTraining Loss: 0.000326\n",
      "Epoch: 16 \tTraining Loss: 0.000326\n",
      "Epoch: 17 \tTraining Loss: 0.000326\n",
      "Epoch: 18 \tTraining Loss: 0.000326\n",
      "Epoch: 1 \tTraining Loss: 0.000177\n",
      "Epoch: 2 \tTraining Loss: 0.000177\n",
      "Epoch: 3 \tTraining Loss: 0.000177\n",
      "Epoch: 4 \tTraining Loss: 0.000177\n",
      "Epoch: 5 \tTraining Loss: 0.000177\n",
      "Epoch: 6 \tTraining Loss: 0.000177\n",
      "Epoch: 7 \tTraining Loss: 0.000177\n",
      "Epoch: 8 \tTraining Loss: 0.000177\n",
      "Epoch: 9 \tTraining Loss: 0.000177\n",
      "Epoch: 10 \tTraining Loss: 0.000177\n",
      "Epoch: 11 \tTraining Loss: 0.000177\n",
      "Epoch: 12 \tTraining Loss: 0.000177\n",
      "Epoch: 13 \tTraining Loss: 0.000177\n",
      "Epoch: 14 \tTraining Loss: 0.000177\n",
      "Epoch: 15 \tTraining Loss: 0.000177\n",
      "Epoch: 16 \tTraining Loss: 0.000177\n",
      "Epoch: 17 \tTraining Loss: 0.000177\n",
      "Epoch: 18 \tTraining Loss: 0.000177\n",
      "Epoch: 1 \tTraining Loss: 0.000260\n",
      "Epoch: 2 \tTraining Loss: 0.000260\n",
      "Epoch: 3 \tTraining Loss: 0.000260\n",
      "Epoch: 4 \tTraining Loss: 0.000260\n",
      "Epoch: 5 \tTraining Loss: 0.000260\n",
      "Epoch: 6 \tTraining Loss: 0.000260\n",
      "Epoch: 7 \tTraining Loss: 0.000260\n",
      "Epoch: 8 \tTraining Loss: 0.000260\n",
      "Epoch: 9 \tTraining Loss: 0.000260\n",
      "Epoch: 10 \tTraining Loss: 0.000260\n",
      "Epoch: 11 \tTraining Loss: 0.000260\n",
      "Epoch: 12 \tTraining Loss: 0.000260\n",
      "Epoch: 13 \tTraining Loss: 0.000260\n",
      "Epoch: 14 \tTraining Loss: 0.000260\n",
      "Epoch: 15 \tTraining Loss: 0.000260\n",
      "Epoch: 16 \tTraining Loss: 0.000260\n",
      "Epoch: 17 \tTraining Loss: 0.000260\n",
      "Epoch: 18 \tTraining Loss: 0.000260\n",
      "Epoch: 1 \tTraining Loss: 0.000233\n",
      "Epoch: 2 \tTraining Loss: 0.000233\n",
      "Epoch: 3 \tTraining Loss: 0.000233\n",
      "Epoch: 4 \tTraining Loss: 0.000233\n",
      "Epoch: 5 \tTraining Loss: 0.000233\n",
      "Epoch: 6 \tTraining Loss: 0.000233\n",
      "Epoch: 7 \tTraining Loss: 0.000233\n",
      "Epoch: 8 \tTraining Loss: 0.000233\n",
      "Epoch: 9 \tTraining Loss: 0.000233\n",
      "Epoch: 10 \tTraining Loss: 0.000233\n",
      "Epoch: 11 \tTraining Loss: 0.000233\n",
      "Epoch: 12 \tTraining Loss: 0.000233\n",
      "Epoch: 13 \tTraining Loss: 0.000233\n",
      "Epoch: 14 \tTraining Loss: 0.000233\n",
      "Epoch: 15 \tTraining Loss: 0.000233\n",
      "Epoch: 16 \tTraining Loss: 0.000233\n",
      "Epoch: 17 \tTraining Loss: 0.000233\n",
      "Epoch: 18 \tTraining Loss: 0.000233\n",
      "Epoch: 1 \tTraining Loss: 0.000112\n",
      "Epoch: 2 \tTraining Loss: 0.000112\n",
      "Epoch: 3 \tTraining Loss: 0.000112\n",
      "Epoch: 4 \tTraining Loss: 0.000112\n",
      "Epoch: 5 \tTraining Loss: 0.000112\n",
      "Epoch: 6 \tTraining Loss: 0.000112\n",
      "Epoch: 7 \tTraining Loss: 0.000112\n",
      "Epoch: 8 \tTraining Loss: 0.000112\n",
      "Epoch: 9 \tTraining Loss: 0.000112\n",
      "Epoch: 10 \tTraining Loss: 0.000112\n",
      "Epoch: 11 \tTraining Loss: 0.000112\n",
      "Epoch: 12 \tTraining Loss: 0.000112\n",
      "Epoch: 13 \tTraining Loss: 0.000112\n",
      "Epoch: 14 \tTraining Loss: 0.000112\n",
      "Epoch: 15 \tTraining Loss: 0.000112\n",
      "Epoch: 16 \tTraining Loss: 0.000112\n",
      "Epoch: 17 \tTraining Loss: 0.000112\n",
      "Epoch: 18 \tTraining Loss: 0.000112\n",
      "Epoch: 1 \tTraining Loss: 0.000171\n",
      "Epoch: 2 \tTraining Loss: 0.000171\n",
      "Epoch: 3 \tTraining Loss: 0.000171\n",
      "Epoch: 4 \tTraining Loss: 0.000171\n",
      "Epoch: 5 \tTraining Loss: 0.000171\n",
      "Epoch: 6 \tTraining Loss: 0.000171\n",
      "Epoch: 7 \tTraining Loss: 0.000171\n",
      "Epoch: 8 \tTraining Loss: 0.000171\n",
      "Epoch: 9 \tTraining Loss: 0.000171\n",
      "Epoch: 10 \tTraining Loss: 0.000171\n",
      "Epoch: 11 \tTraining Loss: 0.000171\n",
      "Epoch: 12 \tTraining Loss: 0.000171\n",
      "Epoch: 13 \tTraining Loss: 0.000171\n",
      "Epoch: 14 \tTraining Loss: 0.000171\n",
      "Epoch: 15 \tTraining Loss: 0.000171\n",
      "Epoch: 16 \tTraining Loss: 0.000171\n",
      "Epoch: 17 \tTraining Loss: 0.000171\n",
      "Epoch: 18 \tTraining Loss: 0.000171\n",
      "Epoch: 1 \tTraining Loss: 0.000167\n",
      "Epoch: 2 \tTraining Loss: 0.000167\n",
      "Epoch: 3 \tTraining Loss: 0.000167\n",
      "Epoch: 4 \tTraining Loss: 0.000167\n",
      "Epoch: 5 \tTraining Loss: 0.000167\n",
      "Epoch: 6 \tTraining Loss: 0.000167\n",
      "Epoch: 7 \tTraining Loss: 0.000167\n",
      "Epoch: 8 \tTraining Loss: 0.000167\n",
      "Epoch: 9 \tTraining Loss: 0.000167\n",
      "Epoch: 10 \tTraining Loss: 0.000167\n",
      "Epoch: 11 \tTraining Loss: 0.000167\n",
      "Epoch: 12 \tTraining Loss: 0.000167\n",
      "Epoch: 13 \tTraining Loss: 0.000167\n",
      "Epoch: 14 \tTraining Loss: 0.000167\n",
      "Epoch: 15 \tTraining Loss: 0.000167\n",
      "Epoch: 16 \tTraining Loss: 0.000167\n",
      "Epoch: 17 \tTraining Loss: 0.000167\n",
      "Epoch: 18 \tTraining Loss: 0.000167\n",
      "Epoch: 1 \tTraining Loss: 0.000147\n",
      "Epoch: 2 \tTraining Loss: 0.000147\n",
      "Epoch: 3 \tTraining Loss: 0.000147\n",
      "Epoch: 4 \tTraining Loss: 0.000147\n",
      "Epoch: 5 \tTraining Loss: 0.000147\n",
      "Epoch: 6 \tTraining Loss: 0.000147\n",
      "Epoch: 7 \tTraining Loss: 0.000147\n",
      "Epoch: 8 \tTraining Loss: 0.000147\n",
      "Epoch: 9 \tTraining Loss: 0.000147\n",
      "Epoch: 10 \tTraining Loss: 0.000147\n",
      "Epoch: 11 \tTraining Loss: 0.000147\n",
      "Epoch: 12 \tTraining Loss: 0.000147\n",
      "Epoch: 13 \tTraining Loss: 0.000147\n",
      "Epoch: 14 \tTraining Loss: 0.000147\n",
      "Epoch: 15 \tTraining Loss: 0.000147\n",
      "Epoch: 16 \tTraining Loss: 0.000147\n",
      "Epoch: 17 \tTraining Loss: 0.000147\n",
      "Epoch: 18 \tTraining Loss: 0.000147\n",
      "Epoch: 1 \tTraining Loss: 0.000229\n",
      "Epoch: 2 \tTraining Loss: 0.000229\n",
      "Epoch: 3 \tTraining Loss: 0.000229\n",
      "Epoch: 4 \tTraining Loss: 0.000229\n",
      "Epoch: 5 \tTraining Loss: 0.000229\n",
      "Epoch: 6 \tTraining Loss: 0.000229\n",
      "Epoch: 7 \tTraining Loss: 0.000229\n",
      "Epoch: 8 \tTraining Loss: 0.000229\n",
      "Epoch: 9 \tTraining Loss: 0.000229\n",
      "Epoch: 10 \tTraining Loss: 0.000229\n",
      "Epoch: 11 \tTraining Loss: 0.000229\n",
      "Epoch: 12 \tTraining Loss: 0.000229\n",
      "Epoch: 13 \tTraining Loss: 0.000229\n",
      "Epoch: 14 \tTraining Loss: 0.000229\n",
      "Epoch: 15 \tTraining Loss: 0.000229\n",
      "Epoch: 16 \tTraining Loss: 0.000229\n",
      "Epoch: 17 \tTraining Loss: 0.000229\n",
      "Epoch: 18 \tTraining Loss: 0.000229\n",
      "Epoch: 1 \tTraining Loss: 0.000096\n",
      "Epoch: 2 \tTraining Loss: 0.000096\n",
      "Epoch: 3 \tTraining Loss: 0.000096\n",
      "Epoch: 4 \tTraining Loss: 0.000096\n",
      "Epoch: 5 \tTraining Loss: 0.000096\n",
      "Epoch: 6 \tTraining Loss: 0.000096\n",
      "Epoch: 7 \tTraining Loss: 0.000096\n",
      "Epoch: 8 \tTraining Loss: 0.000096\n",
      "Epoch: 9 \tTraining Loss: 0.000096\n",
      "Epoch: 10 \tTraining Loss: 0.000096\n",
      "Epoch: 11 \tTraining Loss: 0.000096\n",
      "Epoch: 12 \tTraining Loss: 0.000096\n",
      "Epoch: 13 \tTraining Loss: 0.000096\n",
      "Epoch: 14 \tTraining Loss: 0.000096\n",
      "Epoch: 15 \tTraining Loss: 0.000096\n",
      "Epoch: 16 \tTraining Loss: 0.000096\n",
      "Epoch: 17 \tTraining Loss: 0.000096\n",
      "Epoch: 18 \tTraining Loss: 0.000096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000081\n",
      "Epoch: 2 \tTraining Loss: 0.000081\n",
      "Epoch: 3 \tTraining Loss: 0.000081\n",
      "Epoch: 4 \tTraining Loss: 0.000081\n",
      "Epoch: 5 \tTraining Loss: 0.000081\n",
      "Epoch: 6 \tTraining Loss: 0.000081\n",
      "Epoch: 7 \tTraining Loss: 0.000081\n",
      "Epoch: 8 \tTraining Loss: 0.000081\n",
      "Epoch: 9 \tTraining Loss: 0.000081\n",
      "Epoch: 10 \tTraining Loss: 0.000081\n",
      "Epoch: 11 \tTraining Loss: 0.000081\n",
      "Epoch: 12 \tTraining Loss: 0.000081\n",
      "Epoch: 13 \tTraining Loss: 0.000081\n",
      "Epoch: 14 \tTraining Loss: 0.000081\n",
      "Epoch: 15 \tTraining Loss: 0.000081\n",
      "Epoch: 16 \tTraining Loss: 0.000081\n",
      "Epoch: 17 \tTraining Loss: 0.000081\n",
      "Epoch: 18 \tTraining Loss: 0.000081\n",
      "Epoch: 1 \tTraining Loss: 0.000240\n",
      "Epoch: 2 \tTraining Loss: 0.000240\n",
      "Epoch: 3 \tTraining Loss: 0.000240\n",
      "Epoch: 4 \tTraining Loss: 0.000240\n",
      "Epoch: 5 \tTraining Loss: 0.000240\n",
      "Epoch: 6 \tTraining Loss: 0.000240\n",
      "Epoch: 7 \tTraining Loss: 0.000240\n",
      "Epoch: 8 \tTraining Loss: 0.000240\n",
      "Epoch: 9 \tTraining Loss: 0.000240\n",
      "Epoch: 10 \tTraining Loss: 0.000240\n",
      "Epoch: 11 \tTraining Loss: 0.000240\n",
      "Epoch: 12 \tTraining Loss: 0.000240\n",
      "Epoch: 13 \tTraining Loss: 0.000240\n",
      "Epoch: 14 \tTraining Loss: 0.000240\n",
      "Epoch: 15 \tTraining Loss: 0.000240\n",
      "Epoch: 16 \tTraining Loss: 0.000240\n",
      "Epoch: 17 \tTraining Loss: 0.000240\n",
      "Epoch: 18 \tTraining Loss: 0.000240\n",
      "Epoch: 1 \tTraining Loss: 0.000159\n",
      "Epoch: 2 \tTraining Loss: 0.000159\n",
      "Epoch: 3 \tTraining Loss: 0.000159\n",
      "Epoch: 4 \tTraining Loss: 0.000159\n",
      "Epoch: 5 \tTraining Loss: 0.000159\n",
      "Epoch: 6 \tTraining Loss: 0.000159\n",
      "Epoch: 7 \tTraining Loss: 0.000159\n",
      "Epoch: 8 \tTraining Loss: 0.000159\n",
      "Epoch: 9 \tTraining Loss: 0.000159\n",
      "Epoch: 10 \tTraining Loss: 0.000159\n",
      "Epoch: 11 \tTraining Loss: 0.000159\n",
      "Epoch: 12 \tTraining Loss: 0.000159\n",
      "Epoch: 13 \tTraining Loss: 0.000159\n",
      "Epoch: 14 \tTraining Loss: 0.000159\n",
      "Epoch: 15 \tTraining Loss: 0.000159\n",
      "Epoch: 16 \tTraining Loss: 0.000159\n",
      "Epoch: 17 \tTraining Loss: 0.000159\n",
      "Epoch: 18 \tTraining Loss: 0.000159\n",
      "Epoch: 1 \tTraining Loss: 0.000126\n",
      "Epoch: 2 \tTraining Loss: 0.000126\n",
      "Epoch: 3 \tTraining Loss: 0.000126\n",
      "Epoch: 4 \tTraining Loss: 0.000126\n",
      "Epoch: 5 \tTraining Loss: 0.000126\n",
      "Epoch: 6 \tTraining Loss: 0.000126\n",
      "Epoch: 7 \tTraining Loss: 0.000126\n",
      "Epoch: 8 \tTraining Loss: 0.000126\n",
      "Epoch: 9 \tTraining Loss: 0.000126\n",
      "Epoch: 10 \tTraining Loss: 0.000126\n",
      "Epoch: 11 \tTraining Loss: 0.000126\n",
      "Epoch: 12 \tTraining Loss: 0.000126\n",
      "Epoch: 13 \tTraining Loss: 0.000126\n",
      "Epoch: 14 \tTraining Loss: 0.000126\n",
      "Epoch: 15 \tTraining Loss: 0.000126\n",
      "Epoch: 16 \tTraining Loss: 0.000126\n",
      "Epoch: 17 \tTraining Loss: 0.000126\n",
      "Epoch: 18 \tTraining Loss: 0.000126\n",
      "Epoch: 1 \tTraining Loss: 0.000148\n",
      "Epoch: 2 \tTraining Loss: 0.000148\n",
      "Epoch: 3 \tTraining Loss: 0.000148\n",
      "Epoch: 4 \tTraining Loss: 0.000148\n",
      "Epoch: 5 \tTraining Loss: 0.000148\n",
      "Epoch: 6 \tTraining Loss: 0.000148\n",
      "Epoch: 7 \tTraining Loss: 0.000148\n",
      "Epoch: 8 \tTraining Loss: 0.000148\n",
      "Epoch: 9 \tTraining Loss: 0.000148\n",
      "Epoch: 10 \tTraining Loss: 0.000148\n",
      "Epoch: 11 \tTraining Loss: 0.000148\n",
      "Epoch: 12 \tTraining Loss: 0.000148\n",
      "Epoch: 13 \tTraining Loss: 0.000148\n",
      "Epoch: 14 \tTraining Loss: 0.000148\n",
      "Epoch: 15 \tTraining Loss: 0.000148\n",
      "Epoch: 16 \tTraining Loss: 0.000148\n",
      "Epoch: 17 \tTraining Loss: 0.000148\n",
      "Epoch: 18 \tTraining Loss: 0.000148\n",
      "Epoch: 1 \tTraining Loss: 0.000161\n",
      "Epoch: 2 \tTraining Loss: 0.000161\n",
      "Epoch: 3 \tTraining Loss: 0.000161\n",
      "Epoch: 4 \tTraining Loss: 0.000161\n",
      "Epoch: 5 \tTraining Loss: 0.000161\n",
      "Epoch: 6 \tTraining Loss: 0.000161\n",
      "Epoch: 7 \tTraining Loss: 0.000161\n",
      "Epoch: 8 \tTraining Loss: 0.000161\n",
      "Epoch: 9 \tTraining Loss: 0.000161\n",
      "Epoch: 10 \tTraining Loss: 0.000161\n",
      "Epoch: 11 \tTraining Loss: 0.000161\n",
      "Epoch: 12 \tTraining Loss: 0.000161\n",
      "Epoch: 13 \tTraining Loss: 0.000161\n",
      "Epoch: 14 \tTraining Loss: 0.000161\n",
      "Epoch: 15 \tTraining Loss: 0.000161\n",
      "Epoch: 16 \tTraining Loss: 0.000161\n",
      "Epoch: 17 \tTraining Loss: 0.000161\n",
      "Epoch: 18 \tTraining Loss: 0.000161\n",
      "Epoch: 1 \tTraining Loss: 0.000085\n",
      "Epoch: 2 \tTraining Loss: 0.000085\n",
      "Epoch: 3 \tTraining Loss: 0.000085\n",
      "Epoch: 4 \tTraining Loss: 0.000085\n",
      "Epoch: 5 \tTraining Loss: 0.000085\n",
      "Epoch: 6 \tTraining Loss: 0.000085\n",
      "Epoch: 7 \tTraining Loss: 0.000085\n",
      "Epoch: 8 \tTraining Loss: 0.000085\n",
      "Epoch: 9 \tTraining Loss: 0.000085\n",
      "Epoch: 10 \tTraining Loss: 0.000085\n",
      "Epoch: 11 \tTraining Loss: 0.000085\n",
      "Epoch: 12 \tTraining Loss: 0.000085\n",
      "Epoch: 13 \tTraining Loss: 0.000085\n",
      "Epoch: 14 \tTraining Loss: 0.000085\n",
      "Epoch: 15 \tTraining Loss: 0.000085\n",
      "Epoch: 16 \tTraining Loss: 0.000085\n",
      "Epoch: 17 \tTraining Loss: 0.000085\n",
      "Epoch: 18 \tTraining Loss: 0.000085\n",
      "Epoch: 1 \tTraining Loss: 0.000402\n",
      "Epoch: 2 \tTraining Loss: 0.000402\n",
      "Epoch: 3 \tTraining Loss: 0.000402\n",
      "Epoch: 4 \tTraining Loss: 0.000402\n",
      "Epoch: 5 \tTraining Loss: 0.000402\n",
      "Epoch: 6 \tTraining Loss: 0.000402\n",
      "Epoch: 7 \tTraining Loss: 0.000402\n",
      "Epoch: 8 \tTraining Loss: 0.000402\n",
      "Epoch: 9 \tTraining Loss: 0.000402\n",
      "Epoch: 10 \tTraining Loss: 0.000402\n",
      "Epoch: 11 \tTraining Loss: 0.000402\n",
      "Epoch: 12 \tTraining Loss: 0.000402\n",
      "Epoch: 13 \tTraining Loss: 0.000402\n",
      "Epoch: 14 \tTraining Loss: 0.000402\n",
      "Epoch: 15 \tTraining Loss: 0.000402\n",
      "Epoch: 16 \tTraining Loss: 0.000402\n",
      "Epoch: 17 \tTraining Loss: 0.000402\n",
      "Epoch: 18 \tTraining Loss: 0.000402\n",
      "Epoch: 1 \tTraining Loss: 0.000214\n",
      "Epoch: 2 \tTraining Loss: 0.000214\n",
      "Epoch: 3 \tTraining Loss: 0.000214\n",
      "Epoch: 4 \tTraining Loss: 0.000214\n",
      "Epoch: 5 \tTraining Loss: 0.000214\n",
      "Epoch: 6 \tTraining Loss: 0.000214\n",
      "Epoch: 7 \tTraining Loss: 0.000214\n",
      "Epoch: 8 \tTraining Loss: 0.000214\n",
      "Epoch: 9 \tTraining Loss: 0.000214\n",
      "Epoch: 10 \tTraining Loss: 0.000214\n",
      "Epoch: 11 \tTraining Loss: 0.000214\n",
      "Epoch: 12 \tTraining Loss: 0.000214\n",
      "Epoch: 13 \tTraining Loss: 0.000214\n",
      "Epoch: 14 \tTraining Loss: 0.000214\n",
      "Epoch: 15 \tTraining Loss: 0.000214\n",
      "Epoch: 16 \tTraining Loss: 0.000214\n",
      "Epoch: 17 \tTraining Loss: 0.000214\n",
      "Epoch: 18 \tTraining Loss: 0.000214\n",
      "Epoch: 1 \tTraining Loss: 0.000261\n",
      "Epoch: 2 \tTraining Loss: 0.000261\n",
      "Epoch: 3 \tTraining Loss: 0.000261\n",
      "Epoch: 4 \tTraining Loss: 0.000261\n",
      "Epoch: 5 \tTraining Loss: 0.000261\n",
      "Epoch: 6 \tTraining Loss: 0.000261\n",
      "Epoch: 7 \tTraining Loss: 0.000261\n",
      "Epoch: 8 \tTraining Loss: 0.000261\n",
      "Epoch: 9 \tTraining Loss: 0.000261\n",
      "Epoch: 10 \tTraining Loss: 0.000261\n",
      "Epoch: 11 \tTraining Loss: 0.000261\n",
      "Epoch: 12 \tTraining Loss: 0.000261\n",
      "Epoch: 13 \tTraining Loss: 0.000261\n",
      "Epoch: 14 \tTraining Loss: 0.000261\n",
      "Epoch: 15 \tTraining Loss: 0.000261\n",
      "Epoch: 16 \tTraining Loss: 0.000261\n",
      "Epoch: 17 \tTraining Loss: 0.000261\n",
      "Epoch: 18 \tTraining Loss: 0.000261\n",
      "Epoch: 1 \tTraining Loss: 0.000130\n",
      "Epoch: 2 \tTraining Loss: 0.000130\n",
      "Epoch: 3 \tTraining Loss: 0.000130\n",
      "Epoch: 4 \tTraining Loss: 0.000130\n",
      "Epoch: 5 \tTraining Loss: 0.000130\n",
      "Epoch: 6 \tTraining Loss: 0.000130\n",
      "Epoch: 7 \tTraining Loss: 0.000130\n",
      "Epoch: 8 \tTraining Loss: 0.000130\n",
      "Epoch: 9 \tTraining Loss: 0.000130\n",
      "Epoch: 10 \tTraining Loss: 0.000130\n",
      "Epoch: 11 \tTraining Loss: 0.000130\n",
      "Epoch: 12 \tTraining Loss: 0.000130\n",
      "Epoch: 13 \tTraining Loss: 0.000130\n",
      "Epoch: 14 \tTraining Loss: 0.000130\n",
      "Epoch: 15 \tTraining Loss: 0.000130\n",
      "Epoch: 16 \tTraining Loss: 0.000130\n",
      "Epoch: 17 \tTraining Loss: 0.000130\n",
      "Epoch: 18 \tTraining Loss: 0.000130\n",
      "Epoch: 1 \tTraining Loss: 0.000230\n",
      "Epoch: 2 \tTraining Loss: 0.000230\n",
      "Epoch: 3 \tTraining Loss: 0.000230\n",
      "Epoch: 4 \tTraining Loss: 0.000230\n",
      "Epoch: 5 \tTraining Loss: 0.000230\n",
      "Epoch: 6 \tTraining Loss: 0.000230\n",
      "Epoch: 7 \tTraining Loss: 0.000230\n",
      "Epoch: 8 \tTraining Loss: 0.000230\n",
      "Epoch: 9 \tTraining Loss: 0.000230\n",
      "Epoch: 10 \tTraining Loss: 0.000230\n",
      "Epoch: 11 \tTraining Loss: 0.000230\n",
      "Epoch: 12 \tTraining Loss: 0.000230\n",
      "Epoch: 13 \tTraining Loss: 0.000230\n",
      "Epoch: 14 \tTraining Loss: 0.000230\n",
      "Epoch: 15 \tTraining Loss: 0.000230\n",
      "Epoch: 16 \tTraining Loss: 0.000230\n",
      "Epoch: 17 \tTraining Loss: 0.000230\n",
      "Epoch: 18 \tTraining Loss: 0.000230\n",
      "Epoch: 1 \tTraining Loss: 0.000237\n",
      "Epoch: 2 \tTraining Loss: 0.000237\n",
      "Epoch: 3 \tTraining Loss: 0.000237\n",
      "Epoch: 4 \tTraining Loss: 0.000237\n",
      "Epoch: 5 \tTraining Loss: 0.000237\n",
      "Epoch: 6 \tTraining Loss: 0.000237\n",
      "Epoch: 7 \tTraining Loss: 0.000237\n",
      "Epoch: 8 \tTraining Loss: 0.000237\n",
      "Epoch: 9 \tTraining Loss: 0.000237\n",
      "Epoch: 10 \tTraining Loss: 0.000237\n",
      "Epoch: 11 \tTraining Loss: 0.000237\n",
      "Epoch: 12 \tTraining Loss: 0.000237\n",
      "Epoch: 13 \tTraining Loss: 0.000237\n",
      "Epoch: 14 \tTraining Loss: 0.000237\n",
      "Epoch: 15 \tTraining Loss: 0.000237\n",
      "Epoch: 16 \tTraining Loss: 0.000237\n",
      "Epoch: 17 \tTraining Loss: 0.000237\n",
      "Epoch: 18 \tTraining Loss: 0.000237\n",
      "Epoch: 1 \tTraining Loss: 0.000097\n",
      "Epoch: 2 \tTraining Loss: 0.000097\n",
      "Epoch: 3 \tTraining Loss: 0.000097\n",
      "Epoch: 4 \tTraining Loss: 0.000097\n",
      "Epoch: 5 \tTraining Loss: 0.000097\n",
      "Epoch: 6 \tTraining Loss: 0.000097\n",
      "Epoch: 7 \tTraining Loss: 0.000097\n",
      "Epoch: 8 \tTraining Loss: 0.000097\n",
      "Epoch: 9 \tTraining Loss: 0.000097\n",
      "Epoch: 10 \tTraining Loss: 0.000097\n",
      "Epoch: 11 \tTraining Loss: 0.000097\n",
      "Epoch: 12 \tTraining Loss: 0.000097\n",
      "Epoch: 13 \tTraining Loss: 0.000097\n",
      "Epoch: 14 \tTraining Loss: 0.000097\n",
      "Epoch: 15 \tTraining Loss: 0.000097\n",
      "Epoch: 16 \tTraining Loss: 0.000097\n",
      "Epoch: 17 \tTraining Loss: 0.000097\n",
      "Epoch: 18 \tTraining Loss: 0.000097\n",
      "Epoch: 1 \tTraining Loss: 0.000195\n",
      "Epoch: 2 \tTraining Loss: 0.000195\n",
      "Epoch: 3 \tTraining Loss: 0.000195\n",
      "Epoch: 4 \tTraining Loss: 0.000195\n",
      "Epoch: 5 \tTraining Loss: 0.000195\n",
      "Epoch: 6 \tTraining Loss: 0.000195\n",
      "Epoch: 7 \tTraining Loss: 0.000195\n",
      "Epoch: 8 \tTraining Loss: 0.000195\n",
      "Epoch: 9 \tTraining Loss: 0.000195\n",
      "Epoch: 10 \tTraining Loss: 0.000195\n",
      "Epoch: 11 \tTraining Loss: 0.000195\n",
      "Epoch: 12 \tTraining Loss: 0.000195\n",
      "Epoch: 13 \tTraining Loss: 0.000195\n",
      "Epoch: 14 \tTraining Loss: 0.000195\n",
      "Epoch: 15 \tTraining Loss: 0.000195\n",
      "Epoch: 16 \tTraining Loss: 0.000195\n",
      "Epoch: 17 \tTraining Loss: 0.000195\n",
      "Epoch: 18 \tTraining Loss: 0.000195\n",
      "Epoch: 1 \tTraining Loss: 0.000244\n",
      "Epoch: 2 \tTraining Loss: 0.000244\n",
      "Epoch: 3 \tTraining Loss: 0.000244\n",
      "Epoch: 4 \tTraining Loss: 0.000244\n",
      "Epoch: 5 \tTraining Loss: 0.000244\n",
      "Epoch: 6 \tTraining Loss: 0.000244\n",
      "Epoch: 7 \tTraining Loss: 0.000244\n",
      "Epoch: 8 \tTraining Loss: 0.000244\n",
      "Epoch: 9 \tTraining Loss: 0.000244\n",
      "Epoch: 10 \tTraining Loss: 0.000244\n",
      "Epoch: 11 \tTraining Loss: 0.000244\n",
      "Epoch: 12 \tTraining Loss: 0.000244\n",
      "Epoch: 13 \tTraining Loss: 0.000244\n",
      "Epoch: 14 \tTraining Loss: 0.000244\n",
      "Epoch: 15 \tTraining Loss: 0.000244\n",
      "Epoch: 16 \tTraining Loss: 0.000244\n",
      "Epoch: 17 \tTraining Loss: 0.000244\n",
      "Epoch: 18 \tTraining Loss: 0.000244\n",
      "Epoch: 1 \tTraining Loss: 0.000175\n",
      "Epoch: 2 \tTraining Loss: 0.000175\n",
      "Epoch: 3 \tTraining Loss: 0.000175\n",
      "Epoch: 4 \tTraining Loss: 0.000175\n",
      "Epoch: 5 \tTraining Loss: 0.000175\n",
      "Epoch: 6 \tTraining Loss: 0.000175\n",
      "Epoch: 7 \tTraining Loss: 0.000175\n",
      "Epoch: 8 \tTraining Loss: 0.000175\n",
      "Epoch: 9 \tTraining Loss: 0.000175\n",
      "Epoch: 10 \tTraining Loss: 0.000175\n",
      "Epoch: 11 \tTraining Loss: 0.000175\n",
      "Epoch: 12 \tTraining Loss: 0.000175\n",
      "Epoch: 13 \tTraining Loss: 0.000175\n",
      "Epoch: 14 \tTraining Loss: 0.000175\n",
      "Epoch: 15 \tTraining Loss: 0.000175\n",
      "Epoch: 16 \tTraining Loss: 0.000175\n",
      "Epoch: 17 \tTraining Loss: 0.000175\n",
      "Epoch: 18 \tTraining Loss: 0.000175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000191\n",
      "Epoch: 2 \tTraining Loss: 0.000191\n",
      "Epoch: 3 \tTraining Loss: 0.000191\n",
      "Epoch: 4 \tTraining Loss: 0.000191\n",
      "Epoch: 5 \tTraining Loss: 0.000191\n",
      "Epoch: 6 \tTraining Loss: 0.000191\n",
      "Epoch: 7 \tTraining Loss: 0.000191\n",
      "Epoch: 8 \tTraining Loss: 0.000191\n",
      "Epoch: 9 \tTraining Loss: 0.000191\n",
      "Epoch: 10 \tTraining Loss: 0.000191\n",
      "Epoch: 11 \tTraining Loss: 0.000191\n",
      "Epoch: 12 \tTraining Loss: 0.000191\n",
      "Epoch: 13 \tTraining Loss: 0.000191\n",
      "Epoch: 14 \tTraining Loss: 0.000191\n",
      "Epoch: 15 \tTraining Loss: 0.000191\n",
      "Epoch: 16 \tTraining Loss: 0.000191\n",
      "Epoch: 17 \tTraining Loss: 0.000191\n",
      "Epoch: 18 \tTraining Loss: 0.000191\n",
      "Epoch: 1 \tTraining Loss: 0.000085\n",
      "Epoch: 2 \tTraining Loss: 0.000085\n",
      "Epoch: 3 \tTraining Loss: 0.000085\n",
      "Epoch: 4 \tTraining Loss: 0.000085\n",
      "Epoch: 5 \tTraining Loss: 0.000085\n",
      "Epoch: 6 \tTraining Loss: 0.000085\n",
      "Epoch: 7 \tTraining Loss: 0.000085\n",
      "Epoch: 8 \tTraining Loss: 0.000085\n",
      "Epoch: 9 \tTraining Loss: 0.000085\n",
      "Epoch: 10 \tTraining Loss: 0.000085\n",
      "Epoch: 11 \tTraining Loss: 0.000085\n",
      "Epoch: 12 \tTraining Loss: 0.000085\n",
      "Epoch: 13 \tTraining Loss: 0.000085\n",
      "Epoch: 14 \tTraining Loss: 0.000085\n",
      "Epoch: 15 \tTraining Loss: 0.000085\n",
      "Epoch: 16 \tTraining Loss: 0.000085\n",
      "Epoch: 17 \tTraining Loss: 0.000085\n",
      "Epoch: 18 \tTraining Loss: 0.000085\n",
      "Epoch: 1 \tTraining Loss: 0.000183\n",
      "Epoch: 2 \tTraining Loss: 0.000183\n",
      "Epoch: 3 \tTraining Loss: 0.000183\n",
      "Epoch: 4 \tTraining Loss: 0.000183\n",
      "Epoch: 5 \tTraining Loss: 0.000183\n",
      "Epoch: 6 \tTraining Loss: 0.000183\n",
      "Epoch: 7 \tTraining Loss: 0.000183\n",
      "Epoch: 8 \tTraining Loss: 0.000183\n",
      "Epoch: 9 \tTraining Loss: 0.000183\n",
      "Epoch: 10 \tTraining Loss: 0.000183\n",
      "Epoch: 11 \tTraining Loss: 0.000183\n",
      "Epoch: 12 \tTraining Loss: 0.000183\n",
      "Epoch: 13 \tTraining Loss: 0.000183\n",
      "Epoch: 14 \tTraining Loss: 0.000183\n",
      "Epoch: 15 \tTraining Loss: 0.000183\n",
      "Epoch: 16 \tTraining Loss: 0.000183\n",
      "Epoch: 17 \tTraining Loss: 0.000183\n",
      "Epoch: 18 \tTraining Loss: 0.000183\n",
      "Epoch: 1 \tTraining Loss: 0.000143\n",
      "Epoch: 2 \tTraining Loss: 0.000143\n",
      "Epoch: 3 \tTraining Loss: 0.000143\n",
      "Epoch: 4 \tTraining Loss: 0.000143\n",
      "Epoch: 5 \tTraining Loss: 0.000143\n",
      "Epoch: 6 \tTraining Loss: 0.000143\n",
      "Epoch: 7 \tTraining Loss: 0.000143\n",
      "Epoch: 8 \tTraining Loss: 0.000143\n",
      "Epoch: 9 \tTraining Loss: 0.000143\n",
      "Epoch: 10 \tTraining Loss: 0.000143\n",
      "Epoch: 11 \tTraining Loss: 0.000143\n",
      "Epoch: 12 \tTraining Loss: 0.000143\n",
      "Epoch: 13 \tTraining Loss: 0.000143\n",
      "Epoch: 14 \tTraining Loss: 0.000143\n",
      "Epoch: 15 \tTraining Loss: 0.000143\n",
      "Epoch: 16 \tTraining Loss: 0.000143\n",
      "Epoch: 17 \tTraining Loss: 0.000143\n",
      "Epoch: 18 \tTraining Loss: 0.000143\n",
      "Epoch: 1 \tTraining Loss: 0.000155\n",
      "Epoch: 2 \tTraining Loss: 0.000155\n",
      "Epoch: 3 \tTraining Loss: 0.000155\n",
      "Epoch: 4 \tTraining Loss: 0.000155\n",
      "Epoch: 5 \tTraining Loss: 0.000155\n",
      "Epoch: 6 \tTraining Loss: 0.000155\n",
      "Epoch: 7 \tTraining Loss: 0.000155\n",
      "Epoch: 8 \tTraining Loss: 0.000155\n",
      "Epoch: 9 \tTraining Loss: 0.000155\n",
      "Epoch: 10 \tTraining Loss: 0.000155\n",
      "Epoch: 11 \tTraining Loss: 0.000155\n",
      "Epoch: 12 \tTraining Loss: 0.000155\n",
      "Epoch: 13 \tTraining Loss: 0.000155\n",
      "Epoch: 14 \tTraining Loss: 0.000155\n",
      "Epoch: 15 \tTraining Loss: 0.000155\n",
      "Epoch: 16 \tTraining Loss: 0.000155\n",
      "Epoch: 17 \tTraining Loss: 0.000155\n",
      "Epoch: 18 \tTraining Loss: 0.000155\n",
      "Epoch: 1 \tTraining Loss: 0.000266\n",
      "Epoch: 2 \tTraining Loss: 0.000266\n",
      "Epoch: 3 \tTraining Loss: 0.000266\n",
      "Epoch: 4 \tTraining Loss: 0.000266\n",
      "Epoch: 5 \tTraining Loss: 0.000266\n",
      "Epoch: 6 \tTraining Loss: 0.000266\n",
      "Epoch: 7 \tTraining Loss: 0.000266\n",
      "Epoch: 8 \tTraining Loss: 0.000266\n",
      "Epoch: 9 \tTraining Loss: 0.000266\n",
      "Epoch: 10 \tTraining Loss: 0.000266\n",
      "Epoch: 11 \tTraining Loss: 0.000266\n",
      "Epoch: 12 \tTraining Loss: 0.000266\n",
      "Epoch: 13 \tTraining Loss: 0.000266\n",
      "Epoch: 14 \tTraining Loss: 0.000266\n",
      "Epoch: 15 \tTraining Loss: 0.000266\n",
      "Epoch: 16 \tTraining Loss: 0.000266\n",
      "Epoch: 17 \tTraining Loss: 0.000266\n",
      "Epoch: 18 \tTraining Loss: 0.000266\n",
      "Epoch: 1 \tTraining Loss: 0.000194\n",
      "Epoch: 2 \tTraining Loss: 0.000194\n",
      "Epoch: 3 \tTraining Loss: 0.000194\n",
      "Epoch: 4 \tTraining Loss: 0.000194\n",
      "Epoch: 5 \tTraining Loss: 0.000194\n",
      "Epoch: 6 \tTraining Loss: 0.000194\n",
      "Epoch: 7 \tTraining Loss: 0.000194\n",
      "Epoch: 8 \tTraining Loss: 0.000194\n",
      "Epoch: 9 \tTraining Loss: 0.000194\n",
      "Epoch: 10 \tTraining Loss: 0.000194\n",
      "Epoch: 11 \tTraining Loss: 0.000194\n",
      "Epoch: 12 \tTraining Loss: 0.000194\n",
      "Epoch: 13 \tTraining Loss: 0.000194\n",
      "Epoch: 14 \tTraining Loss: 0.000194\n",
      "Epoch: 15 \tTraining Loss: 0.000194\n",
      "Epoch: 16 \tTraining Loss: 0.000194\n",
      "Epoch: 17 \tTraining Loss: 0.000194\n",
      "Epoch: 18 \tTraining Loss: 0.000194\n",
      "Epoch: 1 \tTraining Loss: 0.000230\n",
      "Epoch: 2 \tTraining Loss: 0.000230\n",
      "Epoch: 3 \tTraining Loss: 0.000230\n",
      "Epoch: 4 \tTraining Loss: 0.000230\n",
      "Epoch: 5 \tTraining Loss: 0.000230\n",
      "Epoch: 6 \tTraining Loss: 0.000230\n",
      "Epoch: 7 \tTraining Loss: 0.000230\n",
      "Epoch: 8 \tTraining Loss: 0.000230\n",
      "Epoch: 9 \tTraining Loss: 0.000230\n",
      "Epoch: 10 \tTraining Loss: 0.000230\n",
      "Epoch: 11 \tTraining Loss: 0.000230\n",
      "Epoch: 12 \tTraining Loss: 0.000230\n",
      "Epoch: 13 \tTraining Loss: 0.000230\n",
      "Epoch: 14 \tTraining Loss: 0.000230\n",
      "Epoch: 15 \tTraining Loss: 0.000230\n",
      "Epoch: 16 \tTraining Loss: 0.000230\n",
      "Epoch: 17 \tTraining Loss: 0.000230\n",
      "Epoch: 18 \tTraining Loss: 0.000230\n",
      "Epoch: 1 \tTraining Loss: 0.000253\n",
      "Epoch: 2 \tTraining Loss: 0.000253\n",
      "Epoch: 3 \tTraining Loss: 0.000253\n",
      "Epoch: 4 \tTraining Loss: 0.000253\n",
      "Epoch: 5 \tTraining Loss: 0.000253\n",
      "Epoch: 6 \tTraining Loss: 0.000253\n",
      "Epoch: 7 \tTraining Loss: 0.000253\n",
      "Epoch: 8 \tTraining Loss: 0.000253\n",
      "Epoch: 9 \tTraining Loss: 0.000253\n",
      "Epoch: 10 \tTraining Loss: 0.000253\n",
      "Epoch: 11 \tTraining Loss: 0.000253\n",
      "Epoch: 12 \tTraining Loss: 0.000253\n",
      "Epoch: 13 \tTraining Loss: 0.000253\n",
      "Epoch: 14 \tTraining Loss: 0.000253\n",
      "Epoch: 15 \tTraining Loss: 0.000253\n",
      "Epoch: 16 \tTraining Loss: 0.000253\n",
      "Epoch: 17 \tTraining Loss: 0.000253\n",
      "Epoch: 18 \tTraining Loss: 0.000253\n",
      "Epoch: 1 \tTraining Loss: 0.000297\n",
      "Epoch: 2 \tTraining Loss: 0.000297\n",
      "Epoch: 3 \tTraining Loss: 0.000297\n",
      "Epoch: 4 \tTraining Loss: 0.000297\n",
      "Epoch: 5 \tTraining Loss: 0.000297\n",
      "Epoch: 6 \tTraining Loss: 0.000297\n",
      "Epoch: 7 \tTraining Loss: 0.000297\n",
      "Epoch: 8 \tTraining Loss: 0.000297\n",
      "Epoch: 9 \tTraining Loss: 0.000297\n",
      "Epoch: 10 \tTraining Loss: 0.000297\n",
      "Epoch: 11 \tTraining Loss: 0.000297\n",
      "Epoch: 12 \tTraining Loss: 0.000297\n",
      "Epoch: 13 \tTraining Loss: 0.000297\n",
      "Epoch: 14 \tTraining Loss: 0.000297\n",
      "Epoch: 15 \tTraining Loss: 0.000297\n",
      "Epoch: 16 \tTraining Loss: 0.000297\n",
      "Epoch: 17 \tTraining Loss: 0.000297\n",
      "Epoch: 18 \tTraining Loss: 0.000297\n",
      "Epoch: 1 \tTraining Loss: 0.000130\n",
      "Epoch: 2 \tTraining Loss: 0.000130\n",
      "Epoch: 3 \tTraining Loss: 0.000130\n",
      "Epoch: 4 \tTraining Loss: 0.000130\n",
      "Epoch: 5 \tTraining Loss: 0.000130\n",
      "Epoch: 6 \tTraining Loss: 0.000130\n",
      "Epoch: 7 \tTraining Loss: 0.000130\n",
      "Epoch: 8 \tTraining Loss: 0.000130\n",
      "Epoch: 9 \tTraining Loss: 0.000130\n",
      "Epoch: 10 \tTraining Loss: 0.000130\n",
      "Epoch: 11 \tTraining Loss: 0.000130\n",
      "Epoch: 12 \tTraining Loss: 0.000130\n",
      "Epoch: 13 \tTraining Loss: 0.000130\n",
      "Epoch: 14 \tTraining Loss: 0.000130\n",
      "Epoch: 15 \tTraining Loss: 0.000130\n",
      "Epoch: 16 \tTraining Loss: 0.000130\n",
      "Epoch: 17 \tTraining Loss: 0.000130\n",
      "Epoch: 18 \tTraining Loss: 0.000130\n",
      "Epoch: 1 \tTraining Loss: 0.000135\n",
      "Epoch: 2 \tTraining Loss: 0.000135\n",
      "Epoch: 3 \tTraining Loss: 0.000135\n",
      "Epoch: 4 \tTraining Loss: 0.000135\n",
      "Epoch: 5 \tTraining Loss: 0.000135\n",
      "Epoch: 6 \tTraining Loss: 0.000135\n",
      "Epoch: 7 \tTraining Loss: 0.000135\n",
      "Epoch: 8 \tTraining Loss: 0.000135\n",
      "Epoch: 9 \tTraining Loss: 0.000135\n",
      "Epoch: 10 \tTraining Loss: 0.000135\n",
      "Epoch: 11 \tTraining Loss: 0.000135\n",
      "Epoch: 12 \tTraining Loss: 0.000135\n",
      "Epoch: 13 \tTraining Loss: 0.000135\n",
      "Epoch: 14 \tTraining Loss: 0.000135\n",
      "Epoch: 15 \tTraining Loss: 0.000135\n",
      "Epoch: 16 \tTraining Loss: 0.000135\n",
      "Epoch: 17 \tTraining Loss: 0.000135\n",
      "Epoch: 18 \tTraining Loss: 0.000135\n",
      "Epoch: 1 \tTraining Loss: 0.000345\n",
      "Epoch: 2 \tTraining Loss: 0.000345\n",
      "Epoch: 3 \tTraining Loss: 0.000345\n",
      "Epoch: 4 \tTraining Loss: 0.000345\n",
      "Epoch: 5 \tTraining Loss: 0.000345\n",
      "Epoch: 6 \tTraining Loss: 0.000345\n",
      "Epoch: 7 \tTraining Loss: 0.000345\n",
      "Epoch: 8 \tTraining Loss: 0.000345\n",
      "Epoch: 9 \tTraining Loss: 0.000345\n",
      "Epoch: 10 \tTraining Loss: 0.000345\n",
      "Epoch: 11 \tTraining Loss: 0.000345\n",
      "Epoch: 12 \tTraining Loss: 0.000345\n",
      "Epoch: 13 \tTraining Loss: 0.000345\n",
      "Epoch: 14 \tTraining Loss: 0.000345\n",
      "Epoch: 15 \tTraining Loss: 0.000345\n",
      "Epoch: 16 \tTraining Loss: 0.000345\n",
      "Epoch: 17 \tTraining Loss: 0.000345\n",
      "Epoch: 18 \tTraining Loss: 0.000345\n",
      "Epoch: 1 \tTraining Loss: 0.000221\n",
      "Epoch: 2 \tTraining Loss: 0.000221\n",
      "Epoch: 3 \tTraining Loss: 0.000221\n",
      "Epoch: 4 \tTraining Loss: 0.000221\n",
      "Epoch: 5 \tTraining Loss: 0.000221\n",
      "Epoch: 6 \tTraining Loss: 0.000221\n",
      "Epoch: 7 \tTraining Loss: 0.000221\n",
      "Epoch: 8 \tTraining Loss: 0.000221\n",
      "Epoch: 9 \tTraining Loss: 0.000221\n",
      "Epoch: 10 \tTraining Loss: 0.000221\n",
      "Epoch: 11 \tTraining Loss: 0.000221\n",
      "Epoch: 12 \tTraining Loss: 0.000221\n",
      "Epoch: 13 \tTraining Loss: 0.000221\n",
      "Epoch: 14 \tTraining Loss: 0.000221\n",
      "Epoch: 15 \tTraining Loss: 0.000221\n",
      "Epoch: 16 \tTraining Loss: 0.000221\n",
      "Epoch: 17 \tTraining Loss: 0.000221\n",
      "Epoch: 18 \tTraining Loss: 0.000221\n",
      "Epoch: 1 \tTraining Loss: 0.000202\n",
      "Epoch: 2 \tTraining Loss: 0.000202\n",
      "Epoch: 3 \tTraining Loss: 0.000202\n",
      "Epoch: 4 \tTraining Loss: 0.000202\n",
      "Epoch: 5 \tTraining Loss: 0.000202\n",
      "Epoch: 6 \tTraining Loss: 0.000202\n",
      "Epoch: 7 \tTraining Loss: 0.000202\n",
      "Epoch: 8 \tTraining Loss: 0.000202\n",
      "Epoch: 9 \tTraining Loss: 0.000202\n",
      "Epoch: 10 \tTraining Loss: 0.000202\n",
      "Epoch: 11 \tTraining Loss: 0.000202\n",
      "Epoch: 12 \tTraining Loss: 0.000202\n",
      "Epoch: 13 \tTraining Loss: 0.000202\n",
      "Epoch: 14 \tTraining Loss: 0.000202\n",
      "Epoch: 15 \tTraining Loss: 0.000202\n",
      "Epoch: 16 \tTraining Loss: 0.000202\n",
      "Epoch: 17 \tTraining Loss: 0.000202\n",
      "Epoch: 18 \tTraining Loss: 0.000202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000146\n",
      "Epoch: 2 \tTraining Loss: 0.000146\n",
      "Epoch: 3 \tTraining Loss: 0.000146\n",
      "Epoch: 4 \tTraining Loss: 0.000146\n",
      "Epoch: 5 \tTraining Loss: 0.000146\n",
      "Epoch: 6 \tTraining Loss: 0.000146\n",
      "Epoch: 7 \tTraining Loss: 0.000146\n",
      "Epoch: 8 \tTraining Loss: 0.000146\n",
      "Epoch: 9 \tTraining Loss: 0.000146\n",
      "Epoch: 10 \tTraining Loss: 0.000146\n",
      "Epoch: 11 \tTraining Loss: 0.000146\n",
      "Epoch: 12 \tTraining Loss: 0.000146\n",
      "Epoch: 13 \tTraining Loss: 0.000146\n",
      "Epoch: 14 \tTraining Loss: 0.000146\n",
      "Epoch: 15 \tTraining Loss: 0.000146\n",
      "Epoch: 16 \tTraining Loss: 0.000146\n",
      "Epoch: 17 \tTraining Loss: 0.000146\n",
      "Epoch: 18 \tTraining Loss: 0.000146\n",
      "Epoch: 1 \tTraining Loss: 0.000131\n",
      "Epoch: 2 \tTraining Loss: 0.000131\n",
      "Epoch: 3 \tTraining Loss: 0.000131\n",
      "Epoch: 4 \tTraining Loss: 0.000131\n",
      "Epoch: 5 \tTraining Loss: 0.000131\n",
      "Epoch: 6 \tTraining Loss: 0.000131\n",
      "Epoch: 7 \tTraining Loss: 0.000131\n",
      "Epoch: 8 \tTraining Loss: 0.000131\n",
      "Epoch: 9 \tTraining Loss: 0.000131\n",
      "Epoch: 10 \tTraining Loss: 0.000131\n",
      "Epoch: 11 \tTraining Loss: 0.000131\n",
      "Epoch: 12 \tTraining Loss: 0.000131\n",
      "Epoch: 13 \tTraining Loss: 0.000131\n",
      "Epoch: 14 \tTraining Loss: 0.000131\n",
      "Epoch: 15 \tTraining Loss: 0.000131\n",
      "Epoch: 16 \tTraining Loss: 0.000131\n",
      "Epoch: 17 \tTraining Loss: 0.000131\n",
      "Epoch: 18 \tTraining Loss: 0.000131\n",
      "Epoch: 1 \tTraining Loss: 0.000248\n",
      "Epoch: 2 \tTraining Loss: 0.000248\n",
      "Epoch: 3 \tTraining Loss: 0.000248\n",
      "Epoch: 4 \tTraining Loss: 0.000248\n",
      "Epoch: 5 \tTraining Loss: 0.000248\n",
      "Epoch: 6 \tTraining Loss: 0.000248\n",
      "Epoch: 7 \tTraining Loss: 0.000248\n",
      "Epoch: 8 \tTraining Loss: 0.000248\n",
      "Epoch: 9 \tTraining Loss: 0.000248\n",
      "Epoch: 10 \tTraining Loss: 0.000248\n",
      "Epoch: 11 \tTraining Loss: 0.000248\n",
      "Epoch: 12 \tTraining Loss: 0.000248\n",
      "Epoch: 13 \tTraining Loss: 0.000248\n",
      "Epoch: 14 \tTraining Loss: 0.000248\n",
      "Epoch: 15 \tTraining Loss: 0.000248\n",
      "Epoch: 16 \tTraining Loss: 0.000248\n",
      "Epoch: 17 \tTraining Loss: 0.000248\n",
      "Epoch: 18 \tTraining Loss: 0.000248\n",
      "Epoch: 1 \tTraining Loss: 0.000203\n",
      "Epoch: 2 \tTraining Loss: 0.000203\n",
      "Epoch: 3 \tTraining Loss: 0.000203\n",
      "Epoch: 4 \tTraining Loss: 0.000203\n",
      "Epoch: 5 \tTraining Loss: 0.000203\n",
      "Epoch: 6 \tTraining Loss: 0.000203\n",
      "Epoch: 7 \tTraining Loss: 0.000203\n",
      "Epoch: 8 \tTraining Loss: 0.000203\n",
      "Epoch: 9 \tTraining Loss: 0.000203\n",
      "Epoch: 10 \tTraining Loss: 0.000203\n",
      "Epoch: 11 \tTraining Loss: 0.000203\n",
      "Epoch: 12 \tTraining Loss: 0.000203\n",
      "Epoch: 13 \tTraining Loss: 0.000203\n",
      "Epoch: 14 \tTraining Loss: 0.000203\n",
      "Epoch: 15 \tTraining Loss: 0.000203\n",
      "Epoch: 16 \tTraining Loss: 0.000203\n",
      "Epoch: 17 \tTraining Loss: 0.000203\n",
      "Epoch: 18 \tTraining Loss: 0.000203\n",
      "Epoch: 1 \tTraining Loss: 0.000104\n",
      "Epoch: 2 \tTraining Loss: 0.000104\n",
      "Epoch: 3 \tTraining Loss: 0.000104\n",
      "Epoch: 4 \tTraining Loss: 0.000104\n",
      "Epoch: 5 \tTraining Loss: 0.000104\n",
      "Epoch: 6 \tTraining Loss: 0.000104\n",
      "Epoch: 7 \tTraining Loss: 0.000104\n",
      "Epoch: 8 \tTraining Loss: 0.000104\n",
      "Epoch: 9 \tTraining Loss: 0.000104\n",
      "Epoch: 10 \tTraining Loss: 0.000104\n",
      "Epoch: 11 \tTraining Loss: 0.000104\n",
      "Epoch: 12 \tTraining Loss: 0.000104\n",
      "Epoch: 13 \tTraining Loss: 0.000104\n",
      "Epoch: 14 \tTraining Loss: 0.000104\n",
      "Epoch: 15 \tTraining Loss: 0.000104\n",
      "Epoch: 16 \tTraining Loss: 0.000104\n",
      "Epoch: 17 \tTraining Loss: 0.000104\n",
      "Epoch: 18 \tTraining Loss: 0.000104\n",
      "Epoch: 1 \tTraining Loss: 0.000229\n",
      "Epoch: 2 \tTraining Loss: 0.000229\n",
      "Epoch: 3 \tTraining Loss: 0.000229\n",
      "Epoch: 4 \tTraining Loss: 0.000229\n",
      "Epoch: 5 \tTraining Loss: 0.000229\n",
      "Epoch: 6 \tTraining Loss: 0.000229\n",
      "Epoch: 7 \tTraining Loss: 0.000229\n",
      "Epoch: 8 \tTraining Loss: 0.000229\n",
      "Epoch: 9 \tTraining Loss: 0.000229\n",
      "Epoch: 10 \tTraining Loss: 0.000229\n",
      "Epoch: 11 \tTraining Loss: 0.000229\n",
      "Epoch: 12 \tTraining Loss: 0.000229\n",
      "Epoch: 13 \tTraining Loss: 0.000229\n",
      "Epoch: 14 \tTraining Loss: 0.000229\n",
      "Epoch: 15 \tTraining Loss: 0.000229\n",
      "Epoch: 16 \tTraining Loss: 0.000229\n",
      "Epoch: 17 \tTraining Loss: 0.000229\n",
      "Epoch: 18 \tTraining Loss: 0.000229\n",
      "Epoch: 1 \tTraining Loss: 0.000409\n",
      "Epoch: 2 \tTraining Loss: 0.000409\n",
      "Epoch: 3 \tTraining Loss: 0.000409\n",
      "Epoch: 4 \tTraining Loss: 0.000409\n",
      "Epoch: 5 \tTraining Loss: 0.000409\n",
      "Epoch: 6 \tTraining Loss: 0.000409\n",
      "Epoch: 7 \tTraining Loss: 0.000409\n",
      "Epoch: 8 \tTraining Loss: 0.000409\n",
      "Epoch: 9 \tTraining Loss: 0.000409\n",
      "Epoch: 10 \tTraining Loss: 0.000409\n",
      "Epoch: 11 \tTraining Loss: 0.000409\n",
      "Epoch: 12 \tTraining Loss: 0.000409\n",
      "Epoch: 13 \tTraining Loss: 0.000409\n",
      "Epoch: 14 \tTraining Loss: 0.000409\n",
      "Epoch: 15 \tTraining Loss: 0.000409\n",
      "Epoch: 16 \tTraining Loss: 0.000409\n",
      "Epoch: 17 \tTraining Loss: 0.000409\n",
      "Epoch: 18 \tTraining Loss: 0.000409\n",
      "Epoch: 1 \tTraining Loss: 0.000179\n",
      "Epoch: 2 \tTraining Loss: 0.000179\n",
      "Epoch: 3 \tTraining Loss: 0.000179\n",
      "Epoch: 4 \tTraining Loss: 0.000179\n",
      "Epoch: 5 \tTraining Loss: 0.000179\n",
      "Epoch: 6 \tTraining Loss: 0.000179\n",
      "Epoch: 7 \tTraining Loss: 0.000179\n",
      "Epoch: 8 \tTraining Loss: 0.000179\n",
      "Epoch: 9 \tTraining Loss: 0.000179\n",
      "Epoch: 10 \tTraining Loss: 0.000179\n",
      "Epoch: 11 \tTraining Loss: 0.000179\n",
      "Epoch: 12 \tTraining Loss: 0.000179\n",
      "Epoch: 13 \tTraining Loss: 0.000179\n",
      "Epoch: 14 \tTraining Loss: 0.000179\n",
      "Epoch: 15 \tTraining Loss: 0.000179\n",
      "Epoch: 16 \tTraining Loss: 0.000179\n",
      "Epoch: 17 \tTraining Loss: 0.000179\n",
      "Epoch: 18 \tTraining Loss: 0.000179\n",
      "Epoch: 1 \tTraining Loss: 0.000163\n",
      "Epoch: 2 \tTraining Loss: 0.000163\n",
      "Epoch: 3 \tTraining Loss: 0.000163\n",
      "Epoch: 4 \tTraining Loss: 0.000163\n",
      "Epoch: 5 \tTraining Loss: 0.000163\n",
      "Epoch: 6 \tTraining Loss: 0.000163\n",
      "Epoch: 7 \tTraining Loss: 0.000163\n",
      "Epoch: 8 \tTraining Loss: 0.000163\n",
      "Epoch: 9 \tTraining Loss: 0.000163\n",
      "Epoch: 10 \tTraining Loss: 0.000163\n",
      "Epoch: 11 \tTraining Loss: 0.000163\n",
      "Epoch: 12 \tTraining Loss: 0.000163\n",
      "Epoch: 13 \tTraining Loss: 0.000163\n",
      "Epoch: 14 \tTraining Loss: 0.000163\n",
      "Epoch: 15 \tTraining Loss: 0.000163\n",
      "Epoch: 16 \tTraining Loss: 0.000163\n",
      "Epoch: 17 \tTraining Loss: 0.000163\n",
      "Epoch: 18 \tTraining Loss: 0.000163\n",
      "Epoch: 1 \tTraining Loss: 0.000229\n",
      "Epoch: 2 \tTraining Loss: 0.000229\n",
      "Epoch: 3 \tTraining Loss: 0.000229\n",
      "Epoch: 4 \tTraining Loss: 0.000229\n",
      "Epoch: 5 \tTraining Loss: 0.000229\n",
      "Epoch: 6 \tTraining Loss: 0.000229\n",
      "Epoch: 7 \tTraining Loss: 0.000229\n",
      "Epoch: 8 \tTraining Loss: 0.000229\n",
      "Epoch: 9 \tTraining Loss: 0.000229\n",
      "Epoch: 10 \tTraining Loss: 0.000229\n",
      "Epoch: 11 \tTraining Loss: 0.000229\n",
      "Epoch: 12 \tTraining Loss: 0.000229\n",
      "Epoch: 13 \tTraining Loss: 0.000229\n",
      "Epoch: 14 \tTraining Loss: 0.000229\n",
      "Epoch: 15 \tTraining Loss: 0.000229\n",
      "Epoch: 16 \tTraining Loss: 0.000229\n",
      "Epoch: 17 \tTraining Loss: 0.000229\n",
      "Epoch: 18 \tTraining Loss: 0.000229\n",
      "Epoch: 1 \tTraining Loss: 0.000104\n",
      "Epoch: 2 \tTraining Loss: 0.000104\n",
      "Epoch: 3 \tTraining Loss: 0.000104\n",
      "Epoch: 4 \tTraining Loss: 0.000104\n",
      "Epoch: 5 \tTraining Loss: 0.000104\n",
      "Epoch: 6 \tTraining Loss: 0.000104\n",
      "Epoch: 7 \tTraining Loss: 0.000104\n",
      "Epoch: 8 \tTraining Loss: 0.000104\n",
      "Epoch: 9 \tTraining Loss: 0.000104\n",
      "Epoch: 10 \tTraining Loss: 0.000104\n",
      "Epoch: 11 \tTraining Loss: 0.000104\n",
      "Epoch: 12 \tTraining Loss: 0.000104\n",
      "Epoch: 13 \tTraining Loss: 0.000104\n",
      "Epoch: 14 \tTraining Loss: 0.000104\n",
      "Epoch: 15 \tTraining Loss: 0.000104\n",
      "Epoch: 16 \tTraining Loss: 0.000104\n",
      "Epoch: 17 \tTraining Loss: 0.000104\n",
      "Epoch: 18 \tTraining Loss: 0.000104\n",
      "Epoch: 1 \tTraining Loss: 0.000201\n",
      "Epoch: 2 \tTraining Loss: 0.000201\n",
      "Epoch: 3 \tTraining Loss: 0.000201\n",
      "Epoch: 4 \tTraining Loss: 0.000201\n",
      "Epoch: 5 \tTraining Loss: 0.000201\n",
      "Epoch: 6 \tTraining Loss: 0.000201\n",
      "Epoch: 7 \tTraining Loss: 0.000201\n",
      "Epoch: 8 \tTraining Loss: 0.000201\n",
      "Epoch: 9 \tTraining Loss: 0.000201\n",
      "Epoch: 10 \tTraining Loss: 0.000201\n",
      "Epoch: 11 \tTraining Loss: 0.000201\n",
      "Epoch: 12 \tTraining Loss: 0.000201\n",
      "Epoch: 13 \tTraining Loss: 0.000201\n",
      "Epoch: 14 \tTraining Loss: 0.000201\n",
      "Epoch: 15 \tTraining Loss: 0.000201\n",
      "Epoch: 16 \tTraining Loss: 0.000201\n",
      "Epoch: 17 \tTraining Loss: 0.000201\n",
      "Epoch: 18 \tTraining Loss: 0.000201\n",
      "Epoch: 1 \tTraining Loss: 0.000182\n",
      "Epoch: 2 \tTraining Loss: 0.000182\n",
      "Epoch: 3 \tTraining Loss: 0.000182\n",
      "Epoch: 4 \tTraining Loss: 0.000182\n",
      "Epoch: 5 \tTraining Loss: 0.000182\n",
      "Epoch: 6 \tTraining Loss: 0.000182\n",
      "Epoch: 7 \tTraining Loss: 0.000182\n",
      "Epoch: 8 \tTraining Loss: 0.000182\n",
      "Epoch: 9 \tTraining Loss: 0.000182\n",
      "Epoch: 10 \tTraining Loss: 0.000182\n",
      "Epoch: 11 \tTraining Loss: 0.000182\n",
      "Epoch: 12 \tTraining Loss: 0.000182\n",
      "Epoch: 13 \tTraining Loss: 0.000182\n",
      "Epoch: 14 \tTraining Loss: 0.000182\n",
      "Epoch: 15 \tTraining Loss: 0.000182\n",
      "Epoch: 16 \tTraining Loss: 0.000182\n",
      "Epoch: 17 \tTraining Loss: 0.000182\n",
      "Epoch: 18 \tTraining Loss: 0.000182\n",
      "Epoch: 1 \tTraining Loss: 0.000161\n",
      "Epoch: 2 \tTraining Loss: 0.000161\n",
      "Epoch: 3 \tTraining Loss: 0.000161\n",
      "Epoch: 4 \tTraining Loss: 0.000161\n",
      "Epoch: 5 \tTraining Loss: 0.000161\n",
      "Epoch: 6 \tTraining Loss: 0.000161\n",
      "Epoch: 7 \tTraining Loss: 0.000161\n",
      "Epoch: 8 \tTraining Loss: 0.000161\n",
      "Epoch: 9 \tTraining Loss: 0.000161\n",
      "Epoch: 10 \tTraining Loss: 0.000161\n",
      "Epoch: 11 \tTraining Loss: 0.000161\n",
      "Epoch: 12 \tTraining Loss: 0.000161\n",
      "Epoch: 13 \tTraining Loss: 0.000161\n",
      "Epoch: 14 \tTraining Loss: 0.000161\n",
      "Epoch: 15 \tTraining Loss: 0.000161\n",
      "Epoch: 16 \tTraining Loss: 0.000161\n",
      "Epoch: 17 \tTraining Loss: 0.000161\n",
      "Epoch: 18 \tTraining Loss: 0.000161\n",
      "Epoch: 1 \tTraining Loss: 0.000221\n",
      "Epoch: 2 \tTraining Loss: 0.000221\n",
      "Epoch: 3 \tTraining Loss: 0.000221\n",
      "Epoch: 4 \tTraining Loss: 0.000221\n",
      "Epoch: 5 \tTraining Loss: 0.000221\n",
      "Epoch: 6 \tTraining Loss: 0.000221\n",
      "Epoch: 7 \tTraining Loss: 0.000221\n",
      "Epoch: 8 \tTraining Loss: 0.000221\n",
      "Epoch: 9 \tTraining Loss: 0.000221\n",
      "Epoch: 10 \tTraining Loss: 0.000221\n",
      "Epoch: 11 \tTraining Loss: 0.000221\n",
      "Epoch: 12 \tTraining Loss: 0.000221\n",
      "Epoch: 13 \tTraining Loss: 0.000221\n",
      "Epoch: 14 \tTraining Loss: 0.000221\n",
      "Epoch: 15 \tTraining Loss: 0.000221\n",
      "Epoch: 16 \tTraining Loss: 0.000221\n",
      "Epoch: 17 \tTraining Loss: 0.000221\n",
      "Epoch: 18 \tTraining Loss: 0.000221\n",
      "Epoch: 1 \tTraining Loss: 0.000194\n",
      "Epoch: 2 \tTraining Loss: 0.000194\n",
      "Epoch: 3 \tTraining Loss: 0.000194\n",
      "Epoch: 4 \tTraining Loss: 0.000194\n",
      "Epoch: 5 \tTraining Loss: 0.000194\n",
      "Epoch: 6 \tTraining Loss: 0.000194\n",
      "Epoch: 7 \tTraining Loss: 0.000194\n",
      "Epoch: 8 \tTraining Loss: 0.000194\n",
      "Epoch: 9 \tTraining Loss: 0.000194\n",
      "Epoch: 10 \tTraining Loss: 0.000194\n",
      "Epoch: 11 \tTraining Loss: 0.000194\n",
      "Epoch: 12 \tTraining Loss: 0.000194\n",
      "Epoch: 13 \tTraining Loss: 0.000194\n",
      "Epoch: 14 \tTraining Loss: 0.000194\n",
      "Epoch: 15 \tTraining Loss: 0.000194\n",
      "Epoch: 16 \tTraining Loss: 0.000194\n",
      "Epoch: 17 \tTraining Loss: 0.000194\n",
      "Epoch: 18 \tTraining Loss: 0.000194\n",
      "Epoch: 1 \tTraining Loss: 0.000168\n",
      "Epoch: 2 \tTraining Loss: 0.000168\n",
      "Epoch: 3 \tTraining Loss: 0.000168\n",
      "Epoch: 4 \tTraining Loss: 0.000168\n",
      "Epoch: 5 \tTraining Loss: 0.000168\n",
      "Epoch: 6 \tTraining Loss: 0.000168\n",
      "Epoch: 7 \tTraining Loss: 0.000168\n",
      "Epoch: 8 \tTraining Loss: 0.000168\n",
      "Epoch: 9 \tTraining Loss: 0.000168\n",
      "Epoch: 10 \tTraining Loss: 0.000168\n",
      "Epoch: 11 \tTraining Loss: 0.000168\n",
      "Epoch: 12 \tTraining Loss: 0.000168\n",
      "Epoch: 13 \tTraining Loss: 0.000168\n",
      "Epoch: 14 \tTraining Loss: 0.000168\n",
      "Epoch: 15 \tTraining Loss: 0.000168\n",
      "Epoch: 16 \tTraining Loss: 0.000168\n",
      "Epoch: 17 \tTraining Loss: 0.000168\n",
      "Epoch: 18 \tTraining Loss: 0.000168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000286\n",
      "Epoch: 2 \tTraining Loss: 0.000286\n",
      "Epoch: 3 \tTraining Loss: 0.000286\n",
      "Epoch: 4 \tTraining Loss: 0.000286\n",
      "Epoch: 5 \tTraining Loss: 0.000286\n",
      "Epoch: 6 \tTraining Loss: 0.000286\n",
      "Epoch: 7 \tTraining Loss: 0.000286\n",
      "Epoch: 8 \tTraining Loss: 0.000286\n",
      "Epoch: 9 \tTraining Loss: 0.000286\n",
      "Epoch: 10 \tTraining Loss: 0.000286\n",
      "Epoch: 11 \tTraining Loss: 0.000286\n",
      "Epoch: 12 \tTraining Loss: 0.000286\n",
      "Epoch: 13 \tTraining Loss: 0.000286\n",
      "Epoch: 14 \tTraining Loss: 0.000286\n",
      "Epoch: 15 \tTraining Loss: 0.000286\n",
      "Epoch: 16 \tTraining Loss: 0.000286\n",
      "Epoch: 17 \tTraining Loss: 0.000286\n",
      "Epoch: 18 \tTraining Loss: 0.000286\n",
      "Epoch: 1 \tTraining Loss: 0.000163\n",
      "Epoch: 2 \tTraining Loss: 0.000163\n",
      "Epoch: 3 \tTraining Loss: 0.000163\n",
      "Epoch: 4 \tTraining Loss: 0.000163\n",
      "Epoch: 5 \tTraining Loss: 0.000163\n",
      "Epoch: 6 \tTraining Loss: 0.000163\n",
      "Epoch: 7 \tTraining Loss: 0.000163\n",
      "Epoch: 8 \tTraining Loss: 0.000163\n",
      "Epoch: 9 \tTraining Loss: 0.000163\n",
      "Epoch: 10 \tTraining Loss: 0.000163\n",
      "Epoch: 11 \tTraining Loss: 0.000163\n",
      "Epoch: 12 \tTraining Loss: 0.000163\n",
      "Epoch: 13 \tTraining Loss: 0.000163\n",
      "Epoch: 14 \tTraining Loss: 0.000163\n",
      "Epoch: 15 \tTraining Loss: 0.000163\n",
      "Epoch: 16 \tTraining Loss: 0.000163\n",
      "Epoch: 17 \tTraining Loss: 0.000163\n",
      "Epoch: 18 \tTraining Loss: 0.000163\n",
      "Epoch: 1 \tTraining Loss: 0.000142\n",
      "Epoch: 2 \tTraining Loss: 0.000142\n",
      "Epoch: 3 \tTraining Loss: 0.000142\n",
      "Epoch: 4 \tTraining Loss: 0.000142\n",
      "Epoch: 5 \tTraining Loss: 0.000142\n",
      "Epoch: 6 \tTraining Loss: 0.000142\n",
      "Epoch: 7 \tTraining Loss: 0.000142\n",
      "Epoch: 8 \tTraining Loss: 0.000142\n",
      "Epoch: 9 \tTraining Loss: 0.000142\n",
      "Epoch: 10 \tTraining Loss: 0.000142\n",
      "Epoch: 11 \tTraining Loss: 0.000142\n",
      "Epoch: 12 \tTraining Loss: 0.000142\n",
      "Epoch: 13 \tTraining Loss: 0.000142\n",
      "Epoch: 14 \tTraining Loss: 0.000142\n",
      "Epoch: 15 \tTraining Loss: 0.000142\n",
      "Epoch: 16 \tTraining Loss: 0.000142\n",
      "Epoch: 17 \tTraining Loss: 0.000142\n",
      "Epoch: 18 \tTraining Loss: 0.000142\n",
      "Epoch: 1 \tTraining Loss: 0.000091\n",
      "Epoch: 2 \tTraining Loss: 0.000091\n",
      "Epoch: 3 \tTraining Loss: 0.000091\n",
      "Epoch: 4 \tTraining Loss: 0.000091\n",
      "Epoch: 5 \tTraining Loss: 0.000091\n",
      "Epoch: 6 \tTraining Loss: 0.000091\n",
      "Epoch: 7 \tTraining Loss: 0.000091\n",
      "Epoch: 8 \tTraining Loss: 0.000091\n",
      "Epoch: 9 \tTraining Loss: 0.000091\n",
      "Epoch: 10 \tTraining Loss: 0.000091\n",
      "Epoch: 11 \tTraining Loss: 0.000091\n",
      "Epoch: 12 \tTraining Loss: 0.000091\n",
      "Epoch: 13 \tTraining Loss: 0.000091\n",
      "Epoch: 14 \tTraining Loss: 0.000091\n",
      "Epoch: 15 \tTraining Loss: 0.000091\n",
      "Epoch: 16 \tTraining Loss: 0.000091\n",
      "Epoch: 17 \tTraining Loss: 0.000091\n",
      "Epoch: 18 \tTraining Loss: 0.000091\n",
      "Epoch: 1 \tTraining Loss: 0.000297\n",
      "Epoch: 2 \tTraining Loss: 0.000297\n",
      "Epoch: 3 \tTraining Loss: 0.000297\n",
      "Epoch: 4 \tTraining Loss: 0.000297\n",
      "Epoch: 5 \tTraining Loss: 0.000297\n",
      "Epoch: 6 \tTraining Loss: 0.000297\n",
      "Epoch: 7 \tTraining Loss: 0.000297\n",
      "Epoch: 8 \tTraining Loss: 0.000297\n",
      "Epoch: 9 \tTraining Loss: 0.000297\n",
      "Epoch: 10 \tTraining Loss: 0.000297\n",
      "Epoch: 11 \tTraining Loss: 0.000297\n",
      "Epoch: 12 \tTraining Loss: 0.000297\n",
      "Epoch: 13 \tTraining Loss: 0.000297\n",
      "Epoch: 14 \tTraining Loss: 0.000297\n",
      "Epoch: 15 \tTraining Loss: 0.000297\n",
      "Epoch: 16 \tTraining Loss: 0.000297\n",
      "Epoch: 17 \tTraining Loss: 0.000297\n",
      "Epoch: 18 \tTraining Loss: 0.000297\n",
      "Epoch: 1 \tTraining Loss: 0.000090\n",
      "Epoch: 2 \tTraining Loss: 0.000090\n",
      "Epoch: 3 \tTraining Loss: 0.000090\n",
      "Epoch: 4 \tTraining Loss: 0.000090\n",
      "Epoch: 5 \tTraining Loss: 0.000090\n",
      "Epoch: 6 \tTraining Loss: 0.000090\n",
      "Epoch: 7 \tTraining Loss: 0.000090\n",
      "Epoch: 8 \tTraining Loss: 0.000090\n",
      "Epoch: 9 \tTraining Loss: 0.000090\n",
      "Epoch: 10 \tTraining Loss: 0.000090\n",
      "Epoch: 11 \tTraining Loss: 0.000090\n",
      "Epoch: 12 \tTraining Loss: 0.000090\n",
      "Epoch: 13 \tTraining Loss: 0.000090\n",
      "Epoch: 14 \tTraining Loss: 0.000090\n",
      "Epoch: 15 \tTraining Loss: 0.000090\n",
      "Epoch: 16 \tTraining Loss: 0.000090\n",
      "Epoch: 17 \tTraining Loss: 0.000090\n",
      "Epoch: 18 \tTraining Loss: 0.000090\n",
      "Epoch: 1 \tTraining Loss: 0.000398\n",
      "Epoch: 2 \tTraining Loss: 0.000398\n",
      "Epoch: 3 \tTraining Loss: 0.000398\n",
      "Epoch: 4 \tTraining Loss: 0.000398\n",
      "Epoch: 5 \tTraining Loss: 0.000398\n",
      "Epoch: 6 \tTraining Loss: 0.000398\n",
      "Epoch: 7 \tTraining Loss: 0.000398\n",
      "Epoch: 8 \tTraining Loss: 0.000398\n",
      "Epoch: 9 \tTraining Loss: 0.000398\n",
      "Epoch: 10 \tTraining Loss: 0.000398\n",
      "Epoch: 11 \tTraining Loss: 0.000398\n",
      "Epoch: 12 \tTraining Loss: 0.000398\n",
      "Epoch: 13 \tTraining Loss: 0.000398\n",
      "Epoch: 14 \tTraining Loss: 0.000398\n",
      "Epoch: 15 \tTraining Loss: 0.000398\n",
      "Epoch: 16 \tTraining Loss: 0.000398\n",
      "Epoch: 17 \tTraining Loss: 0.000398\n",
      "Epoch: 18 \tTraining Loss: 0.000398\n",
      "Epoch: 1 \tTraining Loss: 0.000224\n",
      "Epoch: 2 \tTraining Loss: 0.000224\n",
      "Epoch: 3 \tTraining Loss: 0.000224\n",
      "Epoch: 4 \tTraining Loss: 0.000224\n",
      "Epoch: 5 \tTraining Loss: 0.000224\n",
      "Epoch: 6 \tTraining Loss: 0.000224\n",
      "Epoch: 7 \tTraining Loss: 0.000224\n",
      "Epoch: 8 \tTraining Loss: 0.000224\n",
      "Epoch: 9 \tTraining Loss: 0.000224\n",
      "Epoch: 10 \tTraining Loss: 0.000224\n",
      "Epoch: 11 \tTraining Loss: 0.000224\n",
      "Epoch: 12 \tTraining Loss: 0.000224\n",
      "Epoch: 13 \tTraining Loss: 0.000224\n",
      "Epoch: 14 \tTraining Loss: 0.000224\n",
      "Epoch: 15 \tTraining Loss: 0.000224\n",
      "Epoch: 16 \tTraining Loss: 0.000224\n",
      "Epoch: 17 \tTraining Loss: 0.000224\n",
      "Epoch: 18 \tTraining Loss: 0.000224\n",
      "Epoch: 1 \tTraining Loss: 0.000148\n",
      "Epoch: 2 \tTraining Loss: 0.000148\n",
      "Epoch: 3 \tTraining Loss: 0.000148\n",
      "Epoch: 4 \tTraining Loss: 0.000148\n",
      "Epoch: 5 \tTraining Loss: 0.000148\n",
      "Epoch: 6 \tTraining Loss: 0.000148\n",
      "Epoch: 7 \tTraining Loss: 0.000148\n",
      "Epoch: 8 \tTraining Loss: 0.000148\n",
      "Epoch: 9 \tTraining Loss: 0.000148\n",
      "Epoch: 10 \tTraining Loss: 0.000148\n",
      "Epoch: 11 \tTraining Loss: 0.000148\n",
      "Epoch: 12 \tTraining Loss: 0.000148\n",
      "Epoch: 13 \tTraining Loss: 0.000148\n",
      "Epoch: 14 \tTraining Loss: 0.000148\n",
      "Epoch: 15 \tTraining Loss: 0.000148\n",
      "Epoch: 16 \tTraining Loss: 0.000148\n",
      "Epoch: 17 \tTraining Loss: 0.000148\n",
      "Epoch: 18 \tTraining Loss: 0.000148\n",
      "Epoch: 1 \tTraining Loss: 0.000298\n",
      "Epoch: 2 \tTraining Loss: 0.000298\n",
      "Epoch: 3 \tTraining Loss: 0.000298\n",
      "Epoch: 4 \tTraining Loss: 0.000298\n",
      "Epoch: 5 \tTraining Loss: 0.000298\n",
      "Epoch: 6 \tTraining Loss: 0.000298\n",
      "Epoch: 7 \tTraining Loss: 0.000298\n",
      "Epoch: 8 \tTraining Loss: 0.000298\n",
      "Epoch: 9 \tTraining Loss: 0.000298\n",
      "Epoch: 10 \tTraining Loss: 0.000298\n",
      "Epoch: 11 \tTraining Loss: 0.000298\n",
      "Epoch: 12 \tTraining Loss: 0.000298\n",
      "Epoch: 13 \tTraining Loss: 0.000298\n",
      "Epoch: 14 \tTraining Loss: 0.000298\n",
      "Epoch: 15 \tTraining Loss: 0.000298\n",
      "Epoch: 16 \tTraining Loss: 0.000298\n",
      "Epoch: 17 \tTraining Loss: 0.000298\n",
      "Epoch: 18 \tTraining Loss: 0.000298\n",
      "Epoch: 1 \tTraining Loss: 0.000160\n",
      "Epoch: 2 \tTraining Loss: 0.000160\n",
      "Epoch: 3 \tTraining Loss: 0.000160\n",
      "Epoch: 4 \tTraining Loss: 0.000160\n",
      "Epoch: 5 \tTraining Loss: 0.000160\n",
      "Epoch: 6 \tTraining Loss: 0.000160\n",
      "Epoch: 7 \tTraining Loss: 0.000160\n",
      "Epoch: 8 \tTraining Loss: 0.000160\n",
      "Epoch: 9 \tTraining Loss: 0.000160\n",
      "Epoch: 10 \tTraining Loss: 0.000160\n",
      "Epoch: 11 \tTraining Loss: 0.000160\n",
      "Epoch: 12 \tTraining Loss: 0.000160\n",
      "Epoch: 13 \tTraining Loss: 0.000160\n",
      "Epoch: 14 \tTraining Loss: 0.000160\n",
      "Epoch: 15 \tTraining Loss: 0.000160\n",
      "Epoch: 16 \tTraining Loss: 0.000160\n",
      "Epoch: 17 \tTraining Loss: 0.000160\n",
      "Epoch: 18 \tTraining Loss: 0.000160\n",
      "Epoch: 1 \tTraining Loss: 0.000235\n",
      "Epoch: 2 \tTraining Loss: 0.000235\n",
      "Epoch: 3 \tTraining Loss: 0.000235\n",
      "Epoch: 4 \tTraining Loss: 0.000235\n",
      "Epoch: 5 \tTraining Loss: 0.000235\n",
      "Epoch: 6 \tTraining Loss: 0.000235\n",
      "Epoch: 7 \tTraining Loss: 0.000235\n",
      "Epoch: 8 \tTraining Loss: 0.000235\n",
      "Epoch: 9 \tTraining Loss: 0.000235\n",
      "Epoch: 10 \tTraining Loss: 0.000235\n",
      "Epoch: 11 \tTraining Loss: 0.000235\n",
      "Epoch: 12 \tTraining Loss: 0.000235\n",
      "Epoch: 13 \tTraining Loss: 0.000235\n",
      "Epoch: 14 \tTraining Loss: 0.000235\n",
      "Epoch: 15 \tTraining Loss: 0.000235\n",
      "Epoch: 16 \tTraining Loss: 0.000235\n",
      "Epoch: 17 \tTraining Loss: 0.000235\n",
      "Epoch: 18 \tTraining Loss: 0.000235\n",
      "Epoch: 1 \tTraining Loss: 0.000308\n",
      "Epoch: 2 \tTraining Loss: 0.000308\n",
      "Epoch: 3 \tTraining Loss: 0.000308\n",
      "Epoch: 4 \tTraining Loss: 0.000308\n",
      "Epoch: 5 \tTraining Loss: 0.000308\n",
      "Epoch: 6 \tTraining Loss: 0.000308\n",
      "Epoch: 7 \tTraining Loss: 0.000308\n",
      "Epoch: 8 \tTraining Loss: 0.000308\n",
      "Epoch: 9 \tTraining Loss: 0.000308\n",
      "Epoch: 10 \tTraining Loss: 0.000308\n",
      "Epoch: 11 \tTraining Loss: 0.000308\n",
      "Epoch: 12 \tTraining Loss: 0.000308\n",
      "Epoch: 13 \tTraining Loss: 0.000308\n",
      "Epoch: 14 \tTraining Loss: 0.000308\n",
      "Epoch: 15 \tTraining Loss: 0.000308\n",
      "Epoch: 16 \tTraining Loss: 0.000308\n",
      "Epoch: 17 \tTraining Loss: 0.000308\n",
      "Epoch: 18 \tTraining Loss: 0.000308\n",
      "Epoch: 1 \tTraining Loss: 0.000160\n",
      "Epoch: 2 \tTraining Loss: 0.000160\n",
      "Epoch: 3 \tTraining Loss: 0.000160\n",
      "Epoch: 4 \tTraining Loss: 0.000160\n",
      "Epoch: 5 \tTraining Loss: 0.000160\n",
      "Epoch: 6 \tTraining Loss: 0.000160\n",
      "Epoch: 7 \tTraining Loss: 0.000160\n",
      "Epoch: 8 \tTraining Loss: 0.000160\n",
      "Epoch: 9 \tTraining Loss: 0.000160\n",
      "Epoch: 10 \tTraining Loss: 0.000160\n",
      "Epoch: 11 \tTraining Loss: 0.000160\n",
      "Epoch: 12 \tTraining Loss: 0.000160\n",
      "Epoch: 13 \tTraining Loss: 0.000160\n",
      "Epoch: 14 \tTraining Loss: 0.000160\n",
      "Epoch: 15 \tTraining Loss: 0.000160\n",
      "Epoch: 16 \tTraining Loss: 0.000160\n",
      "Epoch: 17 \tTraining Loss: 0.000160\n",
      "Epoch: 18 \tTraining Loss: 0.000160\n",
      "Epoch: 1 \tTraining Loss: 0.000257\n",
      "Epoch: 2 \tTraining Loss: 0.000257\n",
      "Epoch: 3 \tTraining Loss: 0.000257\n",
      "Epoch: 4 \tTraining Loss: 0.000257\n",
      "Epoch: 5 \tTraining Loss: 0.000257\n",
      "Epoch: 6 \tTraining Loss: 0.000257\n",
      "Epoch: 7 \tTraining Loss: 0.000257\n",
      "Epoch: 8 \tTraining Loss: 0.000257\n",
      "Epoch: 9 \tTraining Loss: 0.000257\n",
      "Epoch: 10 \tTraining Loss: 0.000257\n",
      "Epoch: 11 \tTraining Loss: 0.000257\n",
      "Epoch: 12 \tTraining Loss: 0.000257\n",
      "Epoch: 13 \tTraining Loss: 0.000257\n",
      "Epoch: 14 \tTraining Loss: 0.000257\n",
      "Epoch: 15 \tTraining Loss: 0.000257\n",
      "Epoch: 16 \tTraining Loss: 0.000257\n",
      "Epoch: 17 \tTraining Loss: 0.000257\n",
      "Epoch: 18 \tTraining Loss: 0.000257\n",
      "Epoch: 1 \tTraining Loss: 0.000269\n",
      "Epoch: 2 \tTraining Loss: 0.000269\n",
      "Epoch: 3 \tTraining Loss: 0.000269\n",
      "Epoch: 4 \tTraining Loss: 0.000269\n",
      "Epoch: 5 \tTraining Loss: 0.000269\n",
      "Epoch: 6 \tTraining Loss: 0.000269\n",
      "Epoch: 7 \tTraining Loss: 0.000269\n",
      "Epoch: 8 \tTraining Loss: 0.000269\n",
      "Epoch: 9 \tTraining Loss: 0.000269\n",
      "Epoch: 10 \tTraining Loss: 0.000269\n",
      "Epoch: 11 \tTraining Loss: 0.000269\n",
      "Epoch: 12 \tTraining Loss: 0.000269\n",
      "Epoch: 13 \tTraining Loss: 0.000269\n",
      "Epoch: 14 \tTraining Loss: 0.000269\n",
      "Epoch: 15 \tTraining Loss: 0.000269\n",
      "Epoch: 16 \tTraining Loss: 0.000269\n",
      "Epoch: 17 \tTraining Loss: 0.000269\n",
      "Epoch: 18 \tTraining Loss: 0.000269\n",
      "Epoch: 1 \tTraining Loss: 0.000250\n",
      "Epoch: 2 \tTraining Loss: 0.000250\n",
      "Epoch: 3 \tTraining Loss: 0.000250\n",
      "Epoch: 4 \tTraining Loss: 0.000250\n",
      "Epoch: 5 \tTraining Loss: 0.000250\n",
      "Epoch: 6 \tTraining Loss: 0.000250\n",
      "Epoch: 7 \tTraining Loss: 0.000250\n",
      "Epoch: 8 \tTraining Loss: 0.000250\n",
      "Epoch: 9 \tTraining Loss: 0.000250\n",
      "Epoch: 10 \tTraining Loss: 0.000250\n",
      "Epoch: 11 \tTraining Loss: 0.000250\n",
      "Epoch: 12 \tTraining Loss: 0.000250\n",
      "Epoch: 13 \tTraining Loss: 0.000250\n",
      "Epoch: 14 \tTraining Loss: 0.000250\n",
      "Epoch: 15 \tTraining Loss: 0.000250\n",
      "Epoch: 16 \tTraining Loss: 0.000250\n",
      "Epoch: 17 \tTraining Loss: 0.000250\n",
      "Epoch: 18 \tTraining Loss: 0.000250\n",
      "Epoch: 1 \tTraining Loss: 0.000186\n",
      "Epoch: 2 \tTraining Loss: 0.000186\n",
      "Epoch: 3 \tTraining Loss: 0.000186\n",
      "Epoch: 4 \tTraining Loss: 0.000186\n",
      "Epoch: 5 \tTraining Loss: 0.000186\n",
      "Epoch: 6 \tTraining Loss: 0.000186\n",
      "Epoch: 7 \tTraining Loss: 0.000186\n",
      "Epoch: 8 \tTraining Loss: 0.000186\n",
      "Epoch: 9 \tTraining Loss: 0.000186\n",
      "Epoch: 10 \tTraining Loss: 0.000186\n",
      "Epoch: 11 \tTraining Loss: 0.000186\n",
      "Epoch: 12 \tTraining Loss: 0.000186\n",
      "Epoch: 13 \tTraining Loss: 0.000186\n",
      "Epoch: 14 \tTraining Loss: 0.000186\n",
      "Epoch: 15 \tTraining Loss: 0.000186\n",
      "Epoch: 16 \tTraining Loss: 0.000186\n",
      "Epoch: 17 \tTraining Loss: 0.000186\n",
      "Epoch: 18 \tTraining Loss: 0.000186\n",
      "Epoch: 1 \tTraining Loss: 0.000168\n",
      "Epoch: 2 \tTraining Loss: 0.000168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \tTraining Loss: 0.000168\n",
      "Epoch: 4 \tTraining Loss: 0.000168\n",
      "Epoch: 5 \tTraining Loss: 0.000168\n",
      "Epoch: 6 \tTraining Loss: 0.000168\n",
      "Epoch: 7 \tTraining Loss: 0.000168\n",
      "Epoch: 8 \tTraining Loss: 0.000168\n",
      "Epoch: 9 \tTraining Loss: 0.000168\n",
      "Epoch: 10 \tTraining Loss: 0.000168\n",
      "Epoch: 11 \tTraining Loss: 0.000168\n",
      "Epoch: 12 \tTraining Loss: 0.000168\n",
      "Epoch: 13 \tTraining Loss: 0.000168\n",
      "Epoch: 14 \tTraining Loss: 0.000168\n",
      "Epoch: 15 \tTraining Loss: 0.000168\n",
      "Epoch: 16 \tTraining Loss: 0.000168\n",
      "Epoch: 17 \tTraining Loss: 0.000168\n",
      "Epoch: 18 \tTraining Loss: 0.000168\n",
      "Epoch: 1 \tTraining Loss: 0.000147\n",
      "Epoch: 2 \tTraining Loss: 0.000147\n",
      "Epoch: 3 \tTraining Loss: 0.000147\n",
      "Epoch: 4 \tTraining Loss: 0.000147\n",
      "Epoch: 5 \tTraining Loss: 0.000147\n",
      "Epoch: 6 \tTraining Loss: 0.000147\n",
      "Epoch: 7 \tTraining Loss: 0.000147\n",
      "Epoch: 8 \tTraining Loss: 0.000147\n",
      "Epoch: 9 \tTraining Loss: 0.000147\n",
      "Epoch: 10 \tTraining Loss: 0.000147\n",
      "Epoch: 11 \tTraining Loss: 0.000147\n",
      "Epoch: 12 \tTraining Loss: 0.000147\n",
      "Epoch: 13 \tTraining Loss: 0.000147\n",
      "Epoch: 14 \tTraining Loss: 0.000147\n",
      "Epoch: 15 \tTraining Loss: 0.000147\n",
      "Epoch: 16 \tTraining Loss: 0.000147\n",
      "Epoch: 17 \tTraining Loss: 0.000147\n",
      "Epoch: 18 \tTraining Loss: 0.000147\n",
      "Epoch: 1 \tTraining Loss: 0.000178\n",
      "Epoch: 2 \tTraining Loss: 0.000178\n",
      "Epoch: 3 \tTraining Loss: 0.000178\n",
      "Epoch: 4 \tTraining Loss: 0.000178\n",
      "Epoch: 5 \tTraining Loss: 0.000178\n",
      "Epoch: 6 \tTraining Loss: 0.000178\n",
      "Epoch: 7 \tTraining Loss: 0.000178\n",
      "Epoch: 8 \tTraining Loss: 0.000178\n",
      "Epoch: 9 \tTraining Loss: 0.000178\n",
      "Epoch: 10 \tTraining Loss: 0.000178\n",
      "Epoch: 11 \tTraining Loss: 0.000178\n",
      "Epoch: 12 \tTraining Loss: 0.000178\n",
      "Epoch: 13 \tTraining Loss: 0.000178\n",
      "Epoch: 14 \tTraining Loss: 0.000178\n",
      "Epoch: 15 \tTraining Loss: 0.000178\n",
      "Epoch: 16 \tTraining Loss: 0.000178\n",
      "Epoch: 17 \tTraining Loss: 0.000178\n",
      "Epoch: 18 \tTraining Loss: 0.000178\n",
      "Epoch: 1 \tTraining Loss: 0.000164\n",
      "Epoch: 2 \tTraining Loss: 0.000164\n",
      "Epoch: 3 \tTraining Loss: 0.000164\n",
      "Epoch: 4 \tTraining Loss: 0.000164\n",
      "Epoch: 5 \tTraining Loss: 0.000164\n",
      "Epoch: 6 \tTraining Loss: 0.000164\n",
      "Epoch: 7 \tTraining Loss: 0.000164\n",
      "Epoch: 8 \tTraining Loss: 0.000164\n",
      "Epoch: 9 \tTraining Loss: 0.000164\n",
      "Epoch: 10 \tTraining Loss: 0.000164\n",
      "Epoch: 11 \tTraining Loss: 0.000164\n",
      "Epoch: 12 \tTraining Loss: 0.000164\n",
      "Epoch: 13 \tTraining Loss: 0.000164\n",
      "Epoch: 14 \tTraining Loss: 0.000164\n",
      "Epoch: 15 \tTraining Loss: 0.000164\n",
      "Epoch: 16 \tTraining Loss: 0.000164\n",
      "Epoch: 17 \tTraining Loss: 0.000164\n",
      "Epoch: 18 \tTraining Loss: 0.000164\n",
      "Epoch: 1 \tTraining Loss: 0.000094\n",
      "Epoch: 2 \tTraining Loss: 0.000094\n",
      "Epoch: 3 \tTraining Loss: 0.000094\n",
      "Epoch: 4 \tTraining Loss: 0.000094\n",
      "Epoch: 5 \tTraining Loss: 0.000094\n",
      "Epoch: 6 \tTraining Loss: 0.000094\n",
      "Epoch: 7 \tTraining Loss: 0.000094\n",
      "Epoch: 8 \tTraining Loss: 0.000094\n",
      "Epoch: 9 \tTraining Loss: 0.000094\n",
      "Epoch: 10 \tTraining Loss: 0.000094\n",
      "Epoch: 11 \tTraining Loss: 0.000094\n",
      "Epoch: 12 \tTraining Loss: 0.000094\n",
      "Epoch: 13 \tTraining Loss: 0.000094\n",
      "Epoch: 14 \tTraining Loss: 0.000094\n",
      "Epoch: 15 \tTraining Loss: 0.000094\n",
      "Epoch: 16 \tTraining Loss: 0.000094\n",
      "Epoch: 17 \tTraining Loss: 0.000094\n",
      "Epoch: 18 \tTraining Loss: 0.000094\n",
      "Epoch: 1 \tTraining Loss: 0.000122\n",
      "Epoch: 2 \tTraining Loss: 0.000122\n",
      "Epoch: 3 \tTraining Loss: 0.000122\n",
      "Epoch: 4 \tTraining Loss: 0.000122\n",
      "Epoch: 5 \tTraining Loss: 0.000122\n",
      "Epoch: 6 \tTraining Loss: 0.000122\n",
      "Epoch: 7 \tTraining Loss: 0.000122\n",
      "Epoch: 8 \tTraining Loss: 0.000122\n",
      "Epoch: 9 \tTraining Loss: 0.000122\n",
      "Epoch: 10 \tTraining Loss: 0.000122\n",
      "Epoch: 11 \tTraining Loss: 0.000122\n",
      "Epoch: 12 \tTraining Loss: 0.000122\n",
      "Epoch: 13 \tTraining Loss: 0.000122\n",
      "Epoch: 14 \tTraining Loss: 0.000122\n",
      "Epoch: 15 \tTraining Loss: 0.000122\n",
      "Epoch: 16 \tTraining Loss: 0.000122\n",
      "Epoch: 17 \tTraining Loss: 0.000122\n",
      "Epoch: 18 \tTraining Loss: 0.000122\n",
      "Epoch: 1 \tTraining Loss: 0.000082\n",
      "Epoch: 2 \tTraining Loss: 0.000082\n",
      "Epoch: 3 \tTraining Loss: 0.000082\n",
      "Epoch: 4 \tTraining Loss: 0.000082\n",
      "Epoch: 5 \tTraining Loss: 0.000082\n",
      "Epoch: 6 \tTraining Loss: 0.000082\n",
      "Epoch: 7 \tTraining Loss: 0.000082\n",
      "Epoch: 8 \tTraining Loss: 0.000082\n",
      "Epoch: 9 \tTraining Loss: 0.000082\n",
      "Epoch: 10 \tTraining Loss: 0.000082\n",
      "Epoch: 11 \tTraining Loss: 0.000082\n",
      "Epoch: 12 \tTraining Loss: 0.000082\n",
      "Epoch: 13 \tTraining Loss: 0.000082\n",
      "Epoch: 14 \tTraining Loss: 0.000082\n",
      "Epoch: 15 \tTraining Loss: 0.000082\n",
      "Epoch: 16 \tTraining Loss: 0.000082\n",
      "Epoch: 17 \tTraining Loss: 0.000082\n",
      "Epoch: 18 \tTraining Loss: 0.000082\n",
      "Epoch: 1 \tTraining Loss: 0.000148\n",
      "Epoch: 2 \tTraining Loss: 0.000148\n",
      "Epoch: 3 \tTraining Loss: 0.000148\n",
      "Epoch: 4 \tTraining Loss: 0.000148\n",
      "Epoch: 5 \tTraining Loss: 0.000148\n",
      "Epoch: 6 \tTraining Loss: 0.000148\n",
      "Epoch: 7 \tTraining Loss: 0.000148\n",
      "Epoch: 8 \tTraining Loss: 0.000148\n",
      "Epoch: 9 \tTraining Loss: 0.000148\n",
      "Epoch: 10 \tTraining Loss: 0.000148\n",
      "Epoch: 11 \tTraining Loss: 0.000148\n",
      "Epoch: 12 \tTraining Loss: 0.000148\n",
      "Epoch: 13 \tTraining Loss: 0.000148\n",
      "Epoch: 14 \tTraining Loss: 0.000148\n",
      "Epoch: 15 \tTraining Loss: 0.000148\n",
      "Epoch: 16 \tTraining Loss: 0.000148\n",
      "Epoch: 17 \tTraining Loss: 0.000148\n",
      "Epoch: 18 \tTraining Loss: 0.000148\n",
      "Epoch: 1 \tTraining Loss: 0.000224\n",
      "Epoch: 2 \tTraining Loss: 0.000224\n",
      "Epoch: 3 \tTraining Loss: 0.000224\n",
      "Epoch: 4 \tTraining Loss: 0.000224\n",
      "Epoch: 5 \tTraining Loss: 0.000224\n",
      "Epoch: 6 \tTraining Loss: 0.000224\n",
      "Epoch: 7 \tTraining Loss: 0.000224\n",
      "Epoch: 8 \tTraining Loss: 0.000224\n",
      "Epoch: 9 \tTraining Loss: 0.000224\n",
      "Epoch: 10 \tTraining Loss: 0.000224\n",
      "Epoch: 11 \tTraining Loss: 0.000224\n",
      "Epoch: 12 \tTraining Loss: 0.000224\n",
      "Epoch: 13 \tTraining Loss: 0.000224\n",
      "Epoch: 14 \tTraining Loss: 0.000224\n",
      "Epoch: 15 \tTraining Loss: 0.000224\n",
      "Epoch: 16 \tTraining Loss: 0.000224\n",
      "Epoch: 17 \tTraining Loss: 0.000224\n",
      "Epoch: 18 \tTraining Loss: 0.000224\n",
      "Epoch: 1 \tTraining Loss: 0.000111\n",
      "Epoch: 2 \tTraining Loss: 0.000111\n",
      "Epoch: 3 \tTraining Loss: 0.000111\n",
      "Epoch: 4 \tTraining Loss: 0.000111\n",
      "Epoch: 5 \tTraining Loss: 0.000111\n",
      "Epoch: 6 \tTraining Loss: 0.000111\n",
      "Epoch: 7 \tTraining Loss: 0.000111\n",
      "Epoch: 8 \tTraining Loss: 0.000111\n",
      "Epoch: 9 \tTraining Loss: 0.000111\n",
      "Epoch: 10 \tTraining Loss: 0.000111\n",
      "Epoch: 11 \tTraining Loss: 0.000111\n",
      "Epoch: 12 \tTraining Loss: 0.000111\n",
      "Epoch: 13 \tTraining Loss: 0.000111\n",
      "Epoch: 14 \tTraining Loss: 0.000111\n",
      "Epoch: 15 \tTraining Loss: 0.000111\n",
      "Epoch: 16 \tTraining Loss: 0.000111\n",
      "Epoch: 17 \tTraining Loss: 0.000111\n",
      "Epoch: 18 \tTraining Loss: 0.000111\n",
      "Epoch: 1 \tTraining Loss: 0.000190\n",
      "Epoch: 2 \tTraining Loss: 0.000190\n",
      "Epoch: 3 \tTraining Loss: 0.000190\n",
      "Epoch: 4 \tTraining Loss: 0.000190\n",
      "Epoch: 5 \tTraining Loss: 0.000190\n",
      "Epoch: 6 \tTraining Loss: 0.000190\n",
      "Epoch: 7 \tTraining Loss: 0.000190\n",
      "Epoch: 8 \tTraining Loss: 0.000190\n",
      "Epoch: 9 \tTraining Loss: 0.000190\n",
      "Epoch: 10 \tTraining Loss: 0.000190\n",
      "Epoch: 11 \tTraining Loss: 0.000190\n",
      "Epoch: 12 \tTraining Loss: 0.000190\n",
      "Epoch: 13 \tTraining Loss: 0.000190\n",
      "Epoch: 14 \tTraining Loss: 0.000190\n",
      "Epoch: 15 \tTraining Loss: 0.000190\n",
      "Epoch: 16 \tTraining Loss: 0.000190\n",
      "Epoch: 17 \tTraining Loss: 0.000190\n",
      "Epoch: 18 \tTraining Loss: 0.000190\n",
      "Epoch: 1 \tTraining Loss: 0.000304\n",
      "Epoch: 2 \tTraining Loss: 0.000304\n",
      "Epoch: 3 \tTraining Loss: 0.000304\n",
      "Epoch: 4 \tTraining Loss: 0.000304\n",
      "Epoch: 5 \tTraining Loss: 0.000304\n",
      "Epoch: 6 \tTraining Loss: 0.000304\n",
      "Epoch: 7 \tTraining Loss: 0.000304\n",
      "Epoch: 8 \tTraining Loss: 0.000304\n",
      "Epoch: 9 \tTraining Loss: 0.000304\n",
      "Epoch: 10 \tTraining Loss: 0.000304\n",
      "Epoch: 11 \tTraining Loss: 0.000304\n",
      "Epoch: 12 \tTraining Loss: 0.000304\n",
      "Epoch: 13 \tTraining Loss: 0.000304\n",
      "Epoch: 14 \tTraining Loss: 0.000304\n",
      "Epoch: 15 \tTraining Loss: 0.000304\n",
      "Epoch: 16 \tTraining Loss: 0.000304\n",
      "Epoch: 17 \tTraining Loss: 0.000304\n",
      "Epoch: 18 \tTraining Loss: 0.000304\n",
      "Epoch: 1 \tTraining Loss: 0.000263\n",
      "Epoch: 2 \tTraining Loss: 0.000263\n",
      "Epoch: 3 \tTraining Loss: 0.000263\n",
      "Epoch: 4 \tTraining Loss: 0.000263\n",
      "Epoch: 5 \tTraining Loss: 0.000263\n",
      "Epoch: 6 \tTraining Loss: 0.000263\n",
      "Epoch: 7 \tTraining Loss: 0.000263\n",
      "Epoch: 8 \tTraining Loss: 0.000263\n",
      "Epoch: 9 \tTraining Loss: 0.000263\n",
      "Epoch: 10 \tTraining Loss: 0.000263\n",
      "Epoch: 11 \tTraining Loss: 0.000263\n",
      "Epoch: 12 \tTraining Loss: 0.000263\n",
      "Epoch: 13 \tTraining Loss: 0.000263\n",
      "Epoch: 14 \tTraining Loss: 0.000263\n",
      "Epoch: 15 \tTraining Loss: 0.000263\n",
      "Epoch: 16 \tTraining Loss: 0.000263\n",
      "Epoch: 17 \tTraining Loss: 0.000263\n",
      "Epoch: 18 \tTraining Loss: 0.000263\n",
      "Epoch: 1 \tTraining Loss: 0.000197\n",
      "Epoch: 2 \tTraining Loss: 0.000197\n",
      "Epoch: 3 \tTraining Loss: 0.000197\n",
      "Epoch: 4 \tTraining Loss: 0.000197\n",
      "Epoch: 5 \tTraining Loss: 0.000197\n",
      "Epoch: 6 \tTraining Loss: 0.000197\n",
      "Epoch: 7 \tTraining Loss: 0.000197\n",
      "Epoch: 8 \tTraining Loss: 0.000197\n",
      "Epoch: 9 \tTraining Loss: 0.000197\n",
      "Epoch: 10 \tTraining Loss: 0.000197\n",
      "Epoch: 11 \tTraining Loss: 0.000197\n",
      "Epoch: 12 \tTraining Loss: 0.000197\n",
      "Epoch: 13 \tTraining Loss: 0.000197\n",
      "Epoch: 14 \tTraining Loss: 0.000197\n",
      "Epoch: 15 \tTraining Loss: 0.000197\n",
      "Epoch: 16 \tTraining Loss: 0.000197\n",
      "Epoch: 17 \tTraining Loss: 0.000197\n",
      "Epoch: 18 \tTraining Loss: 0.000197\n",
      "Epoch: 1 \tTraining Loss: 0.000153\n",
      "Epoch: 2 \tTraining Loss: 0.000153\n",
      "Epoch: 3 \tTraining Loss: 0.000153\n",
      "Epoch: 4 \tTraining Loss: 0.000153\n",
      "Epoch: 5 \tTraining Loss: 0.000153\n",
      "Epoch: 6 \tTraining Loss: 0.000153\n",
      "Epoch: 7 \tTraining Loss: 0.000153\n",
      "Epoch: 8 \tTraining Loss: 0.000153\n",
      "Epoch: 9 \tTraining Loss: 0.000153\n",
      "Epoch: 10 \tTraining Loss: 0.000153\n",
      "Epoch: 11 \tTraining Loss: 0.000153\n",
      "Epoch: 12 \tTraining Loss: 0.000153\n",
      "Epoch: 13 \tTraining Loss: 0.000153\n",
      "Epoch: 14 \tTraining Loss: 0.000153\n",
      "Epoch: 15 \tTraining Loss: 0.000153\n",
      "Epoch: 16 \tTraining Loss: 0.000153\n",
      "Epoch: 17 \tTraining Loss: 0.000153\n",
      "Epoch: 18 \tTraining Loss: 0.000153\n",
      "Epoch: 1 \tTraining Loss: 0.000348\n",
      "Epoch: 2 \tTraining Loss: 0.000348\n",
      "Epoch: 3 \tTraining Loss: 0.000348\n",
      "Epoch: 4 \tTraining Loss: 0.000348\n",
      "Epoch: 5 \tTraining Loss: 0.000348\n",
      "Epoch: 6 \tTraining Loss: 0.000348\n",
      "Epoch: 7 \tTraining Loss: 0.000348\n",
      "Epoch: 8 \tTraining Loss: 0.000348\n",
      "Epoch: 9 \tTraining Loss: 0.000348\n",
      "Epoch: 10 \tTraining Loss: 0.000348\n",
      "Epoch: 11 \tTraining Loss: 0.000348\n",
      "Epoch: 12 \tTraining Loss: 0.000348\n",
      "Epoch: 13 \tTraining Loss: 0.000348\n",
      "Epoch: 14 \tTraining Loss: 0.000348\n",
      "Epoch: 15 \tTraining Loss: 0.000348\n",
      "Epoch: 16 \tTraining Loss: 0.000348\n",
      "Epoch: 17 \tTraining Loss: 0.000348\n",
      "Epoch: 18 \tTraining Loss: 0.000348\n",
      "Epoch: 1 \tTraining Loss: 0.000268\n",
      "Epoch: 2 \tTraining Loss: 0.000268\n",
      "Epoch: 3 \tTraining Loss: 0.000268\n",
      "Epoch: 4 \tTraining Loss: 0.000268\n",
      "Epoch: 5 \tTraining Loss: 0.000268\n",
      "Epoch: 6 \tTraining Loss: 0.000268\n",
      "Epoch: 7 \tTraining Loss: 0.000268\n",
      "Epoch: 8 \tTraining Loss: 0.000268\n",
      "Epoch: 9 \tTraining Loss: 0.000268\n",
      "Epoch: 10 \tTraining Loss: 0.000268\n",
      "Epoch: 11 \tTraining Loss: 0.000268\n",
      "Epoch: 12 \tTraining Loss: 0.000268\n",
      "Epoch: 13 \tTraining Loss: 0.000268\n",
      "Epoch: 14 \tTraining Loss: 0.000268\n",
      "Epoch: 15 \tTraining Loss: 0.000268\n",
      "Epoch: 16 \tTraining Loss: 0.000268\n",
      "Epoch: 17 \tTraining Loss: 0.000268\n",
      "Epoch: 18 \tTraining Loss: 0.000268\n",
      "Epoch: 1 \tTraining Loss: 0.000086\n",
      "Epoch: 2 \tTraining Loss: 0.000086\n",
      "Epoch: 3 \tTraining Loss: 0.000086\n",
      "Epoch: 4 \tTraining Loss: 0.000086\n",
      "Epoch: 5 \tTraining Loss: 0.000086\n",
      "Epoch: 6 \tTraining Loss: 0.000086\n",
      "Epoch: 7 \tTraining Loss: 0.000086\n",
      "Epoch: 8 \tTraining Loss: 0.000086\n",
      "Epoch: 9 \tTraining Loss: 0.000086\n",
      "Epoch: 10 \tTraining Loss: 0.000086\n",
      "Epoch: 11 \tTraining Loss: 0.000086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 \tTraining Loss: 0.000086\n",
      "Epoch: 13 \tTraining Loss: 0.000086\n",
      "Epoch: 14 \tTraining Loss: 0.000086\n",
      "Epoch: 15 \tTraining Loss: 0.000086\n",
      "Epoch: 16 \tTraining Loss: 0.000086\n",
      "Epoch: 17 \tTraining Loss: 0.000086\n",
      "Epoch: 18 \tTraining Loss: 0.000086\n",
      "Epoch: 1 \tTraining Loss: 0.000119\n",
      "Epoch: 2 \tTraining Loss: 0.000119\n",
      "Epoch: 3 \tTraining Loss: 0.000119\n",
      "Epoch: 4 \tTraining Loss: 0.000119\n",
      "Epoch: 5 \tTraining Loss: 0.000119\n",
      "Epoch: 6 \tTraining Loss: 0.000119\n",
      "Epoch: 7 \tTraining Loss: 0.000119\n",
      "Epoch: 8 \tTraining Loss: 0.000119\n",
      "Epoch: 9 \tTraining Loss: 0.000119\n",
      "Epoch: 10 \tTraining Loss: 0.000119\n",
      "Epoch: 11 \tTraining Loss: 0.000119\n",
      "Epoch: 12 \tTraining Loss: 0.000119\n",
      "Epoch: 13 \tTraining Loss: 0.000119\n",
      "Epoch: 14 \tTraining Loss: 0.000119\n",
      "Epoch: 15 \tTraining Loss: 0.000119\n",
      "Epoch: 16 \tTraining Loss: 0.000119\n",
      "Epoch: 17 \tTraining Loss: 0.000119\n",
      "Epoch: 18 \tTraining Loss: 0.000119\n",
      "Epoch: 1 \tTraining Loss: 0.000196\n",
      "Epoch: 2 \tTraining Loss: 0.000196\n",
      "Epoch: 3 \tTraining Loss: 0.000196\n",
      "Epoch: 4 \tTraining Loss: 0.000196\n",
      "Epoch: 5 \tTraining Loss: 0.000196\n",
      "Epoch: 6 \tTraining Loss: 0.000196\n",
      "Epoch: 7 \tTraining Loss: 0.000196\n",
      "Epoch: 8 \tTraining Loss: 0.000196\n",
      "Epoch: 9 \tTraining Loss: 0.000196\n",
      "Epoch: 10 \tTraining Loss: 0.000196\n",
      "Epoch: 11 \tTraining Loss: 0.000196\n",
      "Epoch: 12 \tTraining Loss: 0.000196\n",
      "Epoch: 13 \tTraining Loss: 0.000196\n",
      "Epoch: 14 \tTraining Loss: 0.000196\n",
      "Epoch: 15 \tTraining Loss: 0.000196\n",
      "Epoch: 16 \tTraining Loss: 0.000196\n",
      "Epoch: 17 \tTraining Loss: 0.000196\n",
      "Epoch: 18 \tTraining Loss: 0.000196\n",
      "Epoch: 1 \tTraining Loss: 0.000213\n",
      "Epoch: 2 \tTraining Loss: 0.000213\n",
      "Epoch: 3 \tTraining Loss: 0.000213\n",
      "Epoch: 4 \tTraining Loss: 0.000213\n",
      "Epoch: 5 \tTraining Loss: 0.000213\n",
      "Epoch: 6 \tTraining Loss: 0.000213\n",
      "Epoch: 7 \tTraining Loss: 0.000213\n",
      "Epoch: 8 \tTraining Loss: 0.000213\n",
      "Epoch: 9 \tTraining Loss: 0.000213\n",
      "Epoch: 10 \tTraining Loss: 0.000213\n",
      "Epoch: 11 \tTraining Loss: 0.000213\n",
      "Epoch: 12 \tTraining Loss: 0.000213\n",
      "Epoch: 13 \tTraining Loss: 0.000213\n",
      "Epoch: 14 \tTraining Loss: 0.000213\n",
      "Epoch: 15 \tTraining Loss: 0.000213\n",
      "Epoch: 16 \tTraining Loss: 0.000213\n",
      "Epoch: 17 \tTraining Loss: 0.000213\n",
      "Epoch: 18 \tTraining Loss: 0.000213\n",
      "Epoch: 1 \tTraining Loss: 0.000133\n",
      "Epoch: 2 \tTraining Loss: 0.000133\n",
      "Epoch: 3 \tTraining Loss: 0.000133\n",
      "Epoch: 4 \tTraining Loss: 0.000133\n",
      "Epoch: 5 \tTraining Loss: 0.000133\n",
      "Epoch: 6 \tTraining Loss: 0.000133\n",
      "Epoch: 7 \tTraining Loss: 0.000133\n",
      "Epoch: 8 \tTraining Loss: 0.000133\n",
      "Epoch: 9 \tTraining Loss: 0.000133\n",
      "Epoch: 10 \tTraining Loss: 0.000133\n",
      "Epoch: 11 \tTraining Loss: 0.000133\n",
      "Epoch: 12 \tTraining Loss: 0.000133\n",
      "Epoch: 13 \tTraining Loss: 0.000133\n",
      "Epoch: 14 \tTraining Loss: 0.000133\n",
      "Epoch: 15 \tTraining Loss: 0.000133\n",
      "Epoch: 16 \tTraining Loss: 0.000133\n",
      "Epoch: 17 \tTraining Loss: 0.000133\n",
      "Epoch: 18 \tTraining Loss: 0.000133\n",
      "Epoch: 1 \tTraining Loss: 0.000185\n",
      "Epoch: 2 \tTraining Loss: 0.000185\n",
      "Epoch: 3 \tTraining Loss: 0.000185\n",
      "Epoch: 4 \tTraining Loss: 0.000185\n",
      "Epoch: 5 \tTraining Loss: 0.000185\n",
      "Epoch: 6 \tTraining Loss: 0.000185\n",
      "Epoch: 7 \tTraining Loss: 0.000185\n",
      "Epoch: 8 \tTraining Loss: 0.000185\n",
      "Epoch: 9 \tTraining Loss: 0.000185\n",
      "Epoch: 10 \tTraining Loss: 0.000185\n",
      "Epoch: 11 \tTraining Loss: 0.000185\n",
      "Epoch: 12 \tTraining Loss: 0.000185\n",
      "Epoch: 13 \tTraining Loss: 0.000185\n",
      "Epoch: 14 \tTraining Loss: 0.000185\n",
      "Epoch: 15 \tTraining Loss: 0.000185\n",
      "Epoch: 16 \tTraining Loss: 0.000185\n",
      "Epoch: 17 \tTraining Loss: 0.000185\n",
      "Epoch: 18 \tTraining Loss: 0.000185\n",
      "Epoch: 1 \tTraining Loss: 0.000188\n",
      "Epoch: 2 \tTraining Loss: 0.000188\n",
      "Epoch: 3 \tTraining Loss: 0.000188\n",
      "Epoch: 4 \tTraining Loss: 0.000188\n",
      "Epoch: 5 \tTraining Loss: 0.000188\n",
      "Epoch: 6 \tTraining Loss: 0.000188\n",
      "Epoch: 7 \tTraining Loss: 0.000188\n",
      "Epoch: 8 \tTraining Loss: 0.000188\n",
      "Epoch: 9 \tTraining Loss: 0.000188\n",
      "Epoch: 10 \tTraining Loss: 0.000188\n",
      "Epoch: 11 \tTraining Loss: 0.000188\n",
      "Epoch: 12 \tTraining Loss: 0.000188\n",
      "Epoch: 13 \tTraining Loss: 0.000188\n",
      "Epoch: 14 \tTraining Loss: 0.000188\n",
      "Epoch: 15 \tTraining Loss: 0.000188\n",
      "Epoch: 16 \tTraining Loss: 0.000188\n",
      "Epoch: 17 \tTraining Loss: 0.000188\n",
      "Epoch: 18 \tTraining Loss: 0.000188\n",
      "Epoch: 1 \tTraining Loss: 0.000315\n",
      "Epoch: 2 \tTraining Loss: 0.000315\n",
      "Epoch: 3 \tTraining Loss: 0.000315\n",
      "Epoch: 4 \tTraining Loss: 0.000315\n",
      "Epoch: 5 \tTraining Loss: 0.000315\n",
      "Epoch: 6 \tTraining Loss: 0.000315\n",
      "Epoch: 7 \tTraining Loss: 0.000315\n",
      "Epoch: 8 \tTraining Loss: 0.000315\n",
      "Epoch: 9 \tTraining Loss: 0.000315\n",
      "Epoch: 10 \tTraining Loss: 0.000315\n",
      "Epoch: 11 \tTraining Loss: 0.000315\n",
      "Epoch: 12 \tTraining Loss: 0.000315\n",
      "Epoch: 13 \tTraining Loss: 0.000315\n",
      "Epoch: 14 \tTraining Loss: 0.000315\n",
      "Epoch: 15 \tTraining Loss: 0.000315\n",
      "Epoch: 16 \tTraining Loss: 0.000315\n",
      "Epoch: 17 \tTraining Loss: 0.000315\n",
      "Epoch: 18 \tTraining Loss: 0.000315\n",
      "Epoch: 1 \tTraining Loss: 0.000201\n",
      "Epoch: 2 \tTraining Loss: 0.000201\n",
      "Epoch: 3 \tTraining Loss: 0.000201\n",
      "Epoch: 4 \tTraining Loss: 0.000201\n",
      "Epoch: 5 \tTraining Loss: 0.000201\n",
      "Epoch: 6 \tTraining Loss: 0.000201\n",
      "Epoch: 7 \tTraining Loss: 0.000201\n",
      "Epoch: 8 \tTraining Loss: 0.000201\n",
      "Epoch: 9 \tTraining Loss: 0.000201\n",
      "Epoch: 10 \tTraining Loss: 0.000201\n",
      "Epoch: 11 \tTraining Loss: 0.000201\n",
      "Epoch: 12 \tTraining Loss: 0.000201\n",
      "Epoch: 13 \tTraining Loss: 0.000201\n",
      "Epoch: 14 \tTraining Loss: 0.000201\n",
      "Epoch: 15 \tTraining Loss: 0.000201\n",
      "Epoch: 16 \tTraining Loss: 0.000201\n",
      "Epoch: 17 \tTraining Loss: 0.000201\n",
      "Epoch: 18 \tTraining Loss: 0.000201\n",
      "Epoch: 1 \tTraining Loss: 0.000218\n",
      "Epoch: 2 \tTraining Loss: 0.000218\n",
      "Epoch: 3 \tTraining Loss: 0.000218\n",
      "Epoch: 4 \tTraining Loss: 0.000218\n",
      "Epoch: 5 \tTraining Loss: 0.000218\n",
      "Epoch: 6 \tTraining Loss: 0.000218\n",
      "Epoch: 7 \tTraining Loss: 0.000218\n",
      "Epoch: 8 \tTraining Loss: 0.000218\n",
      "Epoch: 9 \tTraining Loss: 0.000218\n",
      "Epoch: 10 \tTraining Loss: 0.000218\n",
      "Epoch: 11 \tTraining Loss: 0.000218\n",
      "Epoch: 12 \tTraining Loss: 0.000218\n",
      "Epoch: 13 \tTraining Loss: 0.000218\n",
      "Epoch: 14 \tTraining Loss: 0.000218\n",
      "Epoch: 15 \tTraining Loss: 0.000218\n",
      "Epoch: 16 \tTraining Loss: 0.000218\n",
      "Epoch: 17 \tTraining Loss: 0.000218\n",
      "Epoch: 18 \tTraining Loss: 0.000218\n",
      "Epoch: 1 \tTraining Loss: 0.000305\n",
      "Epoch: 2 \tTraining Loss: 0.000305\n",
      "Epoch: 3 \tTraining Loss: 0.000305\n",
      "Epoch: 4 \tTraining Loss: 0.000305\n",
      "Epoch: 5 \tTraining Loss: 0.000305\n",
      "Epoch: 6 \tTraining Loss: 0.000305\n",
      "Epoch: 7 \tTraining Loss: 0.000305\n",
      "Epoch: 8 \tTraining Loss: 0.000305\n",
      "Epoch: 9 \tTraining Loss: 0.000305\n",
      "Epoch: 10 \tTraining Loss: 0.000305\n",
      "Epoch: 11 \tTraining Loss: 0.000305\n",
      "Epoch: 12 \tTraining Loss: 0.000305\n",
      "Epoch: 13 \tTraining Loss: 0.000305\n",
      "Epoch: 14 \tTraining Loss: 0.000305\n",
      "Epoch: 15 \tTraining Loss: 0.000305\n",
      "Epoch: 16 \tTraining Loss: 0.000305\n",
      "Epoch: 17 \tTraining Loss: 0.000305\n",
      "Epoch: 18 \tTraining Loss: 0.000305\n",
      "Epoch: 1 \tTraining Loss: 0.000194\n",
      "Epoch: 2 \tTraining Loss: 0.000194\n",
      "Epoch: 3 \tTraining Loss: 0.000194\n",
      "Epoch: 4 \tTraining Loss: 0.000194\n",
      "Epoch: 5 \tTraining Loss: 0.000194\n",
      "Epoch: 6 \tTraining Loss: 0.000194\n",
      "Epoch: 7 \tTraining Loss: 0.000194\n",
      "Epoch: 8 \tTraining Loss: 0.000194\n",
      "Epoch: 9 \tTraining Loss: 0.000194\n",
      "Epoch: 10 \tTraining Loss: 0.000194\n",
      "Epoch: 11 \tTraining Loss: 0.000194\n",
      "Epoch: 12 \tTraining Loss: 0.000194\n",
      "Epoch: 13 \tTraining Loss: 0.000194\n",
      "Epoch: 14 \tTraining Loss: 0.000194\n",
      "Epoch: 15 \tTraining Loss: 0.000194\n",
      "Epoch: 16 \tTraining Loss: 0.000194\n",
      "Epoch: 17 \tTraining Loss: 0.000194\n",
      "Epoch: 18 \tTraining Loss: 0.000194\n",
      "Epoch: 1 \tTraining Loss: 0.000148\n",
      "Epoch: 2 \tTraining Loss: 0.000148\n",
      "Epoch: 3 \tTraining Loss: 0.000148\n",
      "Epoch: 4 \tTraining Loss: 0.000148\n",
      "Epoch: 5 \tTraining Loss: 0.000148\n",
      "Epoch: 6 \tTraining Loss: 0.000148\n",
      "Epoch: 7 \tTraining Loss: 0.000148\n",
      "Epoch: 8 \tTraining Loss: 0.000148\n",
      "Epoch: 9 \tTraining Loss: 0.000148\n",
      "Epoch: 10 \tTraining Loss: 0.000148\n",
      "Epoch: 11 \tTraining Loss: 0.000148\n",
      "Epoch: 12 \tTraining Loss: 0.000148\n",
      "Epoch: 13 \tTraining Loss: 0.000148\n",
      "Epoch: 14 \tTraining Loss: 0.000148\n",
      "Epoch: 15 \tTraining Loss: 0.000148\n",
      "Epoch: 16 \tTraining Loss: 0.000148\n",
      "Epoch: 17 \tTraining Loss: 0.000148\n",
      "Epoch: 18 \tTraining Loss: 0.000148\n",
      "Epoch: 1 \tTraining Loss: 0.000158\n",
      "Epoch: 2 \tTraining Loss: 0.000158\n",
      "Epoch: 3 \tTraining Loss: 0.000158\n",
      "Epoch: 4 \tTraining Loss: 0.000158\n",
      "Epoch: 5 \tTraining Loss: 0.000158\n",
      "Epoch: 6 \tTraining Loss: 0.000158\n",
      "Epoch: 7 \tTraining Loss: 0.000158\n",
      "Epoch: 8 \tTraining Loss: 0.000158\n",
      "Epoch: 9 \tTraining Loss: 0.000158\n",
      "Epoch: 10 \tTraining Loss: 0.000158\n",
      "Epoch: 11 \tTraining Loss: 0.000158\n",
      "Epoch: 12 \tTraining Loss: 0.000158\n",
      "Epoch: 13 \tTraining Loss: 0.000158\n",
      "Epoch: 14 \tTraining Loss: 0.000158\n",
      "Epoch: 15 \tTraining Loss: 0.000158\n",
      "Epoch: 16 \tTraining Loss: 0.000158\n",
      "Epoch: 17 \tTraining Loss: 0.000158\n",
      "Epoch: 18 \tTraining Loss: 0.000158\n",
      "Epoch: 1 \tTraining Loss: 0.000229\n",
      "Epoch: 2 \tTraining Loss: 0.000229\n",
      "Epoch: 3 \tTraining Loss: 0.000229\n",
      "Epoch: 4 \tTraining Loss: 0.000229\n",
      "Epoch: 5 \tTraining Loss: 0.000229\n",
      "Epoch: 6 \tTraining Loss: 0.000229\n",
      "Epoch: 7 \tTraining Loss: 0.000229\n",
      "Epoch: 8 \tTraining Loss: 0.000229\n",
      "Epoch: 9 \tTraining Loss: 0.000229\n",
      "Epoch: 10 \tTraining Loss: 0.000229\n",
      "Epoch: 11 \tTraining Loss: 0.000229\n",
      "Epoch: 12 \tTraining Loss: 0.000229\n",
      "Epoch: 13 \tTraining Loss: 0.000229\n",
      "Epoch: 14 \tTraining Loss: 0.000229\n",
      "Epoch: 15 \tTraining Loss: 0.000229\n",
      "Epoch: 16 \tTraining Loss: 0.000229\n",
      "Epoch: 17 \tTraining Loss: 0.000229\n",
      "Epoch: 18 \tTraining Loss: 0.000229\n",
      "Epoch: 1 \tTraining Loss: 0.000208\n",
      "Epoch: 2 \tTraining Loss: 0.000208\n",
      "Epoch: 3 \tTraining Loss: 0.000208\n",
      "Epoch: 4 \tTraining Loss: 0.000208\n",
      "Epoch: 5 \tTraining Loss: 0.000208\n",
      "Epoch: 6 \tTraining Loss: 0.000208\n",
      "Epoch: 7 \tTraining Loss: 0.000208\n",
      "Epoch: 8 \tTraining Loss: 0.000208\n",
      "Epoch: 9 \tTraining Loss: 0.000208\n",
      "Epoch: 10 \tTraining Loss: 0.000208\n",
      "Epoch: 11 \tTraining Loss: 0.000208\n",
      "Epoch: 12 \tTraining Loss: 0.000208\n",
      "Epoch: 13 \tTraining Loss: 0.000208\n",
      "Epoch: 14 \tTraining Loss: 0.000208\n",
      "Epoch: 15 \tTraining Loss: 0.000208\n",
      "Epoch: 16 \tTraining Loss: 0.000208\n",
      "Epoch: 17 \tTraining Loss: 0.000208\n",
      "Epoch: 18 \tTraining Loss: 0.000208\n",
      "Epoch: 1 \tTraining Loss: 0.000099\n",
      "Epoch: 2 \tTraining Loss: 0.000099\n",
      "Epoch: 3 \tTraining Loss: 0.000099\n",
      "Epoch: 4 \tTraining Loss: 0.000099\n",
      "Epoch: 5 \tTraining Loss: 0.000099\n",
      "Epoch: 6 \tTraining Loss: 0.000099\n",
      "Epoch: 7 \tTraining Loss: 0.000099\n",
      "Epoch: 8 \tTraining Loss: 0.000099\n",
      "Epoch: 9 \tTraining Loss: 0.000099\n",
      "Epoch: 10 \tTraining Loss: 0.000099\n",
      "Epoch: 11 \tTraining Loss: 0.000099\n",
      "Epoch: 12 \tTraining Loss: 0.000099\n",
      "Epoch: 13 \tTraining Loss: 0.000099\n",
      "Epoch: 14 \tTraining Loss: 0.000099\n",
      "Epoch: 15 \tTraining Loss: 0.000099\n",
      "Epoch: 16 \tTraining Loss: 0.000099\n",
      "Epoch: 17 \tTraining Loss: 0.000099\n",
      "Epoch: 18 \tTraining Loss: 0.000099\n",
      "Epoch: 1 \tTraining Loss: 0.000174\n",
      "Epoch: 2 \tTraining Loss: 0.000174\n",
      "Epoch: 3 \tTraining Loss: 0.000174\n",
      "Epoch: 4 \tTraining Loss: 0.000174\n",
      "Epoch: 5 \tTraining Loss: 0.000174\n",
      "Epoch: 6 \tTraining Loss: 0.000174\n",
      "Epoch: 7 \tTraining Loss: 0.000174\n",
      "Epoch: 8 \tTraining Loss: 0.000174\n",
      "Epoch: 9 \tTraining Loss: 0.000174\n",
      "Epoch: 10 \tTraining Loss: 0.000174\n",
      "Epoch: 11 \tTraining Loss: 0.000174\n",
      "Epoch: 12 \tTraining Loss: 0.000174\n",
      "Epoch: 13 \tTraining Loss: 0.000174\n",
      "Epoch: 14 \tTraining Loss: 0.000174\n",
      "Epoch: 15 \tTraining Loss: 0.000174\n",
      "Epoch: 16 \tTraining Loss: 0.000174\n",
      "Epoch: 17 \tTraining Loss: 0.000174\n",
      "Epoch: 18 \tTraining Loss: 0.000174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000142\n",
      "Epoch: 2 \tTraining Loss: 0.000142\n",
      "Epoch: 3 \tTraining Loss: 0.000142\n",
      "Epoch: 4 \tTraining Loss: 0.000142\n",
      "Epoch: 5 \tTraining Loss: 0.000142\n",
      "Epoch: 6 \tTraining Loss: 0.000142\n",
      "Epoch: 7 \tTraining Loss: 0.000142\n",
      "Epoch: 8 \tTraining Loss: 0.000142\n",
      "Epoch: 9 \tTraining Loss: 0.000142\n",
      "Epoch: 10 \tTraining Loss: 0.000142\n",
      "Epoch: 11 \tTraining Loss: 0.000142\n",
      "Epoch: 12 \tTraining Loss: 0.000142\n",
      "Epoch: 13 \tTraining Loss: 0.000142\n",
      "Epoch: 14 \tTraining Loss: 0.000142\n",
      "Epoch: 15 \tTraining Loss: 0.000142\n",
      "Epoch: 16 \tTraining Loss: 0.000142\n",
      "Epoch: 17 \tTraining Loss: 0.000142\n",
      "Epoch: 18 \tTraining Loss: 0.000142\n",
      "Epoch: 1 \tTraining Loss: 0.000216\n",
      "Epoch: 2 \tTraining Loss: 0.000216\n",
      "Epoch: 3 \tTraining Loss: 0.000216\n",
      "Epoch: 4 \tTraining Loss: 0.000216\n",
      "Epoch: 5 \tTraining Loss: 0.000216\n",
      "Epoch: 6 \tTraining Loss: 0.000216\n",
      "Epoch: 7 \tTraining Loss: 0.000216\n",
      "Epoch: 8 \tTraining Loss: 0.000216\n",
      "Epoch: 9 \tTraining Loss: 0.000216\n",
      "Epoch: 10 \tTraining Loss: 0.000216\n",
      "Epoch: 11 \tTraining Loss: 0.000216\n",
      "Epoch: 12 \tTraining Loss: 0.000216\n",
      "Epoch: 13 \tTraining Loss: 0.000216\n",
      "Epoch: 14 \tTraining Loss: 0.000216\n",
      "Epoch: 15 \tTraining Loss: 0.000216\n",
      "Epoch: 16 \tTraining Loss: 0.000216\n",
      "Epoch: 17 \tTraining Loss: 0.000216\n",
      "Epoch: 18 \tTraining Loss: 0.000216\n",
      "Epoch: 1 \tTraining Loss: 0.000082\n",
      "Epoch: 2 \tTraining Loss: 0.000082\n",
      "Epoch: 3 \tTraining Loss: 0.000082\n",
      "Epoch: 4 \tTraining Loss: 0.000082\n",
      "Epoch: 5 \tTraining Loss: 0.000082\n",
      "Epoch: 6 \tTraining Loss: 0.000082\n",
      "Epoch: 7 \tTraining Loss: 0.000082\n",
      "Epoch: 8 \tTraining Loss: 0.000082\n",
      "Epoch: 9 \tTraining Loss: 0.000082\n",
      "Epoch: 10 \tTraining Loss: 0.000082\n",
      "Epoch: 11 \tTraining Loss: 0.000082\n",
      "Epoch: 12 \tTraining Loss: 0.000082\n",
      "Epoch: 13 \tTraining Loss: 0.000082\n",
      "Epoch: 14 \tTraining Loss: 0.000082\n",
      "Epoch: 15 \tTraining Loss: 0.000082\n",
      "Epoch: 16 \tTraining Loss: 0.000082\n",
      "Epoch: 17 \tTraining Loss: 0.000082\n",
      "Epoch: 18 \tTraining Loss: 0.000082\n",
      "Epoch: 1 \tTraining Loss: 0.000217\n",
      "Epoch: 2 \tTraining Loss: 0.000217\n",
      "Epoch: 3 \tTraining Loss: 0.000217\n",
      "Epoch: 4 \tTraining Loss: 0.000217\n",
      "Epoch: 5 \tTraining Loss: 0.000217\n",
      "Epoch: 6 \tTraining Loss: 0.000217\n",
      "Epoch: 7 \tTraining Loss: 0.000217\n",
      "Epoch: 8 \tTraining Loss: 0.000217\n",
      "Epoch: 9 \tTraining Loss: 0.000217\n",
      "Epoch: 10 \tTraining Loss: 0.000217\n",
      "Epoch: 11 \tTraining Loss: 0.000217\n",
      "Epoch: 12 \tTraining Loss: 0.000217\n",
      "Epoch: 13 \tTraining Loss: 0.000217\n",
      "Epoch: 14 \tTraining Loss: 0.000217\n",
      "Epoch: 15 \tTraining Loss: 0.000217\n",
      "Epoch: 16 \tTraining Loss: 0.000217\n",
      "Epoch: 17 \tTraining Loss: 0.000217\n",
      "Epoch: 18 \tTraining Loss: 0.000217\n",
      "Epoch: 1 \tTraining Loss: 0.000136\n",
      "Epoch: 2 \tTraining Loss: 0.000136\n",
      "Epoch: 3 \tTraining Loss: 0.000136\n",
      "Epoch: 4 \tTraining Loss: 0.000136\n",
      "Epoch: 5 \tTraining Loss: 0.000136\n",
      "Epoch: 6 \tTraining Loss: 0.000136\n",
      "Epoch: 7 \tTraining Loss: 0.000136\n",
      "Epoch: 8 \tTraining Loss: 0.000136\n",
      "Epoch: 9 \tTraining Loss: 0.000136\n",
      "Epoch: 10 \tTraining Loss: 0.000136\n",
      "Epoch: 11 \tTraining Loss: 0.000136\n",
      "Epoch: 12 \tTraining Loss: 0.000136\n",
      "Epoch: 13 \tTraining Loss: 0.000136\n",
      "Epoch: 14 \tTraining Loss: 0.000136\n",
      "Epoch: 15 \tTraining Loss: 0.000136\n",
      "Epoch: 16 \tTraining Loss: 0.000136\n",
      "Epoch: 17 \tTraining Loss: 0.000136\n",
      "Epoch: 18 \tTraining Loss: 0.000136\n",
      "Epoch: 1 \tTraining Loss: 0.000159\n",
      "Epoch: 2 \tTraining Loss: 0.000159\n",
      "Epoch: 3 \tTraining Loss: 0.000159\n",
      "Epoch: 4 \tTraining Loss: 0.000159\n",
      "Epoch: 5 \tTraining Loss: 0.000159\n",
      "Epoch: 6 \tTraining Loss: 0.000159\n",
      "Epoch: 7 \tTraining Loss: 0.000159\n",
      "Epoch: 8 \tTraining Loss: 0.000159\n",
      "Epoch: 9 \tTraining Loss: 0.000159\n",
      "Epoch: 10 \tTraining Loss: 0.000159\n",
      "Epoch: 11 \tTraining Loss: 0.000159\n",
      "Epoch: 12 \tTraining Loss: 0.000159\n",
      "Epoch: 13 \tTraining Loss: 0.000159\n",
      "Epoch: 14 \tTraining Loss: 0.000159\n",
      "Epoch: 15 \tTraining Loss: 0.000159\n",
      "Epoch: 16 \tTraining Loss: 0.000159\n",
      "Epoch: 17 \tTraining Loss: 0.000159\n",
      "Epoch: 18 \tTraining Loss: 0.000159\n",
      "Epoch: 1 \tTraining Loss: 0.000263\n",
      "Epoch: 2 \tTraining Loss: 0.000263\n",
      "Epoch: 3 \tTraining Loss: 0.000263\n",
      "Epoch: 4 \tTraining Loss: 0.000263\n",
      "Epoch: 5 \tTraining Loss: 0.000263\n",
      "Epoch: 6 \tTraining Loss: 0.000263\n",
      "Epoch: 7 \tTraining Loss: 0.000263\n",
      "Epoch: 8 \tTraining Loss: 0.000263\n",
      "Epoch: 9 \tTraining Loss: 0.000263\n",
      "Epoch: 10 \tTraining Loss: 0.000263\n",
      "Epoch: 11 \tTraining Loss: 0.000263\n",
      "Epoch: 12 \tTraining Loss: 0.000263\n",
      "Epoch: 13 \tTraining Loss: 0.000263\n",
      "Epoch: 14 \tTraining Loss: 0.000263\n",
      "Epoch: 15 \tTraining Loss: 0.000263\n",
      "Epoch: 16 \tTraining Loss: 0.000263\n",
      "Epoch: 17 \tTraining Loss: 0.000263\n",
      "Epoch: 18 \tTraining Loss: 0.000263\n",
      "Epoch: 1 \tTraining Loss: 0.000105\n",
      "Epoch: 2 \tTraining Loss: 0.000105\n",
      "Epoch: 3 \tTraining Loss: 0.000105\n",
      "Epoch: 4 \tTraining Loss: 0.000105\n",
      "Epoch: 5 \tTraining Loss: 0.000105\n",
      "Epoch: 6 \tTraining Loss: 0.000105\n",
      "Epoch: 7 \tTraining Loss: 0.000105\n",
      "Epoch: 8 \tTraining Loss: 0.000105\n",
      "Epoch: 9 \tTraining Loss: 0.000105\n",
      "Epoch: 10 \tTraining Loss: 0.000105\n",
      "Epoch: 11 \tTraining Loss: 0.000105\n",
      "Epoch: 12 \tTraining Loss: 0.000105\n",
      "Epoch: 13 \tTraining Loss: 0.000105\n",
      "Epoch: 14 \tTraining Loss: 0.000105\n",
      "Epoch: 15 \tTraining Loss: 0.000105\n",
      "Epoch: 16 \tTraining Loss: 0.000105\n",
      "Epoch: 17 \tTraining Loss: 0.000105\n",
      "Epoch: 18 \tTraining Loss: 0.000105\n",
      "Epoch: 1 \tTraining Loss: 0.000264\n",
      "Epoch: 2 \tTraining Loss: 0.000264\n",
      "Epoch: 3 \tTraining Loss: 0.000264\n",
      "Epoch: 4 \tTraining Loss: 0.000264\n",
      "Epoch: 5 \tTraining Loss: 0.000264\n",
      "Epoch: 6 \tTraining Loss: 0.000264\n",
      "Epoch: 7 \tTraining Loss: 0.000264\n",
      "Epoch: 8 \tTraining Loss: 0.000264\n",
      "Epoch: 9 \tTraining Loss: 0.000264\n",
      "Epoch: 10 \tTraining Loss: 0.000264\n",
      "Epoch: 11 \tTraining Loss: 0.000264\n",
      "Epoch: 12 \tTraining Loss: 0.000264\n",
      "Epoch: 13 \tTraining Loss: 0.000264\n",
      "Epoch: 14 \tTraining Loss: 0.000264\n",
      "Epoch: 15 \tTraining Loss: 0.000264\n",
      "Epoch: 16 \tTraining Loss: 0.000264\n",
      "Epoch: 17 \tTraining Loss: 0.000264\n",
      "Epoch: 18 \tTraining Loss: 0.000264\n",
      "Epoch: 1 \tTraining Loss: 0.000129\n",
      "Epoch: 2 \tTraining Loss: 0.000129\n",
      "Epoch: 3 \tTraining Loss: 0.000129\n",
      "Epoch: 4 \tTraining Loss: 0.000129\n",
      "Epoch: 5 \tTraining Loss: 0.000129\n",
      "Epoch: 6 \tTraining Loss: 0.000129\n",
      "Epoch: 7 \tTraining Loss: 0.000129\n",
      "Epoch: 8 \tTraining Loss: 0.000129\n",
      "Epoch: 9 \tTraining Loss: 0.000129\n",
      "Epoch: 10 \tTraining Loss: 0.000129\n",
      "Epoch: 11 \tTraining Loss: 0.000129\n",
      "Epoch: 12 \tTraining Loss: 0.000129\n",
      "Epoch: 13 \tTraining Loss: 0.000129\n",
      "Epoch: 14 \tTraining Loss: 0.000129\n",
      "Epoch: 15 \tTraining Loss: 0.000129\n",
      "Epoch: 16 \tTraining Loss: 0.000129\n",
      "Epoch: 17 \tTraining Loss: 0.000129\n",
      "Epoch: 18 \tTraining Loss: 0.000129\n",
      "Epoch: 1 \tTraining Loss: 0.000124\n",
      "Epoch: 2 \tTraining Loss: 0.000124\n",
      "Epoch: 3 \tTraining Loss: 0.000124\n",
      "Epoch: 4 \tTraining Loss: 0.000124\n",
      "Epoch: 5 \tTraining Loss: 0.000124\n",
      "Epoch: 6 \tTraining Loss: 0.000124\n",
      "Epoch: 7 \tTraining Loss: 0.000124\n",
      "Epoch: 8 \tTraining Loss: 0.000124\n",
      "Epoch: 9 \tTraining Loss: 0.000124\n",
      "Epoch: 10 \tTraining Loss: 0.000124\n",
      "Epoch: 11 \tTraining Loss: 0.000124\n",
      "Epoch: 12 \tTraining Loss: 0.000124\n",
      "Epoch: 13 \tTraining Loss: 0.000124\n",
      "Epoch: 14 \tTraining Loss: 0.000124\n",
      "Epoch: 15 \tTraining Loss: 0.000124\n",
      "Epoch: 16 \tTraining Loss: 0.000124\n",
      "Epoch: 17 \tTraining Loss: 0.000124\n",
      "Epoch: 18 \tTraining Loss: 0.000124\n",
      "Epoch: 1 \tTraining Loss: 0.000289\n",
      "Epoch: 2 \tTraining Loss: 0.000289\n",
      "Epoch: 3 \tTraining Loss: 0.000289\n",
      "Epoch: 4 \tTraining Loss: 0.000289\n",
      "Epoch: 5 \tTraining Loss: 0.000289\n",
      "Epoch: 6 \tTraining Loss: 0.000289\n",
      "Epoch: 7 \tTraining Loss: 0.000289\n",
      "Epoch: 8 \tTraining Loss: 0.000289\n",
      "Epoch: 9 \tTraining Loss: 0.000289\n",
      "Epoch: 10 \tTraining Loss: 0.000289\n",
      "Epoch: 11 \tTraining Loss: 0.000289\n",
      "Epoch: 12 \tTraining Loss: 0.000289\n",
      "Epoch: 13 \tTraining Loss: 0.000289\n",
      "Epoch: 14 \tTraining Loss: 0.000289\n",
      "Epoch: 15 \tTraining Loss: 0.000289\n",
      "Epoch: 16 \tTraining Loss: 0.000289\n",
      "Epoch: 17 \tTraining Loss: 0.000289\n",
      "Epoch: 18 \tTraining Loss: 0.000289\n",
      "Epoch: 1 \tTraining Loss: 0.000285\n",
      "Epoch: 2 \tTraining Loss: 0.000285\n",
      "Epoch: 3 \tTraining Loss: 0.000285\n",
      "Epoch: 4 \tTraining Loss: 0.000285\n",
      "Epoch: 5 \tTraining Loss: 0.000285\n",
      "Epoch: 6 \tTraining Loss: 0.000285\n",
      "Epoch: 7 \tTraining Loss: 0.000285\n",
      "Epoch: 8 \tTraining Loss: 0.000285\n",
      "Epoch: 9 \tTraining Loss: 0.000285\n",
      "Epoch: 10 \tTraining Loss: 0.000285\n",
      "Epoch: 11 \tTraining Loss: 0.000285\n",
      "Epoch: 12 \tTraining Loss: 0.000285\n",
      "Epoch: 13 \tTraining Loss: 0.000285\n",
      "Epoch: 14 \tTraining Loss: 0.000285\n",
      "Epoch: 15 \tTraining Loss: 0.000285\n",
      "Epoch: 16 \tTraining Loss: 0.000285\n",
      "Epoch: 17 \tTraining Loss: 0.000285\n",
      "Epoch: 18 \tTraining Loss: 0.000285\n",
      "Epoch: 1 \tTraining Loss: 0.000110\n",
      "Epoch: 2 \tTraining Loss: 0.000110\n",
      "Epoch: 3 \tTraining Loss: 0.000110\n",
      "Epoch: 4 \tTraining Loss: 0.000110\n",
      "Epoch: 5 \tTraining Loss: 0.000110\n",
      "Epoch: 6 \tTraining Loss: 0.000110\n",
      "Epoch: 7 \tTraining Loss: 0.000110\n",
      "Epoch: 8 \tTraining Loss: 0.000110\n",
      "Epoch: 9 \tTraining Loss: 0.000110\n",
      "Epoch: 10 \tTraining Loss: 0.000110\n",
      "Epoch: 11 \tTraining Loss: 0.000110\n",
      "Epoch: 12 \tTraining Loss: 0.000110\n",
      "Epoch: 13 \tTraining Loss: 0.000110\n",
      "Epoch: 14 \tTraining Loss: 0.000110\n",
      "Epoch: 15 \tTraining Loss: 0.000110\n",
      "Epoch: 16 \tTraining Loss: 0.000110\n",
      "Epoch: 17 \tTraining Loss: 0.000110\n",
      "Epoch: 18 \tTraining Loss: 0.000110\n",
      "Epoch: 1 \tTraining Loss: 0.000106\n",
      "Epoch: 2 \tTraining Loss: 0.000106\n",
      "Epoch: 3 \tTraining Loss: 0.000106\n",
      "Epoch: 4 \tTraining Loss: 0.000106\n",
      "Epoch: 5 \tTraining Loss: 0.000106\n",
      "Epoch: 6 \tTraining Loss: 0.000106\n",
      "Epoch: 7 \tTraining Loss: 0.000106\n",
      "Epoch: 8 \tTraining Loss: 0.000106\n",
      "Epoch: 9 \tTraining Loss: 0.000106\n",
      "Epoch: 10 \tTraining Loss: 0.000106\n",
      "Epoch: 11 \tTraining Loss: 0.000106\n",
      "Epoch: 12 \tTraining Loss: 0.000106\n",
      "Epoch: 13 \tTraining Loss: 0.000106\n",
      "Epoch: 14 \tTraining Loss: 0.000106\n",
      "Epoch: 15 \tTraining Loss: 0.000106\n",
      "Epoch: 16 \tTraining Loss: 0.000106\n",
      "Epoch: 17 \tTraining Loss: 0.000106\n",
      "Epoch: 18 \tTraining Loss: 0.000106\n",
      "Epoch: 1 \tTraining Loss: 0.000169\n",
      "Epoch: 2 \tTraining Loss: 0.000169\n",
      "Epoch: 3 \tTraining Loss: 0.000169\n",
      "Epoch: 4 \tTraining Loss: 0.000169\n",
      "Epoch: 5 \tTraining Loss: 0.000169\n",
      "Epoch: 6 \tTraining Loss: 0.000169\n",
      "Epoch: 7 \tTraining Loss: 0.000169\n",
      "Epoch: 8 \tTraining Loss: 0.000169\n",
      "Epoch: 9 \tTraining Loss: 0.000169\n",
      "Epoch: 10 \tTraining Loss: 0.000169\n",
      "Epoch: 11 \tTraining Loss: 0.000169\n",
      "Epoch: 12 \tTraining Loss: 0.000169\n",
      "Epoch: 13 \tTraining Loss: 0.000169\n",
      "Epoch: 14 \tTraining Loss: 0.000169\n",
      "Epoch: 15 \tTraining Loss: 0.000169\n",
      "Epoch: 16 \tTraining Loss: 0.000169\n",
      "Epoch: 17 \tTraining Loss: 0.000169\n",
      "Epoch: 18 \tTraining Loss: 0.000169\n",
      "Epoch: 1 \tTraining Loss: 0.000146\n",
      "Epoch: 2 \tTraining Loss: 0.000146\n",
      "Epoch: 3 \tTraining Loss: 0.000146\n",
      "Epoch: 4 \tTraining Loss: 0.000146\n",
      "Epoch: 5 \tTraining Loss: 0.000146\n",
      "Epoch: 6 \tTraining Loss: 0.000146\n",
      "Epoch: 7 \tTraining Loss: 0.000146\n",
      "Epoch: 8 \tTraining Loss: 0.000146\n",
      "Epoch: 9 \tTraining Loss: 0.000146\n",
      "Epoch: 10 \tTraining Loss: 0.000146\n",
      "Epoch: 11 \tTraining Loss: 0.000146\n",
      "Epoch: 12 \tTraining Loss: 0.000146\n",
      "Epoch: 13 \tTraining Loss: 0.000146\n",
      "Epoch: 14 \tTraining Loss: 0.000146\n",
      "Epoch: 15 \tTraining Loss: 0.000146\n",
      "Epoch: 16 \tTraining Loss: 0.000146\n",
      "Epoch: 17 \tTraining Loss: 0.000146\n",
      "Epoch: 18 \tTraining Loss: 0.000146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000153\n",
      "Epoch: 2 \tTraining Loss: 0.000153\n",
      "Epoch: 3 \tTraining Loss: 0.000153\n",
      "Epoch: 4 \tTraining Loss: 0.000153\n",
      "Epoch: 5 \tTraining Loss: 0.000153\n",
      "Epoch: 6 \tTraining Loss: 0.000153\n",
      "Epoch: 7 \tTraining Loss: 0.000153\n",
      "Epoch: 8 \tTraining Loss: 0.000153\n",
      "Epoch: 9 \tTraining Loss: 0.000153\n",
      "Epoch: 10 \tTraining Loss: 0.000153\n",
      "Epoch: 11 \tTraining Loss: 0.000153\n",
      "Epoch: 12 \tTraining Loss: 0.000153\n",
      "Epoch: 13 \tTraining Loss: 0.000153\n",
      "Epoch: 14 \tTraining Loss: 0.000153\n",
      "Epoch: 15 \tTraining Loss: 0.000153\n",
      "Epoch: 16 \tTraining Loss: 0.000153\n",
      "Epoch: 17 \tTraining Loss: 0.000153\n",
      "Epoch: 18 \tTraining Loss: 0.000153\n",
      "Epoch: 1 \tTraining Loss: 0.000217\n",
      "Epoch: 2 \tTraining Loss: 0.000217\n",
      "Epoch: 3 \tTraining Loss: 0.000217\n",
      "Epoch: 4 \tTraining Loss: 0.000217\n",
      "Epoch: 5 \tTraining Loss: 0.000217\n",
      "Epoch: 6 \tTraining Loss: 0.000217\n",
      "Epoch: 7 \tTraining Loss: 0.000217\n",
      "Epoch: 8 \tTraining Loss: 0.000217\n",
      "Epoch: 9 \tTraining Loss: 0.000217\n",
      "Epoch: 10 \tTraining Loss: 0.000217\n",
      "Epoch: 11 \tTraining Loss: 0.000217\n",
      "Epoch: 12 \tTraining Loss: 0.000217\n",
      "Epoch: 13 \tTraining Loss: 0.000217\n",
      "Epoch: 14 \tTraining Loss: 0.000217\n",
      "Epoch: 15 \tTraining Loss: 0.000217\n",
      "Epoch: 16 \tTraining Loss: 0.000217\n",
      "Epoch: 17 \tTraining Loss: 0.000217\n",
      "Epoch: 18 \tTraining Loss: 0.000217\n",
      "Epoch: 1 \tTraining Loss: 0.000136\n",
      "Epoch: 2 \tTraining Loss: 0.000136\n",
      "Epoch: 3 \tTraining Loss: 0.000136\n",
      "Epoch: 4 \tTraining Loss: 0.000136\n",
      "Epoch: 5 \tTraining Loss: 0.000136\n",
      "Epoch: 6 \tTraining Loss: 0.000136\n",
      "Epoch: 7 \tTraining Loss: 0.000136\n",
      "Epoch: 8 \tTraining Loss: 0.000136\n",
      "Epoch: 9 \tTraining Loss: 0.000136\n",
      "Epoch: 10 \tTraining Loss: 0.000136\n",
      "Epoch: 11 \tTraining Loss: 0.000136\n",
      "Epoch: 12 \tTraining Loss: 0.000136\n",
      "Epoch: 13 \tTraining Loss: 0.000136\n",
      "Epoch: 14 \tTraining Loss: 0.000136\n",
      "Epoch: 15 \tTraining Loss: 0.000136\n",
      "Epoch: 16 \tTraining Loss: 0.000136\n",
      "Epoch: 17 \tTraining Loss: 0.000136\n",
      "Epoch: 18 \tTraining Loss: 0.000136\n",
      "Epoch: 1 \tTraining Loss: 0.000156\n",
      "Epoch: 2 \tTraining Loss: 0.000156\n",
      "Epoch: 3 \tTraining Loss: 0.000156\n",
      "Epoch: 4 \tTraining Loss: 0.000156\n",
      "Epoch: 5 \tTraining Loss: 0.000156\n",
      "Epoch: 6 \tTraining Loss: 0.000156\n",
      "Epoch: 7 \tTraining Loss: 0.000156\n",
      "Epoch: 8 \tTraining Loss: 0.000156\n",
      "Epoch: 9 \tTraining Loss: 0.000156\n",
      "Epoch: 10 \tTraining Loss: 0.000156\n",
      "Epoch: 11 \tTraining Loss: 0.000156\n",
      "Epoch: 12 \tTraining Loss: 0.000156\n",
      "Epoch: 13 \tTraining Loss: 0.000156\n",
      "Epoch: 14 \tTraining Loss: 0.000156\n",
      "Epoch: 15 \tTraining Loss: 0.000156\n",
      "Epoch: 16 \tTraining Loss: 0.000156\n",
      "Epoch: 17 \tTraining Loss: 0.000156\n",
      "Epoch: 18 \tTraining Loss: 0.000156\n",
      "Epoch: 1 \tTraining Loss: 0.000353\n",
      "Epoch: 2 \tTraining Loss: 0.000353\n",
      "Epoch: 3 \tTraining Loss: 0.000353\n",
      "Epoch: 4 \tTraining Loss: 0.000353\n",
      "Epoch: 5 \tTraining Loss: 0.000353\n",
      "Epoch: 6 \tTraining Loss: 0.000353\n",
      "Epoch: 7 \tTraining Loss: 0.000353\n",
      "Epoch: 8 \tTraining Loss: 0.000353\n",
      "Epoch: 9 \tTraining Loss: 0.000353\n",
      "Epoch: 10 \tTraining Loss: 0.000353\n",
      "Epoch: 11 \tTraining Loss: 0.000353\n",
      "Epoch: 12 \tTraining Loss: 0.000353\n",
      "Epoch: 13 \tTraining Loss: 0.000353\n",
      "Epoch: 14 \tTraining Loss: 0.000353\n",
      "Epoch: 15 \tTraining Loss: 0.000353\n",
      "Epoch: 16 \tTraining Loss: 0.000353\n",
      "Epoch: 17 \tTraining Loss: 0.000353\n",
      "Epoch: 18 \tTraining Loss: 0.000353\n",
      "Epoch: 1 \tTraining Loss: 0.000199\n",
      "Epoch: 2 \tTraining Loss: 0.000199\n",
      "Epoch: 3 \tTraining Loss: 0.000199\n",
      "Epoch: 4 \tTraining Loss: 0.000199\n",
      "Epoch: 5 \tTraining Loss: 0.000199\n",
      "Epoch: 6 \tTraining Loss: 0.000199\n",
      "Epoch: 7 \tTraining Loss: 0.000199\n",
      "Epoch: 8 \tTraining Loss: 0.000199\n",
      "Epoch: 9 \tTraining Loss: 0.000199\n",
      "Epoch: 10 \tTraining Loss: 0.000199\n",
      "Epoch: 11 \tTraining Loss: 0.000199\n",
      "Epoch: 12 \tTraining Loss: 0.000199\n",
      "Epoch: 13 \tTraining Loss: 0.000199\n",
      "Epoch: 14 \tTraining Loss: 0.000199\n",
      "Epoch: 15 \tTraining Loss: 0.000199\n",
      "Epoch: 16 \tTraining Loss: 0.000199\n",
      "Epoch: 17 \tTraining Loss: 0.000199\n",
      "Epoch: 18 \tTraining Loss: 0.000199\n",
      "Epoch: 1 \tTraining Loss: 0.000080\n",
      "Epoch: 2 \tTraining Loss: 0.000080\n",
      "Epoch: 3 \tTraining Loss: 0.000080\n",
      "Epoch: 4 \tTraining Loss: 0.000080\n",
      "Epoch: 5 \tTraining Loss: 0.000080\n",
      "Epoch: 6 \tTraining Loss: 0.000080\n",
      "Epoch: 7 \tTraining Loss: 0.000080\n",
      "Epoch: 8 \tTraining Loss: 0.000080\n",
      "Epoch: 9 \tTraining Loss: 0.000080\n",
      "Epoch: 10 \tTraining Loss: 0.000080\n",
      "Epoch: 11 \tTraining Loss: 0.000080\n",
      "Epoch: 12 \tTraining Loss: 0.000080\n",
      "Epoch: 13 \tTraining Loss: 0.000080\n",
      "Epoch: 14 \tTraining Loss: 0.000080\n",
      "Epoch: 15 \tTraining Loss: 0.000080\n",
      "Epoch: 16 \tTraining Loss: 0.000080\n",
      "Epoch: 17 \tTraining Loss: 0.000080\n",
      "Epoch: 18 \tTraining Loss: 0.000080\n",
      "Epoch: 1 \tTraining Loss: 0.000064\n",
      "Epoch: 2 \tTraining Loss: 0.000064\n",
      "Epoch: 3 \tTraining Loss: 0.000064\n",
      "Epoch: 4 \tTraining Loss: 0.000064\n",
      "Epoch: 5 \tTraining Loss: 0.000064\n",
      "Epoch: 6 \tTraining Loss: 0.000064\n",
      "Epoch: 7 \tTraining Loss: 0.000064\n",
      "Epoch: 8 \tTraining Loss: 0.000064\n",
      "Epoch: 9 \tTraining Loss: 0.000064\n",
      "Epoch: 10 \tTraining Loss: 0.000064\n",
      "Epoch: 11 \tTraining Loss: 0.000064\n",
      "Epoch: 12 \tTraining Loss: 0.000064\n",
      "Epoch: 13 \tTraining Loss: 0.000064\n",
      "Epoch: 14 \tTraining Loss: 0.000064\n",
      "Epoch: 15 \tTraining Loss: 0.000064\n",
      "Epoch: 16 \tTraining Loss: 0.000064\n",
      "Epoch: 17 \tTraining Loss: 0.000064\n",
      "Epoch: 18 \tTraining Loss: 0.000064\n",
      "Epoch: 1 \tTraining Loss: 0.000407\n",
      "Epoch: 2 \tTraining Loss: 0.000407\n",
      "Epoch: 3 \tTraining Loss: 0.000407\n",
      "Epoch: 4 \tTraining Loss: 0.000407\n",
      "Epoch: 5 \tTraining Loss: 0.000407\n",
      "Epoch: 6 \tTraining Loss: 0.000407\n",
      "Epoch: 7 \tTraining Loss: 0.000407\n",
      "Epoch: 8 \tTraining Loss: 0.000407\n",
      "Epoch: 9 \tTraining Loss: 0.000407\n",
      "Epoch: 10 \tTraining Loss: 0.000407\n",
      "Epoch: 11 \tTraining Loss: 0.000407\n",
      "Epoch: 12 \tTraining Loss: 0.000407\n",
      "Epoch: 13 \tTraining Loss: 0.000407\n",
      "Epoch: 14 \tTraining Loss: 0.000407\n",
      "Epoch: 15 \tTraining Loss: 0.000407\n",
      "Epoch: 16 \tTraining Loss: 0.000407\n",
      "Epoch: 17 \tTraining Loss: 0.000407\n",
      "Epoch: 18 \tTraining Loss: 0.000407\n",
      "Epoch: 1 \tTraining Loss: 0.000152\n",
      "Epoch: 2 \tTraining Loss: 0.000152\n",
      "Epoch: 3 \tTraining Loss: 0.000152\n",
      "Epoch: 4 \tTraining Loss: 0.000152\n",
      "Epoch: 5 \tTraining Loss: 0.000152\n",
      "Epoch: 6 \tTraining Loss: 0.000152\n",
      "Epoch: 7 \tTraining Loss: 0.000152\n",
      "Epoch: 8 \tTraining Loss: 0.000152\n",
      "Epoch: 9 \tTraining Loss: 0.000152\n",
      "Epoch: 10 \tTraining Loss: 0.000152\n",
      "Epoch: 11 \tTraining Loss: 0.000152\n",
      "Epoch: 12 \tTraining Loss: 0.000152\n",
      "Epoch: 13 \tTraining Loss: 0.000152\n",
      "Epoch: 14 \tTraining Loss: 0.000152\n",
      "Epoch: 15 \tTraining Loss: 0.000152\n",
      "Epoch: 16 \tTraining Loss: 0.000152\n",
      "Epoch: 17 \tTraining Loss: 0.000152\n",
      "Epoch: 18 \tTraining Loss: 0.000152\n",
      "Epoch: 1 \tTraining Loss: 0.000192\n",
      "Epoch: 2 \tTraining Loss: 0.000192\n",
      "Epoch: 3 \tTraining Loss: 0.000192\n",
      "Epoch: 4 \tTraining Loss: 0.000192\n",
      "Epoch: 5 \tTraining Loss: 0.000192\n",
      "Epoch: 6 \tTraining Loss: 0.000192\n",
      "Epoch: 7 \tTraining Loss: 0.000192\n",
      "Epoch: 8 \tTraining Loss: 0.000192\n",
      "Epoch: 9 \tTraining Loss: 0.000192\n",
      "Epoch: 10 \tTraining Loss: 0.000192\n",
      "Epoch: 11 \tTraining Loss: 0.000192\n",
      "Epoch: 12 \tTraining Loss: 0.000192\n",
      "Epoch: 13 \tTraining Loss: 0.000192\n",
      "Epoch: 14 \tTraining Loss: 0.000192\n",
      "Epoch: 15 \tTraining Loss: 0.000192\n",
      "Epoch: 16 \tTraining Loss: 0.000192\n",
      "Epoch: 17 \tTraining Loss: 0.000192\n",
      "Epoch: 18 \tTraining Loss: 0.000192\n",
      "Epoch: 1 \tTraining Loss: 0.000175\n",
      "Epoch: 2 \tTraining Loss: 0.000175\n",
      "Epoch: 3 \tTraining Loss: 0.000175\n",
      "Epoch: 4 \tTraining Loss: 0.000175\n",
      "Epoch: 5 \tTraining Loss: 0.000175\n",
      "Epoch: 6 \tTraining Loss: 0.000175\n",
      "Epoch: 7 \tTraining Loss: 0.000175\n",
      "Epoch: 8 \tTraining Loss: 0.000175\n",
      "Epoch: 9 \tTraining Loss: 0.000175\n",
      "Epoch: 10 \tTraining Loss: 0.000175\n",
      "Epoch: 11 \tTraining Loss: 0.000175\n",
      "Epoch: 12 \tTraining Loss: 0.000175\n",
      "Epoch: 13 \tTraining Loss: 0.000175\n",
      "Epoch: 14 \tTraining Loss: 0.000175\n",
      "Epoch: 15 \tTraining Loss: 0.000175\n",
      "Epoch: 16 \tTraining Loss: 0.000175\n",
      "Epoch: 17 \tTraining Loss: 0.000175\n",
      "Epoch: 18 \tTraining Loss: 0.000175\n",
      "Epoch: 1 \tTraining Loss: 0.000230\n",
      "Epoch: 2 \tTraining Loss: 0.000230\n",
      "Epoch: 3 \tTraining Loss: 0.000230\n",
      "Epoch: 4 \tTraining Loss: 0.000230\n",
      "Epoch: 5 \tTraining Loss: 0.000230\n",
      "Epoch: 6 \tTraining Loss: 0.000230\n",
      "Epoch: 7 \tTraining Loss: 0.000230\n",
      "Epoch: 8 \tTraining Loss: 0.000230\n",
      "Epoch: 9 \tTraining Loss: 0.000230\n",
      "Epoch: 10 \tTraining Loss: 0.000230\n",
      "Epoch: 11 \tTraining Loss: 0.000230\n",
      "Epoch: 12 \tTraining Loss: 0.000230\n",
      "Epoch: 13 \tTraining Loss: 0.000230\n",
      "Epoch: 14 \tTraining Loss: 0.000230\n",
      "Epoch: 15 \tTraining Loss: 0.000230\n",
      "Epoch: 16 \tTraining Loss: 0.000230\n",
      "Epoch: 17 \tTraining Loss: 0.000230\n",
      "Epoch: 18 \tTraining Loss: 0.000230\n",
      "Epoch: 1 \tTraining Loss: 0.000239\n",
      "Epoch: 2 \tTraining Loss: 0.000239\n",
      "Epoch: 3 \tTraining Loss: 0.000239\n",
      "Epoch: 4 \tTraining Loss: 0.000239\n",
      "Epoch: 5 \tTraining Loss: 0.000239\n",
      "Epoch: 6 \tTraining Loss: 0.000239\n",
      "Epoch: 7 \tTraining Loss: 0.000239\n",
      "Epoch: 8 \tTraining Loss: 0.000239\n",
      "Epoch: 9 \tTraining Loss: 0.000239\n",
      "Epoch: 10 \tTraining Loss: 0.000239\n",
      "Epoch: 11 \tTraining Loss: 0.000239\n",
      "Epoch: 12 \tTraining Loss: 0.000239\n",
      "Epoch: 13 \tTraining Loss: 0.000239\n",
      "Epoch: 14 \tTraining Loss: 0.000239\n",
      "Epoch: 15 \tTraining Loss: 0.000239\n",
      "Epoch: 16 \tTraining Loss: 0.000239\n",
      "Epoch: 17 \tTraining Loss: 0.000239\n",
      "Epoch: 18 \tTraining Loss: 0.000239\n",
      "Epoch: 1 \tTraining Loss: 0.000196\n",
      "Epoch: 2 \tTraining Loss: 0.000196\n",
      "Epoch: 3 \tTraining Loss: 0.000196\n",
      "Epoch: 4 \tTraining Loss: 0.000196\n",
      "Epoch: 5 \tTraining Loss: 0.000196\n",
      "Epoch: 6 \tTraining Loss: 0.000196\n",
      "Epoch: 7 \tTraining Loss: 0.000196\n",
      "Epoch: 8 \tTraining Loss: 0.000196\n",
      "Epoch: 9 \tTraining Loss: 0.000196\n",
      "Epoch: 10 \tTraining Loss: 0.000196\n",
      "Epoch: 11 \tTraining Loss: 0.000196\n",
      "Epoch: 12 \tTraining Loss: 0.000196\n",
      "Epoch: 13 \tTraining Loss: 0.000196\n",
      "Epoch: 14 \tTraining Loss: 0.000196\n",
      "Epoch: 15 \tTraining Loss: 0.000196\n",
      "Epoch: 16 \tTraining Loss: 0.000196\n",
      "Epoch: 17 \tTraining Loss: 0.000196\n",
      "Epoch: 18 \tTraining Loss: 0.000196\n",
      "Epoch: 1 \tTraining Loss: 0.000197\n",
      "Epoch: 2 \tTraining Loss: 0.000197\n",
      "Epoch: 3 \tTraining Loss: 0.000197\n",
      "Epoch: 4 \tTraining Loss: 0.000197\n",
      "Epoch: 5 \tTraining Loss: 0.000197\n",
      "Epoch: 6 \tTraining Loss: 0.000197\n",
      "Epoch: 7 \tTraining Loss: 0.000197\n",
      "Epoch: 8 \tTraining Loss: 0.000197\n",
      "Epoch: 9 \tTraining Loss: 0.000197\n",
      "Epoch: 10 \tTraining Loss: 0.000197\n",
      "Epoch: 11 \tTraining Loss: 0.000197\n",
      "Epoch: 12 \tTraining Loss: 0.000197\n",
      "Epoch: 13 \tTraining Loss: 0.000197\n",
      "Epoch: 14 \tTraining Loss: 0.000197\n",
      "Epoch: 15 \tTraining Loss: 0.000197\n",
      "Epoch: 16 \tTraining Loss: 0.000197\n",
      "Epoch: 17 \tTraining Loss: 0.000197\n",
      "Epoch: 18 \tTraining Loss: 0.000197\n",
      "Epoch: 1 \tTraining Loss: 0.000152\n",
      "Epoch: 2 \tTraining Loss: 0.000152\n",
      "Epoch: 3 \tTraining Loss: 0.000152\n",
      "Epoch: 4 \tTraining Loss: 0.000152\n",
      "Epoch: 5 \tTraining Loss: 0.000152\n",
      "Epoch: 6 \tTraining Loss: 0.000152\n",
      "Epoch: 7 \tTraining Loss: 0.000152\n",
      "Epoch: 8 \tTraining Loss: 0.000152\n",
      "Epoch: 9 \tTraining Loss: 0.000152\n",
      "Epoch: 10 \tTraining Loss: 0.000152\n",
      "Epoch: 11 \tTraining Loss: 0.000152\n",
      "Epoch: 12 \tTraining Loss: 0.000152\n",
      "Epoch: 13 \tTraining Loss: 0.000152\n",
      "Epoch: 14 \tTraining Loss: 0.000152\n",
      "Epoch: 15 \tTraining Loss: 0.000152\n",
      "Epoch: 16 \tTraining Loss: 0.000152\n",
      "Epoch: 17 \tTraining Loss: 0.000152\n",
      "Epoch: 18 \tTraining Loss: 0.000152\n",
      "Epoch: 1 \tTraining Loss: 0.000123\n",
      "Epoch: 2 \tTraining Loss: 0.000123\n",
      "Epoch: 3 \tTraining Loss: 0.000123\n",
      "Epoch: 4 \tTraining Loss: 0.000123\n",
      "Epoch: 5 \tTraining Loss: 0.000123\n",
      "Epoch: 6 \tTraining Loss: 0.000123\n",
      "Epoch: 7 \tTraining Loss: 0.000123\n",
      "Epoch: 8 \tTraining Loss: 0.000123\n",
      "Epoch: 9 \tTraining Loss: 0.000123\n",
      "Epoch: 10 \tTraining Loss: 0.000123\n",
      "Epoch: 11 \tTraining Loss: 0.000123\n",
      "Epoch: 12 \tTraining Loss: 0.000123\n",
      "Epoch: 13 \tTraining Loss: 0.000123\n",
      "Epoch: 14 \tTraining Loss: 0.000123\n",
      "Epoch: 15 \tTraining Loss: 0.000123\n",
      "Epoch: 16 \tTraining Loss: 0.000123\n",
      "Epoch: 17 \tTraining Loss: 0.000123\n",
      "Epoch: 18 \tTraining Loss: 0.000123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000217\n",
      "Epoch: 2 \tTraining Loss: 0.000217\n",
      "Epoch: 3 \tTraining Loss: 0.000217\n",
      "Epoch: 4 \tTraining Loss: 0.000217\n",
      "Epoch: 5 \tTraining Loss: 0.000217\n",
      "Epoch: 6 \tTraining Loss: 0.000217\n",
      "Epoch: 7 \tTraining Loss: 0.000217\n",
      "Epoch: 8 \tTraining Loss: 0.000217\n",
      "Epoch: 9 \tTraining Loss: 0.000217\n",
      "Epoch: 10 \tTraining Loss: 0.000217\n",
      "Epoch: 11 \tTraining Loss: 0.000217\n",
      "Epoch: 12 \tTraining Loss: 0.000217\n",
      "Epoch: 13 \tTraining Loss: 0.000217\n",
      "Epoch: 14 \tTraining Loss: 0.000217\n",
      "Epoch: 15 \tTraining Loss: 0.000217\n",
      "Epoch: 16 \tTraining Loss: 0.000217\n",
      "Epoch: 17 \tTraining Loss: 0.000217\n",
      "Epoch: 18 \tTraining Loss: 0.000217\n",
      "Epoch: 1 \tTraining Loss: 0.000244\n",
      "Epoch: 2 \tTraining Loss: 0.000244\n",
      "Epoch: 3 \tTraining Loss: 0.000244\n",
      "Epoch: 4 \tTraining Loss: 0.000244\n",
      "Epoch: 5 \tTraining Loss: 0.000244\n",
      "Epoch: 6 \tTraining Loss: 0.000244\n",
      "Epoch: 7 \tTraining Loss: 0.000244\n",
      "Epoch: 8 \tTraining Loss: 0.000244\n",
      "Epoch: 9 \tTraining Loss: 0.000244\n",
      "Epoch: 10 \tTraining Loss: 0.000244\n",
      "Epoch: 11 \tTraining Loss: 0.000244\n",
      "Epoch: 12 \tTraining Loss: 0.000244\n",
      "Epoch: 13 \tTraining Loss: 0.000244\n",
      "Epoch: 14 \tTraining Loss: 0.000244\n",
      "Epoch: 15 \tTraining Loss: 0.000244\n",
      "Epoch: 16 \tTraining Loss: 0.000244\n",
      "Epoch: 17 \tTraining Loss: 0.000244\n",
      "Epoch: 18 \tTraining Loss: 0.000244\n",
      "Epoch: 1 \tTraining Loss: 0.000139\n",
      "Epoch: 2 \tTraining Loss: 0.000139\n",
      "Epoch: 3 \tTraining Loss: 0.000139\n",
      "Epoch: 4 \tTraining Loss: 0.000139\n",
      "Epoch: 5 \tTraining Loss: 0.000139\n",
      "Epoch: 6 \tTraining Loss: 0.000139\n",
      "Epoch: 7 \tTraining Loss: 0.000139\n",
      "Epoch: 8 \tTraining Loss: 0.000139\n",
      "Epoch: 9 \tTraining Loss: 0.000139\n",
      "Epoch: 10 \tTraining Loss: 0.000139\n",
      "Epoch: 11 \tTraining Loss: 0.000139\n",
      "Epoch: 12 \tTraining Loss: 0.000139\n",
      "Epoch: 13 \tTraining Loss: 0.000139\n",
      "Epoch: 14 \tTraining Loss: 0.000139\n",
      "Epoch: 15 \tTraining Loss: 0.000139\n",
      "Epoch: 16 \tTraining Loss: 0.000139\n",
      "Epoch: 17 \tTraining Loss: 0.000139\n",
      "Epoch: 18 \tTraining Loss: 0.000139\n",
      "Epoch: 1 \tTraining Loss: 0.000231\n",
      "Epoch: 2 \tTraining Loss: 0.000231\n",
      "Epoch: 3 \tTraining Loss: 0.000231\n",
      "Epoch: 4 \tTraining Loss: 0.000231\n",
      "Epoch: 5 \tTraining Loss: 0.000231\n",
      "Epoch: 6 \tTraining Loss: 0.000231\n",
      "Epoch: 7 \tTraining Loss: 0.000231\n",
      "Epoch: 8 \tTraining Loss: 0.000231\n",
      "Epoch: 9 \tTraining Loss: 0.000231\n",
      "Epoch: 10 \tTraining Loss: 0.000231\n",
      "Epoch: 11 \tTraining Loss: 0.000231\n",
      "Epoch: 12 \tTraining Loss: 0.000231\n",
      "Epoch: 13 \tTraining Loss: 0.000231\n",
      "Epoch: 14 \tTraining Loss: 0.000231\n",
      "Epoch: 15 \tTraining Loss: 0.000231\n",
      "Epoch: 16 \tTraining Loss: 0.000231\n",
      "Epoch: 17 \tTraining Loss: 0.000231\n",
      "Epoch: 18 \tTraining Loss: 0.000231\n",
      "Epoch: 1 \tTraining Loss: 0.000133\n",
      "Epoch: 2 \tTraining Loss: 0.000133\n",
      "Epoch: 3 \tTraining Loss: 0.000133\n",
      "Epoch: 4 \tTraining Loss: 0.000133\n",
      "Epoch: 5 \tTraining Loss: 0.000133\n",
      "Epoch: 6 \tTraining Loss: 0.000133\n",
      "Epoch: 7 \tTraining Loss: 0.000133\n",
      "Epoch: 8 \tTraining Loss: 0.000133\n",
      "Epoch: 9 \tTraining Loss: 0.000133\n",
      "Epoch: 10 \tTraining Loss: 0.000133\n",
      "Epoch: 11 \tTraining Loss: 0.000133\n",
      "Epoch: 12 \tTraining Loss: 0.000133\n",
      "Epoch: 13 \tTraining Loss: 0.000133\n",
      "Epoch: 14 \tTraining Loss: 0.000133\n",
      "Epoch: 15 \tTraining Loss: 0.000133\n",
      "Epoch: 16 \tTraining Loss: 0.000133\n",
      "Epoch: 17 \tTraining Loss: 0.000133\n",
      "Epoch: 18 \tTraining Loss: 0.000133\n",
      "Epoch: 1 \tTraining Loss: 0.000193\n",
      "Epoch: 2 \tTraining Loss: 0.000193\n",
      "Epoch: 3 \tTraining Loss: 0.000193\n",
      "Epoch: 4 \tTraining Loss: 0.000193\n",
      "Epoch: 5 \tTraining Loss: 0.000193\n",
      "Epoch: 6 \tTraining Loss: 0.000193\n",
      "Epoch: 7 \tTraining Loss: 0.000193\n",
      "Epoch: 8 \tTraining Loss: 0.000193\n",
      "Epoch: 9 \tTraining Loss: 0.000193\n",
      "Epoch: 10 \tTraining Loss: 0.000193\n",
      "Epoch: 11 \tTraining Loss: 0.000193\n",
      "Epoch: 12 \tTraining Loss: 0.000193\n",
      "Epoch: 13 \tTraining Loss: 0.000193\n",
      "Epoch: 14 \tTraining Loss: 0.000193\n",
      "Epoch: 15 \tTraining Loss: 0.000193\n",
      "Epoch: 16 \tTraining Loss: 0.000193\n",
      "Epoch: 17 \tTraining Loss: 0.000193\n",
      "Epoch: 18 \tTraining Loss: 0.000193\n",
      "Epoch: 1 \tTraining Loss: 0.000212\n",
      "Epoch: 2 \tTraining Loss: 0.000212\n",
      "Epoch: 3 \tTraining Loss: 0.000212\n",
      "Epoch: 4 \tTraining Loss: 0.000212\n",
      "Epoch: 5 \tTraining Loss: 0.000212\n",
      "Epoch: 6 \tTraining Loss: 0.000212\n",
      "Epoch: 7 \tTraining Loss: 0.000212\n",
      "Epoch: 8 \tTraining Loss: 0.000212\n",
      "Epoch: 9 \tTraining Loss: 0.000212\n",
      "Epoch: 10 \tTraining Loss: 0.000212\n",
      "Epoch: 11 \tTraining Loss: 0.000212\n",
      "Epoch: 12 \tTraining Loss: 0.000212\n",
      "Epoch: 13 \tTraining Loss: 0.000212\n",
      "Epoch: 14 \tTraining Loss: 0.000212\n",
      "Epoch: 15 \tTraining Loss: 0.000212\n",
      "Epoch: 16 \tTraining Loss: 0.000212\n",
      "Epoch: 17 \tTraining Loss: 0.000212\n",
      "Epoch: 18 \tTraining Loss: 0.000212\n",
      "Epoch: 1 \tTraining Loss: 0.000134\n",
      "Epoch: 2 \tTraining Loss: 0.000134\n",
      "Epoch: 3 \tTraining Loss: 0.000134\n",
      "Epoch: 4 \tTraining Loss: 0.000134\n",
      "Epoch: 5 \tTraining Loss: 0.000134\n",
      "Epoch: 6 \tTraining Loss: 0.000134\n",
      "Epoch: 7 \tTraining Loss: 0.000134\n",
      "Epoch: 8 \tTraining Loss: 0.000134\n",
      "Epoch: 9 \tTraining Loss: 0.000134\n",
      "Epoch: 10 \tTraining Loss: 0.000134\n",
      "Epoch: 11 \tTraining Loss: 0.000134\n",
      "Epoch: 12 \tTraining Loss: 0.000134\n",
      "Epoch: 13 \tTraining Loss: 0.000134\n",
      "Epoch: 14 \tTraining Loss: 0.000134\n",
      "Epoch: 15 \tTraining Loss: 0.000134\n",
      "Epoch: 16 \tTraining Loss: 0.000134\n",
      "Epoch: 17 \tTraining Loss: 0.000134\n",
      "Epoch: 18 \tTraining Loss: 0.000134\n",
      "Epoch: 1 \tTraining Loss: 0.000259\n",
      "Epoch: 2 \tTraining Loss: 0.000259\n",
      "Epoch: 3 \tTraining Loss: 0.000259\n",
      "Epoch: 4 \tTraining Loss: 0.000259\n",
      "Epoch: 5 \tTraining Loss: 0.000259\n",
      "Epoch: 6 \tTraining Loss: 0.000259\n",
      "Epoch: 7 \tTraining Loss: 0.000259\n",
      "Epoch: 8 \tTraining Loss: 0.000259\n",
      "Epoch: 9 \tTraining Loss: 0.000259\n",
      "Epoch: 10 \tTraining Loss: 0.000259\n",
      "Epoch: 11 \tTraining Loss: 0.000259\n",
      "Epoch: 12 \tTraining Loss: 0.000259\n",
      "Epoch: 13 \tTraining Loss: 0.000259\n",
      "Epoch: 14 \tTraining Loss: 0.000259\n",
      "Epoch: 15 \tTraining Loss: 0.000259\n",
      "Epoch: 16 \tTraining Loss: 0.000259\n",
      "Epoch: 17 \tTraining Loss: 0.000259\n",
      "Epoch: 18 \tTraining Loss: 0.000259\n",
      "Epoch: 1 \tTraining Loss: 0.000169\n",
      "Epoch: 2 \tTraining Loss: 0.000169\n",
      "Epoch: 3 \tTraining Loss: 0.000169\n",
      "Epoch: 4 \tTraining Loss: 0.000169\n",
      "Epoch: 5 \tTraining Loss: 0.000169\n",
      "Epoch: 6 \tTraining Loss: 0.000169\n",
      "Epoch: 7 \tTraining Loss: 0.000169\n",
      "Epoch: 8 \tTraining Loss: 0.000169\n",
      "Epoch: 9 \tTraining Loss: 0.000169\n",
      "Epoch: 10 \tTraining Loss: 0.000169\n",
      "Epoch: 11 \tTraining Loss: 0.000169\n",
      "Epoch: 12 \tTraining Loss: 0.000169\n",
      "Epoch: 13 \tTraining Loss: 0.000169\n",
      "Epoch: 14 \tTraining Loss: 0.000169\n",
      "Epoch: 15 \tTraining Loss: 0.000169\n",
      "Epoch: 16 \tTraining Loss: 0.000169\n",
      "Epoch: 17 \tTraining Loss: 0.000169\n",
      "Epoch: 18 \tTraining Loss: 0.000169\n",
      "Epoch: 1 \tTraining Loss: 0.000180\n",
      "Epoch: 2 \tTraining Loss: 0.000180\n",
      "Epoch: 3 \tTraining Loss: 0.000180\n",
      "Epoch: 4 \tTraining Loss: 0.000180\n",
      "Epoch: 5 \tTraining Loss: 0.000180\n",
      "Epoch: 6 \tTraining Loss: 0.000180\n",
      "Epoch: 7 \tTraining Loss: 0.000180\n",
      "Epoch: 8 \tTraining Loss: 0.000180\n",
      "Epoch: 9 \tTraining Loss: 0.000180\n",
      "Epoch: 10 \tTraining Loss: 0.000180\n",
      "Epoch: 11 \tTraining Loss: 0.000180\n",
      "Epoch: 12 \tTraining Loss: 0.000180\n",
      "Epoch: 13 \tTraining Loss: 0.000180\n",
      "Epoch: 14 \tTraining Loss: 0.000180\n",
      "Epoch: 15 \tTraining Loss: 0.000180\n",
      "Epoch: 16 \tTraining Loss: 0.000180\n",
      "Epoch: 17 \tTraining Loss: 0.000180\n",
      "Epoch: 18 \tTraining Loss: 0.000180\n",
      "Epoch: 1 \tTraining Loss: 0.000168\n",
      "Epoch: 2 \tTraining Loss: 0.000168\n",
      "Epoch: 3 \tTraining Loss: 0.000168\n",
      "Epoch: 4 \tTraining Loss: 0.000168\n",
      "Epoch: 5 \tTraining Loss: 0.000168\n",
      "Epoch: 6 \tTraining Loss: 0.000168\n",
      "Epoch: 7 \tTraining Loss: 0.000168\n",
      "Epoch: 8 \tTraining Loss: 0.000168\n",
      "Epoch: 9 \tTraining Loss: 0.000168\n",
      "Epoch: 10 \tTraining Loss: 0.000168\n",
      "Epoch: 11 \tTraining Loss: 0.000168\n",
      "Epoch: 12 \tTraining Loss: 0.000168\n",
      "Epoch: 13 \tTraining Loss: 0.000168\n",
      "Epoch: 14 \tTraining Loss: 0.000168\n",
      "Epoch: 15 \tTraining Loss: 0.000168\n",
      "Epoch: 16 \tTraining Loss: 0.000168\n",
      "Epoch: 17 \tTraining Loss: 0.000168\n",
      "Epoch: 18 \tTraining Loss: 0.000168\n",
      "Epoch: 1 \tTraining Loss: 0.000117\n",
      "Epoch: 2 \tTraining Loss: 0.000117\n",
      "Epoch: 3 \tTraining Loss: 0.000117\n",
      "Epoch: 4 \tTraining Loss: 0.000117\n",
      "Epoch: 5 \tTraining Loss: 0.000117\n",
      "Epoch: 6 \tTraining Loss: 0.000117\n",
      "Epoch: 7 \tTraining Loss: 0.000117\n",
      "Epoch: 8 \tTraining Loss: 0.000117\n",
      "Epoch: 9 \tTraining Loss: 0.000117\n",
      "Epoch: 10 \tTraining Loss: 0.000117\n",
      "Epoch: 11 \tTraining Loss: 0.000117\n",
      "Epoch: 12 \tTraining Loss: 0.000117\n",
      "Epoch: 13 \tTraining Loss: 0.000117\n",
      "Epoch: 14 \tTraining Loss: 0.000117\n",
      "Epoch: 15 \tTraining Loss: 0.000117\n",
      "Epoch: 16 \tTraining Loss: 0.000117\n",
      "Epoch: 17 \tTraining Loss: 0.000117\n",
      "Epoch: 18 \tTraining Loss: 0.000117\n",
      "Epoch: 1 \tTraining Loss: 0.000324\n",
      "Epoch: 2 \tTraining Loss: 0.000324\n",
      "Epoch: 3 \tTraining Loss: 0.000324\n",
      "Epoch: 4 \tTraining Loss: 0.000324\n",
      "Epoch: 5 \tTraining Loss: 0.000324\n",
      "Epoch: 6 \tTraining Loss: 0.000324\n",
      "Epoch: 7 \tTraining Loss: 0.000324\n",
      "Epoch: 8 \tTraining Loss: 0.000324\n",
      "Epoch: 9 \tTraining Loss: 0.000324\n",
      "Epoch: 10 \tTraining Loss: 0.000324\n",
      "Epoch: 11 \tTraining Loss: 0.000324\n",
      "Epoch: 12 \tTraining Loss: 0.000324\n",
      "Epoch: 13 \tTraining Loss: 0.000324\n",
      "Epoch: 14 \tTraining Loss: 0.000324\n",
      "Epoch: 15 \tTraining Loss: 0.000324\n",
      "Epoch: 16 \tTraining Loss: 0.000324\n",
      "Epoch: 17 \tTraining Loss: 0.000324\n",
      "Epoch: 18 \tTraining Loss: 0.000324\n",
      "Epoch: 1 \tTraining Loss: 0.000233\n",
      "Epoch: 2 \tTraining Loss: 0.000233\n",
      "Epoch: 3 \tTraining Loss: 0.000233\n",
      "Epoch: 4 \tTraining Loss: 0.000233\n",
      "Epoch: 5 \tTraining Loss: 0.000233\n",
      "Epoch: 6 \tTraining Loss: 0.000233\n",
      "Epoch: 7 \tTraining Loss: 0.000233\n",
      "Epoch: 8 \tTraining Loss: 0.000233\n",
      "Epoch: 9 \tTraining Loss: 0.000233\n",
      "Epoch: 10 \tTraining Loss: 0.000233\n",
      "Epoch: 11 \tTraining Loss: 0.000233\n",
      "Epoch: 12 \tTraining Loss: 0.000233\n",
      "Epoch: 13 \tTraining Loss: 0.000233\n",
      "Epoch: 14 \tTraining Loss: 0.000233\n",
      "Epoch: 15 \tTraining Loss: 0.000233\n",
      "Epoch: 16 \tTraining Loss: 0.000233\n",
      "Epoch: 17 \tTraining Loss: 0.000233\n",
      "Epoch: 18 \tTraining Loss: 0.000233\n",
      "Epoch: 1 \tTraining Loss: 0.000092\n",
      "Epoch: 2 \tTraining Loss: 0.000092\n",
      "Epoch: 3 \tTraining Loss: 0.000092\n",
      "Epoch: 4 \tTraining Loss: 0.000092\n",
      "Epoch: 5 \tTraining Loss: 0.000092\n",
      "Epoch: 6 \tTraining Loss: 0.000092\n",
      "Epoch: 7 \tTraining Loss: 0.000092\n",
      "Epoch: 8 \tTraining Loss: 0.000092\n",
      "Epoch: 9 \tTraining Loss: 0.000092\n",
      "Epoch: 10 \tTraining Loss: 0.000092\n",
      "Epoch: 11 \tTraining Loss: 0.000092\n",
      "Epoch: 12 \tTraining Loss: 0.000092\n",
      "Epoch: 13 \tTraining Loss: 0.000092\n",
      "Epoch: 14 \tTraining Loss: 0.000092\n",
      "Epoch: 15 \tTraining Loss: 0.000092\n",
      "Epoch: 16 \tTraining Loss: 0.000092\n",
      "Epoch: 17 \tTraining Loss: 0.000092\n",
      "Epoch: 18 \tTraining Loss: 0.000092\n",
      "Epoch: 1 \tTraining Loss: 0.000172\n",
      "Epoch: 2 \tTraining Loss: 0.000172\n",
      "Epoch: 3 \tTraining Loss: 0.000172\n",
      "Epoch: 4 \tTraining Loss: 0.000172\n",
      "Epoch: 5 \tTraining Loss: 0.000172\n",
      "Epoch: 6 \tTraining Loss: 0.000172\n",
      "Epoch: 7 \tTraining Loss: 0.000172\n",
      "Epoch: 8 \tTraining Loss: 0.000172\n",
      "Epoch: 9 \tTraining Loss: 0.000172\n",
      "Epoch: 10 \tTraining Loss: 0.000172\n",
      "Epoch: 11 \tTraining Loss: 0.000172\n",
      "Epoch: 12 \tTraining Loss: 0.000172\n",
      "Epoch: 13 \tTraining Loss: 0.000172\n",
      "Epoch: 14 \tTraining Loss: 0.000172\n",
      "Epoch: 15 \tTraining Loss: 0.000172\n",
      "Epoch: 16 \tTraining Loss: 0.000172\n",
      "Epoch: 17 \tTraining Loss: 0.000172\n",
      "Epoch: 18 \tTraining Loss: 0.000172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000089\n",
      "Epoch: 2 \tTraining Loss: 0.000089\n",
      "Epoch: 3 \tTraining Loss: 0.000089\n",
      "Epoch: 4 \tTraining Loss: 0.000089\n",
      "Epoch: 5 \tTraining Loss: 0.000089\n",
      "Epoch: 6 \tTraining Loss: 0.000089\n",
      "Epoch: 7 \tTraining Loss: 0.000089\n",
      "Epoch: 8 \tTraining Loss: 0.000089\n",
      "Epoch: 9 \tTraining Loss: 0.000089\n",
      "Epoch: 10 \tTraining Loss: 0.000089\n",
      "Epoch: 11 \tTraining Loss: 0.000089\n",
      "Epoch: 12 \tTraining Loss: 0.000089\n",
      "Epoch: 13 \tTraining Loss: 0.000089\n",
      "Epoch: 14 \tTraining Loss: 0.000089\n",
      "Epoch: 15 \tTraining Loss: 0.000089\n",
      "Epoch: 16 \tTraining Loss: 0.000089\n",
      "Epoch: 17 \tTraining Loss: 0.000089\n",
      "Epoch: 18 \tTraining Loss: 0.000089\n",
      "Epoch: 1 \tTraining Loss: 0.000078\n",
      "Epoch: 2 \tTraining Loss: 0.000078\n",
      "Epoch: 3 \tTraining Loss: 0.000078\n",
      "Epoch: 4 \tTraining Loss: 0.000078\n",
      "Epoch: 5 \tTraining Loss: 0.000078\n",
      "Epoch: 6 \tTraining Loss: 0.000078\n",
      "Epoch: 7 \tTraining Loss: 0.000078\n",
      "Epoch: 8 \tTraining Loss: 0.000078\n",
      "Epoch: 9 \tTraining Loss: 0.000078\n",
      "Epoch: 10 \tTraining Loss: 0.000078\n",
      "Epoch: 11 \tTraining Loss: 0.000078\n",
      "Epoch: 12 \tTraining Loss: 0.000078\n",
      "Epoch: 13 \tTraining Loss: 0.000078\n",
      "Epoch: 14 \tTraining Loss: 0.000078\n",
      "Epoch: 15 \tTraining Loss: 0.000078\n",
      "Epoch: 16 \tTraining Loss: 0.000078\n",
      "Epoch: 17 \tTraining Loss: 0.000078\n",
      "Epoch: 18 \tTraining Loss: 0.000078\n",
      "Epoch: 1 \tTraining Loss: 0.000122\n",
      "Epoch: 2 \tTraining Loss: 0.000122\n",
      "Epoch: 3 \tTraining Loss: 0.000122\n",
      "Epoch: 4 \tTraining Loss: 0.000122\n",
      "Epoch: 5 \tTraining Loss: 0.000122\n",
      "Epoch: 6 \tTraining Loss: 0.000122\n",
      "Epoch: 7 \tTraining Loss: 0.000122\n",
      "Epoch: 8 \tTraining Loss: 0.000122\n",
      "Epoch: 9 \tTraining Loss: 0.000122\n",
      "Epoch: 10 \tTraining Loss: 0.000122\n",
      "Epoch: 11 \tTraining Loss: 0.000122\n",
      "Epoch: 12 \tTraining Loss: 0.000122\n",
      "Epoch: 13 \tTraining Loss: 0.000122\n",
      "Epoch: 14 \tTraining Loss: 0.000122\n",
      "Epoch: 15 \tTraining Loss: 0.000122\n",
      "Epoch: 16 \tTraining Loss: 0.000122\n",
      "Epoch: 17 \tTraining Loss: 0.000122\n",
      "Epoch: 18 \tTraining Loss: 0.000122\n",
      "Epoch: 1 \tTraining Loss: 0.000310\n",
      "Epoch: 2 \tTraining Loss: 0.000310\n",
      "Epoch: 3 \tTraining Loss: 0.000310\n",
      "Epoch: 4 \tTraining Loss: 0.000310\n",
      "Epoch: 5 \tTraining Loss: 0.000310\n",
      "Epoch: 6 \tTraining Loss: 0.000310\n",
      "Epoch: 7 \tTraining Loss: 0.000310\n",
      "Epoch: 8 \tTraining Loss: 0.000310\n",
      "Epoch: 9 \tTraining Loss: 0.000310\n",
      "Epoch: 10 \tTraining Loss: 0.000310\n",
      "Epoch: 11 \tTraining Loss: 0.000310\n",
      "Epoch: 12 \tTraining Loss: 0.000310\n",
      "Epoch: 13 \tTraining Loss: 0.000310\n",
      "Epoch: 14 \tTraining Loss: 0.000310\n",
      "Epoch: 15 \tTraining Loss: 0.000310\n",
      "Epoch: 16 \tTraining Loss: 0.000310\n",
      "Epoch: 17 \tTraining Loss: 0.000310\n",
      "Epoch: 18 \tTraining Loss: 0.000310\n",
      "Epoch: 1 \tTraining Loss: 0.000143\n",
      "Epoch: 2 \tTraining Loss: 0.000143\n",
      "Epoch: 3 \tTraining Loss: 0.000143\n",
      "Epoch: 4 \tTraining Loss: 0.000143\n",
      "Epoch: 5 \tTraining Loss: 0.000143\n",
      "Epoch: 6 \tTraining Loss: 0.000143\n",
      "Epoch: 7 \tTraining Loss: 0.000143\n",
      "Epoch: 8 \tTraining Loss: 0.000143\n",
      "Epoch: 9 \tTraining Loss: 0.000143\n",
      "Epoch: 10 \tTraining Loss: 0.000143\n",
      "Epoch: 11 \tTraining Loss: 0.000143\n",
      "Epoch: 12 \tTraining Loss: 0.000143\n",
      "Epoch: 13 \tTraining Loss: 0.000143\n",
      "Epoch: 14 \tTraining Loss: 0.000143\n",
      "Epoch: 15 \tTraining Loss: 0.000143\n",
      "Epoch: 16 \tTraining Loss: 0.000143\n",
      "Epoch: 17 \tTraining Loss: 0.000143\n",
      "Epoch: 18 \tTraining Loss: 0.000143\n",
      "Epoch: 1 \tTraining Loss: 0.000229\n",
      "Epoch: 2 \tTraining Loss: 0.000229\n",
      "Epoch: 3 \tTraining Loss: 0.000229\n",
      "Epoch: 4 \tTraining Loss: 0.000229\n",
      "Epoch: 5 \tTraining Loss: 0.000229\n",
      "Epoch: 6 \tTraining Loss: 0.000229\n",
      "Epoch: 7 \tTraining Loss: 0.000229\n",
      "Epoch: 8 \tTraining Loss: 0.000229\n",
      "Epoch: 9 \tTraining Loss: 0.000229\n",
      "Epoch: 10 \tTraining Loss: 0.000229\n",
      "Epoch: 11 \tTraining Loss: 0.000229\n",
      "Epoch: 12 \tTraining Loss: 0.000229\n",
      "Epoch: 13 \tTraining Loss: 0.000229\n",
      "Epoch: 14 \tTraining Loss: 0.000229\n",
      "Epoch: 15 \tTraining Loss: 0.000229\n",
      "Epoch: 16 \tTraining Loss: 0.000229\n",
      "Epoch: 17 \tTraining Loss: 0.000229\n",
      "Epoch: 18 \tTraining Loss: 0.000229\n",
      "Epoch: 1 \tTraining Loss: 0.000090\n",
      "Epoch: 2 \tTraining Loss: 0.000090\n",
      "Epoch: 3 \tTraining Loss: 0.000090\n",
      "Epoch: 4 \tTraining Loss: 0.000090\n",
      "Epoch: 5 \tTraining Loss: 0.000090\n",
      "Epoch: 6 \tTraining Loss: 0.000090\n",
      "Epoch: 7 \tTraining Loss: 0.000090\n",
      "Epoch: 8 \tTraining Loss: 0.000090\n",
      "Epoch: 9 \tTraining Loss: 0.000090\n",
      "Epoch: 10 \tTraining Loss: 0.000090\n",
      "Epoch: 11 \tTraining Loss: 0.000090\n",
      "Epoch: 12 \tTraining Loss: 0.000090\n",
      "Epoch: 13 \tTraining Loss: 0.000090\n",
      "Epoch: 14 \tTraining Loss: 0.000090\n",
      "Epoch: 15 \tTraining Loss: 0.000090\n",
      "Epoch: 16 \tTraining Loss: 0.000090\n",
      "Epoch: 17 \tTraining Loss: 0.000090\n",
      "Epoch: 18 \tTraining Loss: 0.000090\n",
      "Epoch: 1 \tTraining Loss: 0.000270\n",
      "Epoch: 2 \tTraining Loss: 0.000270\n",
      "Epoch: 3 \tTraining Loss: 0.000270\n",
      "Epoch: 4 \tTraining Loss: 0.000270\n",
      "Epoch: 5 \tTraining Loss: 0.000270\n",
      "Epoch: 6 \tTraining Loss: 0.000270\n",
      "Epoch: 7 \tTraining Loss: 0.000270\n",
      "Epoch: 8 \tTraining Loss: 0.000270\n",
      "Epoch: 9 \tTraining Loss: 0.000270\n",
      "Epoch: 10 \tTraining Loss: 0.000270\n",
      "Epoch: 11 \tTraining Loss: 0.000270\n",
      "Epoch: 12 \tTraining Loss: 0.000270\n",
      "Epoch: 13 \tTraining Loss: 0.000270\n",
      "Epoch: 14 \tTraining Loss: 0.000270\n",
      "Epoch: 15 \tTraining Loss: 0.000270\n",
      "Epoch: 16 \tTraining Loss: 0.000270\n",
      "Epoch: 17 \tTraining Loss: 0.000270\n",
      "Epoch: 18 \tTraining Loss: 0.000270\n",
      "Epoch: 1 \tTraining Loss: 0.000127\n",
      "Epoch: 2 \tTraining Loss: 0.000127\n",
      "Epoch: 3 \tTraining Loss: 0.000127\n",
      "Epoch: 4 \tTraining Loss: 0.000127\n",
      "Epoch: 5 \tTraining Loss: 0.000127\n",
      "Epoch: 6 \tTraining Loss: 0.000127\n",
      "Epoch: 7 \tTraining Loss: 0.000127\n",
      "Epoch: 8 \tTraining Loss: 0.000127\n",
      "Epoch: 9 \tTraining Loss: 0.000127\n",
      "Epoch: 10 \tTraining Loss: 0.000127\n",
      "Epoch: 11 \tTraining Loss: 0.000127\n",
      "Epoch: 12 \tTraining Loss: 0.000127\n",
      "Epoch: 13 \tTraining Loss: 0.000127\n",
      "Epoch: 14 \tTraining Loss: 0.000127\n",
      "Epoch: 15 \tTraining Loss: 0.000127\n",
      "Epoch: 16 \tTraining Loss: 0.000127\n",
      "Epoch: 17 \tTraining Loss: 0.000127\n",
      "Epoch: 18 \tTraining Loss: 0.000127\n",
      "Epoch: 1 \tTraining Loss: 0.000251\n",
      "Epoch: 2 \tTraining Loss: 0.000251\n",
      "Epoch: 3 \tTraining Loss: 0.000251\n",
      "Epoch: 4 \tTraining Loss: 0.000251\n",
      "Epoch: 5 \tTraining Loss: 0.000251\n",
      "Epoch: 6 \tTraining Loss: 0.000251\n",
      "Epoch: 7 \tTraining Loss: 0.000251\n",
      "Epoch: 8 \tTraining Loss: 0.000251\n",
      "Epoch: 9 \tTraining Loss: 0.000251\n",
      "Epoch: 10 \tTraining Loss: 0.000251\n",
      "Epoch: 11 \tTraining Loss: 0.000251\n",
      "Epoch: 12 \tTraining Loss: 0.000251\n",
      "Epoch: 13 \tTraining Loss: 0.000251\n",
      "Epoch: 14 \tTraining Loss: 0.000251\n",
      "Epoch: 15 \tTraining Loss: 0.000251\n",
      "Epoch: 16 \tTraining Loss: 0.000251\n",
      "Epoch: 17 \tTraining Loss: 0.000251\n",
      "Epoch: 18 \tTraining Loss: 0.000251\n",
      "Epoch: 1 \tTraining Loss: 0.000114\n",
      "Epoch: 2 \tTraining Loss: 0.000114\n",
      "Epoch: 3 \tTraining Loss: 0.000114\n",
      "Epoch: 4 \tTraining Loss: 0.000114\n",
      "Epoch: 5 \tTraining Loss: 0.000114\n",
      "Epoch: 6 \tTraining Loss: 0.000114\n",
      "Epoch: 7 \tTraining Loss: 0.000114\n",
      "Epoch: 8 \tTraining Loss: 0.000114\n",
      "Epoch: 9 \tTraining Loss: 0.000114\n",
      "Epoch: 10 \tTraining Loss: 0.000114\n",
      "Epoch: 11 \tTraining Loss: 0.000114\n",
      "Epoch: 12 \tTraining Loss: 0.000114\n",
      "Epoch: 13 \tTraining Loss: 0.000114\n",
      "Epoch: 14 \tTraining Loss: 0.000114\n",
      "Epoch: 15 \tTraining Loss: 0.000114\n",
      "Epoch: 16 \tTraining Loss: 0.000114\n",
      "Epoch: 17 \tTraining Loss: 0.000114\n",
      "Epoch: 18 \tTraining Loss: 0.000114\n",
      "Epoch: 1 \tTraining Loss: 0.000319\n",
      "Epoch: 2 \tTraining Loss: 0.000319\n",
      "Epoch: 3 \tTraining Loss: 0.000319\n",
      "Epoch: 4 \tTraining Loss: 0.000319\n",
      "Epoch: 5 \tTraining Loss: 0.000319\n",
      "Epoch: 6 \tTraining Loss: 0.000319\n",
      "Epoch: 7 \tTraining Loss: 0.000319\n",
      "Epoch: 8 \tTraining Loss: 0.000319\n",
      "Epoch: 9 \tTraining Loss: 0.000319\n",
      "Epoch: 10 \tTraining Loss: 0.000319\n",
      "Epoch: 11 \tTraining Loss: 0.000319\n",
      "Epoch: 12 \tTraining Loss: 0.000319\n",
      "Epoch: 13 \tTraining Loss: 0.000319\n",
      "Epoch: 14 \tTraining Loss: 0.000319\n",
      "Epoch: 15 \tTraining Loss: 0.000319\n",
      "Epoch: 16 \tTraining Loss: 0.000319\n",
      "Epoch: 17 \tTraining Loss: 0.000319\n",
      "Epoch: 18 \tTraining Loss: 0.000319\n",
      "Epoch: 1 \tTraining Loss: 0.000186\n",
      "Epoch: 2 \tTraining Loss: 0.000186\n",
      "Epoch: 3 \tTraining Loss: 0.000186\n",
      "Epoch: 4 \tTraining Loss: 0.000186\n",
      "Epoch: 5 \tTraining Loss: 0.000186\n",
      "Epoch: 6 \tTraining Loss: 0.000186\n",
      "Epoch: 7 \tTraining Loss: 0.000186\n",
      "Epoch: 8 \tTraining Loss: 0.000186\n",
      "Epoch: 9 \tTraining Loss: 0.000186\n",
      "Epoch: 10 \tTraining Loss: 0.000186\n",
      "Epoch: 11 \tTraining Loss: 0.000186\n",
      "Epoch: 12 \tTraining Loss: 0.000186\n",
      "Epoch: 13 \tTraining Loss: 0.000186\n",
      "Epoch: 14 \tTraining Loss: 0.000186\n",
      "Epoch: 15 \tTraining Loss: 0.000186\n",
      "Epoch: 16 \tTraining Loss: 0.000186\n",
      "Epoch: 17 \tTraining Loss: 0.000186\n",
      "Epoch: 18 \tTraining Loss: 0.000186\n",
      "Epoch: 1 \tTraining Loss: 0.000188\n",
      "Epoch: 2 \tTraining Loss: 0.000188\n",
      "Epoch: 3 \tTraining Loss: 0.000188\n",
      "Epoch: 4 \tTraining Loss: 0.000188\n",
      "Epoch: 5 \tTraining Loss: 0.000188\n",
      "Epoch: 6 \tTraining Loss: 0.000188\n",
      "Epoch: 7 \tTraining Loss: 0.000188\n",
      "Epoch: 8 \tTraining Loss: 0.000188\n",
      "Epoch: 9 \tTraining Loss: 0.000188\n",
      "Epoch: 10 \tTraining Loss: 0.000188\n",
      "Epoch: 11 \tTraining Loss: 0.000188\n",
      "Epoch: 12 \tTraining Loss: 0.000188\n",
      "Epoch: 13 \tTraining Loss: 0.000188\n",
      "Epoch: 14 \tTraining Loss: 0.000188\n",
      "Epoch: 15 \tTraining Loss: 0.000188\n",
      "Epoch: 16 \tTraining Loss: 0.000188\n",
      "Epoch: 17 \tTraining Loss: 0.000188\n",
      "Epoch: 18 \tTraining Loss: 0.000188\n",
      "Epoch: 1 \tTraining Loss: 0.000154\n",
      "Epoch: 2 \tTraining Loss: 0.000154\n",
      "Epoch: 3 \tTraining Loss: 0.000154\n",
      "Epoch: 4 \tTraining Loss: 0.000154\n",
      "Epoch: 5 \tTraining Loss: 0.000154\n",
      "Epoch: 6 \tTraining Loss: 0.000154\n",
      "Epoch: 7 \tTraining Loss: 0.000154\n",
      "Epoch: 8 \tTraining Loss: 0.000154\n",
      "Epoch: 9 \tTraining Loss: 0.000154\n",
      "Epoch: 10 \tTraining Loss: 0.000154\n",
      "Epoch: 11 \tTraining Loss: 0.000154\n",
      "Epoch: 12 \tTraining Loss: 0.000154\n",
      "Epoch: 13 \tTraining Loss: 0.000154\n",
      "Epoch: 14 \tTraining Loss: 0.000154\n",
      "Epoch: 15 \tTraining Loss: 0.000154\n",
      "Epoch: 16 \tTraining Loss: 0.000154\n",
      "Epoch: 17 \tTraining Loss: 0.000154\n",
      "Epoch: 18 \tTraining Loss: 0.000154\n",
      "Epoch: 1 \tTraining Loss: 0.000264\n",
      "Epoch: 2 \tTraining Loss: 0.000264\n",
      "Epoch: 3 \tTraining Loss: 0.000264\n",
      "Epoch: 4 \tTraining Loss: 0.000264\n",
      "Epoch: 5 \tTraining Loss: 0.000264\n",
      "Epoch: 6 \tTraining Loss: 0.000264\n",
      "Epoch: 7 \tTraining Loss: 0.000264\n",
      "Epoch: 8 \tTraining Loss: 0.000264\n",
      "Epoch: 9 \tTraining Loss: 0.000264\n",
      "Epoch: 10 \tTraining Loss: 0.000264\n",
      "Epoch: 11 \tTraining Loss: 0.000264\n",
      "Epoch: 12 \tTraining Loss: 0.000264\n",
      "Epoch: 13 \tTraining Loss: 0.000264\n",
      "Epoch: 14 \tTraining Loss: 0.000264\n",
      "Epoch: 15 \tTraining Loss: 0.000264\n",
      "Epoch: 16 \tTraining Loss: 0.000264\n",
      "Epoch: 17 \tTraining Loss: 0.000264\n",
      "Epoch: 18 \tTraining Loss: 0.000264\n",
      "Epoch: 1 \tTraining Loss: 0.000171\n",
      "Epoch: 2 \tTraining Loss: 0.000171\n",
      "Epoch: 3 \tTraining Loss: 0.000171\n",
      "Epoch: 4 \tTraining Loss: 0.000171\n",
      "Epoch: 5 \tTraining Loss: 0.000171\n",
      "Epoch: 6 \tTraining Loss: 0.000171\n",
      "Epoch: 7 \tTraining Loss: 0.000171\n",
      "Epoch: 8 \tTraining Loss: 0.000171\n",
      "Epoch: 9 \tTraining Loss: 0.000171\n",
      "Epoch: 10 \tTraining Loss: 0.000171\n",
      "Epoch: 11 \tTraining Loss: 0.000171\n",
      "Epoch: 12 \tTraining Loss: 0.000171\n",
      "Epoch: 13 \tTraining Loss: 0.000171\n",
      "Epoch: 14 \tTraining Loss: 0.000171\n",
      "Epoch: 15 \tTraining Loss: 0.000171\n",
      "Epoch: 16 \tTraining Loss: 0.000171\n",
      "Epoch: 17 \tTraining Loss: 0.000171\n",
      "Epoch: 18 \tTraining Loss: 0.000171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000223\n",
      "Epoch: 2 \tTraining Loss: 0.000223\n",
      "Epoch: 3 \tTraining Loss: 0.000223\n",
      "Epoch: 4 \tTraining Loss: 0.000223\n",
      "Epoch: 5 \tTraining Loss: 0.000223\n",
      "Epoch: 6 \tTraining Loss: 0.000223\n",
      "Epoch: 7 \tTraining Loss: 0.000223\n",
      "Epoch: 8 \tTraining Loss: 0.000223\n",
      "Epoch: 9 \tTraining Loss: 0.000223\n",
      "Epoch: 10 \tTraining Loss: 0.000223\n",
      "Epoch: 11 \tTraining Loss: 0.000223\n",
      "Epoch: 12 \tTraining Loss: 0.000223\n",
      "Epoch: 13 \tTraining Loss: 0.000223\n",
      "Epoch: 14 \tTraining Loss: 0.000223\n",
      "Epoch: 15 \tTraining Loss: 0.000223\n",
      "Epoch: 16 \tTraining Loss: 0.000223\n",
      "Epoch: 17 \tTraining Loss: 0.000223\n",
      "Epoch: 18 \tTraining Loss: 0.000223\n",
      "Epoch: 1 \tTraining Loss: 0.000206\n",
      "Epoch: 2 \tTraining Loss: 0.000206\n",
      "Epoch: 3 \tTraining Loss: 0.000206\n",
      "Epoch: 4 \tTraining Loss: 0.000206\n",
      "Epoch: 5 \tTraining Loss: 0.000206\n",
      "Epoch: 6 \tTraining Loss: 0.000206\n",
      "Epoch: 7 \tTraining Loss: 0.000206\n",
      "Epoch: 8 \tTraining Loss: 0.000206\n",
      "Epoch: 9 \tTraining Loss: 0.000206\n",
      "Epoch: 10 \tTraining Loss: 0.000206\n",
      "Epoch: 11 \tTraining Loss: 0.000206\n",
      "Epoch: 12 \tTraining Loss: 0.000206\n",
      "Epoch: 13 \tTraining Loss: 0.000206\n",
      "Epoch: 14 \tTraining Loss: 0.000206\n",
      "Epoch: 15 \tTraining Loss: 0.000206\n",
      "Epoch: 16 \tTraining Loss: 0.000206\n",
      "Epoch: 17 \tTraining Loss: 0.000206\n",
      "Epoch: 18 \tTraining Loss: 0.000206\n",
      "Epoch: 1 \tTraining Loss: 0.000185\n",
      "Epoch: 2 \tTraining Loss: 0.000185\n",
      "Epoch: 3 \tTraining Loss: 0.000185\n",
      "Epoch: 4 \tTraining Loss: 0.000185\n",
      "Epoch: 5 \tTraining Loss: 0.000185\n",
      "Epoch: 6 \tTraining Loss: 0.000185\n",
      "Epoch: 7 \tTraining Loss: 0.000185\n",
      "Epoch: 8 \tTraining Loss: 0.000185\n",
      "Epoch: 9 \tTraining Loss: 0.000185\n",
      "Epoch: 10 \tTraining Loss: 0.000185\n",
      "Epoch: 11 \tTraining Loss: 0.000185\n",
      "Epoch: 12 \tTraining Loss: 0.000185\n",
      "Epoch: 13 \tTraining Loss: 0.000185\n",
      "Epoch: 14 \tTraining Loss: 0.000185\n",
      "Epoch: 15 \tTraining Loss: 0.000185\n",
      "Epoch: 16 \tTraining Loss: 0.000185\n",
      "Epoch: 17 \tTraining Loss: 0.000185\n",
      "Epoch: 18 \tTraining Loss: 0.000185\n",
      "Epoch: 1 \tTraining Loss: 0.000221\n",
      "Epoch: 2 \tTraining Loss: 0.000221\n",
      "Epoch: 3 \tTraining Loss: 0.000221\n",
      "Epoch: 4 \tTraining Loss: 0.000221\n",
      "Epoch: 5 \tTraining Loss: 0.000221\n",
      "Epoch: 6 \tTraining Loss: 0.000221\n",
      "Epoch: 7 \tTraining Loss: 0.000221\n",
      "Epoch: 8 \tTraining Loss: 0.000221\n",
      "Epoch: 9 \tTraining Loss: 0.000221\n",
      "Epoch: 10 \tTraining Loss: 0.000221\n",
      "Epoch: 11 \tTraining Loss: 0.000221\n",
      "Epoch: 12 \tTraining Loss: 0.000221\n",
      "Epoch: 13 \tTraining Loss: 0.000221\n",
      "Epoch: 14 \tTraining Loss: 0.000221\n",
      "Epoch: 15 \tTraining Loss: 0.000221\n",
      "Epoch: 16 \tTraining Loss: 0.000221\n",
      "Epoch: 17 \tTraining Loss: 0.000221\n",
      "Epoch: 18 \tTraining Loss: 0.000221\n",
      "Epoch: 1 \tTraining Loss: 0.000189\n",
      "Epoch: 2 \tTraining Loss: 0.000189\n",
      "Epoch: 3 \tTraining Loss: 0.000189\n",
      "Epoch: 4 \tTraining Loss: 0.000189\n",
      "Epoch: 5 \tTraining Loss: 0.000189\n",
      "Epoch: 6 \tTraining Loss: 0.000189\n",
      "Epoch: 7 \tTraining Loss: 0.000189\n",
      "Epoch: 8 \tTraining Loss: 0.000189\n",
      "Epoch: 9 \tTraining Loss: 0.000189\n",
      "Epoch: 10 \tTraining Loss: 0.000189\n",
      "Epoch: 11 \tTraining Loss: 0.000189\n",
      "Epoch: 12 \tTraining Loss: 0.000189\n",
      "Epoch: 13 \tTraining Loss: 0.000189\n",
      "Epoch: 14 \tTraining Loss: 0.000189\n",
      "Epoch: 15 \tTraining Loss: 0.000189\n",
      "Epoch: 16 \tTraining Loss: 0.000189\n",
      "Epoch: 17 \tTraining Loss: 0.000189\n",
      "Epoch: 18 \tTraining Loss: 0.000189\n",
      "Epoch: 1 \tTraining Loss: 0.000061\n",
      "Epoch: 2 \tTraining Loss: 0.000061\n",
      "Epoch: 3 \tTraining Loss: 0.000061\n",
      "Epoch: 4 \tTraining Loss: 0.000061\n",
      "Epoch: 5 \tTraining Loss: 0.000061\n",
      "Epoch: 6 \tTraining Loss: 0.000061\n",
      "Epoch: 7 \tTraining Loss: 0.000061\n",
      "Epoch: 8 \tTraining Loss: 0.000061\n",
      "Epoch: 9 \tTraining Loss: 0.000061\n",
      "Epoch: 10 \tTraining Loss: 0.000061\n",
      "Epoch: 11 \tTraining Loss: 0.000061\n",
      "Epoch: 12 \tTraining Loss: 0.000061\n",
      "Epoch: 13 \tTraining Loss: 0.000061\n",
      "Epoch: 14 \tTraining Loss: 0.000061\n",
      "Epoch: 15 \tTraining Loss: 0.000061\n",
      "Epoch: 16 \tTraining Loss: 0.000061\n",
      "Epoch: 17 \tTraining Loss: 0.000061\n",
      "Epoch: 18 \tTraining Loss: 0.000061\n",
      "Epoch: 1 \tTraining Loss: 0.000290\n",
      "Epoch: 2 \tTraining Loss: 0.000290\n",
      "Epoch: 3 \tTraining Loss: 0.000290\n",
      "Epoch: 4 \tTraining Loss: 0.000290\n",
      "Epoch: 5 \tTraining Loss: 0.000290\n",
      "Epoch: 6 \tTraining Loss: 0.000290\n",
      "Epoch: 7 \tTraining Loss: 0.000290\n",
      "Epoch: 8 \tTraining Loss: 0.000290\n",
      "Epoch: 9 \tTraining Loss: 0.000290\n",
      "Epoch: 10 \tTraining Loss: 0.000290\n",
      "Epoch: 11 \tTraining Loss: 0.000290\n",
      "Epoch: 12 \tTraining Loss: 0.000290\n",
      "Epoch: 13 \tTraining Loss: 0.000290\n",
      "Epoch: 14 \tTraining Loss: 0.000290\n",
      "Epoch: 15 \tTraining Loss: 0.000290\n",
      "Epoch: 16 \tTraining Loss: 0.000290\n",
      "Epoch: 17 \tTraining Loss: 0.000290\n",
      "Epoch: 18 \tTraining Loss: 0.000290\n",
      "Epoch: 1 \tTraining Loss: 0.000087\n",
      "Epoch: 2 \tTraining Loss: 0.000087\n",
      "Epoch: 3 \tTraining Loss: 0.000087\n",
      "Epoch: 4 \tTraining Loss: 0.000087\n",
      "Epoch: 5 \tTraining Loss: 0.000087\n",
      "Epoch: 6 \tTraining Loss: 0.000087\n",
      "Epoch: 7 \tTraining Loss: 0.000087\n",
      "Epoch: 8 \tTraining Loss: 0.000087\n",
      "Epoch: 9 \tTraining Loss: 0.000087\n",
      "Epoch: 10 \tTraining Loss: 0.000087\n",
      "Epoch: 11 \tTraining Loss: 0.000087\n",
      "Epoch: 12 \tTraining Loss: 0.000087\n",
      "Epoch: 13 \tTraining Loss: 0.000087\n",
      "Epoch: 14 \tTraining Loss: 0.000087\n",
      "Epoch: 15 \tTraining Loss: 0.000087\n",
      "Epoch: 16 \tTraining Loss: 0.000087\n",
      "Epoch: 17 \tTraining Loss: 0.000087\n",
      "Epoch: 18 \tTraining Loss: 0.000087\n",
      "Epoch: 1 \tTraining Loss: 0.000246\n",
      "Epoch: 2 \tTraining Loss: 0.000246\n",
      "Epoch: 3 \tTraining Loss: 0.000246\n",
      "Epoch: 4 \tTraining Loss: 0.000246\n",
      "Epoch: 5 \tTraining Loss: 0.000246\n",
      "Epoch: 6 \tTraining Loss: 0.000246\n",
      "Epoch: 7 \tTraining Loss: 0.000246\n",
      "Epoch: 8 \tTraining Loss: 0.000246\n",
      "Epoch: 9 \tTraining Loss: 0.000246\n",
      "Epoch: 10 \tTraining Loss: 0.000246\n",
      "Epoch: 11 \tTraining Loss: 0.000246\n",
      "Epoch: 12 \tTraining Loss: 0.000246\n",
      "Epoch: 13 \tTraining Loss: 0.000246\n",
      "Epoch: 14 \tTraining Loss: 0.000246\n",
      "Epoch: 15 \tTraining Loss: 0.000246\n",
      "Epoch: 16 \tTraining Loss: 0.000246\n",
      "Epoch: 17 \tTraining Loss: 0.000246\n",
      "Epoch: 18 \tTraining Loss: 0.000246\n",
      "Epoch: 1 \tTraining Loss: 0.000287\n",
      "Epoch: 2 \tTraining Loss: 0.000287\n",
      "Epoch: 3 \tTraining Loss: 0.000287\n",
      "Epoch: 4 \tTraining Loss: 0.000287\n",
      "Epoch: 5 \tTraining Loss: 0.000287\n",
      "Epoch: 6 \tTraining Loss: 0.000287\n",
      "Epoch: 7 \tTraining Loss: 0.000287\n",
      "Epoch: 8 \tTraining Loss: 0.000287\n",
      "Epoch: 9 \tTraining Loss: 0.000287\n",
      "Epoch: 10 \tTraining Loss: 0.000287\n",
      "Epoch: 11 \tTraining Loss: 0.000287\n",
      "Epoch: 12 \tTraining Loss: 0.000287\n",
      "Epoch: 13 \tTraining Loss: 0.000287\n",
      "Epoch: 14 \tTraining Loss: 0.000287\n",
      "Epoch: 15 \tTraining Loss: 0.000287\n",
      "Epoch: 16 \tTraining Loss: 0.000287\n",
      "Epoch: 17 \tTraining Loss: 0.000287\n",
      "Epoch: 18 \tTraining Loss: 0.000287\n",
      "Epoch: 1 \tTraining Loss: 0.000337\n",
      "Epoch: 2 \tTraining Loss: 0.000337\n",
      "Epoch: 3 \tTraining Loss: 0.000337\n",
      "Epoch: 4 \tTraining Loss: 0.000337\n",
      "Epoch: 5 \tTraining Loss: 0.000337\n",
      "Epoch: 6 \tTraining Loss: 0.000337\n",
      "Epoch: 7 \tTraining Loss: 0.000337\n",
      "Epoch: 8 \tTraining Loss: 0.000337\n",
      "Epoch: 9 \tTraining Loss: 0.000337\n",
      "Epoch: 10 \tTraining Loss: 0.000337\n",
      "Epoch: 11 \tTraining Loss: 0.000337\n",
      "Epoch: 12 \tTraining Loss: 0.000337\n",
      "Epoch: 13 \tTraining Loss: 0.000337\n",
      "Epoch: 14 \tTraining Loss: 0.000337\n",
      "Epoch: 15 \tTraining Loss: 0.000337\n",
      "Epoch: 16 \tTraining Loss: 0.000337\n",
      "Epoch: 17 \tTraining Loss: 0.000337\n",
      "Epoch: 18 \tTraining Loss: 0.000337\n",
      "Epoch: 1 \tTraining Loss: 0.000121\n",
      "Epoch: 2 \tTraining Loss: 0.000121\n",
      "Epoch: 3 \tTraining Loss: 0.000121\n",
      "Epoch: 4 \tTraining Loss: 0.000121\n",
      "Epoch: 5 \tTraining Loss: 0.000121\n",
      "Epoch: 6 \tTraining Loss: 0.000121\n",
      "Epoch: 7 \tTraining Loss: 0.000121\n",
      "Epoch: 8 \tTraining Loss: 0.000121\n",
      "Epoch: 9 \tTraining Loss: 0.000121\n",
      "Epoch: 10 \tTraining Loss: 0.000121\n",
      "Epoch: 11 \tTraining Loss: 0.000121\n",
      "Epoch: 12 \tTraining Loss: 0.000121\n",
      "Epoch: 13 \tTraining Loss: 0.000121\n",
      "Epoch: 14 \tTraining Loss: 0.000121\n",
      "Epoch: 15 \tTraining Loss: 0.000121\n",
      "Epoch: 16 \tTraining Loss: 0.000121\n",
      "Epoch: 17 \tTraining Loss: 0.000121\n",
      "Epoch: 18 \tTraining Loss: 0.000121\n",
      "Epoch: 1 \tTraining Loss: 0.000158\n",
      "Epoch: 2 \tTraining Loss: 0.000158\n",
      "Epoch: 3 \tTraining Loss: 0.000158\n",
      "Epoch: 4 \tTraining Loss: 0.000158\n",
      "Epoch: 5 \tTraining Loss: 0.000158\n",
      "Epoch: 6 \tTraining Loss: 0.000158\n",
      "Epoch: 7 \tTraining Loss: 0.000158\n",
      "Epoch: 8 \tTraining Loss: 0.000158\n",
      "Epoch: 9 \tTraining Loss: 0.000158\n",
      "Epoch: 10 \tTraining Loss: 0.000158\n",
      "Epoch: 11 \tTraining Loss: 0.000158\n",
      "Epoch: 12 \tTraining Loss: 0.000158\n",
      "Epoch: 13 \tTraining Loss: 0.000158\n",
      "Epoch: 14 \tTraining Loss: 0.000158\n",
      "Epoch: 15 \tTraining Loss: 0.000158\n",
      "Epoch: 16 \tTraining Loss: 0.000158\n",
      "Epoch: 17 \tTraining Loss: 0.000158\n",
      "Epoch: 18 \tTraining Loss: 0.000158\n",
      "Epoch: 1 \tTraining Loss: 0.000294\n",
      "Epoch: 2 \tTraining Loss: 0.000294\n",
      "Epoch: 3 \tTraining Loss: 0.000294\n",
      "Epoch: 4 \tTraining Loss: 0.000294\n",
      "Epoch: 5 \tTraining Loss: 0.000294\n",
      "Epoch: 6 \tTraining Loss: 0.000294\n",
      "Epoch: 7 \tTraining Loss: 0.000294\n",
      "Epoch: 8 \tTraining Loss: 0.000294\n",
      "Epoch: 9 \tTraining Loss: 0.000294\n",
      "Epoch: 10 \tTraining Loss: 0.000294\n",
      "Epoch: 11 \tTraining Loss: 0.000294\n",
      "Epoch: 12 \tTraining Loss: 0.000294\n",
      "Epoch: 13 \tTraining Loss: 0.000294\n",
      "Epoch: 14 \tTraining Loss: 0.000294\n",
      "Epoch: 15 \tTraining Loss: 0.000294\n",
      "Epoch: 16 \tTraining Loss: 0.000294\n",
      "Epoch: 17 \tTraining Loss: 0.000294\n",
      "Epoch: 18 \tTraining Loss: 0.000294\n",
      "Epoch: 1 \tTraining Loss: 0.000145\n",
      "Epoch: 2 \tTraining Loss: 0.000145\n",
      "Epoch: 3 \tTraining Loss: 0.000145\n",
      "Epoch: 4 \tTraining Loss: 0.000145\n",
      "Epoch: 5 \tTraining Loss: 0.000145\n",
      "Epoch: 6 \tTraining Loss: 0.000145\n",
      "Epoch: 7 \tTraining Loss: 0.000145\n",
      "Epoch: 8 \tTraining Loss: 0.000145\n",
      "Epoch: 9 \tTraining Loss: 0.000145\n",
      "Epoch: 10 \tTraining Loss: 0.000145\n",
      "Epoch: 11 \tTraining Loss: 0.000145\n",
      "Epoch: 12 \tTraining Loss: 0.000145\n",
      "Epoch: 13 \tTraining Loss: 0.000145\n",
      "Epoch: 14 \tTraining Loss: 0.000145\n",
      "Epoch: 15 \tTraining Loss: 0.000145\n",
      "Epoch: 16 \tTraining Loss: 0.000145\n",
      "Epoch: 17 \tTraining Loss: 0.000145\n",
      "Epoch: 18 \tTraining Loss: 0.000145\n",
      "Epoch: 1 \tTraining Loss: 0.000206\n",
      "Epoch: 2 \tTraining Loss: 0.000206\n",
      "Epoch: 3 \tTraining Loss: 0.000206\n",
      "Epoch: 4 \tTraining Loss: 0.000206\n",
      "Epoch: 5 \tTraining Loss: 0.000206\n",
      "Epoch: 6 \tTraining Loss: 0.000206\n",
      "Epoch: 7 \tTraining Loss: 0.000206\n",
      "Epoch: 8 \tTraining Loss: 0.000206\n",
      "Epoch: 9 \tTraining Loss: 0.000206\n",
      "Epoch: 10 \tTraining Loss: 0.000206\n",
      "Epoch: 11 \tTraining Loss: 0.000206\n",
      "Epoch: 12 \tTraining Loss: 0.000206\n",
      "Epoch: 13 \tTraining Loss: 0.000206\n",
      "Epoch: 14 \tTraining Loss: 0.000206\n",
      "Epoch: 15 \tTraining Loss: 0.000206\n",
      "Epoch: 16 \tTraining Loss: 0.000206\n",
      "Epoch: 17 \tTraining Loss: 0.000206\n",
      "Epoch: 18 \tTraining Loss: 0.000206\n",
      "Epoch: 1 \tTraining Loss: 0.000490\n",
      "Epoch: 2 \tTraining Loss: 0.000490\n",
      "Epoch: 3 \tTraining Loss: 0.000490\n",
      "Epoch: 4 \tTraining Loss: 0.000490\n",
      "Epoch: 5 \tTraining Loss: 0.000490\n",
      "Epoch: 6 \tTraining Loss: 0.000490\n",
      "Epoch: 7 \tTraining Loss: 0.000490\n",
      "Epoch: 8 \tTraining Loss: 0.000490\n",
      "Epoch: 9 \tTraining Loss: 0.000490\n",
      "Epoch: 10 \tTraining Loss: 0.000490\n",
      "Epoch: 11 \tTraining Loss: 0.000490\n",
      "Epoch: 12 \tTraining Loss: 0.000490\n",
      "Epoch: 13 \tTraining Loss: 0.000490\n",
      "Epoch: 14 \tTraining Loss: 0.000490\n",
      "Epoch: 15 \tTraining Loss: 0.000490\n",
      "Epoch: 16 \tTraining Loss: 0.000490\n",
      "Epoch: 17 \tTraining Loss: 0.000490\n",
      "Epoch: 18 \tTraining Loss: 0.000490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000125\n",
      "Epoch: 2 \tTraining Loss: 0.000125\n",
      "Epoch: 3 \tTraining Loss: 0.000125\n",
      "Epoch: 4 \tTraining Loss: 0.000125\n",
      "Epoch: 5 \tTraining Loss: 0.000125\n",
      "Epoch: 6 \tTraining Loss: 0.000125\n",
      "Epoch: 7 \tTraining Loss: 0.000125\n",
      "Epoch: 8 \tTraining Loss: 0.000125\n",
      "Epoch: 9 \tTraining Loss: 0.000125\n",
      "Epoch: 10 \tTraining Loss: 0.000125\n",
      "Epoch: 11 \tTraining Loss: 0.000125\n",
      "Epoch: 12 \tTraining Loss: 0.000125\n",
      "Epoch: 13 \tTraining Loss: 0.000125\n",
      "Epoch: 14 \tTraining Loss: 0.000125\n",
      "Epoch: 15 \tTraining Loss: 0.000125\n",
      "Epoch: 16 \tTraining Loss: 0.000125\n",
      "Epoch: 17 \tTraining Loss: 0.000125\n",
      "Epoch: 18 \tTraining Loss: 0.000125\n",
      "Epoch: 1 \tTraining Loss: 0.000159\n",
      "Epoch: 2 \tTraining Loss: 0.000159\n",
      "Epoch: 3 \tTraining Loss: 0.000159\n",
      "Epoch: 4 \tTraining Loss: 0.000159\n",
      "Epoch: 5 \tTraining Loss: 0.000159\n",
      "Epoch: 6 \tTraining Loss: 0.000159\n",
      "Epoch: 7 \tTraining Loss: 0.000159\n",
      "Epoch: 8 \tTraining Loss: 0.000159\n",
      "Epoch: 9 \tTraining Loss: 0.000159\n",
      "Epoch: 10 \tTraining Loss: 0.000159\n",
      "Epoch: 11 \tTraining Loss: 0.000159\n",
      "Epoch: 12 \tTraining Loss: 0.000159\n",
      "Epoch: 13 \tTraining Loss: 0.000159\n",
      "Epoch: 14 \tTraining Loss: 0.000159\n",
      "Epoch: 15 \tTraining Loss: 0.000159\n",
      "Epoch: 16 \tTraining Loss: 0.000159\n",
      "Epoch: 17 \tTraining Loss: 0.000159\n",
      "Epoch: 18 \tTraining Loss: 0.000159\n",
      "Epoch: 1 \tTraining Loss: 0.000209\n",
      "Epoch: 2 \tTraining Loss: 0.000209\n",
      "Epoch: 3 \tTraining Loss: 0.000209\n",
      "Epoch: 4 \tTraining Loss: 0.000209\n",
      "Epoch: 5 \tTraining Loss: 0.000209\n",
      "Epoch: 6 \tTraining Loss: 0.000209\n",
      "Epoch: 7 \tTraining Loss: 0.000209\n",
      "Epoch: 8 \tTraining Loss: 0.000209\n",
      "Epoch: 9 \tTraining Loss: 0.000209\n",
      "Epoch: 10 \tTraining Loss: 0.000209\n",
      "Epoch: 11 \tTraining Loss: 0.000209\n",
      "Epoch: 12 \tTraining Loss: 0.000209\n",
      "Epoch: 13 \tTraining Loss: 0.000209\n",
      "Epoch: 14 \tTraining Loss: 0.000209\n",
      "Epoch: 15 \tTraining Loss: 0.000209\n",
      "Epoch: 16 \tTraining Loss: 0.000209\n",
      "Epoch: 17 \tTraining Loss: 0.000209\n",
      "Epoch: 18 \tTraining Loss: 0.000209\n",
      "Epoch: 1 \tTraining Loss: 0.000232\n",
      "Epoch: 2 \tTraining Loss: 0.000232\n",
      "Epoch: 3 \tTraining Loss: 0.000232\n",
      "Epoch: 4 \tTraining Loss: 0.000232\n",
      "Epoch: 5 \tTraining Loss: 0.000232\n",
      "Epoch: 6 \tTraining Loss: 0.000232\n",
      "Epoch: 7 \tTraining Loss: 0.000232\n",
      "Epoch: 8 \tTraining Loss: 0.000232\n",
      "Epoch: 9 \tTraining Loss: 0.000232\n",
      "Epoch: 10 \tTraining Loss: 0.000232\n",
      "Epoch: 11 \tTraining Loss: 0.000232\n",
      "Epoch: 12 \tTraining Loss: 0.000232\n",
      "Epoch: 13 \tTraining Loss: 0.000232\n",
      "Epoch: 14 \tTraining Loss: 0.000232\n",
      "Epoch: 15 \tTraining Loss: 0.000232\n",
      "Epoch: 16 \tTraining Loss: 0.000232\n",
      "Epoch: 17 \tTraining Loss: 0.000232\n",
      "Epoch: 18 \tTraining Loss: 0.000232\n",
      "Epoch: 1 \tTraining Loss: 0.000274\n",
      "Epoch: 2 \tTraining Loss: 0.000274\n",
      "Epoch: 3 \tTraining Loss: 0.000274\n",
      "Epoch: 4 \tTraining Loss: 0.000274\n",
      "Epoch: 5 \tTraining Loss: 0.000274\n",
      "Epoch: 6 \tTraining Loss: 0.000274\n",
      "Epoch: 7 \tTraining Loss: 0.000274\n",
      "Epoch: 8 \tTraining Loss: 0.000274\n",
      "Epoch: 9 \tTraining Loss: 0.000274\n",
      "Epoch: 10 \tTraining Loss: 0.000274\n",
      "Epoch: 11 \tTraining Loss: 0.000274\n",
      "Epoch: 12 \tTraining Loss: 0.000274\n",
      "Epoch: 13 \tTraining Loss: 0.000274\n",
      "Epoch: 14 \tTraining Loss: 0.000274\n",
      "Epoch: 15 \tTraining Loss: 0.000274\n",
      "Epoch: 16 \tTraining Loss: 0.000274\n",
      "Epoch: 17 \tTraining Loss: 0.000274\n",
      "Epoch: 18 \tTraining Loss: 0.000274\n",
      "Epoch: 1 \tTraining Loss: 0.000187\n",
      "Epoch: 2 \tTraining Loss: 0.000187\n",
      "Epoch: 3 \tTraining Loss: 0.000187\n",
      "Epoch: 4 \tTraining Loss: 0.000187\n",
      "Epoch: 5 \tTraining Loss: 0.000187\n",
      "Epoch: 6 \tTraining Loss: 0.000187\n",
      "Epoch: 7 \tTraining Loss: 0.000187\n",
      "Epoch: 8 \tTraining Loss: 0.000187\n",
      "Epoch: 9 \tTraining Loss: 0.000187\n",
      "Epoch: 10 \tTraining Loss: 0.000187\n",
      "Epoch: 11 \tTraining Loss: 0.000187\n",
      "Epoch: 12 \tTraining Loss: 0.000187\n",
      "Epoch: 13 \tTraining Loss: 0.000187\n",
      "Epoch: 14 \tTraining Loss: 0.000187\n",
      "Epoch: 15 \tTraining Loss: 0.000187\n",
      "Epoch: 16 \tTraining Loss: 0.000187\n",
      "Epoch: 17 \tTraining Loss: 0.000187\n",
      "Epoch: 18 \tTraining Loss: 0.000187\n",
      "Epoch: 1 \tTraining Loss: 0.000152\n",
      "Epoch: 2 \tTraining Loss: 0.000152\n",
      "Epoch: 3 \tTraining Loss: 0.000152\n",
      "Epoch: 4 \tTraining Loss: 0.000152\n",
      "Epoch: 5 \tTraining Loss: 0.000152\n",
      "Epoch: 6 \tTraining Loss: 0.000152\n",
      "Epoch: 7 \tTraining Loss: 0.000152\n",
      "Epoch: 8 \tTraining Loss: 0.000152\n",
      "Epoch: 9 \tTraining Loss: 0.000152\n",
      "Epoch: 10 \tTraining Loss: 0.000152\n",
      "Epoch: 11 \tTraining Loss: 0.000152\n",
      "Epoch: 12 \tTraining Loss: 0.000152\n",
      "Epoch: 13 \tTraining Loss: 0.000152\n",
      "Epoch: 14 \tTraining Loss: 0.000152\n",
      "Epoch: 15 \tTraining Loss: 0.000152\n",
      "Epoch: 16 \tTraining Loss: 0.000152\n",
      "Epoch: 17 \tTraining Loss: 0.000152\n",
      "Epoch: 18 \tTraining Loss: 0.000152\n",
      "Epoch: 1 \tTraining Loss: 0.000247\n",
      "Epoch: 2 \tTraining Loss: 0.000247\n",
      "Epoch: 3 \tTraining Loss: 0.000247\n",
      "Epoch: 4 \tTraining Loss: 0.000247\n",
      "Epoch: 5 \tTraining Loss: 0.000247\n",
      "Epoch: 6 \tTraining Loss: 0.000247\n",
      "Epoch: 7 \tTraining Loss: 0.000247\n",
      "Epoch: 8 \tTraining Loss: 0.000247\n",
      "Epoch: 9 \tTraining Loss: 0.000247\n",
      "Epoch: 10 \tTraining Loss: 0.000247\n",
      "Epoch: 11 \tTraining Loss: 0.000247\n",
      "Epoch: 12 \tTraining Loss: 0.000247\n",
      "Epoch: 13 \tTraining Loss: 0.000247\n",
      "Epoch: 14 \tTraining Loss: 0.000247\n",
      "Epoch: 15 \tTraining Loss: 0.000247\n",
      "Epoch: 16 \tTraining Loss: 0.000247\n",
      "Epoch: 17 \tTraining Loss: 0.000247\n",
      "Epoch: 18 \tTraining Loss: 0.000247\n",
      "Epoch: 1 \tTraining Loss: 0.000184\n",
      "Epoch: 2 \tTraining Loss: 0.000184\n",
      "Epoch: 3 \tTraining Loss: 0.000184\n",
      "Epoch: 4 \tTraining Loss: 0.000184\n",
      "Epoch: 5 \tTraining Loss: 0.000184\n",
      "Epoch: 6 \tTraining Loss: 0.000184\n",
      "Epoch: 7 \tTraining Loss: 0.000184\n",
      "Epoch: 8 \tTraining Loss: 0.000184\n",
      "Epoch: 9 \tTraining Loss: 0.000184\n",
      "Epoch: 10 \tTraining Loss: 0.000184\n",
      "Epoch: 11 \tTraining Loss: 0.000184\n",
      "Epoch: 12 \tTraining Loss: 0.000184\n",
      "Epoch: 13 \tTraining Loss: 0.000184\n",
      "Epoch: 14 \tTraining Loss: 0.000184\n",
      "Epoch: 15 \tTraining Loss: 0.000184\n",
      "Epoch: 16 \tTraining Loss: 0.000184\n",
      "Epoch: 17 \tTraining Loss: 0.000184\n",
      "Epoch: 18 \tTraining Loss: 0.000184\n",
      "Epoch: 1 \tTraining Loss: 0.000200\n",
      "Epoch: 2 \tTraining Loss: 0.000200\n",
      "Epoch: 3 \tTraining Loss: 0.000200\n",
      "Epoch: 4 \tTraining Loss: 0.000200\n",
      "Epoch: 5 \tTraining Loss: 0.000200\n",
      "Epoch: 6 \tTraining Loss: 0.000200\n",
      "Epoch: 7 \tTraining Loss: 0.000200\n",
      "Epoch: 8 \tTraining Loss: 0.000200\n",
      "Epoch: 9 \tTraining Loss: 0.000200\n",
      "Epoch: 10 \tTraining Loss: 0.000200\n",
      "Epoch: 11 \tTraining Loss: 0.000200\n",
      "Epoch: 12 \tTraining Loss: 0.000200\n",
      "Epoch: 13 \tTraining Loss: 0.000200\n",
      "Epoch: 14 \tTraining Loss: 0.000200\n",
      "Epoch: 15 \tTraining Loss: 0.000200\n",
      "Epoch: 16 \tTraining Loss: 0.000200\n",
      "Epoch: 17 \tTraining Loss: 0.000200\n",
      "Epoch: 18 \tTraining Loss: 0.000200\n",
      "Epoch: 1 \tTraining Loss: 0.000126\n",
      "Epoch: 2 \tTraining Loss: 0.000126\n",
      "Epoch: 3 \tTraining Loss: 0.000126\n",
      "Epoch: 4 \tTraining Loss: 0.000126\n",
      "Epoch: 5 \tTraining Loss: 0.000126\n",
      "Epoch: 6 \tTraining Loss: 0.000126\n",
      "Epoch: 7 \tTraining Loss: 0.000126\n",
      "Epoch: 8 \tTraining Loss: 0.000126\n",
      "Epoch: 9 \tTraining Loss: 0.000126\n",
      "Epoch: 10 \tTraining Loss: 0.000126\n",
      "Epoch: 11 \tTraining Loss: 0.000126\n",
      "Epoch: 12 \tTraining Loss: 0.000126\n",
      "Epoch: 13 \tTraining Loss: 0.000126\n",
      "Epoch: 14 \tTraining Loss: 0.000126\n",
      "Epoch: 15 \tTraining Loss: 0.000126\n",
      "Epoch: 16 \tTraining Loss: 0.000126\n",
      "Epoch: 17 \tTraining Loss: 0.000126\n",
      "Epoch: 18 \tTraining Loss: 0.000126\n",
      "Epoch: 1 \tTraining Loss: 0.000296\n",
      "Epoch: 2 \tTraining Loss: 0.000296\n",
      "Epoch: 3 \tTraining Loss: 0.000296\n",
      "Epoch: 4 \tTraining Loss: 0.000296\n",
      "Epoch: 5 \tTraining Loss: 0.000296\n",
      "Epoch: 6 \tTraining Loss: 0.000296\n",
      "Epoch: 7 \tTraining Loss: 0.000296\n",
      "Epoch: 8 \tTraining Loss: 0.000296\n",
      "Epoch: 9 \tTraining Loss: 0.000296\n",
      "Epoch: 10 \tTraining Loss: 0.000296\n",
      "Epoch: 11 \tTraining Loss: 0.000296\n",
      "Epoch: 12 \tTraining Loss: 0.000296\n",
      "Epoch: 13 \tTraining Loss: 0.000296\n",
      "Epoch: 14 \tTraining Loss: 0.000296\n",
      "Epoch: 15 \tTraining Loss: 0.000296\n",
      "Epoch: 16 \tTraining Loss: 0.000296\n",
      "Epoch: 17 \tTraining Loss: 0.000296\n",
      "Epoch: 18 \tTraining Loss: 0.000296\n",
      "Epoch: 1 \tTraining Loss: 0.000210\n",
      "Epoch: 2 \tTraining Loss: 0.000210\n",
      "Epoch: 3 \tTraining Loss: 0.000210\n",
      "Epoch: 4 \tTraining Loss: 0.000210\n",
      "Epoch: 5 \tTraining Loss: 0.000210\n",
      "Epoch: 6 \tTraining Loss: 0.000210\n",
      "Epoch: 7 \tTraining Loss: 0.000210\n",
      "Epoch: 8 \tTraining Loss: 0.000210\n",
      "Epoch: 9 \tTraining Loss: 0.000210\n",
      "Epoch: 10 \tTraining Loss: 0.000210\n",
      "Epoch: 11 \tTraining Loss: 0.000210\n",
      "Epoch: 12 \tTraining Loss: 0.000210\n",
      "Epoch: 13 \tTraining Loss: 0.000210\n",
      "Epoch: 14 \tTraining Loss: 0.000210\n",
      "Epoch: 15 \tTraining Loss: 0.000210\n",
      "Epoch: 16 \tTraining Loss: 0.000210\n",
      "Epoch: 17 \tTraining Loss: 0.000210\n",
      "Epoch: 18 \tTraining Loss: 0.000210\n",
      "Epoch: 1 \tTraining Loss: 0.000119\n",
      "Epoch: 2 \tTraining Loss: 0.000119\n",
      "Epoch: 3 \tTraining Loss: 0.000119\n",
      "Epoch: 4 \tTraining Loss: 0.000119\n",
      "Epoch: 5 \tTraining Loss: 0.000119\n",
      "Epoch: 6 \tTraining Loss: 0.000119\n",
      "Epoch: 7 \tTraining Loss: 0.000119\n",
      "Epoch: 8 \tTraining Loss: 0.000119\n",
      "Epoch: 9 \tTraining Loss: 0.000119\n",
      "Epoch: 10 \tTraining Loss: 0.000119\n",
      "Epoch: 11 \tTraining Loss: 0.000119\n",
      "Epoch: 12 \tTraining Loss: 0.000119\n",
      "Epoch: 13 \tTraining Loss: 0.000119\n",
      "Epoch: 14 \tTraining Loss: 0.000119\n",
      "Epoch: 15 \tTraining Loss: 0.000119\n",
      "Epoch: 16 \tTraining Loss: 0.000119\n",
      "Epoch: 17 \tTraining Loss: 0.000119\n",
      "Epoch: 18 \tTraining Loss: 0.000119\n",
      "Epoch: 1 \tTraining Loss: 0.000107\n",
      "Epoch: 2 \tTraining Loss: 0.000107\n",
      "Epoch: 3 \tTraining Loss: 0.000107\n",
      "Epoch: 4 \tTraining Loss: 0.000107\n",
      "Epoch: 5 \tTraining Loss: 0.000107\n",
      "Epoch: 6 \tTraining Loss: 0.000107\n",
      "Epoch: 7 \tTraining Loss: 0.000107\n",
      "Epoch: 8 \tTraining Loss: 0.000107\n",
      "Epoch: 9 \tTraining Loss: 0.000107\n",
      "Epoch: 10 \tTraining Loss: 0.000107\n",
      "Epoch: 11 \tTraining Loss: 0.000107\n",
      "Epoch: 12 \tTraining Loss: 0.000107\n",
      "Epoch: 13 \tTraining Loss: 0.000107\n",
      "Epoch: 14 \tTraining Loss: 0.000107\n",
      "Epoch: 15 \tTraining Loss: 0.000107\n",
      "Epoch: 16 \tTraining Loss: 0.000107\n",
      "Epoch: 17 \tTraining Loss: 0.000107\n",
      "Epoch: 18 \tTraining Loss: 0.000107\n",
      "Epoch: 1 \tTraining Loss: 0.000190\n",
      "Epoch: 2 \tTraining Loss: 0.000190\n",
      "Epoch: 3 \tTraining Loss: 0.000190\n",
      "Epoch: 4 \tTraining Loss: 0.000190\n",
      "Epoch: 5 \tTraining Loss: 0.000190\n",
      "Epoch: 6 \tTraining Loss: 0.000190\n",
      "Epoch: 7 \tTraining Loss: 0.000190\n",
      "Epoch: 8 \tTraining Loss: 0.000190\n",
      "Epoch: 9 \tTraining Loss: 0.000190\n",
      "Epoch: 10 \tTraining Loss: 0.000190\n",
      "Epoch: 11 \tTraining Loss: 0.000190\n",
      "Epoch: 12 \tTraining Loss: 0.000190\n",
      "Epoch: 13 \tTraining Loss: 0.000190\n",
      "Epoch: 14 \tTraining Loss: 0.000190\n",
      "Epoch: 15 \tTraining Loss: 0.000190\n",
      "Epoch: 16 \tTraining Loss: 0.000190\n",
      "Epoch: 17 \tTraining Loss: 0.000190\n",
      "Epoch: 18 \tTraining Loss: 0.000190\n",
      "Epoch: 1 \tTraining Loss: 0.000324\n",
      "Epoch: 2 \tTraining Loss: 0.000324\n",
      "Epoch: 3 \tTraining Loss: 0.000324\n",
      "Epoch: 4 \tTraining Loss: 0.000324\n",
      "Epoch: 5 \tTraining Loss: 0.000324\n",
      "Epoch: 6 \tTraining Loss: 0.000324\n",
      "Epoch: 7 \tTraining Loss: 0.000324\n",
      "Epoch: 8 \tTraining Loss: 0.000324\n",
      "Epoch: 9 \tTraining Loss: 0.000324\n",
      "Epoch: 10 \tTraining Loss: 0.000324\n",
      "Epoch: 11 \tTraining Loss: 0.000324\n",
      "Epoch: 12 \tTraining Loss: 0.000324\n",
      "Epoch: 13 \tTraining Loss: 0.000324\n",
      "Epoch: 14 \tTraining Loss: 0.000324\n",
      "Epoch: 15 \tTraining Loss: 0.000324\n",
      "Epoch: 16 \tTraining Loss: 0.000324\n",
      "Epoch: 17 \tTraining Loss: 0.000324\n",
      "Epoch: 18 \tTraining Loss: 0.000324\n",
      "Epoch: 1 \tTraining Loss: 0.000245\n",
      "Epoch: 2 \tTraining Loss: 0.000245\n",
      "Epoch: 3 \tTraining Loss: 0.000245\n",
      "Epoch: 4 \tTraining Loss: 0.000245\n",
      "Epoch: 5 \tTraining Loss: 0.000245\n",
      "Epoch: 6 \tTraining Loss: 0.000245\n",
      "Epoch: 7 \tTraining Loss: 0.000245\n",
      "Epoch: 8 \tTraining Loss: 0.000245\n",
      "Epoch: 9 \tTraining Loss: 0.000245\n",
      "Epoch: 10 \tTraining Loss: 0.000245\n",
      "Epoch: 11 \tTraining Loss: 0.000245\n",
      "Epoch: 12 \tTraining Loss: 0.000245\n",
      "Epoch: 13 \tTraining Loss: 0.000245\n",
      "Epoch: 14 \tTraining Loss: 0.000245\n",
      "Epoch: 15 \tTraining Loss: 0.000245\n",
      "Epoch: 16 \tTraining Loss: 0.000245\n",
      "Epoch: 17 \tTraining Loss: 0.000245\n",
      "Epoch: 18 \tTraining Loss: 0.000245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000188\n",
      "Epoch: 2 \tTraining Loss: 0.000188\n",
      "Epoch: 3 \tTraining Loss: 0.000188\n",
      "Epoch: 4 \tTraining Loss: 0.000188\n",
      "Epoch: 5 \tTraining Loss: 0.000188\n",
      "Epoch: 6 \tTraining Loss: 0.000188\n",
      "Epoch: 7 \tTraining Loss: 0.000188\n",
      "Epoch: 8 \tTraining Loss: 0.000188\n",
      "Epoch: 9 \tTraining Loss: 0.000188\n",
      "Epoch: 10 \tTraining Loss: 0.000188\n",
      "Epoch: 11 \tTraining Loss: 0.000188\n",
      "Epoch: 12 \tTraining Loss: 0.000188\n",
      "Epoch: 13 \tTraining Loss: 0.000188\n",
      "Epoch: 14 \tTraining Loss: 0.000188\n",
      "Epoch: 15 \tTraining Loss: 0.000188\n",
      "Epoch: 16 \tTraining Loss: 0.000188\n",
      "Epoch: 17 \tTraining Loss: 0.000188\n",
      "Epoch: 18 \tTraining Loss: 0.000188\n",
      "Epoch: 1 \tTraining Loss: 0.000343\n",
      "Epoch: 2 \tTraining Loss: 0.000343\n",
      "Epoch: 3 \tTraining Loss: 0.000343\n",
      "Epoch: 4 \tTraining Loss: 0.000343\n",
      "Epoch: 5 \tTraining Loss: 0.000343\n",
      "Epoch: 6 \tTraining Loss: 0.000343\n",
      "Epoch: 7 \tTraining Loss: 0.000343\n",
      "Epoch: 8 \tTraining Loss: 0.000343\n",
      "Epoch: 9 \tTraining Loss: 0.000343\n",
      "Epoch: 10 \tTraining Loss: 0.000343\n",
      "Epoch: 11 \tTraining Loss: 0.000343\n",
      "Epoch: 12 \tTraining Loss: 0.000343\n",
      "Epoch: 13 \tTraining Loss: 0.000343\n",
      "Epoch: 14 \tTraining Loss: 0.000343\n",
      "Epoch: 15 \tTraining Loss: 0.000343\n",
      "Epoch: 16 \tTraining Loss: 0.000343\n",
      "Epoch: 17 \tTraining Loss: 0.000343\n",
      "Epoch: 18 \tTraining Loss: 0.000343\n",
      "Epoch: 1 \tTraining Loss: 0.000190\n",
      "Epoch: 2 \tTraining Loss: 0.000190\n",
      "Epoch: 3 \tTraining Loss: 0.000190\n",
      "Epoch: 4 \tTraining Loss: 0.000190\n",
      "Epoch: 5 \tTraining Loss: 0.000190\n",
      "Epoch: 6 \tTraining Loss: 0.000190\n",
      "Epoch: 7 \tTraining Loss: 0.000190\n",
      "Epoch: 8 \tTraining Loss: 0.000190\n",
      "Epoch: 9 \tTraining Loss: 0.000190\n",
      "Epoch: 10 \tTraining Loss: 0.000190\n",
      "Epoch: 11 \tTraining Loss: 0.000190\n",
      "Epoch: 12 \tTraining Loss: 0.000190\n",
      "Epoch: 13 \tTraining Loss: 0.000190\n",
      "Epoch: 14 \tTraining Loss: 0.000190\n",
      "Epoch: 15 \tTraining Loss: 0.000190\n",
      "Epoch: 16 \tTraining Loss: 0.000190\n",
      "Epoch: 17 \tTraining Loss: 0.000190\n",
      "Epoch: 18 \tTraining Loss: 0.000190\n",
      "Epoch: 1 \tTraining Loss: 0.000225\n",
      "Epoch: 2 \tTraining Loss: 0.000225\n",
      "Epoch: 3 \tTraining Loss: 0.000225\n",
      "Epoch: 4 \tTraining Loss: 0.000225\n",
      "Epoch: 5 \tTraining Loss: 0.000225\n",
      "Epoch: 6 \tTraining Loss: 0.000225\n",
      "Epoch: 7 \tTraining Loss: 0.000225\n",
      "Epoch: 8 \tTraining Loss: 0.000225\n",
      "Epoch: 9 \tTraining Loss: 0.000225\n",
      "Epoch: 10 \tTraining Loss: 0.000225\n",
      "Epoch: 11 \tTraining Loss: 0.000225\n",
      "Epoch: 12 \tTraining Loss: 0.000225\n",
      "Epoch: 13 \tTraining Loss: 0.000225\n",
      "Epoch: 14 \tTraining Loss: 0.000225\n",
      "Epoch: 15 \tTraining Loss: 0.000225\n",
      "Epoch: 16 \tTraining Loss: 0.000225\n",
      "Epoch: 17 \tTraining Loss: 0.000225\n",
      "Epoch: 18 \tTraining Loss: 0.000225\n",
      "Epoch: 1 \tTraining Loss: 0.000167\n",
      "Epoch: 2 \tTraining Loss: 0.000167\n",
      "Epoch: 3 \tTraining Loss: 0.000167\n",
      "Epoch: 4 \tTraining Loss: 0.000167\n",
      "Epoch: 5 \tTraining Loss: 0.000167\n",
      "Epoch: 6 \tTraining Loss: 0.000167\n",
      "Epoch: 7 \tTraining Loss: 0.000167\n",
      "Epoch: 8 \tTraining Loss: 0.000167\n",
      "Epoch: 9 \tTraining Loss: 0.000167\n",
      "Epoch: 10 \tTraining Loss: 0.000167\n",
      "Epoch: 11 \tTraining Loss: 0.000167\n",
      "Epoch: 12 \tTraining Loss: 0.000167\n",
      "Epoch: 13 \tTraining Loss: 0.000167\n",
      "Epoch: 14 \tTraining Loss: 0.000167\n",
      "Epoch: 15 \tTraining Loss: 0.000167\n",
      "Epoch: 16 \tTraining Loss: 0.000167\n",
      "Epoch: 17 \tTraining Loss: 0.000167\n",
      "Epoch: 18 \tTraining Loss: 0.000167\n",
      "Epoch: 1 \tTraining Loss: 0.000439\n",
      "Epoch: 2 \tTraining Loss: 0.000439\n",
      "Epoch: 3 \tTraining Loss: 0.000439\n",
      "Epoch: 4 \tTraining Loss: 0.000439\n",
      "Epoch: 5 \tTraining Loss: 0.000439\n",
      "Epoch: 6 \tTraining Loss: 0.000439\n",
      "Epoch: 7 \tTraining Loss: 0.000439\n",
      "Epoch: 8 \tTraining Loss: 0.000439\n",
      "Epoch: 9 \tTraining Loss: 0.000439\n",
      "Epoch: 10 \tTraining Loss: 0.000439\n",
      "Epoch: 11 \tTraining Loss: 0.000439\n",
      "Epoch: 12 \tTraining Loss: 0.000439\n",
      "Epoch: 13 \tTraining Loss: 0.000439\n",
      "Epoch: 14 \tTraining Loss: 0.000439\n",
      "Epoch: 15 \tTraining Loss: 0.000439\n",
      "Epoch: 16 \tTraining Loss: 0.000439\n",
      "Epoch: 17 \tTraining Loss: 0.000439\n",
      "Epoch: 18 \tTraining Loss: 0.000439\n",
      "Epoch: 1 \tTraining Loss: 0.000166\n",
      "Epoch: 2 \tTraining Loss: 0.000166\n",
      "Epoch: 3 \tTraining Loss: 0.000166\n",
      "Epoch: 4 \tTraining Loss: 0.000166\n",
      "Epoch: 5 \tTraining Loss: 0.000166\n",
      "Epoch: 6 \tTraining Loss: 0.000166\n",
      "Epoch: 7 \tTraining Loss: 0.000166\n",
      "Epoch: 8 \tTraining Loss: 0.000166\n",
      "Epoch: 9 \tTraining Loss: 0.000166\n",
      "Epoch: 10 \tTraining Loss: 0.000166\n",
      "Epoch: 11 \tTraining Loss: 0.000166\n",
      "Epoch: 12 \tTraining Loss: 0.000166\n",
      "Epoch: 13 \tTraining Loss: 0.000166\n",
      "Epoch: 14 \tTraining Loss: 0.000166\n",
      "Epoch: 15 \tTraining Loss: 0.000166\n",
      "Epoch: 16 \tTraining Loss: 0.000166\n",
      "Epoch: 17 \tTraining Loss: 0.000166\n",
      "Epoch: 18 \tTraining Loss: 0.000166\n",
      "Epoch: 1 \tTraining Loss: 0.000105\n",
      "Epoch: 2 \tTraining Loss: 0.000105\n",
      "Epoch: 3 \tTraining Loss: 0.000105\n",
      "Epoch: 4 \tTraining Loss: 0.000105\n",
      "Epoch: 5 \tTraining Loss: 0.000105\n",
      "Epoch: 6 \tTraining Loss: 0.000105\n",
      "Epoch: 7 \tTraining Loss: 0.000105\n",
      "Epoch: 8 \tTraining Loss: 0.000105\n",
      "Epoch: 9 \tTraining Loss: 0.000105\n",
      "Epoch: 10 \tTraining Loss: 0.000105\n",
      "Epoch: 11 \tTraining Loss: 0.000105\n",
      "Epoch: 12 \tTraining Loss: 0.000105\n",
      "Epoch: 13 \tTraining Loss: 0.000105\n",
      "Epoch: 14 \tTraining Loss: 0.000105\n",
      "Epoch: 15 \tTraining Loss: 0.000105\n",
      "Epoch: 16 \tTraining Loss: 0.000105\n",
      "Epoch: 17 \tTraining Loss: 0.000105\n",
      "Epoch: 18 \tTraining Loss: 0.000105\n",
      "Epoch: 1 \tTraining Loss: 0.000090\n",
      "Epoch: 2 \tTraining Loss: 0.000090\n",
      "Epoch: 3 \tTraining Loss: 0.000090\n",
      "Epoch: 4 \tTraining Loss: 0.000090\n",
      "Epoch: 5 \tTraining Loss: 0.000090\n",
      "Epoch: 6 \tTraining Loss: 0.000090\n",
      "Epoch: 7 \tTraining Loss: 0.000090\n",
      "Epoch: 8 \tTraining Loss: 0.000090\n",
      "Epoch: 9 \tTraining Loss: 0.000090\n",
      "Epoch: 10 \tTraining Loss: 0.000090\n",
      "Epoch: 11 \tTraining Loss: 0.000090\n",
      "Epoch: 12 \tTraining Loss: 0.000090\n",
      "Epoch: 13 \tTraining Loss: 0.000090\n",
      "Epoch: 14 \tTraining Loss: 0.000090\n",
      "Epoch: 15 \tTraining Loss: 0.000090\n",
      "Epoch: 16 \tTraining Loss: 0.000090\n",
      "Epoch: 17 \tTraining Loss: 0.000090\n",
      "Epoch: 18 \tTraining Loss: 0.000090\n",
      "Epoch: 1 \tTraining Loss: 0.000235\n",
      "Epoch: 2 \tTraining Loss: 0.000235\n",
      "Epoch: 3 \tTraining Loss: 0.000235\n",
      "Epoch: 4 \tTraining Loss: 0.000235\n",
      "Epoch: 5 \tTraining Loss: 0.000235\n",
      "Epoch: 6 \tTraining Loss: 0.000235\n",
      "Epoch: 7 \tTraining Loss: 0.000235\n",
      "Epoch: 8 \tTraining Loss: 0.000235\n",
      "Epoch: 9 \tTraining Loss: 0.000235\n",
      "Epoch: 10 \tTraining Loss: 0.000235\n",
      "Epoch: 11 \tTraining Loss: 0.000235\n",
      "Epoch: 12 \tTraining Loss: 0.000235\n",
      "Epoch: 13 \tTraining Loss: 0.000235\n",
      "Epoch: 14 \tTraining Loss: 0.000235\n",
      "Epoch: 15 \tTraining Loss: 0.000235\n",
      "Epoch: 16 \tTraining Loss: 0.000235\n",
      "Epoch: 17 \tTraining Loss: 0.000235\n",
      "Epoch: 18 \tTraining Loss: 0.000235\n",
      "Epoch: 1 \tTraining Loss: 0.000062\n",
      "Epoch: 2 \tTraining Loss: 0.000062\n",
      "Epoch: 3 \tTraining Loss: 0.000062\n",
      "Epoch: 4 \tTraining Loss: 0.000062\n",
      "Epoch: 5 \tTraining Loss: 0.000062\n",
      "Epoch: 6 \tTraining Loss: 0.000062\n",
      "Epoch: 7 \tTraining Loss: 0.000062\n",
      "Epoch: 8 \tTraining Loss: 0.000062\n",
      "Epoch: 9 \tTraining Loss: 0.000062\n",
      "Epoch: 10 \tTraining Loss: 0.000062\n",
      "Epoch: 11 \tTraining Loss: 0.000062\n",
      "Epoch: 12 \tTraining Loss: 0.000062\n",
      "Epoch: 13 \tTraining Loss: 0.000062\n",
      "Epoch: 14 \tTraining Loss: 0.000062\n",
      "Epoch: 15 \tTraining Loss: 0.000062\n",
      "Epoch: 16 \tTraining Loss: 0.000062\n",
      "Epoch: 17 \tTraining Loss: 0.000062\n",
      "Epoch: 18 \tTraining Loss: 0.000062\n",
      "Epoch: 1 \tTraining Loss: 0.000137\n",
      "Epoch: 2 \tTraining Loss: 0.000137\n",
      "Epoch: 3 \tTraining Loss: 0.000137\n",
      "Epoch: 4 \tTraining Loss: 0.000137\n",
      "Epoch: 5 \tTraining Loss: 0.000137\n",
      "Epoch: 6 \tTraining Loss: 0.000137\n",
      "Epoch: 7 \tTraining Loss: 0.000137\n",
      "Epoch: 8 \tTraining Loss: 0.000137\n",
      "Epoch: 9 \tTraining Loss: 0.000137\n",
      "Epoch: 10 \tTraining Loss: 0.000137\n",
      "Epoch: 11 \tTraining Loss: 0.000137\n",
      "Epoch: 12 \tTraining Loss: 0.000137\n",
      "Epoch: 13 \tTraining Loss: 0.000137\n",
      "Epoch: 14 \tTraining Loss: 0.000137\n",
      "Epoch: 15 \tTraining Loss: 0.000137\n",
      "Epoch: 16 \tTraining Loss: 0.000137\n",
      "Epoch: 17 \tTraining Loss: 0.000137\n",
      "Epoch: 18 \tTraining Loss: 0.000137\n",
      "Epoch: 1 \tTraining Loss: 0.000169\n",
      "Epoch: 2 \tTraining Loss: 0.000169\n",
      "Epoch: 3 \tTraining Loss: 0.000169\n",
      "Epoch: 4 \tTraining Loss: 0.000169\n",
      "Epoch: 5 \tTraining Loss: 0.000169\n",
      "Epoch: 6 \tTraining Loss: 0.000169\n",
      "Epoch: 7 \tTraining Loss: 0.000169\n",
      "Epoch: 8 \tTraining Loss: 0.000169\n",
      "Epoch: 9 \tTraining Loss: 0.000169\n",
      "Epoch: 10 \tTraining Loss: 0.000169\n",
      "Epoch: 11 \tTraining Loss: 0.000169\n",
      "Epoch: 12 \tTraining Loss: 0.000169\n",
      "Epoch: 13 \tTraining Loss: 0.000169\n",
      "Epoch: 14 \tTraining Loss: 0.000169\n",
      "Epoch: 15 \tTraining Loss: 0.000169\n",
      "Epoch: 16 \tTraining Loss: 0.000169\n",
      "Epoch: 17 \tTraining Loss: 0.000169\n",
      "Epoch: 18 \tTraining Loss: 0.000169\n",
      "Epoch: 1 \tTraining Loss: 0.000169\n",
      "Epoch: 2 \tTraining Loss: 0.000169\n",
      "Epoch: 3 \tTraining Loss: 0.000169\n",
      "Epoch: 4 \tTraining Loss: 0.000169\n",
      "Epoch: 5 \tTraining Loss: 0.000169\n",
      "Epoch: 6 \tTraining Loss: 0.000169\n",
      "Epoch: 7 \tTraining Loss: 0.000169\n",
      "Epoch: 8 \tTraining Loss: 0.000169\n",
      "Epoch: 9 \tTraining Loss: 0.000169\n",
      "Epoch: 10 \tTraining Loss: 0.000169\n",
      "Epoch: 11 \tTraining Loss: 0.000169\n",
      "Epoch: 12 \tTraining Loss: 0.000169\n",
      "Epoch: 13 \tTraining Loss: 0.000169\n",
      "Epoch: 14 \tTraining Loss: 0.000169\n",
      "Epoch: 15 \tTraining Loss: 0.000169\n",
      "Epoch: 16 \tTraining Loss: 0.000169\n",
      "Epoch: 17 \tTraining Loss: 0.000169\n",
      "Epoch: 18 \tTraining Loss: 0.000169\n",
      "Epoch: 1 \tTraining Loss: 0.000140\n",
      "Epoch: 2 \tTraining Loss: 0.000140\n",
      "Epoch: 3 \tTraining Loss: 0.000140\n",
      "Epoch: 4 \tTraining Loss: 0.000140\n",
      "Epoch: 5 \tTraining Loss: 0.000140\n",
      "Epoch: 6 \tTraining Loss: 0.000140\n",
      "Epoch: 7 \tTraining Loss: 0.000140\n",
      "Epoch: 8 \tTraining Loss: 0.000140\n",
      "Epoch: 9 \tTraining Loss: 0.000140\n",
      "Epoch: 10 \tTraining Loss: 0.000140\n",
      "Epoch: 11 \tTraining Loss: 0.000140\n",
      "Epoch: 12 \tTraining Loss: 0.000140\n",
      "Epoch: 13 \tTraining Loss: 0.000140\n",
      "Epoch: 14 \tTraining Loss: 0.000140\n",
      "Epoch: 15 \tTraining Loss: 0.000140\n",
      "Epoch: 16 \tTraining Loss: 0.000140\n",
      "Epoch: 17 \tTraining Loss: 0.000140\n",
      "Epoch: 18 \tTraining Loss: 0.000140\n",
      "Epoch: 1 \tTraining Loss: 0.000312\n",
      "Epoch: 2 \tTraining Loss: 0.000312\n",
      "Epoch: 3 \tTraining Loss: 0.000312\n",
      "Epoch: 4 \tTraining Loss: 0.000312\n",
      "Epoch: 5 \tTraining Loss: 0.000312\n",
      "Epoch: 6 \tTraining Loss: 0.000312\n",
      "Epoch: 7 \tTraining Loss: 0.000312\n",
      "Epoch: 8 \tTraining Loss: 0.000312\n",
      "Epoch: 9 \tTraining Loss: 0.000312\n",
      "Epoch: 10 \tTraining Loss: 0.000312\n",
      "Epoch: 11 \tTraining Loss: 0.000312\n",
      "Epoch: 12 \tTraining Loss: 0.000312\n",
      "Epoch: 13 \tTraining Loss: 0.000312\n",
      "Epoch: 14 \tTraining Loss: 0.000312\n",
      "Epoch: 15 \tTraining Loss: 0.000312\n",
      "Epoch: 16 \tTraining Loss: 0.000312\n",
      "Epoch: 17 \tTraining Loss: 0.000312\n",
      "Epoch: 18 \tTraining Loss: 0.000312\n",
      "Epoch: 1 \tTraining Loss: 0.000193\n",
      "Epoch: 2 \tTraining Loss: 0.000193\n",
      "Epoch: 3 \tTraining Loss: 0.000193\n",
      "Epoch: 4 \tTraining Loss: 0.000193\n",
      "Epoch: 5 \tTraining Loss: 0.000193\n",
      "Epoch: 6 \tTraining Loss: 0.000193\n",
      "Epoch: 7 \tTraining Loss: 0.000193\n",
      "Epoch: 8 \tTraining Loss: 0.000193\n",
      "Epoch: 9 \tTraining Loss: 0.000193\n",
      "Epoch: 10 \tTraining Loss: 0.000193\n",
      "Epoch: 11 \tTraining Loss: 0.000193\n",
      "Epoch: 12 \tTraining Loss: 0.000193\n",
      "Epoch: 13 \tTraining Loss: 0.000193\n",
      "Epoch: 14 \tTraining Loss: 0.000193\n",
      "Epoch: 15 \tTraining Loss: 0.000193\n",
      "Epoch: 16 \tTraining Loss: 0.000193\n",
      "Epoch: 17 \tTraining Loss: 0.000193\n",
      "Epoch: 18 \tTraining Loss: 0.000193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000150\n",
      "Epoch: 2 \tTraining Loss: 0.000150\n",
      "Epoch: 3 \tTraining Loss: 0.000150\n",
      "Epoch: 4 \tTraining Loss: 0.000150\n",
      "Epoch: 5 \tTraining Loss: 0.000150\n",
      "Epoch: 6 \tTraining Loss: 0.000150\n",
      "Epoch: 7 \tTraining Loss: 0.000150\n",
      "Epoch: 8 \tTraining Loss: 0.000150\n",
      "Epoch: 9 \tTraining Loss: 0.000150\n",
      "Epoch: 10 \tTraining Loss: 0.000150\n",
      "Epoch: 11 \tTraining Loss: 0.000150\n",
      "Epoch: 12 \tTraining Loss: 0.000150\n",
      "Epoch: 13 \tTraining Loss: 0.000150\n",
      "Epoch: 14 \tTraining Loss: 0.000150\n",
      "Epoch: 15 \tTraining Loss: 0.000150\n",
      "Epoch: 16 \tTraining Loss: 0.000150\n",
      "Epoch: 17 \tTraining Loss: 0.000150\n",
      "Epoch: 18 \tTraining Loss: 0.000150\n",
      "Epoch: 1 \tTraining Loss: 0.000145\n",
      "Epoch: 2 \tTraining Loss: 0.000145\n",
      "Epoch: 3 \tTraining Loss: 0.000145\n",
      "Epoch: 4 \tTraining Loss: 0.000145\n",
      "Epoch: 5 \tTraining Loss: 0.000145\n",
      "Epoch: 6 \tTraining Loss: 0.000145\n",
      "Epoch: 7 \tTraining Loss: 0.000145\n",
      "Epoch: 8 \tTraining Loss: 0.000145\n",
      "Epoch: 9 \tTraining Loss: 0.000145\n",
      "Epoch: 10 \tTraining Loss: 0.000145\n",
      "Epoch: 11 \tTraining Loss: 0.000145\n",
      "Epoch: 12 \tTraining Loss: 0.000145\n",
      "Epoch: 13 \tTraining Loss: 0.000145\n",
      "Epoch: 14 \tTraining Loss: 0.000145\n",
      "Epoch: 15 \tTraining Loss: 0.000145\n",
      "Epoch: 16 \tTraining Loss: 0.000145\n",
      "Epoch: 17 \tTraining Loss: 0.000145\n",
      "Epoch: 18 \tTraining Loss: 0.000145\n",
      "Epoch: 1 \tTraining Loss: 0.000186\n",
      "Epoch: 2 \tTraining Loss: 0.000186\n",
      "Epoch: 3 \tTraining Loss: 0.000186\n",
      "Epoch: 4 \tTraining Loss: 0.000186\n",
      "Epoch: 5 \tTraining Loss: 0.000186\n",
      "Epoch: 6 \tTraining Loss: 0.000186\n",
      "Epoch: 7 \tTraining Loss: 0.000186\n",
      "Epoch: 8 \tTraining Loss: 0.000186\n",
      "Epoch: 9 \tTraining Loss: 0.000186\n",
      "Epoch: 10 \tTraining Loss: 0.000186\n",
      "Epoch: 11 \tTraining Loss: 0.000186\n",
      "Epoch: 12 \tTraining Loss: 0.000186\n",
      "Epoch: 13 \tTraining Loss: 0.000186\n",
      "Epoch: 14 \tTraining Loss: 0.000186\n",
      "Epoch: 15 \tTraining Loss: 0.000186\n",
      "Epoch: 16 \tTraining Loss: 0.000186\n",
      "Epoch: 17 \tTraining Loss: 0.000186\n",
      "Epoch: 18 \tTraining Loss: 0.000186\n",
      "Epoch: 1 \tTraining Loss: 0.000159\n",
      "Epoch: 2 \tTraining Loss: 0.000159\n",
      "Epoch: 3 \tTraining Loss: 0.000159\n",
      "Epoch: 4 \tTraining Loss: 0.000159\n",
      "Epoch: 5 \tTraining Loss: 0.000159\n",
      "Epoch: 6 \tTraining Loss: 0.000159\n",
      "Epoch: 7 \tTraining Loss: 0.000159\n",
      "Epoch: 8 \tTraining Loss: 0.000159\n",
      "Epoch: 9 \tTraining Loss: 0.000159\n",
      "Epoch: 10 \tTraining Loss: 0.000159\n",
      "Epoch: 11 \tTraining Loss: 0.000159\n",
      "Epoch: 12 \tTraining Loss: 0.000159\n",
      "Epoch: 13 \tTraining Loss: 0.000159\n",
      "Epoch: 14 \tTraining Loss: 0.000159\n",
      "Epoch: 15 \tTraining Loss: 0.000159\n",
      "Epoch: 16 \tTraining Loss: 0.000159\n",
      "Epoch: 17 \tTraining Loss: 0.000159\n",
      "Epoch: 18 \tTraining Loss: 0.000159\n",
      "Epoch: 1 \tTraining Loss: 0.000267\n",
      "Epoch: 2 \tTraining Loss: 0.000267\n",
      "Epoch: 3 \tTraining Loss: 0.000267\n",
      "Epoch: 4 \tTraining Loss: 0.000267\n",
      "Epoch: 5 \tTraining Loss: 0.000267\n",
      "Epoch: 6 \tTraining Loss: 0.000267\n",
      "Epoch: 7 \tTraining Loss: 0.000267\n",
      "Epoch: 8 \tTraining Loss: 0.000267\n",
      "Epoch: 9 \tTraining Loss: 0.000267\n",
      "Epoch: 10 \tTraining Loss: 0.000267\n",
      "Epoch: 11 \tTraining Loss: 0.000267\n",
      "Epoch: 12 \tTraining Loss: 0.000267\n",
      "Epoch: 13 \tTraining Loss: 0.000267\n",
      "Epoch: 14 \tTraining Loss: 0.000267\n",
      "Epoch: 15 \tTraining Loss: 0.000267\n",
      "Epoch: 16 \tTraining Loss: 0.000267\n",
      "Epoch: 17 \tTraining Loss: 0.000267\n",
      "Epoch: 18 \tTraining Loss: 0.000267\n",
      "Epoch: 1 \tTraining Loss: 0.000208\n",
      "Epoch: 2 \tTraining Loss: 0.000208\n",
      "Epoch: 3 \tTraining Loss: 0.000208\n",
      "Epoch: 4 \tTraining Loss: 0.000208\n",
      "Epoch: 5 \tTraining Loss: 0.000208\n",
      "Epoch: 6 \tTraining Loss: 0.000208\n",
      "Epoch: 7 \tTraining Loss: 0.000208\n",
      "Epoch: 8 \tTraining Loss: 0.000208\n",
      "Epoch: 9 \tTraining Loss: 0.000208\n",
      "Epoch: 10 \tTraining Loss: 0.000208\n",
      "Epoch: 11 \tTraining Loss: 0.000208\n",
      "Epoch: 12 \tTraining Loss: 0.000208\n",
      "Epoch: 13 \tTraining Loss: 0.000208\n",
      "Epoch: 14 \tTraining Loss: 0.000208\n",
      "Epoch: 15 \tTraining Loss: 0.000208\n",
      "Epoch: 16 \tTraining Loss: 0.000208\n",
      "Epoch: 17 \tTraining Loss: 0.000208\n",
      "Epoch: 18 \tTraining Loss: 0.000208\n",
      "Epoch: 1 \tTraining Loss: 0.000113\n",
      "Epoch: 2 \tTraining Loss: 0.000113\n",
      "Epoch: 3 \tTraining Loss: 0.000113\n",
      "Epoch: 4 \tTraining Loss: 0.000113\n",
      "Epoch: 5 \tTraining Loss: 0.000113\n",
      "Epoch: 6 \tTraining Loss: 0.000113\n",
      "Epoch: 7 \tTraining Loss: 0.000113\n",
      "Epoch: 8 \tTraining Loss: 0.000113\n",
      "Epoch: 9 \tTraining Loss: 0.000113\n",
      "Epoch: 10 \tTraining Loss: 0.000113\n",
      "Epoch: 11 \tTraining Loss: 0.000113\n",
      "Epoch: 12 \tTraining Loss: 0.000113\n",
      "Epoch: 13 \tTraining Loss: 0.000113\n",
      "Epoch: 14 \tTraining Loss: 0.000113\n",
      "Epoch: 15 \tTraining Loss: 0.000113\n",
      "Epoch: 16 \tTraining Loss: 0.000113\n",
      "Epoch: 17 \tTraining Loss: 0.000113\n",
      "Epoch: 18 \tTraining Loss: 0.000113\n",
      "Epoch: 1 \tTraining Loss: 0.000113\n",
      "Epoch: 2 \tTraining Loss: 0.000113\n",
      "Epoch: 3 \tTraining Loss: 0.000113\n",
      "Epoch: 4 \tTraining Loss: 0.000113\n",
      "Epoch: 5 \tTraining Loss: 0.000113\n",
      "Epoch: 6 \tTraining Loss: 0.000113\n",
      "Epoch: 7 \tTraining Loss: 0.000113\n",
      "Epoch: 8 \tTraining Loss: 0.000113\n",
      "Epoch: 9 \tTraining Loss: 0.000113\n",
      "Epoch: 10 \tTraining Loss: 0.000113\n",
      "Epoch: 11 \tTraining Loss: 0.000113\n",
      "Epoch: 12 \tTraining Loss: 0.000113\n",
      "Epoch: 13 \tTraining Loss: 0.000113\n",
      "Epoch: 14 \tTraining Loss: 0.000113\n",
      "Epoch: 15 \tTraining Loss: 0.000113\n",
      "Epoch: 16 \tTraining Loss: 0.000113\n",
      "Epoch: 17 \tTraining Loss: 0.000113\n",
      "Epoch: 18 \tTraining Loss: 0.000113\n",
      "Epoch: 1 \tTraining Loss: 0.000263\n",
      "Epoch: 2 \tTraining Loss: 0.000263\n",
      "Epoch: 3 \tTraining Loss: 0.000263\n",
      "Epoch: 4 \tTraining Loss: 0.000263\n",
      "Epoch: 5 \tTraining Loss: 0.000263\n",
      "Epoch: 6 \tTraining Loss: 0.000263\n",
      "Epoch: 7 \tTraining Loss: 0.000263\n",
      "Epoch: 8 \tTraining Loss: 0.000263\n",
      "Epoch: 9 \tTraining Loss: 0.000263\n",
      "Epoch: 10 \tTraining Loss: 0.000263\n",
      "Epoch: 11 \tTraining Loss: 0.000263\n",
      "Epoch: 12 \tTraining Loss: 0.000263\n",
      "Epoch: 13 \tTraining Loss: 0.000263\n",
      "Epoch: 14 \tTraining Loss: 0.000263\n",
      "Epoch: 15 \tTraining Loss: 0.000263\n",
      "Epoch: 16 \tTraining Loss: 0.000263\n",
      "Epoch: 17 \tTraining Loss: 0.000263\n",
      "Epoch: 18 \tTraining Loss: 0.000263\n",
      "Epoch: 1 \tTraining Loss: 0.000252\n",
      "Epoch: 2 \tTraining Loss: 0.000252\n",
      "Epoch: 3 \tTraining Loss: 0.000252\n",
      "Epoch: 4 \tTraining Loss: 0.000252\n",
      "Epoch: 5 \tTraining Loss: 0.000252\n",
      "Epoch: 6 \tTraining Loss: 0.000252\n",
      "Epoch: 7 \tTraining Loss: 0.000252\n",
      "Epoch: 8 \tTraining Loss: 0.000252\n",
      "Epoch: 9 \tTraining Loss: 0.000252\n",
      "Epoch: 10 \tTraining Loss: 0.000252\n",
      "Epoch: 11 \tTraining Loss: 0.000252\n",
      "Epoch: 12 \tTraining Loss: 0.000252\n",
      "Epoch: 13 \tTraining Loss: 0.000252\n",
      "Epoch: 14 \tTraining Loss: 0.000252\n",
      "Epoch: 15 \tTraining Loss: 0.000252\n",
      "Epoch: 16 \tTraining Loss: 0.000252\n",
      "Epoch: 17 \tTraining Loss: 0.000252\n",
      "Epoch: 18 \tTraining Loss: 0.000252\n",
      "Epoch: 1 \tTraining Loss: 0.000176\n",
      "Epoch: 2 \tTraining Loss: 0.000176\n",
      "Epoch: 3 \tTraining Loss: 0.000176\n",
      "Epoch: 4 \tTraining Loss: 0.000176\n",
      "Epoch: 5 \tTraining Loss: 0.000176\n",
      "Epoch: 6 \tTraining Loss: 0.000176\n",
      "Epoch: 7 \tTraining Loss: 0.000176\n",
      "Epoch: 8 \tTraining Loss: 0.000176\n",
      "Epoch: 9 \tTraining Loss: 0.000176\n",
      "Epoch: 10 \tTraining Loss: 0.000176\n",
      "Epoch: 11 \tTraining Loss: 0.000176\n",
      "Epoch: 12 \tTraining Loss: 0.000176\n",
      "Epoch: 13 \tTraining Loss: 0.000176\n",
      "Epoch: 14 \tTraining Loss: 0.000176\n",
      "Epoch: 15 \tTraining Loss: 0.000176\n",
      "Epoch: 16 \tTraining Loss: 0.000176\n",
      "Epoch: 17 \tTraining Loss: 0.000176\n",
      "Epoch: 18 \tTraining Loss: 0.000176\n",
      "Epoch: 1 \tTraining Loss: 0.000161\n",
      "Epoch: 2 \tTraining Loss: 0.000161\n",
      "Epoch: 3 \tTraining Loss: 0.000161\n",
      "Epoch: 4 \tTraining Loss: 0.000161\n",
      "Epoch: 5 \tTraining Loss: 0.000161\n",
      "Epoch: 6 \tTraining Loss: 0.000161\n",
      "Epoch: 7 \tTraining Loss: 0.000161\n",
      "Epoch: 8 \tTraining Loss: 0.000161\n",
      "Epoch: 9 \tTraining Loss: 0.000161\n",
      "Epoch: 10 \tTraining Loss: 0.000161\n",
      "Epoch: 11 \tTraining Loss: 0.000161\n",
      "Epoch: 12 \tTraining Loss: 0.000161\n",
      "Epoch: 13 \tTraining Loss: 0.000161\n",
      "Epoch: 14 \tTraining Loss: 0.000161\n",
      "Epoch: 15 \tTraining Loss: 0.000161\n",
      "Epoch: 16 \tTraining Loss: 0.000161\n",
      "Epoch: 17 \tTraining Loss: 0.000161\n",
      "Epoch: 18 \tTraining Loss: 0.000161\n",
      "Epoch: 1 \tTraining Loss: 0.000137\n",
      "Epoch: 2 \tTraining Loss: 0.000137\n",
      "Epoch: 3 \tTraining Loss: 0.000137\n",
      "Epoch: 4 \tTraining Loss: 0.000137\n",
      "Epoch: 5 \tTraining Loss: 0.000137\n",
      "Epoch: 6 \tTraining Loss: 0.000137\n",
      "Epoch: 7 \tTraining Loss: 0.000137\n",
      "Epoch: 8 \tTraining Loss: 0.000137\n",
      "Epoch: 9 \tTraining Loss: 0.000137\n",
      "Epoch: 10 \tTraining Loss: 0.000137\n",
      "Epoch: 11 \tTraining Loss: 0.000137\n",
      "Epoch: 12 \tTraining Loss: 0.000137\n",
      "Epoch: 13 \tTraining Loss: 0.000137\n",
      "Epoch: 14 \tTraining Loss: 0.000137\n",
      "Epoch: 15 \tTraining Loss: 0.000137\n",
      "Epoch: 16 \tTraining Loss: 0.000137\n",
      "Epoch: 17 \tTraining Loss: 0.000137\n",
      "Epoch: 18 \tTraining Loss: 0.000137\n",
      "Epoch: 1 \tTraining Loss: 0.000234\n",
      "Epoch: 2 \tTraining Loss: 0.000234\n",
      "Epoch: 3 \tTraining Loss: 0.000234\n",
      "Epoch: 4 \tTraining Loss: 0.000234\n",
      "Epoch: 5 \tTraining Loss: 0.000234\n",
      "Epoch: 6 \tTraining Loss: 0.000234\n",
      "Epoch: 7 \tTraining Loss: 0.000234\n",
      "Epoch: 8 \tTraining Loss: 0.000234\n",
      "Epoch: 9 \tTraining Loss: 0.000234\n",
      "Epoch: 10 \tTraining Loss: 0.000234\n",
      "Epoch: 11 \tTraining Loss: 0.000234\n",
      "Epoch: 12 \tTraining Loss: 0.000234\n",
      "Epoch: 13 \tTraining Loss: 0.000234\n",
      "Epoch: 14 \tTraining Loss: 0.000234\n",
      "Epoch: 15 \tTraining Loss: 0.000234\n",
      "Epoch: 16 \tTraining Loss: 0.000234\n",
      "Epoch: 17 \tTraining Loss: 0.000234\n",
      "Epoch: 18 \tTraining Loss: 0.000234\n",
      "Epoch: 1 \tTraining Loss: 0.000204\n",
      "Epoch: 2 \tTraining Loss: 0.000204\n",
      "Epoch: 3 \tTraining Loss: 0.000204\n",
      "Epoch: 4 \tTraining Loss: 0.000204\n",
      "Epoch: 5 \tTraining Loss: 0.000204\n",
      "Epoch: 6 \tTraining Loss: 0.000204\n",
      "Epoch: 7 \tTraining Loss: 0.000204\n",
      "Epoch: 8 \tTraining Loss: 0.000204\n",
      "Epoch: 9 \tTraining Loss: 0.000204\n",
      "Epoch: 10 \tTraining Loss: 0.000204\n",
      "Epoch: 11 \tTraining Loss: 0.000204\n",
      "Epoch: 12 \tTraining Loss: 0.000204\n",
      "Epoch: 13 \tTraining Loss: 0.000204\n",
      "Epoch: 14 \tTraining Loss: 0.000204\n",
      "Epoch: 15 \tTraining Loss: 0.000204\n",
      "Epoch: 16 \tTraining Loss: 0.000204\n",
      "Epoch: 17 \tTraining Loss: 0.000204\n",
      "Epoch: 18 \tTraining Loss: 0.000204\n",
      "Epoch: 1 \tTraining Loss: 0.000218\n",
      "Epoch: 2 \tTraining Loss: 0.000218\n",
      "Epoch: 3 \tTraining Loss: 0.000218\n",
      "Epoch: 4 \tTraining Loss: 0.000218\n",
      "Epoch: 5 \tTraining Loss: 0.000218\n",
      "Epoch: 6 \tTraining Loss: 0.000218\n",
      "Epoch: 7 \tTraining Loss: 0.000218\n",
      "Epoch: 8 \tTraining Loss: 0.000218\n",
      "Epoch: 9 \tTraining Loss: 0.000218\n",
      "Epoch: 10 \tTraining Loss: 0.000218\n",
      "Epoch: 11 \tTraining Loss: 0.000218\n",
      "Epoch: 12 \tTraining Loss: 0.000218\n",
      "Epoch: 13 \tTraining Loss: 0.000218\n",
      "Epoch: 14 \tTraining Loss: 0.000218\n",
      "Epoch: 15 \tTraining Loss: 0.000218\n",
      "Epoch: 16 \tTraining Loss: 0.000218\n",
      "Epoch: 17 \tTraining Loss: 0.000218\n",
      "Epoch: 18 \tTraining Loss: 0.000218\n",
      "Epoch: 1 \tTraining Loss: 0.000266\n",
      "Epoch: 2 \tTraining Loss: 0.000266\n",
      "Epoch: 3 \tTraining Loss: 0.000266\n",
      "Epoch: 4 \tTraining Loss: 0.000266\n",
      "Epoch: 5 \tTraining Loss: 0.000266\n",
      "Epoch: 6 \tTraining Loss: 0.000266\n",
      "Epoch: 7 \tTraining Loss: 0.000266\n",
      "Epoch: 8 \tTraining Loss: 0.000266\n",
      "Epoch: 9 \tTraining Loss: 0.000266\n",
      "Epoch: 10 \tTraining Loss: 0.000266\n",
      "Epoch: 11 \tTraining Loss: 0.000266\n",
      "Epoch: 12 \tTraining Loss: 0.000266\n",
      "Epoch: 13 \tTraining Loss: 0.000266\n",
      "Epoch: 14 \tTraining Loss: 0.000266\n",
      "Epoch: 15 \tTraining Loss: 0.000266\n",
      "Epoch: 16 \tTraining Loss: 0.000266\n",
      "Epoch: 17 \tTraining Loss: 0.000266\n",
      "Epoch: 18 \tTraining Loss: 0.000266\n",
      "Epoch: 1 \tTraining Loss: 0.000108\n",
      "Epoch: 2 \tTraining Loss: 0.000108\n",
      "Epoch: 3 \tTraining Loss: 0.000108\n",
      "Epoch: 4 \tTraining Loss: 0.000108\n",
      "Epoch: 5 \tTraining Loss: 0.000108\n",
      "Epoch: 6 \tTraining Loss: 0.000108\n",
      "Epoch: 7 \tTraining Loss: 0.000108\n",
      "Epoch: 8 \tTraining Loss: 0.000108\n",
      "Epoch: 9 \tTraining Loss: 0.000108\n",
      "Epoch: 10 \tTraining Loss: 0.000108\n",
      "Epoch: 11 \tTraining Loss: 0.000108\n",
      "Epoch: 12 \tTraining Loss: 0.000108\n",
      "Epoch: 13 \tTraining Loss: 0.000108\n",
      "Epoch: 14 \tTraining Loss: 0.000108\n",
      "Epoch: 15 \tTraining Loss: 0.000108\n",
      "Epoch: 16 \tTraining Loss: 0.000108\n",
      "Epoch: 17 \tTraining Loss: 0.000108\n",
      "Epoch: 18 \tTraining Loss: 0.000108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000189\n",
      "Epoch: 2 \tTraining Loss: 0.000189\n",
      "Epoch: 3 \tTraining Loss: 0.000189\n",
      "Epoch: 4 \tTraining Loss: 0.000189\n",
      "Epoch: 5 \tTraining Loss: 0.000189\n",
      "Epoch: 6 \tTraining Loss: 0.000189\n",
      "Epoch: 7 \tTraining Loss: 0.000189\n",
      "Epoch: 8 \tTraining Loss: 0.000189\n",
      "Epoch: 9 \tTraining Loss: 0.000189\n",
      "Epoch: 10 \tTraining Loss: 0.000189\n",
      "Epoch: 11 \tTraining Loss: 0.000189\n",
      "Epoch: 12 \tTraining Loss: 0.000189\n",
      "Epoch: 13 \tTraining Loss: 0.000189\n",
      "Epoch: 14 \tTraining Loss: 0.000189\n",
      "Epoch: 15 \tTraining Loss: 0.000189\n",
      "Epoch: 16 \tTraining Loss: 0.000189\n",
      "Epoch: 17 \tTraining Loss: 0.000189\n",
      "Epoch: 18 \tTraining Loss: 0.000189\n",
      "Epoch: 1 \tTraining Loss: 0.000135\n",
      "Epoch: 2 \tTraining Loss: 0.000135\n",
      "Epoch: 3 \tTraining Loss: 0.000135\n",
      "Epoch: 4 \tTraining Loss: 0.000135\n",
      "Epoch: 5 \tTraining Loss: 0.000135\n",
      "Epoch: 6 \tTraining Loss: 0.000135\n",
      "Epoch: 7 \tTraining Loss: 0.000135\n",
      "Epoch: 8 \tTraining Loss: 0.000135\n",
      "Epoch: 9 \tTraining Loss: 0.000135\n",
      "Epoch: 10 \tTraining Loss: 0.000135\n",
      "Epoch: 11 \tTraining Loss: 0.000135\n",
      "Epoch: 12 \tTraining Loss: 0.000135\n",
      "Epoch: 13 \tTraining Loss: 0.000135\n",
      "Epoch: 14 \tTraining Loss: 0.000135\n",
      "Epoch: 15 \tTraining Loss: 0.000135\n",
      "Epoch: 16 \tTraining Loss: 0.000135\n",
      "Epoch: 17 \tTraining Loss: 0.000135\n",
      "Epoch: 18 \tTraining Loss: 0.000135\n",
      "Epoch: 1 \tTraining Loss: 0.000142\n",
      "Epoch: 2 \tTraining Loss: 0.000142\n",
      "Epoch: 3 \tTraining Loss: 0.000142\n",
      "Epoch: 4 \tTraining Loss: 0.000142\n",
      "Epoch: 5 \tTraining Loss: 0.000142\n",
      "Epoch: 6 \tTraining Loss: 0.000142\n",
      "Epoch: 7 \tTraining Loss: 0.000142\n",
      "Epoch: 8 \tTraining Loss: 0.000142\n",
      "Epoch: 9 \tTraining Loss: 0.000142\n",
      "Epoch: 10 \tTraining Loss: 0.000142\n",
      "Epoch: 11 \tTraining Loss: 0.000142\n",
      "Epoch: 12 \tTraining Loss: 0.000142\n",
      "Epoch: 13 \tTraining Loss: 0.000142\n",
      "Epoch: 14 \tTraining Loss: 0.000142\n",
      "Epoch: 15 \tTraining Loss: 0.000142\n",
      "Epoch: 16 \tTraining Loss: 0.000142\n",
      "Epoch: 17 \tTraining Loss: 0.000142\n",
      "Epoch: 18 \tTraining Loss: 0.000142\n",
      "Epoch: 1 \tTraining Loss: 0.000273\n",
      "Epoch: 2 \tTraining Loss: 0.000273\n",
      "Epoch: 3 \tTraining Loss: 0.000273\n",
      "Epoch: 4 \tTraining Loss: 0.000273\n",
      "Epoch: 5 \tTraining Loss: 0.000273\n",
      "Epoch: 6 \tTraining Loss: 0.000273\n",
      "Epoch: 7 \tTraining Loss: 0.000273\n",
      "Epoch: 8 \tTraining Loss: 0.000273\n",
      "Epoch: 9 \tTraining Loss: 0.000273\n",
      "Epoch: 10 \tTraining Loss: 0.000273\n",
      "Epoch: 11 \tTraining Loss: 0.000273\n",
      "Epoch: 12 \tTraining Loss: 0.000273\n",
      "Epoch: 13 \tTraining Loss: 0.000273\n",
      "Epoch: 14 \tTraining Loss: 0.000273\n",
      "Epoch: 15 \tTraining Loss: 0.000273\n",
      "Epoch: 16 \tTraining Loss: 0.000273\n",
      "Epoch: 17 \tTraining Loss: 0.000273\n",
      "Epoch: 18 \tTraining Loss: 0.000273\n",
      "Epoch: 1 \tTraining Loss: 0.000257\n",
      "Epoch: 2 \tTraining Loss: 0.000257\n",
      "Epoch: 3 \tTraining Loss: 0.000257\n",
      "Epoch: 4 \tTraining Loss: 0.000257\n",
      "Epoch: 5 \tTraining Loss: 0.000257\n",
      "Epoch: 6 \tTraining Loss: 0.000257\n",
      "Epoch: 7 \tTraining Loss: 0.000257\n",
      "Epoch: 8 \tTraining Loss: 0.000257\n",
      "Epoch: 9 \tTraining Loss: 0.000257\n",
      "Epoch: 10 \tTraining Loss: 0.000257\n",
      "Epoch: 11 \tTraining Loss: 0.000257\n",
      "Epoch: 12 \tTraining Loss: 0.000257\n",
      "Epoch: 13 \tTraining Loss: 0.000257\n",
      "Epoch: 14 \tTraining Loss: 0.000257\n",
      "Epoch: 15 \tTraining Loss: 0.000257\n",
      "Epoch: 16 \tTraining Loss: 0.000257\n",
      "Epoch: 17 \tTraining Loss: 0.000257\n",
      "Epoch: 18 \tTraining Loss: 0.000257\n",
      "Epoch: 1 \tTraining Loss: 0.000224\n",
      "Epoch: 2 \tTraining Loss: 0.000224\n",
      "Epoch: 3 \tTraining Loss: 0.000224\n",
      "Epoch: 4 \tTraining Loss: 0.000224\n",
      "Epoch: 5 \tTraining Loss: 0.000224\n",
      "Epoch: 6 \tTraining Loss: 0.000224\n",
      "Epoch: 7 \tTraining Loss: 0.000224\n",
      "Epoch: 8 \tTraining Loss: 0.000224\n",
      "Epoch: 9 \tTraining Loss: 0.000224\n",
      "Epoch: 10 \tTraining Loss: 0.000224\n",
      "Epoch: 11 \tTraining Loss: 0.000224\n",
      "Epoch: 12 \tTraining Loss: 0.000224\n",
      "Epoch: 13 \tTraining Loss: 0.000224\n",
      "Epoch: 14 \tTraining Loss: 0.000224\n",
      "Epoch: 15 \tTraining Loss: 0.000224\n",
      "Epoch: 16 \tTraining Loss: 0.000224\n",
      "Epoch: 17 \tTraining Loss: 0.000224\n",
      "Epoch: 18 \tTraining Loss: 0.000224\n",
      "Epoch: 1 \tTraining Loss: 0.000214\n",
      "Epoch: 2 \tTraining Loss: 0.000214\n",
      "Epoch: 3 \tTraining Loss: 0.000214\n",
      "Epoch: 4 \tTraining Loss: 0.000214\n",
      "Epoch: 5 \tTraining Loss: 0.000214\n",
      "Epoch: 6 \tTraining Loss: 0.000214\n",
      "Epoch: 7 \tTraining Loss: 0.000214\n",
      "Epoch: 8 \tTraining Loss: 0.000214\n",
      "Epoch: 9 \tTraining Loss: 0.000214\n",
      "Epoch: 10 \tTraining Loss: 0.000214\n",
      "Epoch: 11 \tTraining Loss: 0.000214\n",
      "Epoch: 12 \tTraining Loss: 0.000214\n",
      "Epoch: 13 \tTraining Loss: 0.000214\n",
      "Epoch: 14 \tTraining Loss: 0.000214\n",
      "Epoch: 15 \tTraining Loss: 0.000214\n",
      "Epoch: 16 \tTraining Loss: 0.000214\n",
      "Epoch: 17 \tTraining Loss: 0.000214\n",
      "Epoch: 18 \tTraining Loss: 0.000214\n",
      "Epoch: 1 \tTraining Loss: 0.000177\n",
      "Epoch: 2 \tTraining Loss: 0.000177\n",
      "Epoch: 3 \tTraining Loss: 0.000177\n",
      "Epoch: 4 \tTraining Loss: 0.000177\n",
      "Epoch: 5 \tTraining Loss: 0.000177\n",
      "Epoch: 6 \tTraining Loss: 0.000177\n",
      "Epoch: 7 \tTraining Loss: 0.000177\n",
      "Epoch: 8 \tTraining Loss: 0.000177\n",
      "Epoch: 9 \tTraining Loss: 0.000177\n",
      "Epoch: 10 \tTraining Loss: 0.000177\n",
      "Epoch: 11 \tTraining Loss: 0.000177\n",
      "Epoch: 12 \tTraining Loss: 0.000177\n",
      "Epoch: 13 \tTraining Loss: 0.000177\n",
      "Epoch: 14 \tTraining Loss: 0.000177\n",
      "Epoch: 15 \tTraining Loss: 0.000177\n",
      "Epoch: 16 \tTraining Loss: 0.000177\n",
      "Epoch: 17 \tTraining Loss: 0.000177\n",
      "Epoch: 18 \tTraining Loss: 0.000177\n",
      "Epoch: 1 \tTraining Loss: 0.000321\n",
      "Epoch: 2 \tTraining Loss: 0.000321\n",
      "Epoch: 3 \tTraining Loss: 0.000321\n",
      "Epoch: 4 \tTraining Loss: 0.000321\n",
      "Epoch: 5 \tTraining Loss: 0.000321\n",
      "Epoch: 6 \tTraining Loss: 0.000321\n",
      "Epoch: 7 \tTraining Loss: 0.000321\n",
      "Epoch: 8 \tTraining Loss: 0.000321\n",
      "Epoch: 9 \tTraining Loss: 0.000321\n",
      "Epoch: 10 \tTraining Loss: 0.000321\n",
      "Epoch: 11 \tTraining Loss: 0.000321\n",
      "Epoch: 12 \tTraining Loss: 0.000321\n",
      "Epoch: 13 \tTraining Loss: 0.000321\n",
      "Epoch: 14 \tTraining Loss: 0.000321\n",
      "Epoch: 15 \tTraining Loss: 0.000321\n",
      "Epoch: 16 \tTraining Loss: 0.000321\n",
      "Epoch: 17 \tTraining Loss: 0.000321\n",
      "Epoch: 18 \tTraining Loss: 0.000321\n",
      "Epoch: 1 \tTraining Loss: 0.000158\n",
      "Epoch: 2 \tTraining Loss: 0.000158\n",
      "Epoch: 3 \tTraining Loss: 0.000158\n",
      "Epoch: 4 \tTraining Loss: 0.000158\n",
      "Epoch: 5 \tTraining Loss: 0.000158\n",
      "Epoch: 6 \tTraining Loss: 0.000158\n",
      "Epoch: 7 \tTraining Loss: 0.000158\n",
      "Epoch: 8 \tTraining Loss: 0.000158\n",
      "Epoch: 9 \tTraining Loss: 0.000158\n",
      "Epoch: 10 \tTraining Loss: 0.000158\n",
      "Epoch: 11 \tTraining Loss: 0.000158\n",
      "Epoch: 12 \tTraining Loss: 0.000158\n",
      "Epoch: 13 \tTraining Loss: 0.000158\n",
      "Epoch: 14 \tTraining Loss: 0.000158\n",
      "Epoch: 15 \tTraining Loss: 0.000158\n",
      "Epoch: 16 \tTraining Loss: 0.000158\n",
      "Epoch: 17 \tTraining Loss: 0.000158\n",
      "Epoch: 18 \tTraining Loss: 0.000158\n",
      "Epoch: 1 \tTraining Loss: 0.000283\n",
      "Epoch: 2 \tTraining Loss: 0.000283\n",
      "Epoch: 3 \tTraining Loss: 0.000283\n",
      "Epoch: 4 \tTraining Loss: 0.000283\n",
      "Epoch: 5 \tTraining Loss: 0.000283\n",
      "Epoch: 6 \tTraining Loss: 0.000283\n",
      "Epoch: 7 \tTraining Loss: 0.000283\n",
      "Epoch: 8 \tTraining Loss: 0.000283\n",
      "Epoch: 9 \tTraining Loss: 0.000283\n",
      "Epoch: 10 \tTraining Loss: 0.000283\n",
      "Epoch: 11 \tTraining Loss: 0.000283\n",
      "Epoch: 12 \tTraining Loss: 0.000283\n",
      "Epoch: 13 \tTraining Loss: 0.000283\n",
      "Epoch: 14 \tTraining Loss: 0.000283\n",
      "Epoch: 15 \tTraining Loss: 0.000283\n",
      "Epoch: 16 \tTraining Loss: 0.000283\n",
      "Epoch: 17 \tTraining Loss: 0.000283\n",
      "Epoch: 18 \tTraining Loss: 0.000283\n",
      "Epoch: 1 \tTraining Loss: 0.000192\n",
      "Epoch: 2 \tTraining Loss: 0.000192\n",
      "Epoch: 3 \tTraining Loss: 0.000192\n",
      "Epoch: 4 \tTraining Loss: 0.000192\n",
      "Epoch: 5 \tTraining Loss: 0.000192\n",
      "Epoch: 6 \tTraining Loss: 0.000192\n",
      "Epoch: 7 \tTraining Loss: 0.000192\n",
      "Epoch: 8 \tTraining Loss: 0.000192\n",
      "Epoch: 9 \tTraining Loss: 0.000192\n",
      "Epoch: 10 \tTraining Loss: 0.000192\n",
      "Epoch: 11 \tTraining Loss: 0.000192\n",
      "Epoch: 12 \tTraining Loss: 0.000192\n",
      "Epoch: 13 \tTraining Loss: 0.000192\n",
      "Epoch: 14 \tTraining Loss: 0.000192\n",
      "Epoch: 15 \tTraining Loss: 0.000192\n",
      "Epoch: 16 \tTraining Loss: 0.000192\n",
      "Epoch: 17 \tTraining Loss: 0.000192\n",
      "Epoch: 18 \tTraining Loss: 0.000192\n",
      "Epoch: 1 \tTraining Loss: 0.000177\n",
      "Epoch: 2 \tTraining Loss: 0.000177\n",
      "Epoch: 3 \tTraining Loss: 0.000177\n",
      "Epoch: 4 \tTraining Loss: 0.000177\n",
      "Epoch: 5 \tTraining Loss: 0.000177\n",
      "Epoch: 6 \tTraining Loss: 0.000177\n",
      "Epoch: 7 \tTraining Loss: 0.000177\n",
      "Epoch: 8 \tTraining Loss: 0.000177\n",
      "Epoch: 9 \tTraining Loss: 0.000177\n",
      "Epoch: 10 \tTraining Loss: 0.000177\n",
      "Epoch: 11 \tTraining Loss: 0.000177\n",
      "Epoch: 12 \tTraining Loss: 0.000177\n",
      "Epoch: 13 \tTraining Loss: 0.000177\n",
      "Epoch: 14 \tTraining Loss: 0.000177\n",
      "Epoch: 15 \tTraining Loss: 0.000177\n",
      "Epoch: 16 \tTraining Loss: 0.000177\n",
      "Epoch: 17 \tTraining Loss: 0.000177\n",
      "Epoch: 18 \tTraining Loss: 0.000177\n",
      "Epoch: 1 \tTraining Loss: 0.000232\n",
      "Epoch: 2 \tTraining Loss: 0.000232\n",
      "Epoch: 3 \tTraining Loss: 0.000232\n",
      "Epoch: 4 \tTraining Loss: 0.000232\n",
      "Epoch: 5 \tTraining Loss: 0.000232\n",
      "Epoch: 6 \tTraining Loss: 0.000232\n",
      "Epoch: 7 \tTraining Loss: 0.000232\n",
      "Epoch: 8 \tTraining Loss: 0.000232\n",
      "Epoch: 9 \tTraining Loss: 0.000232\n",
      "Epoch: 10 \tTraining Loss: 0.000232\n",
      "Epoch: 11 \tTraining Loss: 0.000232\n",
      "Epoch: 12 \tTraining Loss: 0.000232\n",
      "Epoch: 13 \tTraining Loss: 0.000232\n",
      "Epoch: 14 \tTraining Loss: 0.000232\n",
      "Epoch: 15 \tTraining Loss: 0.000232\n",
      "Epoch: 16 \tTraining Loss: 0.000232\n",
      "Epoch: 17 \tTraining Loss: 0.000232\n",
      "Epoch: 18 \tTraining Loss: 0.000232\n",
      "Epoch: 1 \tTraining Loss: 0.000232\n",
      "Epoch: 2 \tTraining Loss: 0.000232\n",
      "Epoch: 3 \tTraining Loss: 0.000232\n",
      "Epoch: 4 \tTraining Loss: 0.000232\n",
      "Epoch: 5 \tTraining Loss: 0.000232\n",
      "Epoch: 6 \tTraining Loss: 0.000232\n",
      "Epoch: 7 \tTraining Loss: 0.000232\n",
      "Epoch: 8 \tTraining Loss: 0.000232\n",
      "Epoch: 9 \tTraining Loss: 0.000232\n",
      "Epoch: 10 \tTraining Loss: 0.000232\n",
      "Epoch: 11 \tTraining Loss: 0.000232\n",
      "Epoch: 12 \tTraining Loss: 0.000232\n",
      "Epoch: 13 \tTraining Loss: 0.000232\n",
      "Epoch: 14 \tTraining Loss: 0.000232\n",
      "Epoch: 15 \tTraining Loss: 0.000232\n",
      "Epoch: 16 \tTraining Loss: 0.000232\n",
      "Epoch: 17 \tTraining Loss: 0.000232\n",
      "Epoch: 18 \tTraining Loss: 0.000232\n",
      "Epoch: 1 \tTraining Loss: 0.000147\n",
      "Epoch: 2 \tTraining Loss: 0.000147\n",
      "Epoch: 3 \tTraining Loss: 0.000147\n",
      "Epoch: 4 \tTraining Loss: 0.000147\n",
      "Epoch: 5 \tTraining Loss: 0.000147\n",
      "Epoch: 6 \tTraining Loss: 0.000147\n",
      "Epoch: 7 \tTraining Loss: 0.000147\n",
      "Epoch: 8 \tTraining Loss: 0.000147\n",
      "Epoch: 9 \tTraining Loss: 0.000147\n",
      "Epoch: 10 \tTraining Loss: 0.000147\n",
      "Epoch: 11 \tTraining Loss: 0.000147\n",
      "Epoch: 12 \tTraining Loss: 0.000147\n",
      "Epoch: 13 \tTraining Loss: 0.000147\n",
      "Epoch: 14 \tTraining Loss: 0.000147\n",
      "Epoch: 15 \tTraining Loss: 0.000147\n",
      "Epoch: 16 \tTraining Loss: 0.000147\n",
      "Epoch: 17 \tTraining Loss: 0.000147\n",
      "Epoch: 18 \tTraining Loss: 0.000147\n",
      "Epoch: 1 \tTraining Loss: 0.000217\n",
      "Epoch: 2 \tTraining Loss: 0.000217\n",
      "Epoch: 3 \tTraining Loss: 0.000217\n",
      "Epoch: 4 \tTraining Loss: 0.000217\n",
      "Epoch: 5 \tTraining Loss: 0.000217\n",
      "Epoch: 6 \tTraining Loss: 0.000217\n",
      "Epoch: 7 \tTraining Loss: 0.000217\n",
      "Epoch: 8 \tTraining Loss: 0.000217\n",
      "Epoch: 9 \tTraining Loss: 0.000217\n",
      "Epoch: 10 \tTraining Loss: 0.000217\n",
      "Epoch: 11 \tTraining Loss: 0.000217\n",
      "Epoch: 12 \tTraining Loss: 0.000217\n",
      "Epoch: 13 \tTraining Loss: 0.000217\n",
      "Epoch: 14 \tTraining Loss: 0.000217\n",
      "Epoch: 15 \tTraining Loss: 0.000217\n",
      "Epoch: 16 \tTraining Loss: 0.000217\n",
      "Epoch: 17 \tTraining Loss: 0.000217\n",
      "Epoch: 18 \tTraining Loss: 0.000217\n",
      "Epoch: 1 \tTraining Loss: 0.000225\n",
      "Epoch: 2 \tTraining Loss: 0.000225\n",
      "Epoch: 3 \tTraining Loss: 0.000225\n",
      "Epoch: 4 \tTraining Loss: 0.000225\n",
      "Epoch: 5 \tTraining Loss: 0.000225\n",
      "Epoch: 6 \tTraining Loss: 0.000225\n",
      "Epoch: 7 \tTraining Loss: 0.000225\n",
      "Epoch: 8 \tTraining Loss: 0.000225\n",
      "Epoch: 9 \tTraining Loss: 0.000225\n",
      "Epoch: 10 \tTraining Loss: 0.000225\n",
      "Epoch: 11 \tTraining Loss: 0.000225\n",
      "Epoch: 12 \tTraining Loss: 0.000225\n",
      "Epoch: 13 \tTraining Loss: 0.000225\n",
      "Epoch: 14 \tTraining Loss: 0.000225\n",
      "Epoch: 15 \tTraining Loss: 0.000225\n",
      "Epoch: 16 \tTraining Loss: 0.000225\n",
      "Epoch: 17 \tTraining Loss: 0.000225\n",
      "Epoch: 18 \tTraining Loss: 0.000225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000069\n",
      "Epoch: 2 \tTraining Loss: 0.000069\n",
      "Epoch: 3 \tTraining Loss: 0.000069\n",
      "Epoch: 4 \tTraining Loss: 0.000069\n",
      "Epoch: 5 \tTraining Loss: 0.000069\n",
      "Epoch: 6 \tTraining Loss: 0.000069\n",
      "Epoch: 7 \tTraining Loss: 0.000069\n",
      "Epoch: 8 \tTraining Loss: 0.000069\n",
      "Epoch: 9 \tTraining Loss: 0.000069\n",
      "Epoch: 10 \tTraining Loss: 0.000069\n",
      "Epoch: 11 \tTraining Loss: 0.000069\n",
      "Epoch: 12 \tTraining Loss: 0.000069\n",
      "Epoch: 13 \tTraining Loss: 0.000069\n",
      "Epoch: 14 \tTraining Loss: 0.000069\n",
      "Epoch: 15 \tTraining Loss: 0.000069\n",
      "Epoch: 16 \tTraining Loss: 0.000069\n",
      "Epoch: 17 \tTraining Loss: 0.000069\n",
      "Epoch: 18 \tTraining Loss: 0.000069\n",
      "Epoch: 1 \tTraining Loss: 0.000158\n",
      "Epoch: 2 \tTraining Loss: 0.000158\n",
      "Epoch: 3 \tTraining Loss: 0.000158\n",
      "Epoch: 4 \tTraining Loss: 0.000158\n",
      "Epoch: 5 \tTraining Loss: 0.000158\n",
      "Epoch: 6 \tTraining Loss: 0.000158\n",
      "Epoch: 7 \tTraining Loss: 0.000158\n",
      "Epoch: 8 \tTraining Loss: 0.000158\n",
      "Epoch: 9 \tTraining Loss: 0.000158\n",
      "Epoch: 10 \tTraining Loss: 0.000158\n",
      "Epoch: 11 \tTraining Loss: 0.000158\n",
      "Epoch: 12 \tTraining Loss: 0.000158\n",
      "Epoch: 13 \tTraining Loss: 0.000158\n",
      "Epoch: 14 \tTraining Loss: 0.000158\n",
      "Epoch: 15 \tTraining Loss: 0.000158\n",
      "Epoch: 16 \tTraining Loss: 0.000158\n",
      "Epoch: 17 \tTraining Loss: 0.000158\n",
      "Epoch: 18 \tTraining Loss: 0.000158\n",
      "Epoch: 1 \tTraining Loss: 0.000274\n",
      "Epoch: 2 \tTraining Loss: 0.000274\n",
      "Epoch: 3 \tTraining Loss: 0.000274\n",
      "Epoch: 4 \tTraining Loss: 0.000274\n",
      "Epoch: 5 \tTraining Loss: 0.000274\n",
      "Epoch: 6 \tTraining Loss: 0.000274\n",
      "Epoch: 7 \tTraining Loss: 0.000274\n",
      "Epoch: 8 \tTraining Loss: 0.000274\n",
      "Epoch: 9 \tTraining Loss: 0.000274\n",
      "Epoch: 10 \tTraining Loss: 0.000274\n",
      "Epoch: 11 \tTraining Loss: 0.000274\n",
      "Epoch: 12 \tTraining Loss: 0.000274\n",
      "Epoch: 13 \tTraining Loss: 0.000274\n",
      "Epoch: 14 \tTraining Loss: 0.000274\n",
      "Epoch: 15 \tTraining Loss: 0.000274\n",
      "Epoch: 16 \tTraining Loss: 0.000274\n",
      "Epoch: 17 \tTraining Loss: 0.000274\n",
      "Epoch: 18 \tTraining Loss: 0.000274\n",
      "Epoch: 1 \tTraining Loss: 0.000215\n",
      "Epoch: 2 \tTraining Loss: 0.000215\n",
      "Epoch: 3 \tTraining Loss: 0.000215\n",
      "Epoch: 4 \tTraining Loss: 0.000215\n",
      "Epoch: 5 \tTraining Loss: 0.000215\n",
      "Epoch: 6 \tTraining Loss: 0.000215\n",
      "Epoch: 7 \tTraining Loss: 0.000215\n",
      "Epoch: 8 \tTraining Loss: 0.000215\n",
      "Epoch: 9 \tTraining Loss: 0.000215\n",
      "Epoch: 10 \tTraining Loss: 0.000215\n",
      "Epoch: 11 \tTraining Loss: 0.000215\n",
      "Epoch: 12 \tTraining Loss: 0.000215\n",
      "Epoch: 13 \tTraining Loss: 0.000215\n",
      "Epoch: 14 \tTraining Loss: 0.000215\n",
      "Epoch: 15 \tTraining Loss: 0.000215\n",
      "Epoch: 16 \tTraining Loss: 0.000215\n",
      "Epoch: 17 \tTraining Loss: 0.000215\n",
      "Epoch: 18 \tTraining Loss: 0.000215\n",
      "Epoch: 1 \tTraining Loss: 0.000150\n",
      "Epoch: 2 \tTraining Loss: 0.000150\n",
      "Epoch: 3 \tTraining Loss: 0.000150\n",
      "Epoch: 4 \tTraining Loss: 0.000150\n",
      "Epoch: 5 \tTraining Loss: 0.000150\n",
      "Epoch: 6 \tTraining Loss: 0.000150\n",
      "Epoch: 7 \tTraining Loss: 0.000150\n",
      "Epoch: 8 \tTraining Loss: 0.000150\n",
      "Epoch: 9 \tTraining Loss: 0.000150\n",
      "Epoch: 10 \tTraining Loss: 0.000150\n",
      "Epoch: 11 \tTraining Loss: 0.000150\n",
      "Epoch: 12 \tTraining Loss: 0.000150\n",
      "Epoch: 13 \tTraining Loss: 0.000150\n",
      "Epoch: 14 \tTraining Loss: 0.000150\n",
      "Epoch: 15 \tTraining Loss: 0.000150\n",
      "Epoch: 16 \tTraining Loss: 0.000150\n",
      "Epoch: 17 \tTraining Loss: 0.000150\n",
      "Epoch: 18 \tTraining Loss: 0.000150\n",
      "Epoch: 1 \tTraining Loss: 0.000097\n",
      "Epoch: 2 \tTraining Loss: 0.000097\n",
      "Epoch: 3 \tTraining Loss: 0.000097\n",
      "Epoch: 4 \tTraining Loss: 0.000097\n",
      "Epoch: 5 \tTraining Loss: 0.000097\n",
      "Epoch: 6 \tTraining Loss: 0.000097\n",
      "Epoch: 7 \tTraining Loss: 0.000097\n",
      "Epoch: 8 \tTraining Loss: 0.000097\n",
      "Epoch: 9 \tTraining Loss: 0.000097\n",
      "Epoch: 10 \tTraining Loss: 0.000097\n",
      "Epoch: 11 \tTraining Loss: 0.000097\n",
      "Epoch: 12 \tTraining Loss: 0.000097\n",
      "Epoch: 13 \tTraining Loss: 0.000097\n",
      "Epoch: 14 \tTraining Loss: 0.000097\n",
      "Epoch: 15 \tTraining Loss: 0.000097\n",
      "Epoch: 16 \tTraining Loss: 0.000097\n",
      "Epoch: 17 \tTraining Loss: 0.000097\n",
      "Epoch: 18 \tTraining Loss: 0.000097\n",
      "Epoch: 1 \tTraining Loss: 0.000195\n",
      "Epoch: 2 \tTraining Loss: 0.000195\n",
      "Epoch: 3 \tTraining Loss: 0.000195\n",
      "Epoch: 4 \tTraining Loss: 0.000195\n",
      "Epoch: 5 \tTraining Loss: 0.000195\n",
      "Epoch: 6 \tTraining Loss: 0.000195\n",
      "Epoch: 7 \tTraining Loss: 0.000195\n",
      "Epoch: 8 \tTraining Loss: 0.000195\n",
      "Epoch: 9 \tTraining Loss: 0.000195\n",
      "Epoch: 10 \tTraining Loss: 0.000195\n",
      "Epoch: 11 \tTraining Loss: 0.000195\n",
      "Epoch: 12 \tTraining Loss: 0.000195\n",
      "Epoch: 13 \tTraining Loss: 0.000195\n",
      "Epoch: 14 \tTraining Loss: 0.000195\n",
      "Epoch: 15 \tTraining Loss: 0.000195\n",
      "Epoch: 16 \tTraining Loss: 0.000195\n",
      "Epoch: 17 \tTraining Loss: 0.000195\n",
      "Epoch: 18 \tTraining Loss: 0.000195\n",
      "Epoch: 1 \tTraining Loss: 0.000201\n",
      "Epoch: 2 \tTraining Loss: 0.000201\n",
      "Epoch: 3 \tTraining Loss: 0.000201\n",
      "Epoch: 4 \tTraining Loss: 0.000201\n",
      "Epoch: 5 \tTraining Loss: 0.000201\n",
      "Epoch: 6 \tTraining Loss: 0.000201\n",
      "Epoch: 7 \tTraining Loss: 0.000201\n",
      "Epoch: 8 \tTraining Loss: 0.000201\n",
      "Epoch: 9 \tTraining Loss: 0.000201\n",
      "Epoch: 10 \tTraining Loss: 0.000201\n",
      "Epoch: 11 \tTraining Loss: 0.000201\n",
      "Epoch: 12 \tTraining Loss: 0.000201\n",
      "Epoch: 13 \tTraining Loss: 0.000201\n",
      "Epoch: 14 \tTraining Loss: 0.000201\n",
      "Epoch: 15 \tTraining Loss: 0.000201\n",
      "Epoch: 16 \tTraining Loss: 0.000201\n",
      "Epoch: 17 \tTraining Loss: 0.000201\n",
      "Epoch: 18 \tTraining Loss: 0.000201\n",
      "Epoch: 1 \tTraining Loss: 0.000267\n",
      "Epoch: 2 \tTraining Loss: 0.000267\n",
      "Epoch: 3 \tTraining Loss: 0.000267\n",
      "Epoch: 4 \tTraining Loss: 0.000267\n",
      "Epoch: 5 \tTraining Loss: 0.000267\n",
      "Epoch: 6 \tTraining Loss: 0.000267\n",
      "Epoch: 7 \tTraining Loss: 0.000267\n",
      "Epoch: 8 \tTraining Loss: 0.000267\n",
      "Epoch: 9 \tTraining Loss: 0.000267\n",
      "Epoch: 10 \tTraining Loss: 0.000267\n",
      "Epoch: 11 \tTraining Loss: 0.000267\n",
      "Epoch: 12 \tTraining Loss: 0.000267\n",
      "Epoch: 13 \tTraining Loss: 0.000267\n",
      "Epoch: 14 \tTraining Loss: 0.000267\n",
      "Epoch: 15 \tTraining Loss: 0.000267\n",
      "Epoch: 16 \tTraining Loss: 0.000267\n",
      "Epoch: 17 \tTraining Loss: 0.000267\n",
      "Epoch: 18 \tTraining Loss: 0.000267\n",
      "Epoch: 1 \tTraining Loss: 0.000093\n",
      "Epoch: 2 \tTraining Loss: 0.000093\n",
      "Epoch: 3 \tTraining Loss: 0.000093\n",
      "Epoch: 4 \tTraining Loss: 0.000093\n",
      "Epoch: 5 \tTraining Loss: 0.000093\n",
      "Epoch: 6 \tTraining Loss: 0.000093\n",
      "Epoch: 7 \tTraining Loss: 0.000093\n",
      "Epoch: 8 \tTraining Loss: 0.000093\n",
      "Epoch: 9 \tTraining Loss: 0.000093\n",
      "Epoch: 10 \tTraining Loss: 0.000093\n",
      "Epoch: 11 \tTraining Loss: 0.000093\n",
      "Epoch: 12 \tTraining Loss: 0.000093\n",
      "Epoch: 13 \tTraining Loss: 0.000093\n",
      "Epoch: 14 \tTraining Loss: 0.000093\n",
      "Epoch: 15 \tTraining Loss: 0.000093\n",
      "Epoch: 16 \tTraining Loss: 0.000093\n",
      "Epoch: 17 \tTraining Loss: 0.000093\n",
      "Epoch: 18 \tTraining Loss: 0.000093\n",
      "Epoch: 1 \tTraining Loss: 0.000167\n",
      "Epoch: 2 \tTraining Loss: 0.000167\n",
      "Epoch: 3 \tTraining Loss: 0.000167\n",
      "Epoch: 4 \tTraining Loss: 0.000167\n",
      "Epoch: 5 \tTraining Loss: 0.000167\n",
      "Epoch: 6 \tTraining Loss: 0.000167\n",
      "Epoch: 7 \tTraining Loss: 0.000167\n",
      "Epoch: 8 \tTraining Loss: 0.000167\n",
      "Epoch: 9 \tTraining Loss: 0.000167\n",
      "Epoch: 10 \tTraining Loss: 0.000167\n",
      "Epoch: 11 \tTraining Loss: 0.000167\n",
      "Epoch: 12 \tTraining Loss: 0.000167\n",
      "Epoch: 13 \tTraining Loss: 0.000167\n",
      "Epoch: 14 \tTraining Loss: 0.000167\n",
      "Epoch: 15 \tTraining Loss: 0.000167\n",
      "Epoch: 16 \tTraining Loss: 0.000167\n",
      "Epoch: 17 \tTraining Loss: 0.000167\n",
      "Epoch: 18 \tTraining Loss: 0.000167\n",
      "Epoch: 1 \tTraining Loss: 0.000126\n",
      "Epoch: 2 \tTraining Loss: 0.000126\n",
      "Epoch: 3 \tTraining Loss: 0.000126\n",
      "Epoch: 4 \tTraining Loss: 0.000126\n",
      "Epoch: 5 \tTraining Loss: 0.000126\n",
      "Epoch: 6 \tTraining Loss: 0.000126\n",
      "Epoch: 7 \tTraining Loss: 0.000126\n",
      "Epoch: 8 \tTraining Loss: 0.000126\n",
      "Epoch: 9 \tTraining Loss: 0.000126\n",
      "Epoch: 10 \tTraining Loss: 0.000126\n",
      "Epoch: 11 \tTraining Loss: 0.000126\n",
      "Epoch: 12 \tTraining Loss: 0.000126\n",
      "Epoch: 13 \tTraining Loss: 0.000126\n",
      "Epoch: 14 \tTraining Loss: 0.000126\n",
      "Epoch: 15 \tTraining Loss: 0.000126\n",
      "Epoch: 16 \tTraining Loss: 0.000126\n",
      "Epoch: 17 \tTraining Loss: 0.000126\n",
      "Epoch: 18 \tTraining Loss: 0.000126\n",
      "Epoch: 1 \tTraining Loss: 0.000277\n",
      "Epoch: 2 \tTraining Loss: 0.000277\n",
      "Epoch: 3 \tTraining Loss: 0.000277\n",
      "Epoch: 4 \tTraining Loss: 0.000277\n",
      "Epoch: 5 \tTraining Loss: 0.000277\n",
      "Epoch: 6 \tTraining Loss: 0.000277\n",
      "Epoch: 7 \tTraining Loss: 0.000277\n",
      "Epoch: 8 \tTraining Loss: 0.000277\n",
      "Epoch: 9 \tTraining Loss: 0.000277\n",
      "Epoch: 10 \tTraining Loss: 0.000277\n",
      "Epoch: 11 \tTraining Loss: 0.000277\n",
      "Epoch: 12 \tTraining Loss: 0.000277\n",
      "Epoch: 13 \tTraining Loss: 0.000277\n",
      "Epoch: 14 \tTraining Loss: 0.000277\n",
      "Epoch: 15 \tTraining Loss: 0.000277\n",
      "Epoch: 16 \tTraining Loss: 0.000277\n",
      "Epoch: 17 \tTraining Loss: 0.000277\n",
      "Epoch: 18 \tTraining Loss: 0.000277\n",
      "Epoch: 1 \tTraining Loss: 0.000452\n",
      "Epoch: 2 \tTraining Loss: 0.000452\n",
      "Epoch: 3 \tTraining Loss: 0.000452\n",
      "Epoch: 4 \tTraining Loss: 0.000452\n",
      "Epoch: 5 \tTraining Loss: 0.000452\n",
      "Epoch: 6 \tTraining Loss: 0.000452\n",
      "Epoch: 7 \tTraining Loss: 0.000452\n",
      "Epoch: 8 \tTraining Loss: 0.000452\n",
      "Epoch: 9 \tTraining Loss: 0.000452\n",
      "Epoch: 10 \tTraining Loss: 0.000452\n",
      "Epoch: 11 \tTraining Loss: 0.000452\n",
      "Epoch: 12 \tTraining Loss: 0.000452\n",
      "Epoch: 13 \tTraining Loss: 0.000452\n",
      "Epoch: 14 \tTraining Loss: 0.000452\n",
      "Epoch: 15 \tTraining Loss: 0.000452\n",
      "Epoch: 16 \tTraining Loss: 0.000452\n",
      "Epoch: 17 \tTraining Loss: 0.000452\n",
      "Epoch: 18 \tTraining Loss: 0.000452\n",
      "Epoch: 1 \tTraining Loss: 0.000113\n",
      "Epoch: 2 \tTraining Loss: 0.000113\n",
      "Epoch: 3 \tTraining Loss: 0.000113\n",
      "Epoch: 4 \tTraining Loss: 0.000113\n",
      "Epoch: 5 \tTraining Loss: 0.000113\n",
      "Epoch: 6 \tTraining Loss: 0.000113\n",
      "Epoch: 7 \tTraining Loss: 0.000113\n",
      "Epoch: 8 \tTraining Loss: 0.000113\n",
      "Epoch: 9 \tTraining Loss: 0.000113\n",
      "Epoch: 10 \tTraining Loss: 0.000113\n",
      "Epoch: 11 \tTraining Loss: 0.000113\n",
      "Epoch: 12 \tTraining Loss: 0.000113\n",
      "Epoch: 13 \tTraining Loss: 0.000113\n",
      "Epoch: 14 \tTraining Loss: 0.000113\n",
      "Epoch: 15 \tTraining Loss: 0.000113\n",
      "Epoch: 16 \tTraining Loss: 0.000113\n",
      "Epoch: 17 \tTraining Loss: 0.000113\n",
      "Epoch: 18 \tTraining Loss: 0.000113\n",
      "Epoch: 1 \tTraining Loss: 0.000132\n",
      "Epoch: 2 \tTraining Loss: 0.000132\n",
      "Epoch: 3 \tTraining Loss: 0.000132\n",
      "Epoch: 4 \tTraining Loss: 0.000132\n",
      "Epoch: 5 \tTraining Loss: 0.000132\n",
      "Epoch: 6 \tTraining Loss: 0.000132\n",
      "Epoch: 7 \tTraining Loss: 0.000132\n",
      "Epoch: 8 \tTraining Loss: 0.000132\n",
      "Epoch: 9 \tTraining Loss: 0.000132\n",
      "Epoch: 10 \tTraining Loss: 0.000132\n",
      "Epoch: 11 \tTraining Loss: 0.000132\n",
      "Epoch: 12 \tTraining Loss: 0.000132\n",
      "Epoch: 13 \tTraining Loss: 0.000132\n",
      "Epoch: 14 \tTraining Loss: 0.000132\n",
      "Epoch: 15 \tTraining Loss: 0.000132\n",
      "Epoch: 16 \tTraining Loss: 0.000132\n",
      "Epoch: 17 \tTraining Loss: 0.000132\n",
      "Epoch: 18 \tTraining Loss: 0.000132\n",
      "Epoch: 1 \tTraining Loss: 0.000220\n",
      "Epoch: 2 \tTraining Loss: 0.000220\n",
      "Epoch: 3 \tTraining Loss: 0.000220\n",
      "Epoch: 4 \tTraining Loss: 0.000220\n",
      "Epoch: 5 \tTraining Loss: 0.000220\n",
      "Epoch: 6 \tTraining Loss: 0.000220\n",
      "Epoch: 7 \tTraining Loss: 0.000220\n",
      "Epoch: 8 \tTraining Loss: 0.000220\n",
      "Epoch: 9 \tTraining Loss: 0.000220\n",
      "Epoch: 10 \tTraining Loss: 0.000220\n",
      "Epoch: 11 \tTraining Loss: 0.000220\n",
      "Epoch: 12 \tTraining Loss: 0.000220\n",
      "Epoch: 13 \tTraining Loss: 0.000220\n",
      "Epoch: 14 \tTraining Loss: 0.000220\n",
      "Epoch: 15 \tTraining Loss: 0.000220\n",
      "Epoch: 16 \tTraining Loss: 0.000220\n",
      "Epoch: 17 \tTraining Loss: 0.000220\n",
      "Epoch: 18 \tTraining Loss: 0.000220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000106\n",
      "Epoch: 2 \tTraining Loss: 0.000106\n",
      "Epoch: 3 \tTraining Loss: 0.000106\n",
      "Epoch: 4 \tTraining Loss: 0.000106\n",
      "Epoch: 5 \tTraining Loss: 0.000106\n",
      "Epoch: 6 \tTraining Loss: 0.000106\n",
      "Epoch: 7 \tTraining Loss: 0.000106\n",
      "Epoch: 8 \tTraining Loss: 0.000106\n",
      "Epoch: 9 \tTraining Loss: 0.000106\n",
      "Epoch: 10 \tTraining Loss: 0.000106\n",
      "Epoch: 11 \tTraining Loss: 0.000106\n",
      "Epoch: 12 \tTraining Loss: 0.000106\n",
      "Epoch: 13 \tTraining Loss: 0.000106\n",
      "Epoch: 14 \tTraining Loss: 0.000106\n",
      "Epoch: 15 \tTraining Loss: 0.000106\n",
      "Epoch: 16 \tTraining Loss: 0.000106\n",
      "Epoch: 17 \tTraining Loss: 0.000106\n",
      "Epoch: 18 \tTraining Loss: 0.000106\n",
      "Epoch: 1 \tTraining Loss: 0.000168\n",
      "Epoch: 2 \tTraining Loss: 0.000168\n",
      "Epoch: 3 \tTraining Loss: 0.000168\n",
      "Epoch: 4 \tTraining Loss: 0.000168\n",
      "Epoch: 5 \tTraining Loss: 0.000168\n",
      "Epoch: 6 \tTraining Loss: 0.000168\n",
      "Epoch: 7 \tTraining Loss: 0.000168\n",
      "Epoch: 8 \tTraining Loss: 0.000168\n",
      "Epoch: 9 \tTraining Loss: 0.000168\n",
      "Epoch: 10 \tTraining Loss: 0.000168\n",
      "Epoch: 11 \tTraining Loss: 0.000168\n",
      "Epoch: 12 \tTraining Loss: 0.000168\n",
      "Epoch: 13 \tTraining Loss: 0.000168\n",
      "Epoch: 14 \tTraining Loss: 0.000168\n",
      "Epoch: 15 \tTraining Loss: 0.000168\n",
      "Epoch: 16 \tTraining Loss: 0.000168\n",
      "Epoch: 17 \tTraining Loss: 0.000168\n",
      "Epoch: 18 \tTraining Loss: 0.000168\n",
      "Epoch: 1 \tTraining Loss: 0.000076\n",
      "Epoch: 2 \tTraining Loss: 0.000076\n",
      "Epoch: 3 \tTraining Loss: 0.000076\n",
      "Epoch: 4 \tTraining Loss: 0.000076\n",
      "Epoch: 5 \tTraining Loss: 0.000076\n",
      "Epoch: 6 \tTraining Loss: 0.000076\n",
      "Epoch: 7 \tTraining Loss: 0.000076\n",
      "Epoch: 8 \tTraining Loss: 0.000076\n",
      "Epoch: 9 \tTraining Loss: 0.000076\n",
      "Epoch: 10 \tTraining Loss: 0.000076\n",
      "Epoch: 11 \tTraining Loss: 0.000076\n",
      "Epoch: 12 \tTraining Loss: 0.000076\n",
      "Epoch: 13 \tTraining Loss: 0.000076\n",
      "Epoch: 14 \tTraining Loss: 0.000076\n",
      "Epoch: 15 \tTraining Loss: 0.000076\n",
      "Epoch: 16 \tTraining Loss: 0.000076\n",
      "Epoch: 17 \tTraining Loss: 0.000076\n",
      "Epoch: 18 \tTraining Loss: 0.000076\n",
      "Epoch: 1 \tTraining Loss: 0.000153\n",
      "Epoch: 2 \tTraining Loss: 0.000153\n",
      "Epoch: 3 \tTraining Loss: 0.000153\n",
      "Epoch: 4 \tTraining Loss: 0.000153\n",
      "Epoch: 5 \tTraining Loss: 0.000153\n",
      "Epoch: 6 \tTraining Loss: 0.000153\n",
      "Epoch: 7 \tTraining Loss: 0.000153\n",
      "Epoch: 8 \tTraining Loss: 0.000153\n",
      "Epoch: 9 \tTraining Loss: 0.000153\n",
      "Epoch: 10 \tTraining Loss: 0.000153\n",
      "Epoch: 11 \tTraining Loss: 0.000153\n",
      "Epoch: 12 \tTraining Loss: 0.000153\n",
      "Epoch: 13 \tTraining Loss: 0.000153\n",
      "Epoch: 14 \tTraining Loss: 0.000153\n",
      "Epoch: 15 \tTraining Loss: 0.000153\n",
      "Epoch: 16 \tTraining Loss: 0.000153\n",
      "Epoch: 17 \tTraining Loss: 0.000153\n",
      "Epoch: 18 \tTraining Loss: 0.000153\n",
      "Epoch: 1 \tTraining Loss: 0.000226\n",
      "Epoch: 2 \tTraining Loss: 0.000226\n",
      "Epoch: 3 \tTraining Loss: 0.000226\n",
      "Epoch: 4 \tTraining Loss: 0.000226\n",
      "Epoch: 5 \tTraining Loss: 0.000226\n",
      "Epoch: 6 \tTraining Loss: 0.000226\n",
      "Epoch: 7 \tTraining Loss: 0.000226\n",
      "Epoch: 8 \tTraining Loss: 0.000226\n",
      "Epoch: 9 \tTraining Loss: 0.000226\n",
      "Epoch: 10 \tTraining Loss: 0.000226\n",
      "Epoch: 11 \tTraining Loss: 0.000226\n",
      "Epoch: 12 \tTraining Loss: 0.000226\n",
      "Epoch: 13 \tTraining Loss: 0.000226\n",
      "Epoch: 14 \tTraining Loss: 0.000226\n",
      "Epoch: 15 \tTraining Loss: 0.000226\n",
      "Epoch: 16 \tTraining Loss: 0.000226\n",
      "Epoch: 17 \tTraining Loss: 0.000226\n",
      "Epoch: 18 \tTraining Loss: 0.000226\n",
      "Epoch: 1 \tTraining Loss: 0.000183\n",
      "Epoch: 2 \tTraining Loss: 0.000183\n",
      "Epoch: 3 \tTraining Loss: 0.000183\n",
      "Epoch: 4 \tTraining Loss: 0.000183\n",
      "Epoch: 5 \tTraining Loss: 0.000183\n",
      "Epoch: 6 \tTraining Loss: 0.000183\n",
      "Epoch: 7 \tTraining Loss: 0.000183\n",
      "Epoch: 8 \tTraining Loss: 0.000183\n",
      "Epoch: 9 \tTraining Loss: 0.000183\n",
      "Epoch: 10 \tTraining Loss: 0.000183\n",
      "Epoch: 11 \tTraining Loss: 0.000183\n",
      "Epoch: 12 \tTraining Loss: 0.000183\n",
      "Epoch: 13 \tTraining Loss: 0.000183\n",
      "Epoch: 14 \tTraining Loss: 0.000183\n",
      "Epoch: 15 \tTraining Loss: 0.000183\n",
      "Epoch: 16 \tTraining Loss: 0.000183\n",
      "Epoch: 17 \tTraining Loss: 0.000183\n",
      "Epoch: 18 \tTraining Loss: 0.000183\n",
      "Epoch: 1 \tTraining Loss: 0.000057\n",
      "Epoch: 2 \tTraining Loss: 0.000057\n",
      "Epoch: 3 \tTraining Loss: 0.000057\n",
      "Epoch: 4 \tTraining Loss: 0.000057\n",
      "Epoch: 5 \tTraining Loss: 0.000057\n",
      "Epoch: 6 \tTraining Loss: 0.000057\n",
      "Epoch: 7 \tTraining Loss: 0.000057\n",
      "Epoch: 8 \tTraining Loss: 0.000057\n",
      "Epoch: 9 \tTraining Loss: 0.000057\n",
      "Epoch: 10 \tTraining Loss: 0.000057\n",
      "Epoch: 11 \tTraining Loss: 0.000057\n",
      "Epoch: 12 \tTraining Loss: 0.000057\n",
      "Epoch: 13 \tTraining Loss: 0.000057\n",
      "Epoch: 14 \tTraining Loss: 0.000057\n",
      "Epoch: 15 \tTraining Loss: 0.000057\n",
      "Epoch: 16 \tTraining Loss: 0.000057\n",
      "Epoch: 17 \tTraining Loss: 0.000057\n",
      "Epoch: 18 \tTraining Loss: 0.000057\n",
      "Epoch: 1 \tTraining Loss: 0.000230\n",
      "Epoch: 2 \tTraining Loss: 0.000230\n",
      "Epoch: 3 \tTraining Loss: 0.000230\n",
      "Epoch: 4 \tTraining Loss: 0.000230\n",
      "Epoch: 5 \tTraining Loss: 0.000230\n",
      "Epoch: 6 \tTraining Loss: 0.000230\n",
      "Epoch: 7 \tTraining Loss: 0.000230\n",
      "Epoch: 8 \tTraining Loss: 0.000230\n",
      "Epoch: 9 \tTraining Loss: 0.000230\n",
      "Epoch: 10 \tTraining Loss: 0.000230\n",
      "Epoch: 11 \tTraining Loss: 0.000230\n",
      "Epoch: 12 \tTraining Loss: 0.000230\n",
      "Epoch: 13 \tTraining Loss: 0.000230\n",
      "Epoch: 14 \tTraining Loss: 0.000230\n",
      "Epoch: 15 \tTraining Loss: 0.000230\n",
      "Epoch: 16 \tTraining Loss: 0.000230\n",
      "Epoch: 17 \tTraining Loss: 0.000230\n",
      "Epoch: 18 \tTraining Loss: 0.000230\n",
      "Epoch: 1 \tTraining Loss: 0.000177\n",
      "Epoch: 2 \tTraining Loss: 0.000177\n",
      "Epoch: 3 \tTraining Loss: 0.000177\n",
      "Epoch: 4 \tTraining Loss: 0.000177\n",
      "Epoch: 5 \tTraining Loss: 0.000177\n",
      "Epoch: 6 \tTraining Loss: 0.000177\n",
      "Epoch: 7 \tTraining Loss: 0.000177\n",
      "Epoch: 8 \tTraining Loss: 0.000177\n",
      "Epoch: 9 \tTraining Loss: 0.000177\n",
      "Epoch: 10 \tTraining Loss: 0.000177\n",
      "Epoch: 11 \tTraining Loss: 0.000177\n",
      "Epoch: 12 \tTraining Loss: 0.000177\n",
      "Epoch: 13 \tTraining Loss: 0.000177\n",
      "Epoch: 14 \tTraining Loss: 0.000177\n",
      "Epoch: 15 \tTraining Loss: 0.000177\n",
      "Epoch: 16 \tTraining Loss: 0.000177\n",
      "Epoch: 17 \tTraining Loss: 0.000177\n",
      "Epoch: 18 \tTraining Loss: 0.000177\n",
      "Epoch: 1 \tTraining Loss: 0.000106\n",
      "Epoch: 2 \tTraining Loss: 0.000106\n",
      "Epoch: 3 \tTraining Loss: 0.000106\n",
      "Epoch: 4 \tTraining Loss: 0.000106\n",
      "Epoch: 5 \tTraining Loss: 0.000106\n",
      "Epoch: 6 \tTraining Loss: 0.000106\n",
      "Epoch: 7 \tTraining Loss: 0.000106\n",
      "Epoch: 8 \tTraining Loss: 0.000106\n",
      "Epoch: 9 \tTraining Loss: 0.000106\n",
      "Epoch: 10 \tTraining Loss: 0.000106\n",
      "Epoch: 11 \tTraining Loss: 0.000106\n",
      "Epoch: 12 \tTraining Loss: 0.000106\n",
      "Epoch: 13 \tTraining Loss: 0.000106\n",
      "Epoch: 14 \tTraining Loss: 0.000106\n",
      "Epoch: 15 \tTraining Loss: 0.000106\n",
      "Epoch: 16 \tTraining Loss: 0.000106\n",
      "Epoch: 17 \tTraining Loss: 0.000106\n",
      "Epoch: 18 \tTraining Loss: 0.000106\n",
      "Epoch: 1 \tTraining Loss: 0.000076\n",
      "Epoch: 2 \tTraining Loss: 0.000076\n",
      "Epoch: 3 \tTraining Loss: 0.000076\n",
      "Epoch: 4 \tTraining Loss: 0.000076\n",
      "Epoch: 5 \tTraining Loss: 0.000076\n",
      "Epoch: 6 \tTraining Loss: 0.000076\n",
      "Epoch: 7 \tTraining Loss: 0.000076\n",
      "Epoch: 8 \tTraining Loss: 0.000076\n",
      "Epoch: 9 \tTraining Loss: 0.000076\n",
      "Epoch: 10 \tTraining Loss: 0.000076\n",
      "Epoch: 11 \tTraining Loss: 0.000076\n",
      "Epoch: 12 \tTraining Loss: 0.000076\n",
      "Epoch: 13 \tTraining Loss: 0.000076\n",
      "Epoch: 14 \tTraining Loss: 0.000076\n",
      "Epoch: 15 \tTraining Loss: 0.000076\n",
      "Epoch: 16 \tTraining Loss: 0.000076\n",
      "Epoch: 17 \tTraining Loss: 0.000076\n",
      "Epoch: 18 \tTraining Loss: 0.000076\n",
      "Epoch: 1 \tTraining Loss: 0.000104\n",
      "Epoch: 2 \tTraining Loss: 0.000104\n",
      "Epoch: 3 \tTraining Loss: 0.000104\n",
      "Epoch: 4 \tTraining Loss: 0.000104\n",
      "Epoch: 5 \tTraining Loss: 0.000104\n",
      "Epoch: 6 \tTraining Loss: 0.000104\n",
      "Epoch: 7 \tTraining Loss: 0.000104\n",
      "Epoch: 8 \tTraining Loss: 0.000104\n",
      "Epoch: 9 \tTraining Loss: 0.000104\n",
      "Epoch: 10 \tTraining Loss: 0.000104\n",
      "Epoch: 11 \tTraining Loss: 0.000104\n",
      "Epoch: 12 \tTraining Loss: 0.000104\n",
      "Epoch: 13 \tTraining Loss: 0.000104\n",
      "Epoch: 14 \tTraining Loss: 0.000104\n",
      "Epoch: 15 \tTraining Loss: 0.000104\n",
      "Epoch: 16 \tTraining Loss: 0.000104\n",
      "Epoch: 17 \tTraining Loss: 0.000104\n",
      "Epoch: 18 \tTraining Loss: 0.000104\n",
      "Epoch: 1 \tTraining Loss: 0.000090\n",
      "Epoch: 2 \tTraining Loss: 0.000090\n",
      "Epoch: 3 \tTraining Loss: 0.000090\n",
      "Epoch: 4 \tTraining Loss: 0.000090\n",
      "Epoch: 5 \tTraining Loss: 0.000090\n",
      "Epoch: 6 \tTraining Loss: 0.000090\n",
      "Epoch: 7 \tTraining Loss: 0.000090\n",
      "Epoch: 8 \tTraining Loss: 0.000090\n",
      "Epoch: 9 \tTraining Loss: 0.000090\n",
      "Epoch: 10 \tTraining Loss: 0.000090\n",
      "Epoch: 11 \tTraining Loss: 0.000090\n",
      "Epoch: 12 \tTraining Loss: 0.000090\n",
      "Epoch: 13 \tTraining Loss: 0.000090\n",
      "Epoch: 14 \tTraining Loss: 0.000090\n",
      "Epoch: 15 \tTraining Loss: 0.000090\n",
      "Epoch: 16 \tTraining Loss: 0.000090\n",
      "Epoch: 17 \tTraining Loss: 0.000090\n",
      "Epoch: 18 \tTraining Loss: 0.000090\n",
      "Epoch: 1 \tTraining Loss: 0.000125\n",
      "Epoch: 2 \tTraining Loss: 0.000125\n",
      "Epoch: 3 \tTraining Loss: 0.000125\n",
      "Epoch: 4 \tTraining Loss: 0.000125\n",
      "Epoch: 5 \tTraining Loss: 0.000125\n",
      "Epoch: 6 \tTraining Loss: 0.000125\n",
      "Epoch: 7 \tTraining Loss: 0.000125\n",
      "Epoch: 8 \tTraining Loss: 0.000125\n",
      "Epoch: 9 \tTraining Loss: 0.000125\n",
      "Epoch: 10 \tTraining Loss: 0.000125\n",
      "Epoch: 11 \tTraining Loss: 0.000125\n",
      "Epoch: 12 \tTraining Loss: 0.000125\n",
      "Epoch: 13 \tTraining Loss: 0.000125\n",
      "Epoch: 14 \tTraining Loss: 0.000125\n",
      "Epoch: 15 \tTraining Loss: 0.000125\n",
      "Epoch: 16 \tTraining Loss: 0.000125\n",
      "Epoch: 17 \tTraining Loss: 0.000125\n",
      "Epoch: 18 \tTraining Loss: 0.000125\n",
      "Epoch: 1 \tTraining Loss: 0.000118\n",
      "Epoch: 2 \tTraining Loss: 0.000118\n",
      "Epoch: 3 \tTraining Loss: 0.000118\n",
      "Epoch: 4 \tTraining Loss: 0.000118\n",
      "Epoch: 5 \tTraining Loss: 0.000118\n",
      "Epoch: 6 \tTraining Loss: 0.000118\n",
      "Epoch: 7 \tTraining Loss: 0.000118\n",
      "Epoch: 8 \tTraining Loss: 0.000118\n",
      "Epoch: 9 \tTraining Loss: 0.000118\n",
      "Epoch: 10 \tTraining Loss: 0.000118\n",
      "Epoch: 11 \tTraining Loss: 0.000118\n",
      "Epoch: 12 \tTraining Loss: 0.000118\n",
      "Epoch: 13 \tTraining Loss: 0.000118\n",
      "Epoch: 14 \tTraining Loss: 0.000118\n",
      "Epoch: 15 \tTraining Loss: 0.000118\n",
      "Epoch: 16 \tTraining Loss: 0.000118\n",
      "Epoch: 17 \tTraining Loss: 0.000118\n",
      "Epoch: 18 \tTraining Loss: 0.000118\n",
      "Epoch: 1 \tTraining Loss: 0.000128\n",
      "Epoch: 2 \tTraining Loss: 0.000128\n",
      "Epoch: 3 \tTraining Loss: 0.000128\n",
      "Epoch: 4 \tTraining Loss: 0.000128\n",
      "Epoch: 5 \tTraining Loss: 0.000128\n",
      "Epoch: 6 \tTraining Loss: 0.000128\n",
      "Epoch: 7 \tTraining Loss: 0.000128\n",
      "Epoch: 8 \tTraining Loss: 0.000128\n",
      "Epoch: 9 \tTraining Loss: 0.000128\n",
      "Epoch: 10 \tTraining Loss: 0.000128\n",
      "Epoch: 11 \tTraining Loss: 0.000128\n",
      "Epoch: 12 \tTraining Loss: 0.000128\n",
      "Epoch: 13 \tTraining Loss: 0.000128\n",
      "Epoch: 14 \tTraining Loss: 0.000128\n",
      "Epoch: 15 \tTraining Loss: 0.000128\n",
      "Epoch: 16 \tTraining Loss: 0.000128\n",
      "Epoch: 17 \tTraining Loss: 0.000128\n",
      "Epoch: 18 \tTraining Loss: 0.000128\n",
      "Epoch: 1 \tTraining Loss: 0.000114\n",
      "Epoch: 2 \tTraining Loss: 0.000114\n",
      "Epoch: 3 \tTraining Loss: 0.000114\n",
      "Epoch: 4 \tTraining Loss: 0.000114\n",
      "Epoch: 5 \tTraining Loss: 0.000114\n",
      "Epoch: 6 \tTraining Loss: 0.000114\n",
      "Epoch: 7 \tTraining Loss: 0.000114\n",
      "Epoch: 8 \tTraining Loss: 0.000114\n",
      "Epoch: 9 \tTraining Loss: 0.000114\n",
      "Epoch: 10 \tTraining Loss: 0.000114\n",
      "Epoch: 11 \tTraining Loss: 0.000114\n",
      "Epoch: 12 \tTraining Loss: 0.000114\n",
      "Epoch: 13 \tTraining Loss: 0.000114\n",
      "Epoch: 14 \tTraining Loss: 0.000114\n",
      "Epoch: 15 \tTraining Loss: 0.000114\n",
      "Epoch: 16 \tTraining Loss: 0.000114\n",
      "Epoch: 17 \tTraining Loss: 0.000114\n",
      "Epoch: 18 \tTraining Loss: 0.000114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000235\n",
      "Epoch: 2 \tTraining Loss: 0.000235\n",
      "Epoch: 3 \tTraining Loss: 0.000235\n",
      "Epoch: 4 \tTraining Loss: 0.000235\n",
      "Epoch: 5 \tTraining Loss: 0.000235\n",
      "Epoch: 6 \tTraining Loss: 0.000235\n",
      "Epoch: 7 \tTraining Loss: 0.000235\n",
      "Epoch: 8 \tTraining Loss: 0.000235\n",
      "Epoch: 9 \tTraining Loss: 0.000235\n",
      "Epoch: 10 \tTraining Loss: 0.000235\n",
      "Epoch: 11 \tTraining Loss: 0.000235\n",
      "Epoch: 12 \tTraining Loss: 0.000235\n",
      "Epoch: 13 \tTraining Loss: 0.000235\n",
      "Epoch: 14 \tTraining Loss: 0.000235\n",
      "Epoch: 15 \tTraining Loss: 0.000235\n",
      "Epoch: 16 \tTraining Loss: 0.000235\n",
      "Epoch: 17 \tTraining Loss: 0.000235\n",
      "Epoch: 18 \tTraining Loss: 0.000235\n",
      "Epoch: 1 \tTraining Loss: 0.000224\n",
      "Epoch: 2 \tTraining Loss: 0.000224\n",
      "Epoch: 3 \tTraining Loss: 0.000224\n",
      "Epoch: 4 \tTraining Loss: 0.000224\n",
      "Epoch: 5 \tTraining Loss: 0.000224\n",
      "Epoch: 6 \tTraining Loss: 0.000224\n",
      "Epoch: 7 \tTraining Loss: 0.000224\n",
      "Epoch: 8 \tTraining Loss: 0.000224\n",
      "Epoch: 9 \tTraining Loss: 0.000224\n",
      "Epoch: 10 \tTraining Loss: 0.000224\n",
      "Epoch: 11 \tTraining Loss: 0.000224\n",
      "Epoch: 12 \tTraining Loss: 0.000224\n",
      "Epoch: 13 \tTraining Loss: 0.000224\n",
      "Epoch: 14 \tTraining Loss: 0.000224\n",
      "Epoch: 15 \tTraining Loss: 0.000224\n",
      "Epoch: 16 \tTraining Loss: 0.000224\n",
      "Epoch: 17 \tTraining Loss: 0.000224\n",
      "Epoch: 18 \tTraining Loss: 0.000224\n",
      "Epoch: 1 \tTraining Loss: 0.000069\n",
      "Epoch: 2 \tTraining Loss: 0.000069\n",
      "Epoch: 3 \tTraining Loss: 0.000069\n",
      "Epoch: 4 \tTraining Loss: 0.000069\n",
      "Epoch: 5 \tTraining Loss: 0.000069\n",
      "Epoch: 6 \tTraining Loss: 0.000069\n",
      "Epoch: 7 \tTraining Loss: 0.000069\n",
      "Epoch: 8 \tTraining Loss: 0.000069\n",
      "Epoch: 9 \tTraining Loss: 0.000069\n",
      "Epoch: 10 \tTraining Loss: 0.000069\n",
      "Epoch: 11 \tTraining Loss: 0.000069\n",
      "Epoch: 12 \tTraining Loss: 0.000069\n",
      "Epoch: 13 \tTraining Loss: 0.000069\n",
      "Epoch: 14 \tTraining Loss: 0.000069\n",
      "Epoch: 15 \tTraining Loss: 0.000069\n",
      "Epoch: 16 \tTraining Loss: 0.000069\n",
      "Epoch: 17 \tTraining Loss: 0.000069\n",
      "Epoch: 18 \tTraining Loss: 0.000069\n",
      "Epoch: 1 \tTraining Loss: 0.000326\n",
      "Epoch: 2 \tTraining Loss: 0.000326\n",
      "Epoch: 3 \tTraining Loss: 0.000326\n",
      "Epoch: 4 \tTraining Loss: 0.000326\n",
      "Epoch: 5 \tTraining Loss: 0.000326\n",
      "Epoch: 6 \tTraining Loss: 0.000326\n",
      "Epoch: 7 \tTraining Loss: 0.000326\n",
      "Epoch: 8 \tTraining Loss: 0.000326\n",
      "Epoch: 9 \tTraining Loss: 0.000326\n",
      "Epoch: 10 \tTraining Loss: 0.000326\n",
      "Epoch: 11 \tTraining Loss: 0.000326\n",
      "Epoch: 12 \tTraining Loss: 0.000326\n",
      "Epoch: 13 \tTraining Loss: 0.000326\n",
      "Epoch: 14 \tTraining Loss: 0.000326\n",
      "Epoch: 15 \tTraining Loss: 0.000326\n",
      "Epoch: 16 \tTraining Loss: 0.000326\n",
      "Epoch: 17 \tTraining Loss: 0.000326\n",
      "Epoch: 18 \tTraining Loss: 0.000326\n",
      "Epoch: 1 \tTraining Loss: 0.000102\n",
      "Epoch: 2 \tTraining Loss: 0.000102\n",
      "Epoch: 3 \tTraining Loss: 0.000102\n",
      "Epoch: 4 \tTraining Loss: 0.000102\n",
      "Epoch: 5 \tTraining Loss: 0.000102\n",
      "Epoch: 6 \tTraining Loss: 0.000102\n",
      "Epoch: 7 \tTraining Loss: 0.000102\n",
      "Epoch: 8 \tTraining Loss: 0.000102\n",
      "Epoch: 9 \tTraining Loss: 0.000102\n",
      "Epoch: 10 \tTraining Loss: 0.000102\n",
      "Epoch: 11 \tTraining Loss: 0.000102\n",
      "Epoch: 12 \tTraining Loss: 0.000102\n",
      "Epoch: 13 \tTraining Loss: 0.000102\n",
      "Epoch: 14 \tTraining Loss: 0.000102\n",
      "Epoch: 15 \tTraining Loss: 0.000102\n",
      "Epoch: 16 \tTraining Loss: 0.000102\n",
      "Epoch: 17 \tTraining Loss: 0.000102\n",
      "Epoch: 18 \tTraining Loss: 0.000102\n",
      "Epoch: 1 \tTraining Loss: 0.000228\n",
      "Epoch: 2 \tTraining Loss: 0.000228\n",
      "Epoch: 3 \tTraining Loss: 0.000228\n",
      "Epoch: 4 \tTraining Loss: 0.000228\n",
      "Epoch: 5 \tTraining Loss: 0.000228\n",
      "Epoch: 6 \tTraining Loss: 0.000228\n",
      "Epoch: 7 \tTraining Loss: 0.000228\n",
      "Epoch: 8 \tTraining Loss: 0.000228\n",
      "Epoch: 9 \tTraining Loss: 0.000228\n",
      "Epoch: 10 \tTraining Loss: 0.000228\n",
      "Epoch: 11 \tTraining Loss: 0.000228\n",
      "Epoch: 12 \tTraining Loss: 0.000228\n",
      "Epoch: 13 \tTraining Loss: 0.000228\n",
      "Epoch: 14 \tTraining Loss: 0.000228\n",
      "Epoch: 15 \tTraining Loss: 0.000228\n",
      "Epoch: 16 \tTraining Loss: 0.000228\n",
      "Epoch: 17 \tTraining Loss: 0.000228\n",
      "Epoch: 18 \tTraining Loss: 0.000228\n",
      "Epoch: 1 \tTraining Loss: 0.000348\n",
      "Epoch: 2 \tTraining Loss: 0.000348\n",
      "Epoch: 3 \tTraining Loss: 0.000348\n",
      "Epoch: 4 \tTraining Loss: 0.000348\n",
      "Epoch: 5 \tTraining Loss: 0.000348\n",
      "Epoch: 6 \tTraining Loss: 0.000348\n",
      "Epoch: 7 \tTraining Loss: 0.000348\n",
      "Epoch: 8 \tTraining Loss: 0.000348\n",
      "Epoch: 9 \tTraining Loss: 0.000348\n",
      "Epoch: 10 \tTraining Loss: 0.000348\n",
      "Epoch: 11 \tTraining Loss: 0.000348\n",
      "Epoch: 12 \tTraining Loss: 0.000348\n",
      "Epoch: 13 \tTraining Loss: 0.000348\n",
      "Epoch: 14 \tTraining Loss: 0.000348\n",
      "Epoch: 15 \tTraining Loss: 0.000348\n",
      "Epoch: 16 \tTraining Loss: 0.000348\n",
      "Epoch: 17 \tTraining Loss: 0.000348\n",
      "Epoch: 18 \tTraining Loss: 0.000348\n",
      "Epoch: 1 \tTraining Loss: 0.000106\n",
      "Epoch: 2 \tTraining Loss: 0.000106\n",
      "Epoch: 3 \tTraining Loss: 0.000106\n",
      "Epoch: 4 \tTraining Loss: 0.000106\n",
      "Epoch: 5 \tTraining Loss: 0.000106\n",
      "Epoch: 6 \tTraining Loss: 0.000106\n",
      "Epoch: 7 \tTraining Loss: 0.000106\n",
      "Epoch: 8 \tTraining Loss: 0.000106\n",
      "Epoch: 9 \tTraining Loss: 0.000106\n",
      "Epoch: 10 \tTraining Loss: 0.000106\n",
      "Epoch: 11 \tTraining Loss: 0.000106\n",
      "Epoch: 12 \tTraining Loss: 0.000106\n",
      "Epoch: 13 \tTraining Loss: 0.000106\n",
      "Epoch: 14 \tTraining Loss: 0.000106\n",
      "Epoch: 15 \tTraining Loss: 0.000106\n",
      "Epoch: 16 \tTraining Loss: 0.000106\n",
      "Epoch: 17 \tTraining Loss: 0.000106\n",
      "Epoch: 18 \tTraining Loss: 0.000106\n",
      "Epoch: 1 \tTraining Loss: 0.000234\n",
      "Epoch: 2 \tTraining Loss: 0.000234\n",
      "Epoch: 3 \tTraining Loss: 0.000234\n",
      "Epoch: 4 \tTraining Loss: 0.000234\n",
      "Epoch: 5 \tTraining Loss: 0.000234\n",
      "Epoch: 6 \tTraining Loss: 0.000234\n",
      "Epoch: 7 \tTraining Loss: 0.000234\n",
      "Epoch: 8 \tTraining Loss: 0.000234\n",
      "Epoch: 9 \tTraining Loss: 0.000234\n",
      "Epoch: 10 \tTraining Loss: 0.000234\n",
      "Epoch: 11 \tTraining Loss: 0.000234\n",
      "Epoch: 12 \tTraining Loss: 0.000234\n",
      "Epoch: 13 \tTraining Loss: 0.000234\n",
      "Epoch: 14 \tTraining Loss: 0.000234\n",
      "Epoch: 15 \tTraining Loss: 0.000234\n",
      "Epoch: 16 \tTraining Loss: 0.000234\n",
      "Epoch: 17 \tTraining Loss: 0.000234\n",
      "Epoch: 18 \tTraining Loss: 0.000234\n",
      "Epoch: 1 \tTraining Loss: 0.000433\n",
      "Epoch: 2 \tTraining Loss: 0.000433\n",
      "Epoch: 3 \tTraining Loss: 0.000433\n",
      "Epoch: 4 \tTraining Loss: 0.000433\n",
      "Epoch: 5 \tTraining Loss: 0.000433\n",
      "Epoch: 6 \tTraining Loss: 0.000433\n",
      "Epoch: 7 \tTraining Loss: 0.000433\n",
      "Epoch: 8 \tTraining Loss: 0.000433\n",
      "Epoch: 9 \tTraining Loss: 0.000433\n",
      "Epoch: 10 \tTraining Loss: 0.000433\n",
      "Epoch: 11 \tTraining Loss: 0.000433\n",
      "Epoch: 12 \tTraining Loss: 0.000433\n",
      "Epoch: 13 \tTraining Loss: 0.000433\n",
      "Epoch: 14 \tTraining Loss: 0.000433\n",
      "Epoch: 15 \tTraining Loss: 0.000433\n",
      "Epoch: 16 \tTraining Loss: 0.000433\n",
      "Epoch: 17 \tTraining Loss: 0.000433\n",
      "Epoch: 18 \tTraining Loss: 0.000433\n",
      "Epoch: 1 \tTraining Loss: 0.000279\n",
      "Epoch: 2 \tTraining Loss: 0.000279\n",
      "Epoch: 3 \tTraining Loss: 0.000279\n",
      "Epoch: 4 \tTraining Loss: 0.000279\n",
      "Epoch: 5 \tTraining Loss: 0.000279\n",
      "Epoch: 6 \tTraining Loss: 0.000279\n",
      "Epoch: 7 \tTraining Loss: 0.000279\n",
      "Epoch: 8 \tTraining Loss: 0.000279\n",
      "Epoch: 9 \tTraining Loss: 0.000279\n",
      "Epoch: 10 \tTraining Loss: 0.000279\n",
      "Epoch: 11 \tTraining Loss: 0.000279\n",
      "Epoch: 12 \tTraining Loss: 0.000279\n",
      "Epoch: 13 \tTraining Loss: 0.000279\n",
      "Epoch: 14 \tTraining Loss: 0.000279\n",
      "Epoch: 15 \tTraining Loss: 0.000279\n",
      "Epoch: 16 \tTraining Loss: 0.000279\n",
      "Epoch: 17 \tTraining Loss: 0.000279\n",
      "Epoch: 18 \tTraining Loss: 0.000279\n",
      "Epoch: 1 \tTraining Loss: 0.000207\n",
      "Epoch: 2 \tTraining Loss: 0.000207\n",
      "Epoch: 3 \tTraining Loss: 0.000207\n",
      "Epoch: 4 \tTraining Loss: 0.000207\n",
      "Epoch: 5 \tTraining Loss: 0.000207\n",
      "Epoch: 6 \tTraining Loss: 0.000207\n",
      "Epoch: 7 \tTraining Loss: 0.000207\n",
      "Epoch: 8 \tTraining Loss: 0.000207\n",
      "Epoch: 9 \tTraining Loss: 0.000207\n",
      "Epoch: 10 \tTraining Loss: 0.000207\n",
      "Epoch: 11 \tTraining Loss: 0.000207\n",
      "Epoch: 12 \tTraining Loss: 0.000207\n",
      "Epoch: 13 \tTraining Loss: 0.000207\n",
      "Epoch: 14 \tTraining Loss: 0.000207\n",
      "Epoch: 15 \tTraining Loss: 0.000207\n",
      "Epoch: 16 \tTraining Loss: 0.000207\n",
      "Epoch: 17 \tTraining Loss: 0.000207\n",
      "Epoch: 18 \tTraining Loss: 0.000207\n",
      "Epoch: 1 \tTraining Loss: 0.000196\n",
      "Epoch: 2 \tTraining Loss: 0.000196\n",
      "Epoch: 3 \tTraining Loss: 0.000196\n",
      "Epoch: 4 \tTraining Loss: 0.000196\n",
      "Epoch: 5 \tTraining Loss: 0.000196\n",
      "Epoch: 6 \tTraining Loss: 0.000196\n",
      "Epoch: 7 \tTraining Loss: 0.000196\n",
      "Epoch: 8 \tTraining Loss: 0.000196\n",
      "Epoch: 9 \tTraining Loss: 0.000196\n",
      "Epoch: 10 \tTraining Loss: 0.000196\n",
      "Epoch: 11 \tTraining Loss: 0.000196\n",
      "Epoch: 12 \tTraining Loss: 0.000196\n",
      "Epoch: 13 \tTraining Loss: 0.000196\n",
      "Epoch: 14 \tTraining Loss: 0.000196\n",
      "Epoch: 15 \tTraining Loss: 0.000196\n",
      "Epoch: 16 \tTraining Loss: 0.000196\n",
      "Epoch: 17 \tTraining Loss: 0.000196\n",
      "Epoch: 18 \tTraining Loss: 0.000196\n",
      "Epoch: 1 \tTraining Loss: 0.000050\n",
      "Epoch: 2 \tTraining Loss: 0.000050\n",
      "Epoch: 3 \tTraining Loss: 0.000050\n",
      "Epoch: 4 \tTraining Loss: 0.000050\n",
      "Epoch: 5 \tTraining Loss: 0.000050\n",
      "Epoch: 6 \tTraining Loss: 0.000050\n",
      "Epoch: 7 \tTraining Loss: 0.000050\n",
      "Epoch: 8 \tTraining Loss: 0.000050\n",
      "Epoch: 9 \tTraining Loss: 0.000050\n",
      "Epoch: 10 \tTraining Loss: 0.000050\n",
      "Epoch: 11 \tTraining Loss: 0.000050\n",
      "Epoch: 12 \tTraining Loss: 0.000050\n",
      "Epoch: 13 \tTraining Loss: 0.000050\n",
      "Epoch: 14 \tTraining Loss: 0.000050\n",
      "Epoch: 15 \tTraining Loss: 0.000050\n",
      "Epoch: 16 \tTraining Loss: 0.000050\n",
      "Epoch: 17 \tTraining Loss: 0.000050\n",
      "Epoch: 18 \tTraining Loss: 0.000050\n",
      "Epoch: 1 \tTraining Loss: 0.000279\n",
      "Epoch: 2 \tTraining Loss: 0.000279\n",
      "Epoch: 3 \tTraining Loss: 0.000279\n",
      "Epoch: 4 \tTraining Loss: 0.000279\n",
      "Epoch: 5 \tTraining Loss: 0.000279\n",
      "Epoch: 6 \tTraining Loss: 0.000279\n",
      "Epoch: 7 \tTraining Loss: 0.000279\n",
      "Epoch: 8 \tTraining Loss: 0.000279\n",
      "Epoch: 9 \tTraining Loss: 0.000279\n",
      "Epoch: 10 \tTraining Loss: 0.000279\n",
      "Epoch: 11 \tTraining Loss: 0.000279\n",
      "Epoch: 12 \tTraining Loss: 0.000279\n",
      "Epoch: 13 \tTraining Loss: 0.000279\n",
      "Epoch: 14 \tTraining Loss: 0.000279\n",
      "Epoch: 15 \tTraining Loss: 0.000279\n",
      "Epoch: 16 \tTraining Loss: 0.000279\n",
      "Epoch: 17 \tTraining Loss: 0.000279\n",
      "Epoch: 18 \tTraining Loss: 0.000279\n",
      "Epoch: 1 \tTraining Loss: 0.000090\n",
      "Epoch: 2 \tTraining Loss: 0.000090\n",
      "Epoch: 3 \tTraining Loss: 0.000090\n",
      "Epoch: 4 \tTraining Loss: 0.000090\n",
      "Epoch: 5 \tTraining Loss: 0.000090\n",
      "Epoch: 6 \tTraining Loss: 0.000090\n",
      "Epoch: 7 \tTraining Loss: 0.000090\n",
      "Epoch: 8 \tTraining Loss: 0.000090\n",
      "Epoch: 9 \tTraining Loss: 0.000090\n",
      "Epoch: 10 \tTraining Loss: 0.000090\n",
      "Epoch: 11 \tTraining Loss: 0.000090\n",
      "Epoch: 12 \tTraining Loss: 0.000090\n",
      "Epoch: 13 \tTraining Loss: 0.000090\n",
      "Epoch: 14 \tTraining Loss: 0.000090\n",
      "Epoch: 15 \tTraining Loss: 0.000090\n",
      "Epoch: 16 \tTraining Loss: 0.000090\n",
      "Epoch: 17 \tTraining Loss: 0.000090\n",
      "Epoch: 18 \tTraining Loss: 0.000090\n",
      "Epoch: 1 \tTraining Loss: 0.000075\n",
      "Epoch: 2 \tTraining Loss: 0.000075\n",
      "Epoch: 3 \tTraining Loss: 0.000075\n",
      "Epoch: 4 \tTraining Loss: 0.000075\n",
      "Epoch: 5 \tTraining Loss: 0.000075\n",
      "Epoch: 6 \tTraining Loss: 0.000075\n",
      "Epoch: 7 \tTraining Loss: 0.000075\n",
      "Epoch: 8 \tTraining Loss: 0.000075\n",
      "Epoch: 9 \tTraining Loss: 0.000075\n",
      "Epoch: 10 \tTraining Loss: 0.000075\n",
      "Epoch: 11 \tTraining Loss: 0.000075\n",
      "Epoch: 12 \tTraining Loss: 0.000075\n",
      "Epoch: 13 \tTraining Loss: 0.000075\n",
      "Epoch: 14 \tTraining Loss: 0.000075\n",
      "Epoch: 15 \tTraining Loss: 0.000075\n",
      "Epoch: 16 \tTraining Loss: 0.000075\n",
      "Epoch: 17 \tTraining Loss: 0.000075\n",
      "Epoch: 18 \tTraining Loss: 0.000075\n",
      "Epoch: 1 \tTraining Loss: 0.000116\n",
      "Epoch: 2 \tTraining Loss: 0.000116\n",
      "Epoch: 3 \tTraining Loss: 0.000116\n",
      "Epoch: 4 \tTraining Loss: 0.000116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 \tTraining Loss: 0.000116\n",
      "Epoch: 6 \tTraining Loss: 0.000116\n",
      "Epoch: 7 \tTraining Loss: 0.000116\n",
      "Epoch: 8 \tTraining Loss: 0.000116\n",
      "Epoch: 9 \tTraining Loss: 0.000116\n",
      "Epoch: 10 \tTraining Loss: 0.000116\n",
      "Epoch: 11 \tTraining Loss: 0.000116\n",
      "Epoch: 12 \tTraining Loss: 0.000116\n",
      "Epoch: 13 \tTraining Loss: 0.000116\n",
      "Epoch: 14 \tTraining Loss: 0.000116\n",
      "Epoch: 15 \tTraining Loss: 0.000116\n",
      "Epoch: 16 \tTraining Loss: 0.000116\n",
      "Epoch: 17 \tTraining Loss: 0.000116\n",
      "Epoch: 18 \tTraining Loss: 0.000116\n",
      "Epoch: 1 \tTraining Loss: 0.000204\n",
      "Epoch: 2 \tTraining Loss: 0.000204\n",
      "Epoch: 3 \tTraining Loss: 0.000204\n",
      "Epoch: 4 \tTraining Loss: 0.000204\n",
      "Epoch: 5 \tTraining Loss: 0.000204\n",
      "Epoch: 6 \tTraining Loss: 0.000204\n",
      "Epoch: 7 \tTraining Loss: 0.000204\n",
      "Epoch: 8 \tTraining Loss: 0.000204\n",
      "Epoch: 9 \tTraining Loss: 0.000204\n",
      "Epoch: 10 \tTraining Loss: 0.000204\n",
      "Epoch: 11 \tTraining Loss: 0.000204\n",
      "Epoch: 12 \tTraining Loss: 0.000204\n",
      "Epoch: 13 \tTraining Loss: 0.000204\n",
      "Epoch: 14 \tTraining Loss: 0.000204\n",
      "Epoch: 15 \tTraining Loss: 0.000204\n",
      "Epoch: 16 \tTraining Loss: 0.000204\n",
      "Epoch: 17 \tTraining Loss: 0.000204\n",
      "Epoch: 18 \tTraining Loss: 0.000204\n",
      "Epoch: 1 \tTraining Loss: 0.000096\n",
      "Epoch: 2 \tTraining Loss: 0.000096\n",
      "Epoch: 3 \tTraining Loss: 0.000096\n",
      "Epoch: 4 \tTraining Loss: 0.000096\n",
      "Epoch: 5 \tTraining Loss: 0.000096\n",
      "Epoch: 6 \tTraining Loss: 0.000096\n",
      "Epoch: 7 \tTraining Loss: 0.000096\n",
      "Epoch: 8 \tTraining Loss: 0.000096\n",
      "Epoch: 9 \tTraining Loss: 0.000096\n",
      "Epoch: 10 \tTraining Loss: 0.000096\n",
      "Epoch: 11 \tTraining Loss: 0.000096\n",
      "Epoch: 12 \tTraining Loss: 0.000096\n",
      "Epoch: 13 \tTraining Loss: 0.000096\n",
      "Epoch: 14 \tTraining Loss: 0.000096\n",
      "Epoch: 15 \tTraining Loss: 0.000096\n",
      "Epoch: 16 \tTraining Loss: 0.000096\n",
      "Epoch: 17 \tTraining Loss: 0.000096\n",
      "Epoch: 18 \tTraining Loss: 0.000096\n",
      "Epoch: 1 \tTraining Loss: 0.000306\n",
      "Epoch: 2 \tTraining Loss: 0.000306\n",
      "Epoch: 3 \tTraining Loss: 0.000306\n",
      "Epoch: 4 \tTraining Loss: 0.000306\n",
      "Epoch: 5 \tTraining Loss: 0.000306\n",
      "Epoch: 6 \tTraining Loss: 0.000306\n",
      "Epoch: 7 \tTraining Loss: 0.000306\n",
      "Epoch: 8 \tTraining Loss: 0.000306\n",
      "Epoch: 9 \tTraining Loss: 0.000306\n",
      "Epoch: 10 \tTraining Loss: 0.000306\n",
      "Epoch: 11 \tTraining Loss: 0.000306\n",
      "Epoch: 12 \tTraining Loss: 0.000306\n",
      "Epoch: 13 \tTraining Loss: 0.000306\n",
      "Epoch: 14 \tTraining Loss: 0.000306\n",
      "Epoch: 15 \tTraining Loss: 0.000306\n",
      "Epoch: 16 \tTraining Loss: 0.000306\n",
      "Epoch: 17 \tTraining Loss: 0.000306\n",
      "Epoch: 18 \tTraining Loss: 0.000306\n",
      "Epoch: 1 \tTraining Loss: 0.000244\n",
      "Epoch: 2 \tTraining Loss: 0.000244\n",
      "Epoch: 3 \tTraining Loss: 0.000244\n",
      "Epoch: 4 \tTraining Loss: 0.000244\n",
      "Epoch: 5 \tTraining Loss: 0.000244\n",
      "Epoch: 6 \tTraining Loss: 0.000244\n",
      "Epoch: 7 \tTraining Loss: 0.000244\n",
      "Epoch: 8 \tTraining Loss: 0.000244\n",
      "Epoch: 9 \tTraining Loss: 0.000244\n",
      "Epoch: 10 \tTraining Loss: 0.000244\n",
      "Epoch: 11 \tTraining Loss: 0.000244\n",
      "Epoch: 12 \tTraining Loss: 0.000244\n",
      "Epoch: 13 \tTraining Loss: 0.000244\n",
      "Epoch: 14 \tTraining Loss: 0.000244\n",
      "Epoch: 15 \tTraining Loss: 0.000244\n",
      "Epoch: 16 \tTraining Loss: 0.000244\n",
      "Epoch: 17 \tTraining Loss: 0.000244\n",
      "Epoch: 18 \tTraining Loss: 0.000244\n",
      "Epoch: 1 \tTraining Loss: 0.000087\n",
      "Epoch: 2 \tTraining Loss: 0.000087\n",
      "Epoch: 3 \tTraining Loss: 0.000087\n",
      "Epoch: 4 \tTraining Loss: 0.000087\n",
      "Epoch: 5 \tTraining Loss: 0.000087\n",
      "Epoch: 6 \tTraining Loss: 0.000087\n",
      "Epoch: 7 \tTraining Loss: 0.000087\n",
      "Epoch: 8 \tTraining Loss: 0.000087\n",
      "Epoch: 9 \tTraining Loss: 0.000087\n",
      "Epoch: 10 \tTraining Loss: 0.000087\n",
      "Epoch: 11 \tTraining Loss: 0.000087\n",
      "Epoch: 12 \tTraining Loss: 0.000087\n",
      "Epoch: 13 \tTraining Loss: 0.000087\n",
      "Epoch: 14 \tTraining Loss: 0.000087\n",
      "Epoch: 15 \tTraining Loss: 0.000087\n",
      "Epoch: 16 \tTraining Loss: 0.000087\n",
      "Epoch: 17 \tTraining Loss: 0.000087\n",
      "Epoch: 18 \tTraining Loss: 0.000087\n",
      "Epoch: 1 \tTraining Loss: 0.000229\n",
      "Epoch: 2 \tTraining Loss: 0.000229\n",
      "Epoch: 3 \tTraining Loss: 0.000229\n",
      "Epoch: 4 \tTraining Loss: 0.000229\n",
      "Epoch: 5 \tTraining Loss: 0.000229\n",
      "Epoch: 6 \tTraining Loss: 0.000229\n",
      "Epoch: 7 \tTraining Loss: 0.000229\n",
      "Epoch: 8 \tTraining Loss: 0.000229\n",
      "Epoch: 9 \tTraining Loss: 0.000229\n",
      "Epoch: 10 \tTraining Loss: 0.000229\n",
      "Epoch: 11 \tTraining Loss: 0.000229\n",
      "Epoch: 12 \tTraining Loss: 0.000229\n",
      "Epoch: 13 \tTraining Loss: 0.000229\n",
      "Epoch: 14 \tTraining Loss: 0.000229\n",
      "Epoch: 15 \tTraining Loss: 0.000229\n",
      "Epoch: 16 \tTraining Loss: 0.000229\n",
      "Epoch: 17 \tTraining Loss: 0.000229\n",
      "Epoch: 18 \tTraining Loss: 0.000229\n",
      "Epoch: 1 \tTraining Loss: 0.000226\n",
      "Epoch: 2 \tTraining Loss: 0.000226\n",
      "Epoch: 3 \tTraining Loss: 0.000226\n",
      "Epoch: 4 \tTraining Loss: 0.000226\n",
      "Epoch: 5 \tTraining Loss: 0.000226\n",
      "Epoch: 6 \tTraining Loss: 0.000226\n",
      "Epoch: 7 \tTraining Loss: 0.000226\n",
      "Epoch: 8 \tTraining Loss: 0.000226\n",
      "Epoch: 9 \tTraining Loss: 0.000226\n",
      "Epoch: 10 \tTraining Loss: 0.000226\n",
      "Epoch: 11 \tTraining Loss: 0.000226\n",
      "Epoch: 12 \tTraining Loss: 0.000226\n",
      "Epoch: 13 \tTraining Loss: 0.000226\n",
      "Epoch: 14 \tTraining Loss: 0.000226\n",
      "Epoch: 15 \tTraining Loss: 0.000226\n",
      "Epoch: 16 \tTraining Loss: 0.000226\n",
      "Epoch: 17 \tTraining Loss: 0.000226\n",
      "Epoch: 18 \tTraining Loss: 0.000226\n",
      "Epoch: 1 \tTraining Loss: 0.000254\n",
      "Epoch: 2 \tTraining Loss: 0.000254\n",
      "Epoch: 3 \tTraining Loss: 0.000254\n",
      "Epoch: 4 \tTraining Loss: 0.000254\n",
      "Epoch: 5 \tTraining Loss: 0.000254\n",
      "Epoch: 6 \tTraining Loss: 0.000254\n",
      "Epoch: 7 \tTraining Loss: 0.000254\n",
      "Epoch: 8 \tTraining Loss: 0.000254\n",
      "Epoch: 9 \tTraining Loss: 0.000254\n",
      "Epoch: 10 \tTraining Loss: 0.000254\n",
      "Epoch: 11 \tTraining Loss: 0.000254\n",
      "Epoch: 12 \tTraining Loss: 0.000254\n",
      "Epoch: 13 \tTraining Loss: 0.000254\n",
      "Epoch: 14 \tTraining Loss: 0.000254\n",
      "Epoch: 15 \tTraining Loss: 0.000254\n",
      "Epoch: 16 \tTraining Loss: 0.000254\n",
      "Epoch: 17 \tTraining Loss: 0.000254\n",
      "Epoch: 18 \tTraining Loss: 0.000254\n",
      "Epoch: 1 \tTraining Loss: 0.000183\n",
      "Epoch: 2 \tTraining Loss: 0.000183\n",
      "Epoch: 3 \tTraining Loss: 0.000183\n",
      "Epoch: 4 \tTraining Loss: 0.000183\n",
      "Epoch: 5 \tTraining Loss: 0.000183\n",
      "Epoch: 6 \tTraining Loss: 0.000183\n",
      "Epoch: 7 \tTraining Loss: 0.000183\n",
      "Epoch: 8 \tTraining Loss: 0.000183\n",
      "Epoch: 9 \tTraining Loss: 0.000183\n",
      "Epoch: 10 \tTraining Loss: 0.000183\n",
      "Epoch: 11 \tTraining Loss: 0.000183\n",
      "Epoch: 12 \tTraining Loss: 0.000183\n",
      "Epoch: 13 \tTraining Loss: 0.000183\n",
      "Epoch: 14 \tTraining Loss: 0.000183\n",
      "Epoch: 15 \tTraining Loss: 0.000183\n",
      "Epoch: 16 \tTraining Loss: 0.000183\n",
      "Epoch: 17 \tTraining Loss: 0.000183\n",
      "Epoch: 18 \tTraining Loss: 0.000183\n",
      "Epoch: 1 \tTraining Loss: 0.000185\n",
      "Epoch: 2 \tTraining Loss: 0.000185\n",
      "Epoch: 3 \tTraining Loss: 0.000185\n",
      "Epoch: 4 \tTraining Loss: 0.000185\n",
      "Epoch: 5 \tTraining Loss: 0.000185\n",
      "Epoch: 6 \tTraining Loss: 0.000185\n",
      "Epoch: 7 \tTraining Loss: 0.000185\n",
      "Epoch: 8 \tTraining Loss: 0.000185\n",
      "Epoch: 9 \tTraining Loss: 0.000185\n",
      "Epoch: 10 \tTraining Loss: 0.000185\n",
      "Epoch: 11 \tTraining Loss: 0.000185\n",
      "Epoch: 12 \tTraining Loss: 0.000185\n",
      "Epoch: 13 \tTraining Loss: 0.000185\n",
      "Epoch: 14 \tTraining Loss: 0.000185\n",
      "Epoch: 15 \tTraining Loss: 0.000185\n",
      "Epoch: 16 \tTraining Loss: 0.000185\n",
      "Epoch: 17 \tTraining Loss: 0.000185\n",
      "Epoch: 18 \tTraining Loss: 0.000185\n",
      "Epoch: 1 \tTraining Loss: 0.000151\n",
      "Epoch: 2 \tTraining Loss: 0.000151\n",
      "Epoch: 3 \tTraining Loss: 0.000151\n",
      "Epoch: 4 \tTraining Loss: 0.000151\n",
      "Epoch: 5 \tTraining Loss: 0.000151\n",
      "Epoch: 6 \tTraining Loss: 0.000151\n",
      "Epoch: 7 \tTraining Loss: 0.000151\n",
      "Epoch: 8 \tTraining Loss: 0.000151\n",
      "Epoch: 9 \tTraining Loss: 0.000151\n",
      "Epoch: 10 \tTraining Loss: 0.000151\n",
      "Epoch: 11 \tTraining Loss: 0.000151\n",
      "Epoch: 12 \tTraining Loss: 0.000151\n",
      "Epoch: 13 \tTraining Loss: 0.000151\n",
      "Epoch: 14 \tTraining Loss: 0.000151\n",
      "Epoch: 15 \tTraining Loss: 0.000151\n",
      "Epoch: 16 \tTraining Loss: 0.000151\n",
      "Epoch: 17 \tTraining Loss: 0.000151\n",
      "Epoch: 18 \tTraining Loss: 0.000151\n",
      "Epoch: 1 \tTraining Loss: 0.000244\n",
      "Epoch: 2 \tTraining Loss: 0.000244\n",
      "Epoch: 3 \tTraining Loss: 0.000244\n",
      "Epoch: 4 \tTraining Loss: 0.000244\n",
      "Epoch: 5 \tTraining Loss: 0.000244\n",
      "Epoch: 6 \tTraining Loss: 0.000244\n",
      "Epoch: 7 \tTraining Loss: 0.000244\n",
      "Epoch: 8 \tTraining Loss: 0.000244\n",
      "Epoch: 9 \tTraining Loss: 0.000244\n",
      "Epoch: 10 \tTraining Loss: 0.000244\n",
      "Epoch: 11 \tTraining Loss: 0.000244\n",
      "Epoch: 12 \tTraining Loss: 0.000244\n",
      "Epoch: 13 \tTraining Loss: 0.000244\n",
      "Epoch: 14 \tTraining Loss: 0.000244\n",
      "Epoch: 15 \tTraining Loss: 0.000244\n",
      "Epoch: 16 \tTraining Loss: 0.000244\n",
      "Epoch: 17 \tTraining Loss: 0.000244\n",
      "Epoch: 18 \tTraining Loss: 0.000244\n",
      "Epoch: 1 \tTraining Loss: 0.000215\n",
      "Epoch: 2 \tTraining Loss: 0.000215\n",
      "Epoch: 3 \tTraining Loss: 0.000215\n",
      "Epoch: 4 \tTraining Loss: 0.000215\n",
      "Epoch: 5 \tTraining Loss: 0.000215\n",
      "Epoch: 6 \tTraining Loss: 0.000215\n",
      "Epoch: 7 \tTraining Loss: 0.000215\n",
      "Epoch: 8 \tTraining Loss: 0.000215\n",
      "Epoch: 9 \tTraining Loss: 0.000215\n",
      "Epoch: 10 \tTraining Loss: 0.000215\n",
      "Epoch: 11 \tTraining Loss: 0.000215\n",
      "Epoch: 12 \tTraining Loss: 0.000215\n",
      "Epoch: 13 \tTraining Loss: 0.000215\n",
      "Epoch: 14 \tTraining Loss: 0.000215\n",
      "Epoch: 15 \tTraining Loss: 0.000215\n",
      "Epoch: 16 \tTraining Loss: 0.000215\n",
      "Epoch: 17 \tTraining Loss: 0.000215\n",
      "Epoch: 18 \tTraining Loss: 0.000215\n",
      "Epoch: 1 \tTraining Loss: 0.000108\n",
      "Epoch: 2 \tTraining Loss: 0.000108\n",
      "Epoch: 3 \tTraining Loss: 0.000108\n",
      "Epoch: 4 \tTraining Loss: 0.000108\n",
      "Epoch: 5 \tTraining Loss: 0.000108\n",
      "Epoch: 6 \tTraining Loss: 0.000108\n",
      "Epoch: 7 \tTraining Loss: 0.000108\n",
      "Epoch: 8 \tTraining Loss: 0.000108\n",
      "Epoch: 9 \tTraining Loss: 0.000108\n",
      "Epoch: 10 \tTraining Loss: 0.000108\n",
      "Epoch: 11 \tTraining Loss: 0.000108\n",
      "Epoch: 12 \tTraining Loss: 0.000108\n",
      "Epoch: 13 \tTraining Loss: 0.000108\n",
      "Epoch: 14 \tTraining Loss: 0.000108\n",
      "Epoch: 15 \tTraining Loss: 0.000108\n",
      "Epoch: 16 \tTraining Loss: 0.000108\n",
      "Epoch: 17 \tTraining Loss: 0.000108\n",
      "Epoch: 18 \tTraining Loss: 0.000108\n",
      "Epoch: 1 \tTraining Loss: 0.000120\n",
      "Epoch: 2 \tTraining Loss: 0.000120\n",
      "Epoch: 3 \tTraining Loss: 0.000120\n",
      "Epoch: 4 \tTraining Loss: 0.000120\n",
      "Epoch: 5 \tTraining Loss: 0.000120\n",
      "Epoch: 6 \tTraining Loss: 0.000120\n",
      "Epoch: 7 \tTraining Loss: 0.000120\n",
      "Epoch: 8 \tTraining Loss: 0.000120\n",
      "Epoch: 9 \tTraining Loss: 0.000120\n",
      "Epoch: 10 \tTraining Loss: 0.000120\n",
      "Epoch: 11 \tTraining Loss: 0.000120\n",
      "Epoch: 12 \tTraining Loss: 0.000120\n",
      "Epoch: 13 \tTraining Loss: 0.000120\n",
      "Epoch: 14 \tTraining Loss: 0.000120\n",
      "Epoch: 15 \tTraining Loss: 0.000120\n",
      "Epoch: 16 \tTraining Loss: 0.000120\n",
      "Epoch: 17 \tTraining Loss: 0.000120\n",
      "Epoch: 18 \tTraining Loss: 0.000120\n",
      "Epoch: 1 \tTraining Loss: 0.000174\n",
      "Epoch: 2 \tTraining Loss: 0.000174\n",
      "Epoch: 3 \tTraining Loss: 0.000174\n",
      "Epoch: 4 \tTraining Loss: 0.000174\n",
      "Epoch: 5 \tTraining Loss: 0.000174\n",
      "Epoch: 6 \tTraining Loss: 0.000174\n",
      "Epoch: 7 \tTraining Loss: 0.000174\n",
      "Epoch: 8 \tTraining Loss: 0.000174\n",
      "Epoch: 9 \tTraining Loss: 0.000174\n",
      "Epoch: 10 \tTraining Loss: 0.000174\n",
      "Epoch: 11 \tTraining Loss: 0.000174\n",
      "Epoch: 12 \tTraining Loss: 0.000174\n",
      "Epoch: 13 \tTraining Loss: 0.000174\n",
      "Epoch: 14 \tTraining Loss: 0.000174\n",
      "Epoch: 15 \tTraining Loss: 0.000174\n",
      "Epoch: 16 \tTraining Loss: 0.000174\n",
      "Epoch: 17 \tTraining Loss: 0.000174\n",
      "Epoch: 18 \tTraining Loss: 0.000174\n",
      "Epoch: 1 \tTraining Loss: 0.000284\n",
      "Epoch: 2 \tTraining Loss: 0.000284\n",
      "Epoch: 3 \tTraining Loss: 0.000284\n",
      "Epoch: 4 \tTraining Loss: 0.000284\n",
      "Epoch: 5 \tTraining Loss: 0.000284\n",
      "Epoch: 6 \tTraining Loss: 0.000284\n",
      "Epoch: 7 \tTraining Loss: 0.000284\n",
      "Epoch: 8 \tTraining Loss: 0.000284\n",
      "Epoch: 9 \tTraining Loss: 0.000284\n",
      "Epoch: 10 \tTraining Loss: 0.000284\n",
      "Epoch: 11 \tTraining Loss: 0.000284\n",
      "Epoch: 12 \tTraining Loss: 0.000284\n",
      "Epoch: 13 \tTraining Loss: 0.000284\n",
      "Epoch: 14 \tTraining Loss: 0.000284\n",
      "Epoch: 15 \tTraining Loss: 0.000284\n",
      "Epoch: 16 \tTraining Loss: 0.000284\n",
      "Epoch: 17 \tTraining Loss: 0.000284\n",
      "Epoch: 18 \tTraining Loss: 0.000284\n",
      "Epoch: 1 \tTraining Loss: 0.000115\n",
      "Epoch: 2 \tTraining Loss: 0.000115\n",
      "Epoch: 3 \tTraining Loss: 0.000115\n",
      "Epoch: 4 \tTraining Loss: 0.000115\n",
      "Epoch: 5 \tTraining Loss: 0.000115\n",
      "Epoch: 6 \tTraining Loss: 0.000115\n",
      "Epoch: 7 \tTraining Loss: 0.000115\n",
      "Epoch: 8 \tTraining Loss: 0.000115\n",
      "Epoch: 9 \tTraining Loss: 0.000115\n",
      "Epoch: 10 \tTraining Loss: 0.000115\n",
      "Epoch: 11 \tTraining Loss: 0.000115\n",
      "Epoch: 12 \tTraining Loss: 0.000115\n",
      "Epoch: 13 \tTraining Loss: 0.000115\n",
      "Epoch: 14 \tTraining Loss: 0.000115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 \tTraining Loss: 0.000115\n",
      "Epoch: 16 \tTraining Loss: 0.000115\n",
      "Epoch: 17 \tTraining Loss: 0.000115\n",
      "Epoch: 18 \tTraining Loss: 0.000115\n",
      "Epoch: 1 \tTraining Loss: 0.000104\n",
      "Epoch: 2 \tTraining Loss: 0.000104\n",
      "Epoch: 3 \tTraining Loss: 0.000104\n",
      "Epoch: 4 \tTraining Loss: 0.000104\n",
      "Epoch: 5 \tTraining Loss: 0.000104\n",
      "Epoch: 6 \tTraining Loss: 0.000104\n",
      "Epoch: 7 \tTraining Loss: 0.000104\n",
      "Epoch: 8 \tTraining Loss: 0.000104\n",
      "Epoch: 9 \tTraining Loss: 0.000104\n",
      "Epoch: 10 \tTraining Loss: 0.000104\n",
      "Epoch: 11 \tTraining Loss: 0.000104\n",
      "Epoch: 12 \tTraining Loss: 0.000104\n",
      "Epoch: 13 \tTraining Loss: 0.000104\n",
      "Epoch: 14 \tTraining Loss: 0.000104\n",
      "Epoch: 15 \tTraining Loss: 0.000104\n",
      "Epoch: 16 \tTraining Loss: 0.000104\n",
      "Epoch: 17 \tTraining Loss: 0.000104\n",
      "Epoch: 18 \tTraining Loss: 0.000104\n",
      "Epoch: 1 \tTraining Loss: 0.000164\n",
      "Epoch: 2 \tTraining Loss: 0.000164\n",
      "Epoch: 3 \tTraining Loss: 0.000164\n",
      "Epoch: 4 \tTraining Loss: 0.000164\n",
      "Epoch: 5 \tTraining Loss: 0.000164\n",
      "Epoch: 6 \tTraining Loss: 0.000164\n",
      "Epoch: 7 \tTraining Loss: 0.000164\n",
      "Epoch: 8 \tTraining Loss: 0.000164\n",
      "Epoch: 9 \tTraining Loss: 0.000164\n",
      "Epoch: 10 \tTraining Loss: 0.000164\n",
      "Epoch: 11 \tTraining Loss: 0.000164\n",
      "Epoch: 12 \tTraining Loss: 0.000164\n",
      "Epoch: 13 \tTraining Loss: 0.000164\n",
      "Epoch: 14 \tTraining Loss: 0.000164\n",
      "Epoch: 15 \tTraining Loss: 0.000164\n",
      "Epoch: 16 \tTraining Loss: 0.000164\n",
      "Epoch: 17 \tTraining Loss: 0.000164\n",
      "Epoch: 18 \tTraining Loss: 0.000164\n",
      "Epoch: 1 \tTraining Loss: 0.000301\n",
      "Epoch: 2 \tTraining Loss: 0.000301\n",
      "Epoch: 3 \tTraining Loss: 0.000301\n",
      "Epoch: 4 \tTraining Loss: 0.000301\n",
      "Epoch: 5 \tTraining Loss: 0.000301\n",
      "Epoch: 6 \tTraining Loss: 0.000301\n",
      "Epoch: 7 \tTraining Loss: 0.000301\n",
      "Epoch: 8 \tTraining Loss: 0.000301\n",
      "Epoch: 9 \tTraining Loss: 0.000301\n",
      "Epoch: 10 \tTraining Loss: 0.000301\n",
      "Epoch: 11 \tTraining Loss: 0.000301\n",
      "Epoch: 12 \tTraining Loss: 0.000301\n",
      "Epoch: 13 \tTraining Loss: 0.000301\n",
      "Epoch: 14 \tTraining Loss: 0.000301\n",
      "Epoch: 15 \tTraining Loss: 0.000301\n",
      "Epoch: 16 \tTraining Loss: 0.000301\n",
      "Epoch: 17 \tTraining Loss: 0.000301\n",
      "Epoch: 18 \tTraining Loss: 0.000301\n",
      "Epoch: 1 \tTraining Loss: 0.000174\n",
      "Epoch: 2 \tTraining Loss: 0.000174\n",
      "Epoch: 3 \tTraining Loss: 0.000174\n",
      "Epoch: 4 \tTraining Loss: 0.000174\n",
      "Epoch: 5 \tTraining Loss: 0.000174\n",
      "Epoch: 6 \tTraining Loss: 0.000174\n",
      "Epoch: 7 \tTraining Loss: 0.000174\n",
      "Epoch: 8 \tTraining Loss: 0.000174\n",
      "Epoch: 9 \tTraining Loss: 0.000174\n",
      "Epoch: 10 \tTraining Loss: 0.000174\n",
      "Epoch: 11 \tTraining Loss: 0.000174\n",
      "Epoch: 12 \tTraining Loss: 0.000174\n",
      "Epoch: 13 \tTraining Loss: 0.000174\n",
      "Epoch: 14 \tTraining Loss: 0.000174\n",
      "Epoch: 15 \tTraining Loss: 0.000174\n",
      "Epoch: 16 \tTraining Loss: 0.000174\n",
      "Epoch: 17 \tTraining Loss: 0.000174\n",
      "Epoch: 18 \tTraining Loss: 0.000174\n",
      "Epoch: 1 \tTraining Loss: 0.000156\n",
      "Epoch: 2 \tTraining Loss: 0.000156\n",
      "Epoch: 3 \tTraining Loss: 0.000156\n",
      "Epoch: 4 \tTraining Loss: 0.000156\n",
      "Epoch: 5 \tTraining Loss: 0.000156\n",
      "Epoch: 6 \tTraining Loss: 0.000156\n",
      "Epoch: 7 \tTraining Loss: 0.000156\n",
      "Epoch: 8 \tTraining Loss: 0.000156\n",
      "Epoch: 9 \tTraining Loss: 0.000156\n",
      "Epoch: 10 \tTraining Loss: 0.000156\n",
      "Epoch: 11 \tTraining Loss: 0.000156\n",
      "Epoch: 12 \tTraining Loss: 0.000156\n",
      "Epoch: 13 \tTraining Loss: 0.000156\n",
      "Epoch: 14 \tTraining Loss: 0.000156\n",
      "Epoch: 15 \tTraining Loss: 0.000156\n",
      "Epoch: 16 \tTraining Loss: 0.000156\n",
      "Epoch: 17 \tTraining Loss: 0.000156\n",
      "Epoch: 18 \tTraining Loss: 0.000156\n",
      "Epoch: 1 \tTraining Loss: 0.000105\n",
      "Epoch: 2 \tTraining Loss: 0.000105\n",
      "Epoch: 3 \tTraining Loss: 0.000105\n",
      "Epoch: 4 \tTraining Loss: 0.000105\n",
      "Epoch: 5 \tTraining Loss: 0.000105\n",
      "Epoch: 6 \tTraining Loss: 0.000105\n",
      "Epoch: 7 \tTraining Loss: 0.000105\n",
      "Epoch: 8 \tTraining Loss: 0.000105\n",
      "Epoch: 9 \tTraining Loss: 0.000105\n",
      "Epoch: 10 \tTraining Loss: 0.000105\n",
      "Epoch: 11 \tTraining Loss: 0.000105\n",
      "Epoch: 12 \tTraining Loss: 0.000105\n",
      "Epoch: 13 \tTraining Loss: 0.000105\n",
      "Epoch: 14 \tTraining Loss: 0.000105\n",
      "Epoch: 15 \tTraining Loss: 0.000105\n",
      "Epoch: 16 \tTraining Loss: 0.000105\n",
      "Epoch: 17 \tTraining Loss: 0.000105\n",
      "Epoch: 18 \tTraining Loss: 0.000105\n",
      "Epoch: 1 \tTraining Loss: 0.000089\n",
      "Epoch: 2 \tTraining Loss: 0.000089\n",
      "Epoch: 3 \tTraining Loss: 0.000089\n",
      "Epoch: 4 \tTraining Loss: 0.000089\n",
      "Epoch: 5 \tTraining Loss: 0.000089\n",
      "Epoch: 6 \tTraining Loss: 0.000089\n",
      "Epoch: 7 \tTraining Loss: 0.000089\n",
      "Epoch: 8 \tTraining Loss: 0.000089\n",
      "Epoch: 9 \tTraining Loss: 0.000089\n",
      "Epoch: 10 \tTraining Loss: 0.000089\n",
      "Epoch: 11 \tTraining Loss: 0.000089\n",
      "Epoch: 12 \tTraining Loss: 0.000089\n",
      "Epoch: 13 \tTraining Loss: 0.000089\n",
      "Epoch: 14 \tTraining Loss: 0.000089\n",
      "Epoch: 15 \tTraining Loss: 0.000089\n",
      "Epoch: 16 \tTraining Loss: 0.000089\n",
      "Epoch: 17 \tTraining Loss: 0.000089\n",
      "Epoch: 18 \tTraining Loss: 0.000089\n",
      "Epoch: 1 \tTraining Loss: 0.000252\n",
      "Epoch: 2 \tTraining Loss: 0.000252\n",
      "Epoch: 3 \tTraining Loss: 0.000252\n",
      "Epoch: 4 \tTraining Loss: 0.000252\n",
      "Epoch: 5 \tTraining Loss: 0.000252\n",
      "Epoch: 6 \tTraining Loss: 0.000252\n",
      "Epoch: 7 \tTraining Loss: 0.000252\n",
      "Epoch: 8 \tTraining Loss: 0.000252\n",
      "Epoch: 9 \tTraining Loss: 0.000252\n",
      "Epoch: 10 \tTraining Loss: 0.000252\n",
      "Epoch: 11 \tTraining Loss: 0.000252\n",
      "Epoch: 12 \tTraining Loss: 0.000252\n",
      "Epoch: 13 \tTraining Loss: 0.000252\n",
      "Epoch: 14 \tTraining Loss: 0.000252\n",
      "Epoch: 15 \tTraining Loss: 0.000252\n",
      "Epoch: 16 \tTraining Loss: 0.000252\n",
      "Epoch: 17 \tTraining Loss: 0.000252\n",
      "Epoch: 18 \tTraining Loss: 0.000252\n",
      "Epoch: 1 \tTraining Loss: 0.000132\n",
      "Epoch: 2 \tTraining Loss: 0.000132\n",
      "Epoch: 3 \tTraining Loss: 0.000132\n",
      "Epoch: 4 \tTraining Loss: 0.000132\n",
      "Epoch: 5 \tTraining Loss: 0.000132\n",
      "Epoch: 6 \tTraining Loss: 0.000132\n",
      "Epoch: 7 \tTraining Loss: 0.000132\n",
      "Epoch: 8 \tTraining Loss: 0.000132\n",
      "Epoch: 9 \tTraining Loss: 0.000132\n",
      "Epoch: 10 \tTraining Loss: 0.000132\n",
      "Epoch: 11 \tTraining Loss: 0.000132\n",
      "Epoch: 12 \tTraining Loss: 0.000132\n",
      "Epoch: 13 \tTraining Loss: 0.000132\n",
      "Epoch: 14 \tTraining Loss: 0.000132\n",
      "Epoch: 15 \tTraining Loss: 0.000132\n",
      "Epoch: 16 \tTraining Loss: 0.000132\n",
      "Epoch: 17 \tTraining Loss: 0.000132\n",
      "Epoch: 18 \tTraining Loss: 0.000132\n",
      "Epoch: 1 \tTraining Loss: 0.000126\n",
      "Epoch: 2 \tTraining Loss: 0.000126\n",
      "Epoch: 3 \tTraining Loss: 0.000126\n",
      "Epoch: 4 \tTraining Loss: 0.000126\n",
      "Epoch: 5 \tTraining Loss: 0.000126\n",
      "Epoch: 6 \tTraining Loss: 0.000126\n",
      "Epoch: 7 \tTraining Loss: 0.000126\n",
      "Epoch: 8 \tTraining Loss: 0.000126\n",
      "Epoch: 9 \tTraining Loss: 0.000126\n",
      "Epoch: 10 \tTraining Loss: 0.000126\n",
      "Epoch: 11 \tTraining Loss: 0.000126\n",
      "Epoch: 12 \tTraining Loss: 0.000126\n",
      "Epoch: 13 \tTraining Loss: 0.000126\n",
      "Epoch: 14 \tTraining Loss: 0.000126\n",
      "Epoch: 15 \tTraining Loss: 0.000126\n",
      "Epoch: 16 \tTraining Loss: 0.000126\n",
      "Epoch: 17 \tTraining Loss: 0.000126\n",
      "Epoch: 18 \tTraining Loss: 0.000126\n",
      "Epoch: 1 \tTraining Loss: 0.000142\n",
      "Epoch: 2 \tTraining Loss: 0.000142\n",
      "Epoch: 3 \tTraining Loss: 0.000142\n",
      "Epoch: 4 \tTraining Loss: 0.000142\n",
      "Epoch: 5 \tTraining Loss: 0.000142\n",
      "Epoch: 6 \tTraining Loss: 0.000142\n",
      "Epoch: 7 \tTraining Loss: 0.000142\n",
      "Epoch: 8 \tTraining Loss: 0.000142\n",
      "Epoch: 9 \tTraining Loss: 0.000142\n",
      "Epoch: 10 \tTraining Loss: 0.000142\n",
      "Epoch: 11 \tTraining Loss: 0.000142\n",
      "Epoch: 12 \tTraining Loss: 0.000142\n",
      "Epoch: 13 \tTraining Loss: 0.000142\n",
      "Epoch: 14 \tTraining Loss: 0.000142\n",
      "Epoch: 15 \tTraining Loss: 0.000142\n",
      "Epoch: 16 \tTraining Loss: 0.000142\n",
      "Epoch: 17 \tTraining Loss: 0.000142\n",
      "Epoch: 18 \tTraining Loss: 0.000142\n",
      "Epoch: 1 \tTraining Loss: 0.000248\n",
      "Epoch: 2 \tTraining Loss: 0.000248\n",
      "Epoch: 3 \tTraining Loss: 0.000248\n",
      "Epoch: 4 \tTraining Loss: 0.000248\n",
      "Epoch: 5 \tTraining Loss: 0.000248\n",
      "Epoch: 6 \tTraining Loss: 0.000248\n",
      "Epoch: 7 \tTraining Loss: 0.000248\n",
      "Epoch: 8 \tTraining Loss: 0.000248\n",
      "Epoch: 9 \tTraining Loss: 0.000248\n",
      "Epoch: 10 \tTraining Loss: 0.000248\n",
      "Epoch: 11 \tTraining Loss: 0.000248\n",
      "Epoch: 12 \tTraining Loss: 0.000248\n",
      "Epoch: 13 \tTraining Loss: 0.000248\n",
      "Epoch: 14 \tTraining Loss: 0.000248\n",
      "Epoch: 15 \tTraining Loss: 0.000248\n",
      "Epoch: 16 \tTraining Loss: 0.000248\n",
      "Epoch: 17 \tTraining Loss: 0.000248\n",
      "Epoch: 18 \tTraining Loss: 0.000248\n",
      "Epoch: 1 \tTraining Loss: 0.000242\n",
      "Epoch: 2 \tTraining Loss: 0.000242\n",
      "Epoch: 3 \tTraining Loss: 0.000242\n",
      "Epoch: 4 \tTraining Loss: 0.000242\n",
      "Epoch: 5 \tTraining Loss: 0.000242\n",
      "Epoch: 6 \tTraining Loss: 0.000242\n",
      "Epoch: 7 \tTraining Loss: 0.000242\n",
      "Epoch: 8 \tTraining Loss: 0.000242\n",
      "Epoch: 9 \tTraining Loss: 0.000242\n",
      "Epoch: 10 \tTraining Loss: 0.000242\n",
      "Epoch: 11 \tTraining Loss: 0.000242\n",
      "Epoch: 12 \tTraining Loss: 0.000242\n",
      "Epoch: 13 \tTraining Loss: 0.000242\n",
      "Epoch: 14 \tTraining Loss: 0.000242\n",
      "Epoch: 15 \tTraining Loss: 0.000242\n",
      "Epoch: 16 \tTraining Loss: 0.000242\n",
      "Epoch: 17 \tTraining Loss: 0.000242\n",
      "Epoch: 18 \tTraining Loss: 0.000242\n",
      "Epoch: 1 \tTraining Loss: 0.000157\n",
      "Epoch: 2 \tTraining Loss: 0.000157\n",
      "Epoch: 3 \tTraining Loss: 0.000157\n",
      "Epoch: 4 \tTraining Loss: 0.000157\n",
      "Epoch: 5 \tTraining Loss: 0.000157\n",
      "Epoch: 6 \tTraining Loss: 0.000157\n",
      "Epoch: 7 \tTraining Loss: 0.000157\n",
      "Epoch: 8 \tTraining Loss: 0.000157\n",
      "Epoch: 9 \tTraining Loss: 0.000157\n",
      "Epoch: 10 \tTraining Loss: 0.000157\n",
      "Epoch: 11 \tTraining Loss: 0.000157\n",
      "Epoch: 12 \tTraining Loss: 0.000157\n",
      "Epoch: 13 \tTraining Loss: 0.000157\n",
      "Epoch: 14 \tTraining Loss: 0.000157\n",
      "Epoch: 15 \tTraining Loss: 0.000157\n",
      "Epoch: 16 \tTraining Loss: 0.000157\n",
      "Epoch: 17 \tTraining Loss: 0.000157\n",
      "Epoch: 18 \tTraining Loss: 0.000157\n",
      "Epoch: 1 \tTraining Loss: 0.000220\n",
      "Epoch: 2 \tTraining Loss: 0.000220\n",
      "Epoch: 3 \tTraining Loss: 0.000220\n",
      "Epoch: 4 \tTraining Loss: 0.000220\n",
      "Epoch: 5 \tTraining Loss: 0.000220\n",
      "Epoch: 6 \tTraining Loss: 0.000220\n",
      "Epoch: 7 \tTraining Loss: 0.000220\n",
      "Epoch: 8 \tTraining Loss: 0.000220\n",
      "Epoch: 9 \tTraining Loss: 0.000220\n",
      "Epoch: 10 \tTraining Loss: 0.000220\n",
      "Epoch: 11 \tTraining Loss: 0.000220\n",
      "Epoch: 12 \tTraining Loss: 0.000220\n",
      "Epoch: 13 \tTraining Loss: 0.000220\n",
      "Epoch: 14 \tTraining Loss: 0.000220\n",
      "Epoch: 15 \tTraining Loss: 0.000220\n",
      "Epoch: 16 \tTraining Loss: 0.000220\n",
      "Epoch: 17 \tTraining Loss: 0.000220\n",
      "Epoch: 18 \tTraining Loss: 0.000220\n",
      "Epoch: 1 \tTraining Loss: 0.000127\n",
      "Epoch: 2 \tTraining Loss: 0.000127\n",
      "Epoch: 3 \tTraining Loss: 0.000127\n",
      "Epoch: 4 \tTraining Loss: 0.000127\n",
      "Epoch: 5 \tTraining Loss: 0.000127\n",
      "Epoch: 6 \tTraining Loss: 0.000127\n",
      "Epoch: 7 \tTraining Loss: 0.000127\n",
      "Epoch: 8 \tTraining Loss: 0.000127\n",
      "Epoch: 9 \tTraining Loss: 0.000127\n",
      "Epoch: 10 \tTraining Loss: 0.000127\n",
      "Epoch: 11 \tTraining Loss: 0.000127\n",
      "Epoch: 12 \tTraining Loss: 0.000127\n",
      "Epoch: 13 \tTraining Loss: 0.000127\n",
      "Epoch: 14 \tTraining Loss: 0.000127\n",
      "Epoch: 15 \tTraining Loss: 0.000127\n",
      "Epoch: 16 \tTraining Loss: 0.000127\n",
      "Epoch: 17 \tTraining Loss: 0.000127\n",
      "Epoch: 18 \tTraining Loss: 0.000127\n",
      "Epoch: 1 \tTraining Loss: 0.000224\n",
      "Epoch: 2 \tTraining Loss: 0.000224\n",
      "Epoch: 3 \tTraining Loss: 0.000224\n",
      "Epoch: 4 \tTraining Loss: 0.000224\n",
      "Epoch: 5 \tTraining Loss: 0.000224\n",
      "Epoch: 6 \tTraining Loss: 0.000224\n",
      "Epoch: 7 \tTraining Loss: 0.000224\n",
      "Epoch: 8 \tTraining Loss: 0.000224\n",
      "Epoch: 9 \tTraining Loss: 0.000224\n",
      "Epoch: 10 \tTraining Loss: 0.000224\n",
      "Epoch: 11 \tTraining Loss: 0.000224\n",
      "Epoch: 12 \tTraining Loss: 0.000224\n",
      "Epoch: 13 \tTraining Loss: 0.000224\n",
      "Epoch: 14 \tTraining Loss: 0.000224\n",
      "Epoch: 15 \tTraining Loss: 0.000224\n",
      "Epoch: 16 \tTraining Loss: 0.000224\n",
      "Epoch: 17 \tTraining Loss: 0.000224\n",
      "Epoch: 18 \tTraining Loss: 0.000224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000280\n",
      "Epoch: 2 \tTraining Loss: 0.000280\n",
      "Epoch: 3 \tTraining Loss: 0.000280\n",
      "Epoch: 4 \tTraining Loss: 0.000280\n",
      "Epoch: 5 \tTraining Loss: 0.000280\n",
      "Epoch: 6 \tTraining Loss: 0.000280\n",
      "Epoch: 7 \tTraining Loss: 0.000280\n",
      "Epoch: 8 \tTraining Loss: 0.000280\n",
      "Epoch: 9 \tTraining Loss: 0.000280\n",
      "Epoch: 10 \tTraining Loss: 0.000280\n",
      "Epoch: 11 \tTraining Loss: 0.000280\n",
      "Epoch: 12 \tTraining Loss: 0.000280\n",
      "Epoch: 13 \tTraining Loss: 0.000280\n",
      "Epoch: 14 \tTraining Loss: 0.000280\n",
      "Epoch: 15 \tTraining Loss: 0.000280\n",
      "Epoch: 16 \tTraining Loss: 0.000280\n",
      "Epoch: 17 \tTraining Loss: 0.000280\n",
      "Epoch: 18 \tTraining Loss: 0.000280\n",
      "Epoch: 1 \tTraining Loss: 0.000161\n",
      "Epoch: 2 \tTraining Loss: 0.000161\n",
      "Epoch: 3 \tTraining Loss: 0.000161\n",
      "Epoch: 4 \tTraining Loss: 0.000161\n",
      "Epoch: 5 \tTraining Loss: 0.000161\n",
      "Epoch: 6 \tTraining Loss: 0.000161\n",
      "Epoch: 7 \tTraining Loss: 0.000161\n",
      "Epoch: 8 \tTraining Loss: 0.000161\n",
      "Epoch: 9 \tTraining Loss: 0.000161\n",
      "Epoch: 10 \tTraining Loss: 0.000161\n",
      "Epoch: 11 \tTraining Loss: 0.000161\n",
      "Epoch: 12 \tTraining Loss: 0.000161\n",
      "Epoch: 13 \tTraining Loss: 0.000161\n",
      "Epoch: 14 \tTraining Loss: 0.000161\n",
      "Epoch: 15 \tTraining Loss: 0.000161\n",
      "Epoch: 16 \tTraining Loss: 0.000161\n",
      "Epoch: 17 \tTraining Loss: 0.000161\n",
      "Epoch: 18 \tTraining Loss: 0.000161\n",
      "Epoch: 1 \tTraining Loss: 0.000066\n",
      "Epoch: 2 \tTraining Loss: 0.000066\n",
      "Epoch: 3 \tTraining Loss: 0.000066\n",
      "Epoch: 4 \tTraining Loss: 0.000066\n",
      "Epoch: 5 \tTraining Loss: 0.000066\n",
      "Epoch: 6 \tTraining Loss: 0.000066\n",
      "Epoch: 7 \tTraining Loss: 0.000066\n",
      "Epoch: 8 \tTraining Loss: 0.000066\n",
      "Epoch: 9 \tTraining Loss: 0.000066\n",
      "Epoch: 10 \tTraining Loss: 0.000066\n",
      "Epoch: 11 \tTraining Loss: 0.000066\n",
      "Epoch: 12 \tTraining Loss: 0.000066\n",
      "Epoch: 13 \tTraining Loss: 0.000066\n",
      "Epoch: 14 \tTraining Loss: 0.000066\n",
      "Epoch: 15 \tTraining Loss: 0.000066\n",
      "Epoch: 16 \tTraining Loss: 0.000066\n",
      "Epoch: 17 \tTraining Loss: 0.000066\n",
      "Epoch: 18 \tTraining Loss: 0.000066\n",
      "Epoch: 1 \tTraining Loss: 0.000121\n",
      "Epoch: 2 \tTraining Loss: 0.000121\n",
      "Epoch: 3 \tTraining Loss: 0.000121\n",
      "Epoch: 4 \tTraining Loss: 0.000121\n",
      "Epoch: 5 \tTraining Loss: 0.000121\n",
      "Epoch: 6 \tTraining Loss: 0.000121\n",
      "Epoch: 7 \tTraining Loss: 0.000121\n",
      "Epoch: 8 \tTraining Loss: 0.000121\n",
      "Epoch: 9 \tTraining Loss: 0.000121\n",
      "Epoch: 10 \tTraining Loss: 0.000121\n",
      "Epoch: 11 \tTraining Loss: 0.000121\n",
      "Epoch: 12 \tTraining Loss: 0.000121\n",
      "Epoch: 13 \tTraining Loss: 0.000121\n",
      "Epoch: 14 \tTraining Loss: 0.000121\n",
      "Epoch: 15 \tTraining Loss: 0.000121\n",
      "Epoch: 16 \tTraining Loss: 0.000121\n",
      "Epoch: 17 \tTraining Loss: 0.000121\n",
      "Epoch: 18 \tTraining Loss: 0.000121\n",
      "Epoch: 1 \tTraining Loss: 0.000118\n",
      "Epoch: 2 \tTraining Loss: 0.000118\n",
      "Epoch: 3 \tTraining Loss: 0.000118\n",
      "Epoch: 4 \tTraining Loss: 0.000118\n",
      "Epoch: 5 \tTraining Loss: 0.000118\n",
      "Epoch: 6 \tTraining Loss: 0.000118\n",
      "Epoch: 7 \tTraining Loss: 0.000118\n",
      "Epoch: 8 \tTraining Loss: 0.000118\n",
      "Epoch: 9 \tTraining Loss: 0.000118\n",
      "Epoch: 10 \tTraining Loss: 0.000118\n",
      "Epoch: 11 \tTraining Loss: 0.000118\n",
      "Epoch: 12 \tTraining Loss: 0.000118\n",
      "Epoch: 13 \tTraining Loss: 0.000118\n",
      "Epoch: 14 \tTraining Loss: 0.000118\n",
      "Epoch: 15 \tTraining Loss: 0.000118\n",
      "Epoch: 16 \tTraining Loss: 0.000118\n",
      "Epoch: 17 \tTraining Loss: 0.000118\n",
      "Epoch: 18 \tTraining Loss: 0.000118\n",
      "Epoch: 1 \tTraining Loss: 0.000076\n",
      "Epoch: 2 \tTraining Loss: 0.000076\n",
      "Epoch: 3 \tTraining Loss: 0.000076\n",
      "Epoch: 4 \tTraining Loss: 0.000076\n",
      "Epoch: 5 \tTraining Loss: 0.000076\n",
      "Epoch: 6 \tTraining Loss: 0.000076\n",
      "Epoch: 7 \tTraining Loss: 0.000076\n",
      "Epoch: 8 \tTraining Loss: 0.000076\n",
      "Epoch: 9 \tTraining Loss: 0.000076\n",
      "Epoch: 10 \tTraining Loss: 0.000076\n",
      "Epoch: 11 \tTraining Loss: 0.000076\n",
      "Epoch: 12 \tTraining Loss: 0.000076\n",
      "Epoch: 13 \tTraining Loss: 0.000076\n",
      "Epoch: 14 \tTraining Loss: 0.000076\n",
      "Epoch: 15 \tTraining Loss: 0.000076\n",
      "Epoch: 16 \tTraining Loss: 0.000076\n",
      "Epoch: 17 \tTraining Loss: 0.000076\n",
      "Epoch: 18 \tTraining Loss: 0.000076\n",
      "Epoch: 1 \tTraining Loss: 0.000107\n",
      "Epoch: 2 \tTraining Loss: 0.000107\n",
      "Epoch: 3 \tTraining Loss: 0.000107\n",
      "Epoch: 4 \tTraining Loss: 0.000107\n",
      "Epoch: 5 \tTraining Loss: 0.000107\n",
      "Epoch: 6 \tTraining Loss: 0.000107\n",
      "Epoch: 7 \tTraining Loss: 0.000107\n",
      "Epoch: 8 \tTraining Loss: 0.000107\n",
      "Epoch: 9 \tTraining Loss: 0.000107\n",
      "Epoch: 10 \tTraining Loss: 0.000107\n",
      "Epoch: 11 \tTraining Loss: 0.000107\n",
      "Epoch: 12 \tTraining Loss: 0.000107\n",
      "Epoch: 13 \tTraining Loss: 0.000107\n",
      "Epoch: 14 \tTraining Loss: 0.000107\n",
      "Epoch: 15 \tTraining Loss: 0.000107\n",
      "Epoch: 16 \tTraining Loss: 0.000107\n",
      "Epoch: 17 \tTraining Loss: 0.000107\n",
      "Epoch: 18 \tTraining Loss: 0.000107\n",
      "Epoch: 1 \tTraining Loss: 0.000162\n",
      "Epoch: 2 \tTraining Loss: 0.000162\n",
      "Epoch: 3 \tTraining Loss: 0.000162\n",
      "Epoch: 4 \tTraining Loss: 0.000162\n",
      "Epoch: 5 \tTraining Loss: 0.000162\n",
      "Epoch: 6 \tTraining Loss: 0.000162\n",
      "Epoch: 7 \tTraining Loss: 0.000162\n",
      "Epoch: 8 \tTraining Loss: 0.000162\n",
      "Epoch: 9 \tTraining Loss: 0.000162\n",
      "Epoch: 10 \tTraining Loss: 0.000162\n",
      "Epoch: 11 \tTraining Loss: 0.000162\n",
      "Epoch: 12 \tTraining Loss: 0.000162\n",
      "Epoch: 13 \tTraining Loss: 0.000162\n",
      "Epoch: 14 \tTraining Loss: 0.000162\n",
      "Epoch: 15 \tTraining Loss: 0.000162\n",
      "Epoch: 16 \tTraining Loss: 0.000162\n",
      "Epoch: 17 \tTraining Loss: 0.000162\n",
      "Epoch: 18 \tTraining Loss: 0.000162\n",
      "Epoch: 1 \tTraining Loss: 0.000145\n",
      "Epoch: 2 \tTraining Loss: 0.000145\n",
      "Epoch: 3 \tTraining Loss: 0.000145\n",
      "Epoch: 4 \tTraining Loss: 0.000145\n",
      "Epoch: 5 \tTraining Loss: 0.000145\n",
      "Epoch: 6 \tTraining Loss: 0.000145\n",
      "Epoch: 7 \tTraining Loss: 0.000145\n",
      "Epoch: 8 \tTraining Loss: 0.000145\n",
      "Epoch: 9 \tTraining Loss: 0.000145\n",
      "Epoch: 10 \tTraining Loss: 0.000145\n",
      "Epoch: 11 \tTraining Loss: 0.000145\n",
      "Epoch: 12 \tTraining Loss: 0.000145\n",
      "Epoch: 13 \tTraining Loss: 0.000145\n",
      "Epoch: 14 \tTraining Loss: 0.000145\n",
      "Epoch: 15 \tTraining Loss: 0.000145\n",
      "Epoch: 16 \tTraining Loss: 0.000145\n",
      "Epoch: 17 \tTraining Loss: 0.000145\n",
      "Epoch: 18 \tTraining Loss: 0.000145\n",
      "Epoch: 1 \tTraining Loss: 0.000053\n",
      "Epoch: 2 \tTraining Loss: 0.000053\n",
      "Epoch: 3 \tTraining Loss: 0.000053\n",
      "Epoch: 4 \tTraining Loss: 0.000053\n",
      "Epoch: 5 \tTraining Loss: 0.000053\n",
      "Epoch: 6 \tTraining Loss: 0.000053\n",
      "Epoch: 7 \tTraining Loss: 0.000053\n",
      "Epoch: 8 \tTraining Loss: 0.000053\n",
      "Epoch: 9 \tTraining Loss: 0.000053\n",
      "Epoch: 10 \tTraining Loss: 0.000053\n",
      "Epoch: 11 \tTraining Loss: 0.000053\n",
      "Epoch: 12 \tTraining Loss: 0.000053\n",
      "Epoch: 13 \tTraining Loss: 0.000053\n",
      "Epoch: 14 \tTraining Loss: 0.000053\n",
      "Epoch: 15 \tTraining Loss: 0.000053\n",
      "Epoch: 16 \tTraining Loss: 0.000053\n",
      "Epoch: 17 \tTraining Loss: 0.000053\n",
      "Epoch: 18 \tTraining Loss: 0.000053\n",
      "Epoch: 1 \tTraining Loss: 0.000358\n",
      "Epoch: 2 \tTraining Loss: 0.000358\n",
      "Epoch: 3 \tTraining Loss: 0.000358\n",
      "Epoch: 4 \tTraining Loss: 0.000358\n",
      "Epoch: 5 \tTraining Loss: 0.000358\n",
      "Epoch: 6 \tTraining Loss: 0.000358\n",
      "Epoch: 7 \tTraining Loss: 0.000358\n",
      "Epoch: 8 \tTraining Loss: 0.000358\n",
      "Epoch: 9 \tTraining Loss: 0.000358\n",
      "Epoch: 10 \tTraining Loss: 0.000358\n",
      "Epoch: 11 \tTraining Loss: 0.000358\n",
      "Epoch: 12 \tTraining Loss: 0.000358\n",
      "Epoch: 13 \tTraining Loss: 0.000358\n",
      "Epoch: 14 \tTraining Loss: 0.000358\n",
      "Epoch: 15 \tTraining Loss: 0.000358\n",
      "Epoch: 16 \tTraining Loss: 0.000358\n",
      "Epoch: 17 \tTraining Loss: 0.000358\n",
      "Epoch: 18 \tTraining Loss: 0.000358\n",
      "Epoch: 1 \tTraining Loss: 0.000273\n",
      "Epoch: 2 \tTraining Loss: 0.000273\n",
      "Epoch: 3 \tTraining Loss: 0.000273\n",
      "Epoch: 4 \tTraining Loss: 0.000273\n",
      "Epoch: 5 \tTraining Loss: 0.000273\n",
      "Epoch: 6 \tTraining Loss: 0.000273\n",
      "Epoch: 7 \tTraining Loss: 0.000273\n",
      "Epoch: 8 \tTraining Loss: 0.000273\n",
      "Epoch: 9 \tTraining Loss: 0.000273\n",
      "Epoch: 10 \tTraining Loss: 0.000273\n",
      "Epoch: 11 \tTraining Loss: 0.000273\n",
      "Epoch: 12 \tTraining Loss: 0.000273\n",
      "Epoch: 13 \tTraining Loss: 0.000273\n",
      "Epoch: 14 \tTraining Loss: 0.000273\n",
      "Epoch: 15 \tTraining Loss: 0.000273\n",
      "Epoch: 16 \tTraining Loss: 0.000273\n",
      "Epoch: 17 \tTraining Loss: 0.000273\n",
      "Epoch: 18 \tTraining Loss: 0.000273\n",
      "Epoch: 1 \tTraining Loss: 0.000245\n",
      "Epoch: 2 \tTraining Loss: 0.000245\n",
      "Epoch: 3 \tTraining Loss: 0.000245\n",
      "Epoch: 4 \tTraining Loss: 0.000245\n",
      "Epoch: 5 \tTraining Loss: 0.000245\n",
      "Epoch: 6 \tTraining Loss: 0.000245\n",
      "Epoch: 7 \tTraining Loss: 0.000245\n",
      "Epoch: 8 \tTraining Loss: 0.000245\n",
      "Epoch: 9 \tTraining Loss: 0.000245\n",
      "Epoch: 10 \tTraining Loss: 0.000245\n",
      "Epoch: 11 \tTraining Loss: 0.000245\n",
      "Epoch: 12 \tTraining Loss: 0.000245\n",
      "Epoch: 13 \tTraining Loss: 0.000245\n",
      "Epoch: 14 \tTraining Loss: 0.000245\n",
      "Epoch: 15 \tTraining Loss: 0.000245\n",
      "Epoch: 16 \tTraining Loss: 0.000245\n",
      "Epoch: 17 \tTraining Loss: 0.000245\n",
      "Epoch: 18 \tTraining Loss: 0.000245\n",
      "Epoch: 1 \tTraining Loss: 0.000111\n",
      "Epoch: 2 \tTraining Loss: 0.000111\n",
      "Epoch: 3 \tTraining Loss: 0.000111\n",
      "Epoch: 4 \tTraining Loss: 0.000111\n",
      "Epoch: 5 \tTraining Loss: 0.000111\n",
      "Epoch: 6 \tTraining Loss: 0.000111\n",
      "Epoch: 7 \tTraining Loss: 0.000111\n",
      "Epoch: 8 \tTraining Loss: 0.000111\n",
      "Epoch: 9 \tTraining Loss: 0.000111\n",
      "Epoch: 10 \tTraining Loss: 0.000111\n",
      "Epoch: 11 \tTraining Loss: 0.000111\n",
      "Epoch: 12 \tTraining Loss: 0.000111\n",
      "Epoch: 13 \tTraining Loss: 0.000111\n",
      "Epoch: 14 \tTraining Loss: 0.000111\n",
      "Epoch: 15 \tTraining Loss: 0.000111\n",
      "Epoch: 16 \tTraining Loss: 0.000111\n",
      "Epoch: 17 \tTraining Loss: 0.000111\n",
      "Epoch: 18 \tTraining Loss: 0.000111\n",
      "Epoch: 1 \tTraining Loss: 0.000221\n",
      "Epoch: 2 \tTraining Loss: 0.000221\n",
      "Epoch: 3 \tTraining Loss: 0.000221\n",
      "Epoch: 4 \tTraining Loss: 0.000221\n",
      "Epoch: 5 \tTraining Loss: 0.000221\n",
      "Epoch: 6 \tTraining Loss: 0.000221\n",
      "Epoch: 7 \tTraining Loss: 0.000221\n",
      "Epoch: 8 \tTraining Loss: 0.000221\n",
      "Epoch: 9 \tTraining Loss: 0.000221\n",
      "Epoch: 10 \tTraining Loss: 0.000221\n",
      "Epoch: 11 \tTraining Loss: 0.000221\n",
      "Epoch: 12 \tTraining Loss: 0.000221\n",
      "Epoch: 13 \tTraining Loss: 0.000221\n",
      "Epoch: 14 \tTraining Loss: 0.000221\n",
      "Epoch: 15 \tTraining Loss: 0.000221\n",
      "Epoch: 16 \tTraining Loss: 0.000221\n",
      "Epoch: 17 \tTraining Loss: 0.000221\n",
      "Epoch: 18 \tTraining Loss: 0.000221\n",
      "Epoch: 1 \tTraining Loss: 0.000106\n",
      "Epoch: 2 \tTraining Loss: 0.000106\n",
      "Epoch: 3 \tTraining Loss: 0.000106\n",
      "Epoch: 4 \tTraining Loss: 0.000106\n",
      "Epoch: 5 \tTraining Loss: 0.000106\n",
      "Epoch: 6 \tTraining Loss: 0.000106\n",
      "Epoch: 7 \tTraining Loss: 0.000106\n",
      "Epoch: 8 \tTraining Loss: 0.000106\n",
      "Epoch: 9 \tTraining Loss: 0.000106\n",
      "Epoch: 10 \tTraining Loss: 0.000106\n",
      "Epoch: 11 \tTraining Loss: 0.000106\n",
      "Epoch: 12 \tTraining Loss: 0.000106\n",
      "Epoch: 13 \tTraining Loss: 0.000106\n",
      "Epoch: 14 \tTraining Loss: 0.000106\n",
      "Epoch: 15 \tTraining Loss: 0.000106\n",
      "Epoch: 16 \tTraining Loss: 0.000106\n",
      "Epoch: 17 \tTraining Loss: 0.000106\n",
      "Epoch: 18 \tTraining Loss: 0.000106\n",
      "Epoch: 1 \tTraining Loss: 0.000222\n",
      "Epoch: 2 \tTraining Loss: 0.000222\n",
      "Epoch: 3 \tTraining Loss: 0.000222\n",
      "Epoch: 4 \tTraining Loss: 0.000222\n",
      "Epoch: 5 \tTraining Loss: 0.000222\n",
      "Epoch: 6 \tTraining Loss: 0.000222\n",
      "Epoch: 7 \tTraining Loss: 0.000222\n",
      "Epoch: 8 \tTraining Loss: 0.000222\n",
      "Epoch: 9 \tTraining Loss: 0.000222\n",
      "Epoch: 10 \tTraining Loss: 0.000222\n",
      "Epoch: 11 \tTraining Loss: 0.000222\n",
      "Epoch: 12 \tTraining Loss: 0.000222\n",
      "Epoch: 13 \tTraining Loss: 0.000222\n",
      "Epoch: 14 \tTraining Loss: 0.000222\n",
      "Epoch: 15 \tTraining Loss: 0.000222\n",
      "Epoch: 16 \tTraining Loss: 0.000222\n",
      "Epoch: 17 \tTraining Loss: 0.000222\n",
      "Epoch: 18 \tTraining Loss: 0.000222\n",
      "Epoch: 1 \tTraining Loss: 0.000340\n",
      "Epoch: 2 \tTraining Loss: 0.000340\n",
      "Epoch: 3 \tTraining Loss: 0.000340\n",
      "Epoch: 4 \tTraining Loss: 0.000340\n",
      "Epoch: 5 \tTraining Loss: 0.000340\n",
      "Epoch: 6 \tTraining Loss: 0.000340\n",
      "Epoch: 7 \tTraining Loss: 0.000340\n",
      "Epoch: 8 \tTraining Loss: 0.000340\n",
      "Epoch: 9 \tTraining Loss: 0.000340\n",
      "Epoch: 10 \tTraining Loss: 0.000340\n",
      "Epoch: 11 \tTraining Loss: 0.000340\n",
      "Epoch: 12 \tTraining Loss: 0.000340\n",
      "Epoch: 13 \tTraining Loss: 0.000340\n",
      "Epoch: 14 \tTraining Loss: 0.000340\n",
      "Epoch: 15 \tTraining Loss: 0.000340\n",
      "Epoch: 16 \tTraining Loss: 0.000340\n",
      "Epoch: 17 \tTraining Loss: 0.000340\n",
      "Epoch: 18 \tTraining Loss: 0.000340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000289\n",
      "Epoch: 2 \tTraining Loss: 0.000289\n",
      "Epoch: 3 \tTraining Loss: 0.000289\n",
      "Epoch: 4 \tTraining Loss: 0.000289\n",
      "Epoch: 5 \tTraining Loss: 0.000289\n",
      "Epoch: 6 \tTraining Loss: 0.000289\n",
      "Epoch: 7 \tTraining Loss: 0.000289\n",
      "Epoch: 8 \tTraining Loss: 0.000289\n",
      "Epoch: 9 \tTraining Loss: 0.000289\n",
      "Epoch: 10 \tTraining Loss: 0.000289\n",
      "Epoch: 11 \tTraining Loss: 0.000289\n",
      "Epoch: 12 \tTraining Loss: 0.000289\n",
      "Epoch: 13 \tTraining Loss: 0.000289\n",
      "Epoch: 14 \tTraining Loss: 0.000289\n",
      "Epoch: 15 \tTraining Loss: 0.000289\n",
      "Epoch: 16 \tTraining Loss: 0.000289\n",
      "Epoch: 17 \tTraining Loss: 0.000289\n",
      "Epoch: 18 \tTraining Loss: 0.000289\n",
      "Epoch: 1 \tTraining Loss: 0.000134\n",
      "Epoch: 2 \tTraining Loss: 0.000134\n",
      "Epoch: 3 \tTraining Loss: 0.000134\n",
      "Epoch: 4 \tTraining Loss: 0.000134\n",
      "Epoch: 5 \tTraining Loss: 0.000134\n",
      "Epoch: 6 \tTraining Loss: 0.000134\n",
      "Epoch: 7 \tTraining Loss: 0.000134\n",
      "Epoch: 8 \tTraining Loss: 0.000134\n",
      "Epoch: 9 \tTraining Loss: 0.000134\n",
      "Epoch: 10 \tTraining Loss: 0.000134\n",
      "Epoch: 11 \tTraining Loss: 0.000134\n",
      "Epoch: 12 \tTraining Loss: 0.000134\n",
      "Epoch: 13 \tTraining Loss: 0.000134\n",
      "Epoch: 14 \tTraining Loss: 0.000134\n",
      "Epoch: 15 \tTraining Loss: 0.000134\n",
      "Epoch: 16 \tTraining Loss: 0.000134\n",
      "Epoch: 17 \tTraining Loss: 0.000134\n",
      "Epoch: 18 \tTraining Loss: 0.000134\n",
      "Epoch: 1 \tTraining Loss: 0.000123\n",
      "Epoch: 2 \tTraining Loss: 0.000123\n",
      "Epoch: 3 \tTraining Loss: 0.000123\n",
      "Epoch: 4 \tTraining Loss: 0.000123\n",
      "Epoch: 5 \tTraining Loss: 0.000123\n",
      "Epoch: 6 \tTraining Loss: 0.000123\n",
      "Epoch: 7 \tTraining Loss: 0.000123\n",
      "Epoch: 8 \tTraining Loss: 0.000123\n",
      "Epoch: 9 \tTraining Loss: 0.000123\n",
      "Epoch: 10 \tTraining Loss: 0.000123\n",
      "Epoch: 11 \tTraining Loss: 0.000123\n",
      "Epoch: 12 \tTraining Loss: 0.000123\n",
      "Epoch: 13 \tTraining Loss: 0.000123\n",
      "Epoch: 14 \tTraining Loss: 0.000123\n",
      "Epoch: 15 \tTraining Loss: 0.000123\n",
      "Epoch: 16 \tTraining Loss: 0.000123\n",
      "Epoch: 17 \tTraining Loss: 0.000123\n",
      "Epoch: 18 \tTraining Loss: 0.000123\n",
      "Epoch: 1 \tTraining Loss: 0.000282\n",
      "Epoch: 2 \tTraining Loss: 0.000282\n",
      "Epoch: 3 \tTraining Loss: 0.000282\n",
      "Epoch: 4 \tTraining Loss: 0.000282\n",
      "Epoch: 5 \tTraining Loss: 0.000282\n",
      "Epoch: 6 \tTraining Loss: 0.000282\n",
      "Epoch: 7 \tTraining Loss: 0.000282\n",
      "Epoch: 8 \tTraining Loss: 0.000282\n",
      "Epoch: 9 \tTraining Loss: 0.000282\n",
      "Epoch: 10 \tTraining Loss: 0.000282\n",
      "Epoch: 11 \tTraining Loss: 0.000282\n",
      "Epoch: 12 \tTraining Loss: 0.000282\n",
      "Epoch: 13 \tTraining Loss: 0.000282\n",
      "Epoch: 14 \tTraining Loss: 0.000282\n",
      "Epoch: 15 \tTraining Loss: 0.000282\n",
      "Epoch: 16 \tTraining Loss: 0.000282\n",
      "Epoch: 17 \tTraining Loss: 0.000282\n",
      "Epoch: 18 \tTraining Loss: 0.000282\n",
      "Epoch: 1 \tTraining Loss: 0.000123\n",
      "Epoch: 2 \tTraining Loss: 0.000123\n",
      "Epoch: 3 \tTraining Loss: 0.000123\n",
      "Epoch: 4 \tTraining Loss: 0.000123\n",
      "Epoch: 5 \tTraining Loss: 0.000123\n",
      "Epoch: 6 \tTraining Loss: 0.000123\n",
      "Epoch: 7 \tTraining Loss: 0.000123\n",
      "Epoch: 8 \tTraining Loss: 0.000123\n",
      "Epoch: 9 \tTraining Loss: 0.000123\n",
      "Epoch: 10 \tTraining Loss: 0.000123\n",
      "Epoch: 11 \tTraining Loss: 0.000123\n",
      "Epoch: 12 \tTraining Loss: 0.000123\n",
      "Epoch: 13 \tTraining Loss: 0.000123\n",
      "Epoch: 14 \tTraining Loss: 0.000123\n",
      "Epoch: 15 \tTraining Loss: 0.000123\n",
      "Epoch: 16 \tTraining Loss: 0.000123\n",
      "Epoch: 17 \tTraining Loss: 0.000123\n",
      "Epoch: 18 \tTraining Loss: 0.000123\n",
      "Epoch: 1 \tTraining Loss: 0.000174\n",
      "Epoch: 2 \tTraining Loss: 0.000174\n",
      "Epoch: 3 \tTraining Loss: 0.000174\n",
      "Epoch: 4 \tTraining Loss: 0.000174\n",
      "Epoch: 5 \tTraining Loss: 0.000174\n",
      "Epoch: 6 \tTraining Loss: 0.000174\n",
      "Epoch: 7 \tTraining Loss: 0.000174\n",
      "Epoch: 8 \tTraining Loss: 0.000174\n",
      "Epoch: 9 \tTraining Loss: 0.000174\n",
      "Epoch: 10 \tTraining Loss: 0.000174\n",
      "Epoch: 11 \tTraining Loss: 0.000174\n",
      "Epoch: 12 \tTraining Loss: 0.000174\n",
      "Epoch: 13 \tTraining Loss: 0.000174\n",
      "Epoch: 14 \tTraining Loss: 0.000174\n",
      "Epoch: 15 \tTraining Loss: 0.000174\n",
      "Epoch: 16 \tTraining Loss: 0.000174\n",
      "Epoch: 17 \tTraining Loss: 0.000174\n",
      "Epoch: 18 \tTraining Loss: 0.000174\n",
      "Epoch: 1 \tTraining Loss: 0.000292\n",
      "Epoch: 2 \tTraining Loss: 0.000292\n",
      "Epoch: 3 \tTraining Loss: 0.000292\n",
      "Epoch: 4 \tTraining Loss: 0.000292\n",
      "Epoch: 5 \tTraining Loss: 0.000292\n",
      "Epoch: 6 \tTraining Loss: 0.000292\n",
      "Epoch: 7 \tTraining Loss: 0.000292\n",
      "Epoch: 8 \tTraining Loss: 0.000292\n",
      "Epoch: 9 \tTraining Loss: 0.000292\n",
      "Epoch: 10 \tTraining Loss: 0.000292\n",
      "Epoch: 11 \tTraining Loss: 0.000292\n",
      "Epoch: 12 \tTraining Loss: 0.000292\n",
      "Epoch: 13 \tTraining Loss: 0.000292\n",
      "Epoch: 14 \tTraining Loss: 0.000292\n",
      "Epoch: 15 \tTraining Loss: 0.000292\n",
      "Epoch: 16 \tTraining Loss: 0.000292\n",
      "Epoch: 17 \tTraining Loss: 0.000292\n",
      "Epoch: 18 \tTraining Loss: 0.000292\n",
      "Epoch: 1 \tTraining Loss: 0.000132\n",
      "Epoch: 2 \tTraining Loss: 0.000132\n",
      "Epoch: 3 \tTraining Loss: 0.000132\n",
      "Epoch: 4 \tTraining Loss: 0.000132\n",
      "Epoch: 5 \tTraining Loss: 0.000132\n",
      "Epoch: 6 \tTraining Loss: 0.000132\n",
      "Epoch: 7 \tTraining Loss: 0.000132\n",
      "Epoch: 8 \tTraining Loss: 0.000132\n",
      "Epoch: 9 \tTraining Loss: 0.000132\n",
      "Epoch: 10 \tTraining Loss: 0.000132\n",
      "Epoch: 11 \tTraining Loss: 0.000132\n",
      "Epoch: 12 \tTraining Loss: 0.000132\n",
      "Epoch: 13 \tTraining Loss: 0.000132\n",
      "Epoch: 14 \tTraining Loss: 0.000132\n",
      "Epoch: 15 \tTraining Loss: 0.000132\n",
      "Epoch: 16 \tTraining Loss: 0.000132\n",
      "Epoch: 17 \tTraining Loss: 0.000132\n",
      "Epoch: 18 \tTraining Loss: 0.000132\n",
      "Epoch: 1 \tTraining Loss: 0.000178\n",
      "Epoch: 2 \tTraining Loss: 0.000178\n",
      "Epoch: 3 \tTraining Loss: 0.000178\n",
      "Epoch: 4 \tTraining Loss: 0.000178\n",
      "Epoch: 5 \tTraining Loss: 0.000178\n",
      "Epoch: 6 \tTraining Loss: 0.000178\n",
      "Epoch: 7 \tTraining Loss: 0.000178\n",
      "Epoch: 8 \tTraining Loss: 0.000178\n",
      "Epoch: 9 \tTraining Loss: 0.000178\n",
      "Epoch: 10 \tTraining Loss: 0.000178\n",
      "Epoch: 11 \tTraining Loss: 0.000178\n",
      "Epoch: 12 \tTraining Loss: 0.000178\n",
      "Epoch: 13 \tTraining Loss: 0.000178\n",
      "Epoch: 14 \tTraining Loss: 0.000178\n",
      "Epoch: 15 \tTraining Loss: 0.000178\n",
      "Epoch: 16 \tTraining Loss: 0.000178\n",
      "Epoch: 17 \tTraining Loss: 0.000178\n",
      "Epoch: 18 \tTraining Loss: 0.000178\n",
      "Epoch: 1 \tTraining Loss: 0.000228\n",
      "Epoch: 2 \tTraining Loss: 0.000228\n",
      "Epoch: 3 \tTraining Loss: 0.000228\n",
      "Epoch: 4 \tTraining Loss: 0.000228\n",
      "Epoch: 5 \tTraining Loss: 0.000228\n",
      "Epoch: 6 \tTraining Loss: 0.000228\n",
      "Epoch: 7 \tTraining Loss: 0.000228\n",
      "Epoch: 8 \tTraining Loss: 0.000228\n",
      "Epoch: 9 \tTraining Loss: 0.000228\n",
      "Epoch: 10 \tTraining Loss: 0.000228\n",
      "Epoch: 11 \tTraining Loss: 0.000228\n",
      "Epoch: 12 \tTraining Loss: 0.000228\n",
      "Epoch: 13 \tTraining Loss: 0.000228\n",
      "Epoch: 14 \tTraining Loss: 0.000228\n",
      "Epoch: 15 \tTraining Loss: 0.000228\n",
      "Epoch: 16 \tTraining Loss: 0.000228\n",
      "Epoch: 17 \tTraining Loss: 0.000228\n",
      "Epoch: 18 \tTraining Loss: 0.000228\n",
      "Epoch: 1 \tTraining Loss: 0.000122\n",
      "Epoch: 2 \tTraining Loss: 0.000122\n",
      "Epoch: 3 \tTraining Loss: 0.000122\n",
      "Epoch: 4 \tTraining Loss: 0.000122\n",
      "Epoch: 5 \tTraining Loss: 0.000122\n",
      "Epoch: 6 \tTraining Loss: 0.000122\n",
      "Epoch: 7 \tTraining Loss: 0.000122\n",
      "Epoch: 8 \tTraining Loss: 0.000122\n",
      "Epoch: 9 \tTraining Loss: 0.000122\n",
      "Epoch: 10 \tTraining Loss: 0.000122\n",
      "Epoch: 11 \tTraining Loss: 0.000122\n",
      "Epoch: 12 \tTraining Loss: 0.000122\n",
      "Epoch: 13 \tTraining Loss: 0.000122\n",
      "Epoch: 14 \tTraining Loss: 0.000122\n",
      "Epoch: 15 \tTraining Loss: 0.000122\n",
      "Epoch: 16 \tTraining Loss: 0.000122\n",
      "Epoch: 17 \tTraining Loss: 0.000122\n",
      "Epoch: 18 \tTraining Loss: 0.000122\n",
      "Epoch: 1 \tTraining Loss: 0.000224\n",
      "Epoch: 2 \tTraining Loss: 0.000224\n",
      "Epoch: 3 \tTraining Loss: 0.000224\n",
      "Epoch: 4 \tTraining Loss: 0.000224\n",
      "Epoch: 5 \tTraining Loss: 0.000224\n",
      "Epoch: 6 \tTraining Loss: 0.000224\n",
      "Epoch: 7 \tTraining Loss: 0.000224\n",
      "Epoch: 8 \tTraining Loss: 0.000224\n",
      "Epoch: 9 \tTraining Loss: 0.000224\n",
      "Epoch: 10 \tTraining Loss: 0.000224\n",
      "Epoch: 11 \tTraining Loss: 0.000224\n",
      "Epoch: 12 \tTraining Loss: 0.000224\n",
      "Epoch: 13 \tTraining Loss: 0.000224\n",
      "Epoch: 14 \tTraining Loss: 0.000224\n",
      "Epoch: 15 \tTraining Loss: 0.000224\n",
      "Epoch: 16 \tTraining Loss: 0.000224\n",
      "Epoch: 17 \tTraining Loss: 0.000224\n",
      "Epoch: 18 \tTraining Loss: 0.000224\n"
     ]
    }
   ],
   "source": [
    "cretarion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 18\n",
    "model.train() #preparing your model for training\n",
    "train_loss = 0\n",
    "\n",
    "\n",
    "for data, label in train_Loader:\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = cretarion(output, label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    train_loss = train_loss/len(train_Loader.dataset)\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = 0\n",
    "\n",
    "\n",
    "for data, taget in test_Loader:\n",
    "    output = model(data)\n",
    "    loss = cretarion(output, target)\n",
    "    test_loss +=loss.item()*data.size(0)\n",
    "    pred = torch.max(output, 1)\n",
    "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "        \n",
    "test_loss = test_loss/len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            str(i), 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x288 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# obtain one batch of test images\n",
    "dataiter = iter(test_Loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# get sample outputs\n",
    "output = model(images)\n",
    "# convert output probabilities to predicted class\n",
    "_, preds = torch.max(output, 1)\n",
    "# prep images for display\n",
    "images = images.numpy()\n",
    "\n",
    "# plot the images in the batch, along with predicted and true labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    ax.set_title(\"{} ({})\".format(str(preds[idx].item()), str(labels[idx].item())),\n",
    "                 color=(\"green\" if preds[idx]==labels[idx] else \"red\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cat/dag.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-0758dcc23edb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cat/dag.png\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36mimread\u001b[1;34m(fname, format)\u001b[0m\n\u001b[0;32m   1431\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mhandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1432\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1433\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfd\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1434\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mhandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1435\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cat/dag.png'"
     ]
    }
   ],
   "source": [
    "# Computer Vision for CNN\n",
    "\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mp\n",
    "\n",
    "image = mp.imread(\"cat/dag.png\")\n",
    "plt.imshow(image)\n",
    "\n",
    "# Changing the color to gray\n",
    "\n",
    "gray = cv.cvtColor(image, cv.COLOR_RGB2GRAY)\n",
    "plt.imshow(gray, cmap=\"gray\")\n",
    "\n",
    "# create a custom kernel\n",
    "\n",
    "sober_y = np.array([-1, 2, 1],[1,2,4], [0,1,2])\n",
    "sober_x = np.array([-1, 0, 1],[2,0,1], [-1,0,2])\n",
    "\n",
    "# Filtering the image\n",
    "filtered_image = cv.filter2D(image, -1, sober_y)\n",
    "\n",
    "plt.imshow(filtered_image, cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')\n",
    "gray = cv2.cvtColor(imag, cv2.COLOR_BGR2GRAY)\n",
    "faces = face_cascade.detectMultiScale(gray) #Face detaction\n",
    "for (x,y,w,h) in faces:\n",
    "    # add bounding box to color image\n",
    "    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "\n",
    "def face_detector(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray)\n",
    "    return len(faces) > 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
